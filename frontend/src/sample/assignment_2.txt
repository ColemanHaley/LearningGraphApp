1. Machine Learning & Neural Networks (8 points)
(a) (4 points) Adam Optimizer
Recall the standard Stochastic Gradient Descent update rule:
θ ← θ − α∇θ
Jminibatch(θ)
where θ is a vector containing all of the model parameters, J is the loss function, ∇θ
Jminibatch(θ)
is the gradient of the loss function with respect to the parameters on a minibatch of data, and α is
the learning rate. Adam Optimization1 uses a more sophisticated update rule with two additional
steps.2
i. (2 points) First, Adam uses a trick called momentum by keeping track of m, a rolling average
of the gradients:
m ← β1m + (1 − β1)∇θ
Jminibatch(θ)
θ ← θ − αm
where β1 is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain (you don’t need
to prove mathematically, just give an intuition) how using m stops the updates from varying
as much and why this low variance may be helpful to learning, overall.
ii. (2 points) Adam extends the idea of momentum with the trick of adaptive learning rates by
keeping track of v, a rolling average of the magnitudes of the gradients:
m ← β1m + (1 − β1)∇θ
Jminibatch(θ)
v ← β2v + (1 − β2)(∇θ
Jminibatch(θ)  ∇θ
Jminibatch(θ))
θ ← θ − α  m/
√
v
where  and / denote elementwise multiplication and division (so zz is elementwise squaring)
and β2 is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update
by √
v, which of the model parameters will get larger updates? Why might this help with
learning?
(b) (4 points) Dropout3
is a regularization technique. During training, dropout randomly sets units
in the hidden layer h to zero with probability pdrop (dropping different units each minibatch), and
then multiplies h by a constant γ. We can write this as
hdrop = γd  h
where d ∈ {0, 1}
Dh (Dh is the size of h) is a mask vector where each entry is 0 with probability
pdrop and 1 with probability (1 − pdrop). γ is chosen such that the expected value of hdrop is h:
Epdrop [hdrop]i = hi
for all i ∈ {1, . . . , Dh}.
i. (2 points) What must γ equal in terms of pdrop? Briefly justify your answer.
ii. (2 points) Why should we apply dropout during training but not during evaluation?