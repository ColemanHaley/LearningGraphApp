Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 21 Lexicons Sentiment , Affect , Connotation “ [ W ] e write , fingers , whole person . nerve controls pen winds itself every fibre , threads heart , pierces liver . ” Virginia Woolf , Orlando “ runs gamut emotions B . ” Dorothy Parker , reviewing Hepburn’s performance Little Women “ Festering’s always bad . There’s good kind festering . ” Adventure Time , Season 5 chapter turn tools interpreting affective meaning , extending ouraffective study sentiment analysis Chapter 4 . word ‘ affective ’ , following tradition affective computing ( Picard , 1995 ) mean emotion , sentiment , per - sonality , mood , attitudes . Affective meaning closely related subjectivity , subjectivity study speaker writer’s evaluations , opinions , emotions , speculations ( Wiebe et al . , 1999 ) . affective meaning defined ? influential typology affec - tive states comes Scherer ( 2000 ) , defines class affective states factors like cognitive realization time course : design extractors kinds affective states . Chapter 4 already introduced sentiment analysis , task extracting positive negative orientation writer expresses text . corresponds Scherer’s typology extraction attitudes : figuring people like dislike , affect - rich texts like consumer reviews books movies , newspaper editorials , public sentiment blogs tweets . Detecting emotion moods useful detecting student con - fused , engaged , certain interacting tutorial system , caller help line frustrated , someone’s blog posts tweets indicated depres - sion . Detecting emotions like fear novels , example , help trace groups situations feared changes time . Detecting different interpersonal stances useful extracting infor - mation human-human conversations . goal detect stances like friendliness awkwardness interviews friendly conversations , example summarizing meetings finding parts conversation people especially excited engaged , conversational hot spots help meeting summariza - tion . Detecting personality user — user extrovert extent open experience — help improve conversa - 2 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION Emotion : Relatively brief episode response evaluation external internal event major significance . ( angry , sad , joyful , fearful , ashamed , proud , elated , desperate ) Mood : Diffuse affect state , pronounced change subjective feeling , low intensity relatively long duration , often apparent cause . ( cheerful , gloomy , irritable , listless , depressed , buoyant ) Interpersonal stance : Affective stance taken toward another person spe - cific interaction , coloring interpersonal exchange situation . ( distant , cold , warm , supportive , contemptuous , friendly ) Attitude : Relatively enduring , affectively colored beliefs , preferences , pre - dispositions objects persons . ( liking , loving , hating , valuing , desiring ) Personality traits : Emotionally laden , stable personality dispositions - havior tendencies , typical person . ( nervous , anxious , reckless , morose , hostile , jealous ) Figure 21.1 Scherer typology affective states ( Scherer , 2000 ) . tional agents , seem work better match users ’ personality expecta - tions ( Mairesse Walker , 2008 ) . affect important generation well recognition ; synthesizing affect important conversational agents various domains , including literacy tutors children’s storybooks , computer games . Chapter 4 introduced naive Bayes classification classify document’s sentiment . Various classifiers successfully applied many tasks , words training set input classifier determines affect status text . chapter focus alternative model , every word feature , focus certain words , ones carry particularly strong cues affect sentiment . call lists words affective lexicons senti - ment lexicons . lexicons presuppose fact semantics : words affective meanings connotations . word connotation different meaningsconnotations different fields , mean aspects word’s meaning related writer reader’s emotions , sentiment , opinions , evaluations . addition ability help determine affective status text , connotation lexicons useful features kinds affective tasks , computa - tional social science analysis . next sections introduce basic theories emotion , show sentiment lexicons special case emotion lexicons , mention useful lexicons . survey three ways building lexicons : human labeling , semi-supervised , supervised . Finally , turn kinds affective meaning like personality , stance , entity-centric affect , introduce connotation frames . 21.1 Defining Emotion important affective classes emotion , Scherer ( 2000 ) definesemotion “ relatively brief episode response evaluation external internal event major significance ” . Detecting emotion potential improve number language processing 21.1 • DEFINING EMOTION 3 tasks . Automatically detecting emotions reviews customer responses ( anger , dissatisfaction , trust ) help businesses recognize specific problem areas ones going well . Emotion recognition help dialog systems like tutoring systems detect student unhappy , bored , hesitant , confident , . Emotion play role medical informatics tasks like detecting depression suicidal intent . Detecting emotions expressed toward characters novels might play role understanding different social groups viewed society different times . two widely-held families theories emotion . family , emo - tions viewed fixed atomic units , limited number , others generated , often called basic emotions ( Tomkins 1962 , Plutchik 1962 ) . Perhapsbasic emotions well-known family theories 6 emotions proposed Ekman ( example Ekman 1999 ) set emotions likely universally present cultures : surprise , happiness , anger , fear , disgust , sadness . Another atomic theory Plutchik ( 1980 ) wheel emotion , consisting 8 basic emo - tions four opposing pairs : joy – sadness , anger – fear , trust – disgust , anticipa - tion – surprise , together emotions derived , shown Fig . 21.2 . Figure 21.2 Plutchik wheel emotion . second class emotion theories views emotion space 2 3 di - mensions ( Russell , 1980 ) . models include two dimensions valence arousal , many add third , dominance . defined : valence : pleasantness stimulus arousal : intensity emotion provoked stimulus dominance : degree control exerted stimulus next sections lexicons kinds theories emotion . Sentiment viewed special case second view emotions points space . particular , valence dimension , measuring pleasant unpleasant word , often directly measure sentiment . 4 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION 21.2 Available Sentiment Affect Lexicons wide variety affect lexicons created released . basic lexicons label words dimension semantic variability , generally called “ sentiment ” “ valence ” . simplest lexicons dimension represented binary fashion , wordlist positive words wordlist negative words . oldest General Inquirer ( Stone et al . , 1966 ) , drew content analysis earlyGeneralInquirer work cognitive psychology word meaning ( Osgood et al . , 1957 ) . Gen - eral Inquirer lexicon 1915 positive words lexicon 2291 negative words ( well lexicons discussed below ) . MPQA Subjectivity lexicon ( Wilson et al . , 2005 ) 2718 positive 4912 negative words drawn prior lexicons plus bootstrapped list subjective words phrases ( Riloff Wiebe , 2003 ) entry lexicon hand-labeled sentiment labeled reliability ( strongly subjective weakly subjective ) . polarity lexicon Hu Liu ( 2004 ) gives 2006 positive 4783 negative words , drawn product reviews , labeled bootstrapping method WordNet . Positive admire , amazing , assure , celebration , charm , eager , enthusiastic , excellent , fancy , fan - tastic , frolic , graceful , happy , joy , luck , majesty , mercy , nice , patience , perfect , proud , rejoice , relief , respect , satisfactorily , sensational , super , terrific , thank , vivid , wise , won - derful , zest Negative abominable , anger , anxious , bad , catastrophe , cheap , complaint , condescending , deceit , defective , disappointment , embarrass , fake , fear , filthy , fool , guilt , hate , idiot , inflict , lazy , miserable , mourn , nervous , objection , pest , plot , reject , scream , silly , terrible , unfriendly , vile , wicked Figure 21.3 samples words consistent sentiment three sentiment lexicons : General Inquirer ( Stone et al . , 1966 ) , MPQA Subjectivity lexicon ( Wilson et al . , 2005 ) , polarity lexicon Hu Liu ( 2004 ) . Slightly general sentiment lexicons lexicons assign word value three affective dimensions . NRC Valence , Arousal , Dominance ( VAD ) lexicon ( Mohammad , 2018a ) assigns valence , arousal , dom - inance scores 20,000 words . examples shown Fig . 21.4 . Valence Arousal Dominance vacation . 840 enraged . 962 powerful . 991 delightful . 918 party . 840 authority . 935 whistle . 653 organized . 337 saxophone . 482 consolation . 408 effortless . 120 discouraged . 0090 torture . 115 napping . 046 weak . 045 Figure 21.4 Samples values selected words three emotional dimensions Mohammad ( 2018a ) . NRC Word-Emotion Association Lexicon , called EmoLex ( Moham-EmoLex mad Turney , 2013 ) , Plutchik ( 1980 ) 8 basic emotions defined . lexicon includes around 14,000 words including words prior lexicons well frequent nouns , verbs , adverbs adjectives . Values lexicon sample words : 21.3 • CREATING AFFECT LEXICONS HUMAN LABELING 5 Word ge r tic ip io n di sg t fe ar jo y sa dn es s su rp ri se tr t po si tiv e ne ga tiv e reward 0 1 0 0 1 0 1 1 1 0 worry 0 1 0 1 0 1 0 0 0 1 tenderness 0 0 0 0 1 0 0 0 1 0 sweetheart 0 1 0 0 1 1 0 1 1 0 suddenly 0 0 0 0 0 0 1 0 0 0 thirst 0 1 0 0 0 1 1 0 0 0 garbage 0 0 1 0 0 0 0 0 0 1 smaller set 5,814 words , NRC Emotion / Affect Intensity Lexicon ( Mohammad , 2018b ) contains real-valued scores association anger , fear , joy , sadness ; Fig . 21.5 shows examples . Anger Fear Joy Sadness outraged 0.964 horror 0.923 superb 0.864 sad 0.844 violence 0.742 anguish 0.703 cheered 0.773 guilt 0.750 coup 0.578 pestilence 0.625 rainbow 0.531 unkind 0.547 oust 0.484 stressed 0.531 gesture 0.387 difficulties 0.421 suspicious 0.484 failing 0.531 warms 0.391 beggar 0.422 nurture 0.059 confident 0.094 hardship . 031 sing 0.017 Figure 21.5 Sample emotional intensities words anger , fear , joy , sadness Mohammad ( 2018b ) . LIWC , Linguistic Inquiry Word Count , widely set 73 lex-LIWC icons containing 2300 words ( Pennebaker et al . , 2007 ) , designed capture aspects lexical meaning relevant social psychological tasks . addition sentiment-related lexicons like ones negative emotion ( bad , weird , hate , prob - lem , tough ) positive emotion ( love , nice , sweet ) , LIWC includes lexicons categories like anger , sadness , cognitive mechanisms , perception , tentative , - hibition , shown Fig . 21.6 . various hand-built affective lexicons . General Inquirer - cludes additional lexicons dimensions like strong vs . weak , active vs . passive , overstated vs . understated , well lexicons categories like pleasure , pain , virtue , vice , motivation , cognitive orientation . Another useful feature various tasks distinction concreteconcrete words like banana bathrobe abstract words like belief although . Theabstract lexicon Brysbaert et al . ( 2014 ) crowdsourcing assign rating 1 5 concreteness 40,000 words , thus assigning banana , bathrobe , bagel 5 , belief 1.19 , although 1.07 , words like brisk 2.5 . 21.3 Creating Affect Lexicons Human Labeling earliest method build affect lexicons , still common , humans label word . commonly via crowdsourcing : crowdsourcing breaking task small pieces distributing large number anno - 6 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION Positive Negative Emotion Emotion Insight Inhibition Family Negate appreciat * anger * aware * avoid * brother * comfort * bore * believe careful * cousin * great cry decid * hesitat * daughter * happy despair * feel limit * family neither interest fail * figur * oppos * father * never joy * fear know prevent * grandf * perfect * griev * knew reluctan * grandm * nobod * please * hate * means safe * husband none safe * panic * notice * stop mom nor terrific suffers recogni * stubborn * mother nothing value terrify sense wait niece * nowhere wow * violent * think wary wife Figure 21.6 Samples 5 73 lexical categories LIWC ( Pennebaker et al . , 2007 ) . * means previous letters word prefix words prefix included category . tators . take look methodological choices two crowdsourced emotion lexicons . NRC Emotion Lexicon ( EmoLex ) ( Mohammad Turney , 2013 ) , labeled emotions two steps . ensure annotators judging correct sense word , first answered multiple-choice synonym question primed correct sense word ( requiring annotator read potentially confusing sense definition ) . created automatically headwords associated thesaurus category sense question Macquarie dictionary headwords 3 random distractor categories . example : word closest meaning ( related ) startle ? • automobile • shake • honesty • entertain word ( e.g . startle ) , annotator asked rate associated word 8 emotions ( joy , fear , anger , etc . ) . associations rated scale , weakly , moderately , strongly associated . Outlier ratings removed , term assigned class chosen ma - jority annotators , ties broken choosing stronger intensity , 4 levels mapped binary label word ( weak mapped 0 , moderate strong mapped 1 ) . NRC VAD Lexicon ( Mohammad , 2018a ) built selecting words emoticons prior lexicons annotating crowd-sourcing best - worst scaling ( Louviere et al . 2015 , Kiritchenko Mohammad 2017 ) . best-best-worstscaling worst scaling , annotators N items ( usually 4 ) asked item best ( highest ) worst ( lowest ) terms property . set words decribe ends scales taken prior literature . valence , example , raters asked : Q1 . four words below associated happi - ness / pleasure / positiveness / satisfaction / contentedness / hopefulness LEAST unhappiness / annoyance / negativeness / dissatisfaction / 21.4 • SEMI-SUPERVISED INDUCTION AFFECT LEXICONS 7 melancholy / despair ? ( Four words listed options . ) Q2 . four words below associated LEAST hap - piness / pleasure / positiveness / satisfaction / contentedness / hopeful - ness unhappiness / annoyance / negativeness / dissatisfaction / melancholy / despair ? ( Four words listed options . ) score word lexicon proportion times item chosen best ( highest V / / D ) minus proportion times item chosen worst ( lowest V / / D ) . agreement annotations evaluated split - half reliability : split corpus half compute correlations thesplit-halfreliability annotations two halves . 21.4 Semi-supervised Induction Affect Lexicons Another common way learn sentiment lexicons start set seed words define two poles semantic axis ( words like good bad ) , find ways label word w similarity two seed sets . summarize two families seed-based semi-supervised lexicon induction algorithms , axis-based graph-based . 21.4.1 Semantic Axis Methods well-known lexicon induction methods , Turney Littman ( 2003 ) algorithm , seed words like good bad , word w labeled , measures similar good different bad . describe slight extension algorithm due et al . ( 2018 ) , based computing semantic axis . first step , choose seed words hand . two methods dealing fact affect word different different contexts : ( 1 ) start single large seed lexicon rely induction algorithm fine-tune domain , ( 2 ) choose different seed words different genres . Hellrich et al . ( 2019 ) suggests modeling affect different historical time periods , starting large modern affect dictionary better small seedsets tuned stable time . example second approach , Hamilton et al . ( 2016 ) define set seed words general sentiment analysis , different set Twitter , yet another set sentiment financial text : Domain Positive seeds Negative seeds General good , lovely , excellent , fortunate , pleas - ant , delightful , perfect , loved , love , happy bad , horrible , poor , unfortunate , un - pleasant , disgusting , evil , hated , hate , unhappy Twitter love , loved , loves , awesome , nice , amazing , best , fantastic , correct , happy hate , hated , hates , terrible , nasty , awful , worst , horrible , wrong , sad Finance successful , excellent , profit , beneficial , improving , improved , success , gains , positive negligent , loss , volatile , wrong , losses , damages , bad , litigation , failure , down , negative second step , compute embeddings pole words . embeddings off-the-shelf word2vec embeddings , computed directly 8 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION specific corpus ( example financial corpus finance lexicon goal ) , fine-tune off-the-shelf embeddings corpus . Fine-tuning espe - cially important specific genre text enough data train good embeddings . fine-tuning , begin off-the-shelf embeddings like word2vec , continue training small target corpus . Once embeddings pole word , create embedding represents pole taking centroid embeddings seed words ; recall centroid multidimensional version mean . set embeddings positive seed words S + = { E ( w + 1 ) , E ( w + 2 ) , . . . , E ( w + n ) } , embeddings negative seed words S − = { E ( w − 1 ) , E ( w − 2 ) , . . . , E ( w − m ) } , pole centroids : V + = 1 n n ∑ 1 E ( w + ) V − = 1 m m ∑ 1 E ( w − ) ( 21.1 ) semantic axis defined poles computed just subtracting two vec - tors : Vaxis = V + − V − ( 21.2 ) Vaxis , semantic axis , vector direction sentiment . Finally , compute close word w sentiment axis , taking cosine w’s embedding axis vector . higher cosine means w aligned S + S − . score ( w ) = ( cos ( E ( w ) , Vaxis ) = E ( w ) · Vaxis ‖ E ( w ) ‖ ‖ Vaxis ‖ ( 21.3 ) dictionary words sentiment scores sufficient , ! need group words positive negative lexicon , threshold method give discrete lexicons . 21.4.2 Label Propagation alternative family methods defines lexicons propagating sentiment labels graphs , idea suggested early work Hatzivassiloglou McKeown ( 1997 ) . describe simple SentProp ( Sentiment Propagation ) algorithm Hamilton et al . ( 2016 ) , four steps : 1 . Define graph : word embeddings , build weighted lexical graph connecting word k nearest neighbors ( according cosine - similarity ) . weights edge words wi w j set : Ei , j = arccos ( − wi > wj ‖ wi ‖ ‖ wj ‖ ) . ( 21.4 ) 2 . Define seed set : Choose positive negative seed words . 21.4 • SEMI-SUPERVISED INDUCTION AFFECT LEXICONS 9 3 . Propagate polarities seed set : perform random walk graph , starting seed set . random walk , start node choose node move probability proportional edge prob - ability . word’s polarity score seed set proportional probability random walk seed set landing word , ( Fig . 21.7 ) . 4 . Create word scores : walk positive negative seed sets , resulting positive ( score + ( wi ) ) negative ( score − ( wi ) ) label scores . combine values positive-polarity score : score + ( wi ) = score + ( wi ) score + ( wi ) + score − ( wi ) ( 21.5 ) often helpful standardize scores zero mean unit variance corpus . 5 . Assign confidence score : sentiment scores influenced seed set , like know score word change different seed set . bootstrap-sampling get confidence regions , computing propagation B times random subsets positive negative seed sets ( example B = 50 choosing 7 10 seed words time ) . standard deviation bootstrap-sampled polarity scores gives confidence measure . idolizelove adore appreciate like find dislikesee notice disapprove abhor hate loathe despise uncover idolizelove adore appreciate like find dislikesee notice disapprove abhor hate loathe despise uncover ( ) ( b ) Figure 21.7 Intuition SENTPROP algorithm . ( ) Run random walks seed words . ( b ) Assign polarity scores ( shown colors green red ) based frequency random walk visits . 21.4.3 Methods core semisupervised algorithms metric measuring similarity seed words . Turney Littman ( 2003 ) Hamilton et al . ( 2016 ) ap - proaches embedding cosine distance metric : words labeled positive basically embeddings high cosines positive seeds low cosines negative seeds . methods chosen kinds distance metrics besides embedding cosine . example Hatzivassiloglou McKeown ( 1997 ) algorithm syntactic cues ; two adjectives considered similar frequently conjoined rarely conjoined . based intuition adjectives conjoined words tend same polarity ; positive adjectives generally coordinated positive , negative negative : fair legitimate , corrupt brutal less often positive adjectives coordinated negative : * fair brutal , * corrupt legitimate 10 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION contrast , adjectives conjoined likely opposite polarity : fair brutal Another cue opposite polarity comes morphological negation ( un - , - , - less ) . Adjectives same root differing morphological negative ( ad - equate / inadequate , thoughtful / thoughtless ) tend opposite polarity . Yet another method finding words similar polarity seed words make thesaurus like WordNet ( Kim Hovy 2004 , Hu Liu 2004 ) . word’s synonyms presumably share polarity word’s antonyms probably opposite polarity . seed lexicon built , lexicon updated follows , possibly iterated . Lex + : Add synonyms positive words ( well ) antonyms ( like fine ) negative words Lex − : Add synonyms negative words ( awful ) antonyms ( like evil ) positive words extension algorithm assigns polarity WordNet senses , called Senti - WordNet ( Baccianella et al . , 2010 ) . Fig . 21.8 shows examples . SentiWordNet Synset Pos Neg Obj good # 6 ‘ agreeable pleasing ’ 1 0 0 respectable # 2 honorable # 4 good # 4 estimable # 2 ‘ deserving esteem ’ 0.75 0 0.25 estimable # 3 computable # 1 ‘ computed estimated ’ 0 0 1 sting # 1 burn # 4 bite # 2 ‘ cause sharp stinging pain ’ 0 0.875 . 125 acute # 6 ‘ critical importance consequence ’ 0.625 0.125 . 250 acute # 4 ‘ angle ; less 90 degrees ’ 0 0 1 acute # 1 ‘ experiencing rapid onset short severe course ’ 0 0.5 0.5 Figure 21.8 Examples SentiWordNet 3.0 ( Baccianella et al . , 2010 ) . Note differences senses homonymous words : estimable # 3 purely objective , estimable # 2 positive ; acute positive ( acute # 6 ) , negative ( acute # 1 ) , neutral ( acute # 4 ) . algorithm , polarity assigned entire synsets rather words . positive lexicon built synsets associated 7 positive words , negative lexicon synsets associated 7 negative words . classifier trained data take WordNet gloss decide sense defined positive , negative neutral . further step ( involving random-walk algorithm ) assigns score WordNet synset degree positivity , negativity , neutrality . summary , semisupervised algorithms human-defined set seed words two poles dimension , similarity metrics like embedding cosine , coordination , morphology , thesaurus structure score words similar positive seeds dissimilar negative seeds . 21.5 Supervised Learning Word Sentiment Semi-supervised methods require minimal human supervision ( form seed sets ) . sometimes supervision signal exists world made . signal scores associated online reviews . web contains enormous number online reviews restaurants , movies , books , products , text review 21.5 • SUPERVISED LEARNING WORD SENTIMENT 11 associated review score : value range 1 star 5 stars , scoring 1 10 . Fig . 21.9 shows samples extracted restaurant , book , movie reviews . Movie review excerpts ( IMDb ) 10 great movie . film just wonderful experience . surreal , zany , witty slapstick same time . terrific performances . 1 probably worst movie ever seen . story went nowhere even though interesting stuff . Restaurant review excerpts ( Yelp ) 5 service impeccable . food cooked seasoned perfectly . . . watermelon perfectly square . . . grilled octopus . . . mouthwatering . . . 2 . . . took get waters , got entree starter , never received silverware napkins requested . . . Book review excerpts ( GoodReads ) 1 going try stop deceived eye-catching titles . wanted like book disappointed . 5 book hilarious . recommend anyone looking satirical read romantic twist narrator keeps butting Product review excerpts ( Amazon ) 5 lid blender though probably like best . . . enables pour something even taking lid off ! . . . perfect pitcher ! . . . works fantastic . 1 hate blender . . . nearly impossible get frozen fruit ice turn smoothie . . . add TON liquid . wish spout . . . Figure 21.9 Excerpts reviews various review websites , scale 1 5 stars except IMDb , scale 1 10 stars . review score supervision : positive words likely appear 5-star reviews ; negative words 1-star reviews . just binary polarity , kind supervision allows assign word complex representation polarity : distribution stars ( scores ) . Thus ten-star system represent sentiment word 10-tuple , number score representing word’s association polarity level . association raw count , likelihood P ( w | c ) , function count , class c 1 10 . example , compute IMDb likelihood word like disap - point ( ed / ing ) occurring 1 star review dividing number times disap - point ( ed / ing ) occurs 1-star reviews IMDb dataset ( 8,557 ) total num - ber words occurring 1-star reviews ( 25,395,214 ) , IMDb estimate P ( disappointing | 1 ) . 0003 . slight modification weighting , normalized likelihood , illuminating visualization ( Potts , 2011 ) 1 : P ( w | c ) = count ( w , c ) ∑ w ∈ C count ( w , c ) PottsScore ( w ) = P ( w | c ) ∑ c P ( w | c ) ( 21.6 ) Dividing IMDb estimate P ( disappointing | 1 ) . 0003 sum likeli - 1 Potts shows normalized likelihood estimate posterior P ( c | w ) make incorrect simplifying assumption categories c equal probability . 12 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION hood P ( w | c ) categories gives Potts score 0.10 . word disappointing thus associated vector [ . 10 , . 12 , . 14 , . 14 , . 13 , . 11 , . 08 , . 06 , . 06 , . 05 ] . Potts diagram ( Potts , 2011 ) visualization word scores , representing thePotts diagram prior sentiment word distribution rating categories . Fig . 21.10 shows Potts diagrams 3 positive 3 negative scalar adjec - tives . Note curve strongly positive scalars shape letter J , strongly negative scalars look like reverse J . contrast , weakly posi - tive negative scalars hump-shape , maximum below mean ( weakly negative words like disappointing ) mean ( weakly pos - itive words like good ) . shapes offer illuminating typology affective meaning . Overview Data Methods Categorization Scale induction Looking ahead Example : attenuators IMDB – 53,775 tokens Category - 0 . 5 0 - 0 . 3 9 - 0 . 2 8 - 0 . 1 7 - 0 . 0 6 0 . 06 0 . 17 0 . 28 0 . 39 0 . 50 0.05 0.09 0.15 Cat = 0.33 ( p = 0.004 ) Cat ^ 2 = - 4.02 ( p < 0.001 ) OpenTable – 3,890 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.38 Cat = 0.11 ( p = 0.707 ) Cat ^ 2 = - 6.2 ( p = 0.014 ) Goodreads – 3,424 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.19 0.36 Cat = - 0.55 ( p = 0.128 ) Cat ^ 2 = - 5.04 ( p = 0.016 ) Amazon / Tripadvisor – 2,060 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.12 0.28 Cat = 0.42 ( p = 0.207 ) Cat ^ 2 = - 2.74 ( p = 0.05 ) somewhat / r IMDB – 33,515 tokens Category - 0 . 5 0 - 0 . 3 9 - 0 . 2 8 - 0 . 1 7 - 0 . 0 6 0 . 06 0 . 17 0 . 28 0 . 39 0 . 50 0.04 0.09 0.17 Cat = - 0.13 ( p = 0.284 ) Cat ^ 2 = - 5.37 ( p < 0.001 ) OpenTable – 2,829 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.31 Cat = 0.2 ( p = 0.265 ) Cat ^ 2 = - 4.16 ( p = 0.007 ) Goodreads – 1,806 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.05 0.12 0.18 0.35 Cat = - 0.87 ( p = 0.016 ) Cat ^ 2 = - 5.74 ( p = 0.004 ) Amazon / Tripadvisor – 2,158 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.11 0.29 Cat = 0.54 ( p = 0.183 ) Cat ^ 2 = - 3.32 ( p = 0.045 ) fairly / r IMDB – 176,264 tokens Category - 0 . 5 0 - 0 . 3 9 - 0 . 2 8 - 0 . 1 7 - 0 . 0 6 0 . 06 0 . 17 0 . 28 0 . 39 0 . 50 0.05 0.09 0.13 Cat = - 0.43 ( p < 0.001 ) Cat ^ 2 = - 3.6 ( p < 0.001 ) OpenTable – 8,982 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.14 0.19 0.32 Cat = - 0.64 ( p = 0.035 ) Cat ^ 2 = - 4.47 ( p = 0.007 ) Goodreads – 11,895 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.07 0.15 0.34 Cat = - 0.71 ( p = 0.072 ) Cat ^ 2 = - 4.59 ( p = 0.018 ) Amazon / Tripadvisor – 5,980 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.15 0.28 Cat = 0.26 ( p = 0.496 ) Cat ^ 2 = - 2.23 ( p = 0.131 ) pretty / r “ Potts & diagrams ” Potts , & Christopher . & 2011 . & NSF & workshop & & restructuring & adjectives . good great excellent disappointing bad terrible totally absolutely utterly somewhat fairly pretty Positive scalars Negative scalars Emphatics Attenuators 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating Figure 21.10 Potts diagrams ( Potts , 2011 ) positive negative scalar adjectives , show - ing J-shape reverse J-shape strongly positive negative adjectives , hump-shape weakly polarized adjectives . Fig . 21.11 shows Potts diagrams emphasizing attenuating adverbs . Note emphatics tend J-shape ( likely occur posi - tive reviews ) U-shape ( likely occur strongly positive nega - tive ) . Attenuators hump-shape , emphasizing middle scale downplaying extremes . diagrams typology lexical sentiment , play role modeling sentiment compositionality . addition functions like posterior P ( c | w ) , likelihood P ( w | c ) , normalized likelihood ( Eq . 21.6 ) many functions count word occurring sentiment label . introduce page 17 , including ideas like normalizing counts per writer Eq . 21.14 . 21.5.1 Log Odds Ratio Informative Dirichlet Prior thing often word polarity distinguish words likely category texts another . , example , know words associated 1 star reviews versus associated 5 star reviews . differences just related senti - 21.5 • SUPERVISED LEARNING WORD SENTIMENT 13 Overview Data Methods Categorization Scale induction Looking ahead Example : attenuators IMDB – 53,775 tokens Category - 0 . 5 0 - 0 . 3 9 - 0 . 2 8 - 0 . 1 7 - 0 . 0 6 0 . 06 0 . 17 0 . 28 0 . 39 0 . 50 0.05 0.09 0.15 Cat = 0.33 ( p = 0.004 ) Cat ^ 2 = - 4.02 ( p < 0.001 ) OpenTable – 3,890 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.38 Cat = 0.11 ( p = 0.707 ) Cat ^ 2 = - 6.2 ( p = 0.014 ) Goodreads – 3,424 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.19 0.36 Cat = - 0.55 ( p = 0.128 ) Cat ^ 2 = - 5.04 ( p = 0.016 ) Amazon / Tripadvisor – 2,060 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.12 0.28 Cat = 0.42 ( p = 0.207 ) Cat ^ 2 = - 2.74 ( p = 0.05 ) somewhat / r IMDB – 33,515 tokens Category - 0 . 5 0 - 0 . 3 9 - 0 . 2 8 - 0 . 1 7 - 0 . 0 6 0 . 06 0 . 17 0 . 28 0 . 39 0 . 50 0.04 0.09 0.17 Cat = - 0.13 ( p = 0.284 ) Cat ^ 2 = - 5.37 ( p < 0.001 ) OpenTable – 2,829 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.31 Cat = 0.2 ( p = 0.265 ) Cat ^ 2 = - 4.16 ( p = 0.007 ) Goodreads – 1,806 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.05 0.12 0.18 0.35 Cat = - 0.87 ( p = 0.016 ) Cat ^ 2 = - 5.74 ( p = 0.004 ) Amazon / Tripadvisor – 2,158 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.11 0.29 Cat = 0.54 ( p = 0.183 ) Cat ^ 2 = - 3.32 ( p = 0.045 ) fairly / r IMDB – 176,264 tokens Category - 0 . 5 0 - 0 . 3 9 - 0 . 2 8 - 0 . 1 7 - 0 . 0 6 0 . 06 0 . 17 0 . 28 0 . 39 0 . 50 0.05 0.09 0.13 Cat = - 0.43 ( p < 0.001 ) Cat ^ 2 = - 3.6 ( p < 0.001 ) OpenTable – 8,982 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.08 0.14 0.19 0.32 Cat = - 0.64 ( p = 0.035 ) Cat ^ 2 = - 4.47 ( p = 0.007 ) Goodreads – 11,895 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.07 0.15 0.34 Cat = - 0.71 ( p = 0.072 ) Cat ^ 2 = - 4.59 ( p = 0.018 ) Amazon / Tripadvisor – 5,980 tokens Category - 0 . 5 0 - 0 . 2 5 0 . 00 0 . 25 0 . 50 0.15 0.28 Cat = 0.26 ( p = 0.496 ) Cat ^ 2 = - 2.23 ( p = 0.131 ) pretty / r “ Potts & diagrams ” Potts , & Christopher . & 2011 . & NSF & workshop & & restructuring & adjectives . good great excellent disappointing bad terrible totally absolutely utterly somewhat fairly pretty Positive scalars Negative scalars Emphatics Attenuators 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating 1 2 3 4 5 6 7 8 9 10 rating Figure 21.11 Potts diagrams ( Potts , 2011 ) emphatic attenuating adverbs . ment . might find words often Democratic Republican members Congress , words often menus expensive restaurants cheap restaurants . two classes documents , find words associated cate - gory another , might choose just compute difference frequencies ( word w frequent class class B ? ) . difference frequencies might compute ratio frequencies , log odds ratio ( log ratio odds two words ) . sort words whichever associations category , ( sorting words overrepresented category words overrepresented category B ) . problem simple log-likelihood log odds methods work well rare words frequent words ; words fre - quent , differences seem large , words rare , differences seem large . section walk details solution problem : “ log odds ratio informative Dirichlet prior ” method Monroe et al . ( 2008 ) particularly useful method finding words statistically overrepresented particular category texts compared another . based idea another large corpus get prior estimate expect frequency word . start goal : assume know word horrible occurs corpus corpus j . compute log likelihood ratio , log likelihoodratio f ( w ) mean frequency word w corpus , ni mean total number words corpus : llr ( horrible ) = log Pi ( horrible ) P j ( horrible ) = logPi ( horrible ) − logP j ( horrible ) = log fi ( horrible ) ni − log f j ( horrible ) n j ( 21.7 ) , compute log odds ratio : horrible higher odds inlog odds ratio 14 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION j : lor ( horrible ) = log ( Pi ( horrible ) 1 − Pi ( horrible ) ) − log ( P j ( horrible ) 1 − P j ( horrible ) ) = log    f ( horrible ) ni 1 − f ( horrible ) ni    − log    f j ( horrible ) n j 1 − f j ( horrible ) n j    = log ( fi ( horrible ) ni − fi ( horrible ) ) − log ( f j ( horrible ) n j − f j ( horrible ) ) ( 21.8 ) Dirichlet intuition large background corpus get prior estimate expect frequency word w . simply adding counts corpus numerator denominator , essentially shrinking counts toward prior . like asking large differences j expect frequencies well-estimated large background corpus . method estimates difference frequency word w two corpora j via prior-modified log odds ratio w , δ ( − j ) w , estimated : δ ( − j ) w = log ( f iw + αw ni + α0 − ( f iw + αw ) ) − log ( f jw + αw n j + α0 − ( f jw + αw ) ) ( 21.9 ) ( ni size corpus , n j size corpus j , f iw count word w corpus , f jw count word w corpus j , α0 size background corpus , αw count word w background corpus . ) addition , Monroe et al . ( 2008 ) make estimate variance log – odds – ratio : σ2 ( δ̂ ( − j ) w ) ≈ 1 f iw + αw + 1 f jw + αw ( 21.10 ) final statistic word z – score log – odds – ratio : δ̂ ( − j ) w √ σ2 ( δ̂ ( − j ) w ) ( 21.11 ) Monroe et al . ( 2008 ) method thus modifies commonly log odds ratio two ways : z-scores log odds ratio , controls amount variance word’s frequency , counts background corpus provide prior count words . Fig . 21.12 shows method applied dataset restaurant reviews Yelp , comparing words 1-star reviews words 5-star reviews ( Jurafsky et al . , 2014 ) . largest difference obvious sentiment words , 1-star reviews negative sentiment words like worse , bad , awful 5-star reviews positive sentiment words like great , best , amazing . illuminating differences . 1-star reviews logical negation ( , ) , 5-star reviews emphatics emphasize universality ( , highly , every , always ) . 1 - star reviews first person plurals ( , , ) 5 star reviews second person . 1-star reviews talk people ( manager , waiter , customer ) 5-star reviews talk dessert properties expensive restaurants like courses atmosphere . Jurafsky et al . ( 2014 ) details . 21.6 • LEXICONS SENTIMENT RECOGNITION 15 Class Words 1-star reviews Class Words 5-star reviews Negative worst , rude , terrible , horrible , bad , awful , disgusting , bland , tasteless , gross , mediocre , overpriced , worse , poor Positive great , best , love ( d ) , delicious , amazing , favorite , perfect , excellent , awesome , friendly , fantastic , fresh , wonderful , - credible , sweet , yum ( ) Negation , Emphatics / universals , highly , perfectly , definitely , abso - lutely , everything , every , always 1Pl pro , , 2 pro 3 pro , , , Articles , Past verb , , asked , told , , , charged , waited , left , took Advice try , recommend Sequencers , Conjunct , , well , , Nouns manager , waitress , waiter , customer , customers , attitude , waste , poisoning , money , bill , minutes Nouns atmosphere , dessert , chocolate , wine , course , menu Irrealis modals , Auxiliaries / ’ s , , ’ ve , Comp , Prep , , , die , city , mouth Figure 21.12 top 50 words associated – star five-star restaurant reviews Yelp dataset 900,000 reviews , Monroe et al . ( 2008 ) method ( Jurafsky et al . , 2014 ) . 21.6 Lexicons Sentiment Recognition Chapter 4 introduced naive Bayes algorithm sentiment analysis . lexicons focused throughout chapter far number ways improve sentiment detection . simplest case , lexicons sufficient training data build supervised sentiment analyzer ; often expensive human assign sentiment document train supervised classifier . situations , lexicons rule-based algorithm classifica - tion . simplest version just ratio positive negative words : document positive negative words ( lexicon decide po - larity word document ) , classified positive . Often threshold λ , document classified positive ratio greater λ . sentiment lexicon includes positive negative weights word , θ + w θ − w , well . Here’s simple sentiment algorithm : f + = ∑ w s.t . w ∈ positivelexicon θ + w count ( w ) f − = ∑ w s.t . w ∈ negativelexicon θ − w count ( w ) sentiment =          + f + f − > λ − f − f + > λ 0 otherwise . ( 21.12 ) supervised training data available , counts computed sentiment lex - icons , sometimes weighted normalized various ways , fea - tures classifier lexical non-lexical features . return algorithms Section 21.8 . 16 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION 21.7 tasks : Personality Many kinds affective meaning extracted text speech . example detecting person’s personality language useful di-personality alog systems ( users tend prefer agents match personality ) , play useful role computational social science questions like understanding per - sonality related kinds behavior . Many theories human personality based around small number dimen - sions , various versions “ Big Five ” dimensions ( Digman , 1990 ) : Extroversion vs . Introversion : sociable , assertive , playful vs . aloof , reserved , shy Emotional stability vs . Neuroticism : calm , unemotional vs . insecure , anxious Agreeableness vs . Disagreeableness : friendly , cooperative vs . antagonistic , fault - finding Conscientiousness vs . Unconscientiousness : self-disciplined , organized vs . - efficient , careless Openness experience : intellectual , insightful vs . shallow , unimaginative few corpora text speech labeled personality author authors take standard personality test . essay corpus Pennebaker King ( 1999 ) consists 2,479 essays ( 1.9 million words ) psy - chology students asked “ write whatever comes mind ” 20 minutes . EAR ( Electronically Activated Recorder ) corpus Mehl et al . ( 2006 ) created volunteers wear recorder throughout day , ran - domly recorded short snippets conversation throughout day , transcribed . Facebook corpus ( Schwartz et al . , 2013 ) includes 309 million words Facebook posts 75,000 volunteers . example , samples Pennebaker King ( 1999 ) essay written someone neurotic end neurotic / emotionally stable scale , friends just barged , jumped seat . crazy . tell again . fastidious actually . certain things annoy . things annoy actually annoy normal human , know freak . someone emotionally stable end scale : excel sport know push body harder anyone know , matter test always push body harder everyone else . best matter sport event . good love ride bike . Another kind affective meaning Scherer ( 2000 ) calls interpersonal stance , ‘ affective stance taken toward another person specific interactioninterpersonalstance coloring interpersonal exchange ’ . Extracting kind meaning means au - tomatically labeling participants friendly , supportive , distant . example Ranganath et al . ( 2013 ) studied corpus speed-dates , par - ticipants went series 4-minute romantic dates , wearing microphones . participant labeled flirtatious , friendly , awkward , assertive . Ranganath et al . ( 2013 ) combination lexicons features detect interpersonal stances text . 21.8 • AFFECT RECOGNITION 17 21.8 Affect Recognition Detection emotion , personality , interactional stance , kinds af - fective meaning described Scherer ( 2000 ) generalizing algo - rithms described detecting sentiment . common algorithms involve supervised classification : training set labeled affective meaning detected , classifier built features extracted training set . sentiment analysis , training set large enough , test set sufficiently similar training set , simply words bigrams features powerful classifier like SVM logistic regression , described Fig . ? ? Chapter 4 , excellent algorithm whose performance hard beat . Thus treat affective meaning classification text sample simple document classification . modifications nonetheless often necessary large datasets . example , Schwartz et al . ( 2013 ) study personality , gender , age 700 million words Facebook posts subset n-grams lengths 1 - 3 . words phrases least 1 % subjects included features , 2-grams 3-grams kept sufficiently high PMI ( PMI greater 2 ∗ length , length number words ) : pmi ( phrase ) = log p ( phrase ) ∏ w ∈ phrase p ( w ) ( 21.13 ) Various weights features , including raw count training set , normalized probability log probability . Schwartz et al . ( 2013 ) , example , turn feature counts phrase likelihoods normalizing subject’s total word . p ( phrase | subject ) = freq ( phrase , subject ) ∑ phrase ′ ∈ vocab ( subject ) freq ( phrase ′ , subject ) ( 21.14 ) training data sparser , similar test set , lexicons discussed play helpful role , alone combination words n-grams . Many possible values lexicon features . simplest just indicator function , value feature fL takes value 1 particular text word relevant lexicon L . notation Chapter 4 , feature value defined particular output class c document x . fL ( c , x ) = { 1 ∃ w : w ∈ L & w ∈ x & class = c 0 otherwise Alternatively value feature fL particular lexicon L total number word tokens document occur L : fL = ∑ w ∈ L count ( w ) lexica word associated score weight , count multiplied weight θ Lw : fL = ∑ w ∈ L θ Lwcount ( w ) 18 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION Counts alternatively logged normalized per writer Eq . 21.14 . defined , lexicon features supervised classifier predict desired affective category text document . Once classifier trained , examine lexicon features associated classes . classifier like logistic regression feature weight gives indication associated feature class . Thus , example , Mairesse Walker ( 2008 ) found classifying per - sonality , dimension Agreeable , LIWC lexicons Family Home positively associated LIWC lexicons anger swear negatively associated . contrast , Extroversion positively associated Friend , Religion Self lexicons , Emotional Stability positively associated Sports negatively associated Negative Emotion . Figure 6 . Words , phrases , topics distinguishing extraversion introversion neuroticism emotional stability . . Language extraversion ( left , e.g . , ‘ party ’ ) introversion ( right , e.g . , ‘ computer ’ ) ; N ~ 72,709 . B . Language distinguishing neuroticism ( left , e.g . ‘ hate ’ ) emotional stability ( right , e.g . , ‘ blessed ’ ) ; N ~ 71,968 ( adjusted age gender , Bonferroni-corrected pv0:001 ) . Figure S8 contains results openness , conscientiousness , agreeableness . doi : 10.1371/journal.pone.0073791.g006 Personality , Gender , Age Social Media Language PLOS | www.plosone.org 12 September 2013 | Volume 8 | Issue 9 | e73791 Figure 6 . Words , phrases , topics distinguishing extraversion introversion neuroticism emotional stability . . Language extraversion ( left , e.g . , ‘ party ’ ) introversion ( right , e.g . , ‘ computer ’ ) ; N ~ 72,709 . B . Language distinguishing neuroticism ( left , e.g . ‘ hate ’ ) emotional stability ( right , e.g . , ‘ blessed ’ ) ; N ~ 71,968 ( adjusted age gender , Bonferroni-corrected pv0:001 ) . Figure S8 contains results openness , conscientiousness , agreeableness . doi : 10.1371/journal.pone.0073791.g006 Personality , Gender , Age Social Media Language PLOS | www.plosone.org 12 September 2013 | Volume 8 | Issue 9 | e73791 ( ) ( b ) Figure 21.13 Word clouds Schwartz et al . ( 2013 ) , showing words highly associated introversion ( left ) extroversion ( right ) . size word represents association strength ( regression coefficient ) , color ( ranging cold hot ) represents relative frequency word / phrase ( low high ) . situation words phrases document potential features , resulting weights learned regression clas - sifier basis affective lexicon . Extroversion / Introversion classifier Schwartz et al . ( 2013 ) , ordinary least-squares regression predict value personality dimension words phrases . resulting re - gression coefficient word phrase association value predicted dimension . word clouds Fig . 21.13 show example words associated introversion ( ) extroversion ( b ) . 21.9 Lexicon-based methods Entity-Centric Affect get affect score entire document , particular entity text ? entity-centric method Field Tsvetkov ( 2019 ) combines affect lexicons contextual embeddings assign affect score entity text . context affect people , relabel Valence / Arousal / Dominance dimension Sentiment / Agency / Power . algorithm first trains classifiers map embeddings scores : 1 . word w training corpus : ( ) off-the-shelf pre-trained language models ( ELMo BERT ) ex - tract contextual embedding e instance word . addi - 21.10 • CONNOTATION FRAMES 19 tional fine-tuning . ( b ) Average e embeddings instance w obtain single embedding vector training point w . ( c ) NRC VAD Lexicon get S , , P scores w . 2 . Train ( three ) regression models words w predict V , , D scores word’s average embedding . entity mention m text , assign affect scores follows : 1 . same pre-trained LM get contextual embeddings m context . 2 . Feed embeddings 3 regression models get S , , P scores entity . results ( S , , P ) tuple entity mention ; get scores resp - resentation entity complete document , run coref average ( S , , P ) scores mentions . find ELMo works better BERT . Fig . 21.14 shows scores algorithm characters movie Dark Knight run Wikpedia plot summary texts gold coreference . 2555 Power Score weakly Rachel Dent Gordan Batman Joker powerfully Sentiment Score negative Joker Dent Gordan Rachel Batman positive Agency Score dull Dent Gordan Rachel Batman Joker scary Figure 1 : Power , sentiment , agency scores char - acters Dark Night learned regres - sion model ELMo embeddings . Scores generally align character archetypes , i.e . antagonist lowest sentiment score . ment resulted effective removal industry . articles #MeToo movement portray men like Weinstein unpow - erful , speculate corpora train ELMo BERT portray powerful . Thus , corpus traditional power roles inverted , embeddings extracted ELMo BERT perform worse ran - dom , biased power struc - tures data trained . Further ev - idence exists performance BERT-masked embeddings - whereas em - beddings generally capture power poorly com - pared unmasked embeddings ( Table 2 ) , outperform unmasked embeddings task , even outperform frequency baseline setting . Nevertheless , outper - form Field et al . ( 2019 ) , likely capture affect information well unmasked embeddings ( Table 2 ) . 4.3 Qualitative Document-level Analysis Finally , qualitatively analyze well method captures affect dimensions analyzing single documents detail . conduct anal - ysis domain expect entities fulfill traditional power roles entity portray - als known . Following Bamman et al . ( 2013 ) , analyze Wikipedia plot summary movie Dark Knight , 7 focusing Batman ( protagonist ) , 8 Joker ( antagonist ) , Jim Gordan ( law enforcement officer , ally Batman ) , Har - 7http :// bit.ly/2XmhRDR 8We consider Batman / Bruce Wayne same entity . Power Score weakly Rachel Joker Dent Gordan Batmanpowerfully Sentiment Score negative Joker Gordan Batman Dent Rachel positive Agency Score dull Rachel Dent GordanBatman Joker scary Figure 2 : Power , sentiment , agency scores char - acters Dark Night learned ASP ELMo embeddings . scores reflect same pat - terns regression model greater separation characters . vey Dent ( ally Batman turns evil ) Rachel Dawes ( primary love interest ) . facil - itate extracting example sentences , score instance entities narrative separately average instances obtain entity score document . 9 maximize data capturing every mention entity , per - form co-reference resolution hand . Addition - ally , based results Table 3 well Wikipedia data training ELMo model ( Peters et al . , 2018 ) , ELMo embed - dings analysis . Figures 1 2 show results . refer - ence , show entity scores compared polar opposite pair identified ASP . regression model ASP show similar pat - terns . Batman high power , Rachel low power . Additionally , Joker associated negative sentiment , high - est agency . Throughout plot summary , movie progresses Joker taking aggres - sive action characters responding . dynamic reflected Joker’s profile score , high-powered , high-agency , low-sentiment character , primary plot - driver . general , ASP shows greater separation characters regression model . hypothesize occurs ASP isolates dimensions interest , regression ap - proach captures confounds , hu - 9When averaging metric evaluations , found significant change results . Thus , sce - narios , compute scores averaged embeddings , rather averaging scores separately computed embed - ding reduce computationally complexity . Figure 21.14 Power ( dominance ) , sentiment ( valence ) agency ( arousal ) characters movie Dark Knight computed ELMo embeddings trained NRC VAD Lexicon . Note protagonst ( Batman ) antagonist ( Joker ) high power agency scores differ sentiment , love interest Rachel low power agency high sentiment . 21.10 Connotation Frames lexicons described far define word point affective space . connotation frame , contrast , lexicon incorporates richer kind gram-connotationframe matical structure , combining affective lexicons frame semantic lexicons Chapter 20 . basic insight connotation frame lexicons predicate like verb expresses connotations verb’s arguments ( Rashkin et al . 2016 , Rashkin et al . 2017 ) . Consider sentences like : ( 21.15 ) Country violated sovereignty Country B ( 21.16 ) teenager . . . survived Boston Marathon bombing ” 20 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION verb violate ( 21.15 ) , author expressing sympathies Country B , portraying Country B victim , expressing antagonism toward agent Country . contrast , verb survive , author ( 21.16 ) expressing bombing negative experience , subject sentence , teenager , sympathetic character . aspects connotation inherent meaning verbs violate survive , shown Fig . 21.15 . Writer Role1 Role2 Role1 sympathetic victim type hardship Reader + _ + _ _ S ( wr ite r → ro le1 ) S ( writer → role2 ) Connotation Frame “ Role1 survives Role2 ” S ( role1 → role2 ) Writer Role1 Role2 Role1 antagonist Role2 sympathetic victim Reader + _ + _ _ S ( wr ite r → ro le1 ) S ( writer → role2 ) Connotation Frame “ Role1 violates Role2 ” S ( role1 → role2 ) ( ) ( b ) Figure 21.15 Connotation frames survive violate . ( ) survive , writer reader positive sentiment toward Role1 , subject , negative sentiment toward Role2 , direct object . ( b ) violate , writer reader positive sentiment toward Role2 , direct object . connotation frame lexicons Rashkin et al . ( 2016 ) Rashkin et al . ( 2017 ) express connotative aspects predicate toward argument , including effect ( something bad happened x ) value : ( x valuable ) , mental state : ( x distressed event ) . Connotation frames mark aspects power agency ; Chapter 20 ( Sap et al . , 2017 ) . Connotation frames built hand ( Sap et al . , 2017 ) , learned supervised learning ( Rashkin et al . , 2016 ) , example hand-labeled train - ing data supervise classifiers individual relations , e.g . , S ( writer → Role1 ) + - , improving accuracy via global constraints relations . 21.11 Summary • Many kinds affective states distinguished , including emotions , moods , attitudes ( include sentiment ) , interpersonal stance , personality . • Emotion represented fixed atomic units often called basic emo - tions , points space defined dimensions like valence arousal . • Words connotational aspects related affective states , connotational aspect word meaning represented lexicons . • Affective lexicons built hand , crowd sourcing label affective content word . • Lexicons built semi-supervised , bootstrapping seed words similarity metrics like embedding cosine . • Lexicons learned fully supervised manner , convenient training signal found world , ratings assigned users review site . BIBLIOGRAPHICAL HISTORICAL NOTES 21 • Words assigned weights lexicon various functions word counts training texts , ratio metrics like log odds ratio informative Dirichlet prior . • Personality often represented point 5-dimensional space . • Affect detected , just like sentiment , standard supervised text classification techniques , words bigrams text features . Additional features drawn counts words lexicons . • Lexicons detect affect rule-based classifier picking simple majority sentiment based counts words lexicon . • Connotation frames express richer relations affective meaning pred - icate encodes arguments . Bibliographical Historical Notes idea formally representing subjective meaning words began Os - good et al . ( 1957 ) , same pioneering study first proposed vector space model meaning described Chapter 6 . Osgood et al . ( 1957 ) participants rate words various scales , ran factor analysis ratings . significant factor uncovered evaluative dimension , distinguished pairs like good / bad , valuable / worthless , pleasant / unpleasant . work influenced development early dictionaries sentiment affective meaning field content analysis ( Stone et al . , 1966 ) . Wiebe ( 1994 ) began influential line work detecting subjectivity text , subjectivity beginning task identifying subjective sentences subjective char - acters described text holding private states , beliefs attitudes . Learned sentiment lexicons polarity lexicons Hatzivassiloglou McKeown ( 1997 ) shown useful feature subjectivity detection ( Hatzi - vassiloglou Wiebe 2000 , Wiebe 2000 ) . term sentiment seems introduced 2001 Das Chen ( 2001 ) , describe task measuring market sentiment looking words stock trading message boards . same paper Das Chen ( 2001 ) proposed sentiment lexicon . list words lexicon created hand , word assigned weights according discriminated particular class ( say buy versus sell ) maximizing across-class variation minimizing within-class variation . term sentiment , lexicons , caught quite quickly ( e.g . , inter alia , Turney 2002 ) . Pang et al . ( 2002 ) first showed power words sentiment lexicon ; Wang Manning ( 2012 ) . semi-supervised methods describe extending sentiment dic - tionaries drew early idea synonyms antonyms tend co-occur same sentence . ( Miller Charles 1991 , Justeson Katz 1991 , Riloff Shep - herd 1997 ) . semi-supervised methods learning cues affective mean - ing rely information extraction techniques , like AutoSlog pattern extractors ( Riloff Wiebe , 2003 ) . Graph based algorithms sentiment first sug - gested Hatzivassiloglou McKeown ( 1997 ) , graph propagation became standard method ( Zhu Ghahramani 2002 , Zhu et al . 2003 , Zhou et al . 2004 , Velikovich et al . 2010 ) . Crowdsourcing improve precision 22 CHAPTER 21 • LEXICONS SENTIMENT , AFFECT , CONNOTATION filtering result semi-supervised lexicon learning ( Riloff Shepherd 1997 , Fast et al . 2016 ) . recent work focuses ways learn embeddings directly encode sen - timent properties , DENSIFIER algorithm Rothe et al . ( 2016 ) learns transform embedding space focus sentiment ( ) infor - mation . Bibliographical Historical Notes 23 , J . , Kwak , H . , Ahn , Y . - Y . ( 2018 ) . SemAxis : lightweight framework characterize domain-specific word semantics beyond sentiment . ACL 2018 . Baccianella , S . , Esuli , . , Sebastiani , F . ( 2010 ) . Senti - wordnet 3.0 : enhanced lexical resource sentiment analysis opinion mining . . LREC-10 , 2200 – 2204 . Brysbaert , M . , Warriner , . B . , Kuperman , V . ( 2014 ) . Concreteness ratings 40 thousand generally known en - glish word lemmas . Behavior Research Methods , 46 ( 3 ) , 904 – 911 . Das , S . R . Chen , M . Y . ( 2001 ) . Yahoo ! Amazon : Sentiment parsing small talk web . EFA 2001 Barcelona Meetings . Available SSRN : http://ssrn.com/abstract=276189 . Digman , J . M . ( 1990 ) . Personality structure : Emergence five-factor model . Annual Review Psychology , 41 ( 1 ) , 417 – 440 . Ekman , P . ( 1999 ) . Basic emotions . Dalgleish , T . Power , M . J . ( Eds . ) , Handbook Cognition Emotion , 45 – 60 . Wiley . Fast , E . , Chen , B . , Bernstein , M . S . ( 2016 ) . Empath : Understanding Topic Signals Large-Scale Text . CHI . Field , . Tsvetkov , Y . ( 2019 ) . Entity-centric contextual affective analysis . ACL 2019 , 2550 – 2560 . Hamilton , W . L . , Clark , K . , Leskovec , J . , Jurafsky , D . ( 2016 ) . Inducing domain-specific sentiment lexicons unlabeled corpora . EMNLP 2016 . Hatzivassiloglou , V . McKeown , K . ( 1997 ) . Predicting semantic orientation adjectives . ACL / EACL-97 , 174 – 181 . Hatzivassiloglou , V . Wiebe , J . ( 2000 ) . Effects adjec - tive orientation gradability sentence subjectivity . COLING-00 , 299 – 305 . Hellrich , J . , Buechel , S . , Hahn , U . ( 2019 ) . Modeling word emotion historical language : Quantity beats sup - posed stability seed word selection . Proceedings 3rd Joint SIGHUM Workshop Computational Lin - guistics Cultural Heritage , Social Sciences , Humanities Literature , 1 – 11 . Hu , M . Liu , B . ( 2004 ) . Mining summarizing cus - tomer reviews . SIGKDD-04 . Jurafsky , D . , Chahuneau , V . , Routledge , B . R . , Smith , N . . ( 2014 ) . Narrative framing consumer sentiment online restaurant reviews . First Monday , 19 ( 4 ) . Justeson , J . S . Katz , S . M . ( 1991 ) . Co-occurrences antonymous adjectives contexts . Computational linguistics , 17 ( 1 ) , 1 – 19 . Kim , S . M . Hovy , E . H . ( 2004 ) . Determining senti - ment opinions . COLING-04 . Kiritchenko , S . Mohammad , S . M . ( 2017 ) . Best-worst scaling reliable rating scales : case study sentiment intensity annotation . ACL 2017 , 465 – 470 . Louviere , J . J . , Flynn , T . N . , Marley , . . J . ( 2015 ) . Best-worst scaling : Theory , methods applications . Cambridge University Press . Mairesse , F . Walker , M . . ( 2008 ) . Trainable generation big-five personality styles data-driven parameter estimation . ACL-08 . Mehl , M . R . , Gosling , S . D . , Pennebaker , J . W . ( 2006 ) . Personality natural habitat : manifestations - plicit folk theories personality daily life . . Journal Personality Social Psychology , 90 ( 5 ) . Miller , G . . Charles , W . G . ( 1991 ) . Contextual cor - relates semantics similarity . Language Cognitive Processes , 6 ( 1 ) , 1 – 28 . Mohammad , S . M . ( 2018a ) . Obtaining reliable human rat - ings valence , arousal , dominance 20,000 english words . ACL 2018 . Mohammad , S . M . ( 2018b ) . Word affect intensities . LREC-18 . Mohammad , S . M . Turney , P . D . ( 2013 ) . Crowdsourcing word-emotion association lexicon . Computational Intel - ligence , 29 ( 3 ) , 436 – 465 . Monroe , B . L . , Colaresi , M . P . , Quinn , K . M . ( 2008 ) . Fightin’words : Lexical feature selection evaluation identifying content political conflict . Political Anal - ysis , 16 ( 4 ) , 372 – 403 . Osgood , C . E . , Suci , G . J . , Tannenbaum , P . H . ( 1957 ) . Measurement Meaning . University Illinois Press . Pang , B . , Lee , L . , Vaithyanathan , S . ( 2002 ) . Thumbs up ? Sentiment classification machine learning tech - niques . EMNLP 2002 , 79 – 86 . Pennebaker , J . W . , Booth , R . J . , Francis , M . E . ( 2007 ) . Linguistic Inquiry Word Count : LIWC 2007 . Austin , TX . Pennebaker , J . W . King , L . . ( 1999 ) . Linguistic styles : language individual difference . Journal Per - sonality Social Psychology , 77 ( 6 ) . Picard , R . W . ( 1995 ) . Affective computing . Tech . rep . 321 , MIT Media Lab Perceputal Computing Technical Report . Revised November 26 , 1995 . Plutchik , R . ( 1962 ) . emotions : Facts , theories , new model . Random House . Plutchik , R . ( 1980 ) . general psychoevolutionary theory emotion . Plutchik , R . Kellerman , H . ( Eds . ) , Emo - tion : Theory , Research , Experience , Volume 1 , 3 – 33 . Academic Press . Potts , C . ( 2011 ) . negativity negation . Li , N . Lutz , D . ( Eds . ) , Proceedings Semantics Linguistic Theory 20 , 636 – 659 . CLC Publications , Ithaca , NY . Ranganath , R . , Jurafsky , D . , McFarland , D . . ( 2013 ) . Detecting friendly , flirtatious , awkward , assertive speech speed-dates . Computer Speech Language , 27 ( 1 ) , 89 – 115 . Rashkin , H . , Bell , E . , Choi , Y . , Volkova , S . ( 2017 ) . Mul - tilingual connotation frames : case study social media targeted sentiment analysis forecast . ACL 2017 , 459 – 464 . Rashkin , H . , Singh , S . , Choi , Y . ( 2016 ) . Connotation frames : data-driven investigation . ACL 2016 , 311 – 321 . Riloff , E . Shepherd , J . ( 1997 ) . corpus-based approach building semantic lexicons . EMNLP 1997 . Riloff , E . Wiebe , J . ( 2003 ) . Learning extraction patterns subjective expressions . EMNLP 2003 . Rothe , S . , Ebert , S . , Schütze , H . ( 2016 ) . Ultradense Word Embeddings Orthogonal Transformation . NAACL HLT 2016 . 24 Chapter 21 • Lexicons Sentiment , Affect , Connotation Russell , J . . ( 1980 ) . circumplex model affect . Journal personality social psychology , 39 ( 6 ) , 1161 – 1178 . Sap , M . , Prasettio , M . C . , Holtzman , . , Rashkin , H . , Choi , Y . ( 2017 ) . Connotation frames power agency modern films . EMNLP 2017 , 2329 – 2334 . Scherer , K . R . ( 2000 ) . Psychological models emotion . Borod , J . C . ( Ed . ) , neuropsychology emotion , 137 – 162 . Oxford . Schwartz , H . . , Eichstaedt , J . C . , Kern , M . L . , Dziurzyn - ski , L . , Ramones , S . M . , Agrawal , M . , Shah , . , Kosin - ski , M . , Stillwell , D . , Seligman , M . E . P . , Ungar , L . H . ( 2013 ) . Personality , gender , age language - cial media : open-vocabulary approach . PloS , 8 ( 9 ) , e73791 . Stone , P . , Dunphry , D . , Smith , M . , Ogilvie , D . ( 1966 ) . General Inquirer : Computer Approach Content Analysis . Cambridge , MA : MIT Press . Tomkins , S . S . ( 1962 ) . Affect , imagery , consciousness : Vol . . positive affects . Springer . Turney , P . D . ( 2002 ) . Thumbs up thumbs down ? seman - tic orientation applied unsupervised classification re - views . ACL-02 . Turney , P . D . Littman , M . ( 2003 ) . Measuring praise criticism : Inference semantic orientation associa - tion . ACM Transactions Information Systems ( TOIS ) , 21 , 315 – 346 . Velikovich , L . , Blair-Goldensohn , S . , Hannan , K . , Mc - Donald , R . ( 2010 ) . viability web-derived polarity lexicons . NAACL HLT 2010 , 777 – 785 . Wang , S . Manning , C . D . ( 2012 ) . Baselines bigrams : Simple , good sentiment topic classification . ACL 2012 , 90 – 94 . Wiebe , J . ( 1994 ) . Tracking point view narrative . Com - putational Linguistics , 20 ( 2 ) , 233 – 287 . Wiebe , J . ( 2000 ) . Learning subjective adjectives cor - pora . AAAI-00 , 735 – 740 . Wiebe , J . , Bruce , R . F . , O’Hara , T . P . ( 1999 ) . Devel - opment gold-standard data set subjectivity classifications . ACL-99 , 246 – 253 . Wilson , T . , Wiebe , J . , Hoffmann , P . ( 2005 ) . Recogniz - ing contextual polarity phrase-level sentiment analysis . HLT-EMNLP-05 , 347 – 354 . Zhou , D . , Bousquet , O . , Lal , T . N . , Weston , J . , Schölkopf , B . ( 2004 ) . Learning local global con - sistency . NIPS 2004 . Zhu , X . Ghahramani , Z . ( 2002 ) . Learning la - beled unlabeled data label propagation . Tech . rep . CMU-CALD-02 , CMU . Zhu , X . , Ghahramani , Z . , Lafferty , J . ( 2003 ) . Semi - supervised learning gaussian fields harmonic functions . ICML 2003 , 912 – 919 .