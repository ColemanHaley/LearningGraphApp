Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 7 Neural Networks NeuralLanguage Models “ [ M ] achines character behave complicated manner number units large . ” Alan Turing ( 1948 ) “ Intelligent Machines ” , page 6 Neural networks fundamental computational tool language process - ing , old . called neural origins lie McCulloch-Pitts neuron ( McCulloch Pitts , 1943 ) , simplified model human neuron kind computing element described terms propositional logic . modern language processing longer draws early biological inspirations . , modern neural network network small computing units , takes vector input values produces single output value . chapter introduce neural net applied classification . architecture introduce called feedforward network computation proceeds iter-feedforward atively layer units next . modern neural nets often called deep learning , modern networks often deep ( many layers ) . deep learning Neural networks share same mathematics logistic regression . neural networks powerful classifier logistic regression , indeed minimal neural network ( technically single ‘ hidden layer ’ ) shown learn function . Neural net classifiers different logistic regression another way . logistic regression , applied regression classifier many different tasks developing many rich kinds feature templates based domain knowledge . working neural networks , common avoid rich hand - derived features , building neural networks take raw words inputs learn induce features part process learning classify . saw examples kind representation learning embeddings Chapter 6 . Nets deep particularly good representation learning . reason deep neural nets right tool large scale problems offer sufficient data learn features automatically . chapter introduce feedforward networks classifiers , ap - ply simple task language modeling : assigning probabilities word sequences predicting upcoming words . subsequent chapters introduce many aspects neural models , recurrent neural networks ( Chap - ter 9 ) , encoder-decoder models , attention Transformer ( Chapter 10 ) . 2 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS 7.1 Units building block neural network single computational unit . unit takes set real valued numbers input , performs computation , produces output . heart , neural unit taking weighted sum inputs , addi - tional term sum called bias term . set inputs x1 . . . xn , unit hasbias term set corresponding weights w1 . . . wn bias b , weighted sum z represented : z = b + ∑ wixi ( 7.1 ) Often convenient express weighted sum vector notation ; recall linear algebra vector , heart , just list array numbers . Thusvector talk z terms weight vector w , scalar bias b , input vector x , replace sum convenient dot product : z = w · x + b ( 7.2 ) defined Eq . 7.2 , z just real valued number . Finally , z , linear function x , output , neural units apply non-linear function f z . refer output function activation value unit , . just modeling single unit , theactivation activation node fact final output network , generally call y . value y defined : y = = f ( z ) discuss three popular non-linear functions f ( ) below ( sigmoid , tanh , rectified linear ReLU ) pedagogically convenient start sigmoid function saw Chapter 5 : sigmoid y = σ ( z ) = 1 1 + e − z ( 7.3 ) sigmoid ( shown Fig . 7.1 ) number advantages ; maps output range [ 0,1 ] , useful squashing outliers toward 0 1 . differentiable , saw Section ? ? handy learning . Figure 7.1 sigmoid function takes real value maps range [ 0,1 ] . nearly linear around 0 outlier values get squashed toward 0 1 . 7.1 • UNITS 3 Substituting Eq . 7.2 Eq . 7.3 gives output neural unit : y = σ ( w · x + b ) = 1 1 + exp ( − ( w · x + b ) ) ( 7.4 ) Fig . 7.2 shows final schematic basic neural unit . example unit takes 3 input values x1 , x2 , x3 , computes weighted sum , multiplying value weight ( w1 , w2 , w3 , respectively ) , adds bias term b , passes resulting sum sigmoid function result number 0 1 . x1 x2 x3 y w1 w2 w3 ∑ b σ + 1 z Figure 7.2 neural unit , taking 3 inputs x1 , x2 , x3 ( bias b represent weight input clamped + 1 ) producing output y . include convenient intermediate variables : output summation , z , output sigmoid , . case output unit y same , deeper networks reserve y mean final output entire network , leaving activation individual node . walk example just get intuition . suppose unit following weight vector bias : w = [ 0.2,0.3,0.9 ] b = 0.5 unit following input vector : x = [ 0.5,0.6,0.1 ] resulting output y : y = σ ( w · x + b ) = 1 1 + e − ( w·x + b ) = 1 1 + e − ( . 5 ∗ . 2 + . 6 ∗ . 3 + . 1 ∗ . 9 + . 5 ) = e − 0.87 = . 70 practice , sigmoid commonly activation function . function similar almost always better tanh function shown Fig . 7.3a ; tanh tanh variant sigmoid ranges - 1 + 1 : y = ez − e − z ez + e − z ( 7.5 ) simplest activation function , perhaps commonly , rec - tified linear unit , called ReLU , shown Fig . 7.3b . just same xReLU x positive , 0 otherwise : y = max ( x , 0 ) ( 7.6 ) 4 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS ( ) ( b ) Figure 7.3 tanh ReLU activation functions . activation functions different properties make useful differ - ent language applications network architectures . example rectifier func - tion nice properties result close linear . sigmoid tanh functions , high values z result values y saturated , i.e . , saturated extremely close 1 , causes problems learning . Rectifiers problem , output values close 1 approaches 1 nice gentle linear way . contrast , tanh function nice properties smoothly differentiable mapping outlier values toward mean . 7.2 XOR problem Early history neural networks realized power neural net - works , real neurons inspired , comes combining units larger networks . clever demonstrations need multi-layer networks proof Minsky Papert ( 1969 ) single neural unit compute simple functions input . Consider task computing elementary logical functions two inputs , like , , XOR . reminder , truth tables functions : XOR x1 x2 y x1 x2 y x1 x2 y 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 example first shown perceptron , simple neuralperceptron unit binary output non-linear activation function . output y perceptron 0 1 , computed follows ( same weight w , input x , bias b Eq . 7.2 ) : y = { 0 , w · x + b ≤ 0 1 , w · x + b > 0 ( 7.7 ) 7.2 • XOR PROBLEM 5 easy build perceptron compute logical functions binary inputs ; Fig . 7.4 shows necessary weights . x1 x2 + 1 - 1 1 1 x1 x2 + 1 0 1 1 ( ) ( b ) Figure 7.4 weights w bias b perceptrons computing logical functions . inputs shown x1 x2 bias special node value + 1 multiplied bias weight b . ( ) logical , showing weights w1 = 1 w2 = 1 bias weight b = − 1 . ( b ) logical , showing weights w1 = 1 w2 = 1 bias weight b = 0 . weights / biases just infinite number possible sets weights biases implement functions . turns , , possible build perceptron compute logical XOR ! ( worth spending moment give try ! ) intuition behind important result relies understanding percep - tron linear classifier . two-dimensional input x1 x2 , perception equation , w1x1 + w2x2 + b = 0 equation line . ( putting standard linear format : x2 = − ( w1 / w2 ) x1 − b . ) line acts decision boundary two-dimensional space output 0 assigned inputsdecisionboundary lying side line , output 1 input points lying side line . 2 inputs , decision boundary becomes hyperplane line , idea same , separating space two categories . Fig . 7.5 shows possible logical inputs ( 00 , 01 , 10 , 11 ) line drawn possible set parameters classifier . Notice simply way draw line separates positive cases XOR ( 01 10 ) negative cases ( 00 11 ) . say XOR linearly separablelinearlyseparable function . course draw boundary curve , function , single line . 7.2.1 solution : neural networks XOR function calculated single perceptron , cal - culated layered network units . example Goodfellow et al . ( 2016 ) computes XOR two layers ReLU-based units . Fig . 7.6 shows figure input processed two layers neural units . middle layer ( called h ) two units , output layer ( called y ) unit . set weights biases shown ReLU correctly computes XOR function . walk happens input x = [ 0 0 ] . multiply input value appropriate weight , sum , add bias b , get vector [ 0 - 1 ] , apply rectified linear transformation give output h layer [ 0 0 ] . once again multiply weights , sum , add bias ( 0 case ) resulting value 0 . reader work computation remaining 3 possible input pairs resulting y values 1 inputs [ 0 1 ] [ 1 0 ] 0 [ 0 0 ] [ 1 1 ] . 6 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS 0 0 1 1 x1 x2 0 0 1 1 x1 x2 0 0 1 1 x1 x2 ) x1 x2 b ) x1 x2 c ) x1 XOR x2 ? Figure 7.5 functions , , XOR , represented input x1 x-axis input x2 y axis . Filled circles represent perceptron outputs 1 , white circles perceptron outputs 0 . way draw line correctly separates two categories XOR . Figure styled Russell Norvig ( 2002 ) . x1 x2 h1 h2 y1 + 1 1 - 11 1 1 - 2 01 + 1 0 Figure 7.6 XOR solution Goodfellow et al . ( 2016 ) . three ReLU units , two layers ; called h1 , h2 ( h “ hidden layer ” ) y1 . , numbers arrows represent weights w unit , represent bias b weight unit clamped + 1 , bias weights / units gray . instructive look intermediate results , outputs two hidden nodes h0 h1 . showed previous paragraph h vector inputs x = [ 0 0 ] [ 0 0 ] . Fig . 7.7b shows values h layer 4 inputs . Notice hidden representations two input points x = [ 0 1 ] x = [ 1 0 ] ( two cases XOR output = 1 ) merged single point h = [ 1 0 ] . merger makes easy linearly separate positive negative cases XOR . words , view hidden layer network forming representation input . example just stipulated weights Fig . 7.6 . real examples weights neural networks learned automatically error backprop - agation algorithm introduced Section 7.4 . means hidden layers learn form useful representations . intuition , neural networks auto - matically learn useful representations input , key advantages , return again again later chapters . Note solution XOR problem requires network units non - linear activation functions . network made up simple linear ( perceptron ) units solve XOR problem . network formed many layers purely linear units always reduced ( shown computationally identical 7.3 • FEED-FORWARD NEURAL NETWORKS 7 0 0 1 1 x0 x1 ) original x space 0 0 1 1 h0 h1 2 b ) new h space Figure 7.7 hidden layer forming new representation input . rep - resentation hidden layer , h , compared original input representation x . Notice input point [ 0 1 ] collapsed input point [ 1 0 ] , making possible linearly separate positive negative cases XOR . Goodfellow et al . ( 2016 ) . ) single layer linear units appropriate weights , already shown ( visually , Fig . 7.5 ) single unit solve XOR problem . 7.3 Feed-Forward Neural Networks walk slightly formal presentation simplest kind neural network , feedforward network . feedforward network multilayerfeedforwardnetwork network units connected cycles ; outputs units layer passed units next higher layer , outputs passed back lower layers . ( Chapter 9 introduce networks cycles , called recurrent neural networks . ) historical reasons multilayer networks , especially feedforward networks , sometimes called multi-layer perceptrons ( MLPs ) ; technical misnomer , multi-layerperceptrons MLP units modern multilayer networks perceptrons ( perceptrons purely linear , modern networks made up units non-linearities like sigmoids ) , point name stuck . Simple feedforward networks three kinds nodes : input units , hidden units , output units . Fig . 7.8 shows picture . input units simply scalar values just saw Fig . 7.2 . core neural network hidden layer formed hidden units , hidden layer neural unit described Section 7.1 , taking weighted sum inputs applying non-linearity . standard architecture , layer fully-connected , meaning unit layer takes input outputsfully-connected units previous layer , link every pair units two adjacent layers . Thus hidden unit sums input units . Recall single hidden unit parameters w ( weight vector ) b ( bias scalar ) . represent parameters entire hidden layer combining weight vector wi bias bi unit single weight matrix W single bias vector b whole layer ( Fig . 7.8 ) . element Wi j weight matrix W represents weight connection ith input unit xi 8 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS x1 x2 h1 h2 y1 xn0 … h3 hn1 … + 1 b … U W y2 yn2 Figure 7.8 simple 2-layer feedforward network , hidden layer , output layer , input layer ( input layer usually counted enumerating layers ) . jth hidden unit h j . advantage single matrix W weights entire layer hidden layer computation feedforward network efficiently simple matrix operations . fact , computation three steps : multiplying weight matrix input vector x , adding bias vector b , applying activation function g ( sigmoid , tanh , ReLU activation function defined ) . output hidden layer , vector h , thus following , sigmoid function σ : h = σ ( Wx + b ) ( 7.8 ) Notice applying σ function vector , Eq . 7.3 applied scalar . thus allowing σ ( · ) , indeed activation function g ( · ) , apply vector element-wise , g [ z1 , z2 , z3 ] = [ g ( z1 ) , g ( z2 ) , g ( z3 ) ] . introduce constants represent dimensionalities vectors matrices . refer input layer layer 0 network , n0 represent number inputs , x vector real numbers dimension n0 , formally x ∈ Rn0 . call hidden layer layer 1 output layer layer 2 . hidden layer dimensionality n1 , h ∈ Rn1 b ∈ Rn1 ( hidden unit take different bias value ) . weight matrix W dimensionality W ∈ Rn1 × n0 . Take moment convince yourself matrix multiplication Eq . 7.8 compute value h j σ ( ∑ nx = 1 wi jxi + b j ) . saw Section 7.2 , resulting value h ( hidden hypoth - esis ) forms representation input . role output layer take new representation h compute final output . output real - valued number , many cases goal network make sort classification decision , focus case classification . binary task like sentiment classification , might single output node , value y probability positive versus negative sentiment . multinomial classification , assigning part-of-speech tag , might output node potential part-of-speech , whose output value probability part-of-speech , values output nodes sum . output layer thus gives probability distribution output 7.3 • FEED-FORWARD NEURAL NETWORKS 9 nodes . happens . Like hidden layer , output layer weight matrix ( call U ) , models include bias vector b output layer , simplify eliminating bias vector example . weight matrix multiplied input vector ( h ) produce intermediate output z . z = Uh n2 output nodes , z ∈ Rn2 , weight matrix U dimensionality U ∈ Rn2 × n1 , element Ui j weight unit j hidden layer unit output layer . , z output classifier , vector real-valued numbers , need classification vector probabilities . convenient function normalizing vector real values , meannormalizing converting vector encodes probability distribution ( numbers lie 0 1 sum 1 ) : softmax function saw page ? ? ofsoftmax Chapter 5 . vector z dimensionality d , softmax defined : softmax ( zi ) = ezi ∑ d j = 1 e z j 1 ≤ ≤ d ( 7.9 ) Thus example vector z =[ 0.6 1.1 - 1.5 1.2 3.2 - 1.1 ] , softmax ( z ) [ 0.055 0.090 0.0067 0.10 0.74 0.010 ] . recall softmax exactly create probability distribution vector real-valued numbers ( computed summing weights times features ) logistic regression Chapter 5 . means think neural network classifier hidden layer building vector h hidden layer representation input , running standard logistic regression features network develops h . contrast , Chapter 5 features mainly designed hand via feature templates . neural network like logistic regression , ( ) many layers , deep neural network like layer layer logistic regression classifiers , ( b ) rather forming features feature templates , prior layers network induce feature representations themselves . final equations feedforward network single hidden layer , takes input vector x , outputs probability distribution y , parameter - ized weight matrices W U bias vector b : h = σ ( Wx + b ) z = Uh y = softmax ( z ) ( 7.10 ) call network 2-layer network ( traditionally count input layer numbering layers , count output layer ) . terminology logistic regression 1-layer network . set up notation make easier talk deeper networks depth 2 . superscripts square brackets mean layer num - bers , starting 0 input layer . W [ 1 ] mean weight matrix ( first ) hidden layer , b [ 1 ] mean bias vector ( first ) hidden layer . n j mean number units layer j . g ( · ) stand activation function , tend ReLU tanh intermediate layers softmax output layers . [ ] mean output layer , z [ ] mean 10 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS combination weights biases W [ ] [ − 1 ] + b [ ] . 0th layer inputs , inputs x refer generally [ 0 ] . Thus re-represent 2-layer net Eq . 7.10 follows : z [ 1 ] = W [ 1 ] [ 0 ] + b [ 1 ] [ 1 ] = g [ 1 ] ( z [ 1 ] ) z [ 2 ] = W [ 2 ] [ 1 ] + b [ 2 ] [ 2 ] = g [ 2 ] ( z [ 2 ] ) ŷ = [ 2 ] ( 7.11 ) Note notation , equations computation layer same . algorithm computing forward step n-layer feedforward network , input vector [ 0 ] thus simply : 1 . . n z [ ] = W [ ] [ − 1 ] + b [ ] [ ] = g [ ] ( z [ ] ) ŷ = [ n ] activation functions g ( · ) generally different final layer . Thus g [ 2 ] might softmax multinomial classification sigmoid binary classification , ReLU tanh might activation function g ( · ) internal layers . 7.4 Training Neural Nets feedforward neural net instance supervised machine learning know correct output y observation x . system produces , via Eq . 7.11 , ŷ , system’s estimate true y . goal training procedure learn parameters W [ ] b [ ] layer make ŷ training observation close possible true y . general , drawing methods introduced Chapter 5 logistic regression , reader comfortable chapter proceeding . First , need loss function models distance system output gold output , common loss function logistic regression , cross-entropy loss . Second , find parameters minimize loss function , gradient descent optimization algorithm introduced Chapter 5 . Third , gradient descent requires knowing gradient loss function , vector contains partial derivative loss function respect parameters . part learning neural networks complex logistic logistic regression . logistic regression , observation directly compute derivative loss function respect individ - ual w b . neural networks , millions parameters many layers , harder compute partial derivative weight layer 1 loss attached later layer . partial loss intermediate layers ? answer algorithm called error backpropagation reverse differen - tiation . 7.4 • TRAINING NEURAL NETS 11 7.4.1 Loss function cross-entropy loss neural networks same saw forcross-entropyloss logistic regression . fact , neural network binary classifier , sig - moid final layer , loss function exactly same saw logistic regression Eq . ? ? : LCE ( ŷ , y ) = − log p ( y | x ) = − [ y log ŷ + ( 1 − y ) log ( 1 − ŷ ) ] ( 7.12 ) neural network multinomial classifier ? Let y vector C classes representing true output probability distribution . cross-entropy loss LCE ( ŷ , y ) = − C ∑ = 1 yi log ŷi ( 7.13 ) simplify equation further . Assume hard classification task , meaning class correct , output unit y class . true class , y vector yi = 1 y j = 0 ∀ j 6 = . vector like , value = 1 rest 0 , called one-hot vector . let ŷ vector output network . sum Eq . 7.13 0 except true class . Hence cross-entropy loss simply log probability correct class , call negative log likelihood loss : negative loglikelihood loss LCE ( ŷ , y ) = − log ŷi ( 7.14 ) Plugging softmax formula Eq . 7.9 , K number classes : LCE ( ŷ , y ) = − log ezi ∑ K j = 1 e z j ( 7.15 ) 7.4.2 Computing Gradient compute gradient loss function ? Computing gradient requires partial derivative loss function respect parameter . network weight layer sigmoid output ( logistic regression ) , simply derivative loss logistic regression Eq . 7.16 ( derived Section ? ? ) : ∂ LCE ( w , b ) ∂ w j = ( ŷ − y ) x j = ( σ ( w · x + b ) − y ) x j ( 7.16 ) network hidden layer softmax output , deriva - tive softmax loss Eq . ? ? : ∂ LCE ∂ wk = ( 1 { y = k } − p ( y = k | x ) ) xk = ( 1 { y = k } − e wk·x + bk ∑ K j = 1 e w j · x + b j ) xk ( 7.17 ) 12 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS derivatives give correct updates weight layer : last ! deep networks , computing gradients weight complex , computing derivative respect weight parameters appear way back early layers network , even though loss computed end network . solution computing gradient algorithm called error backprop - agation backprop ( Rumelhart et al . , 1986 ) . backprop invented spe-error back-propagation cially neural networks , turns same general procedure called backward differentiation , depends notion computation graphs . works next subsection . 7.4.3 Computation Graphs computation graph representation process computing mathematical expression , computation broken down separate operations , modeled node graph . Consider computing function L ( , b , c ) = c ( + 2b ) . make component addition multiplication operations explicit , add names ( d e ) intermediate outputs , resulting series computations : d = 2 ∗ b e = + d L = c ∗ e represent graph , nodes operation , di - rected edges showing outputs operation inputs next , Fig . 7.9 . simplest computation graphs compute value function inputs . figure , assumed inputs = 3 , b = 1 , c = − 2 , shown result forward pass compute re - sult L ( 3,1 , − 2 ) = 10 . forward pass computation graph , apply operation left right , passing outputs computation input next node . e = d + d = 2b L = ce 3 1 - 2 e = 5 d = 2 L = - 10 forward pass b c Figure 7.9 Computation graph function L ( , b , c ) = c ( + 2b ) , values input nodes = 3 , b = 1 , c = − 2 , showing forward pass computation L . 7.4.4 Backward differentiation computation graphs importance computation graph comes backward pass , compute derivatives need weight update . example goal compute derivative output function L respect 7.4 • TRAINING NEURAL NETS 13 input variables , i.e . , ∂ L ∂ , ∂ L ∂ b , ∂ L ∂ c . derivative ∂ L ∂ , tells small change affects L . Backwards differentiation makes chain rule calculus . Suppose wechain rule computing derivative composite function f ( x ) = u ( v ( x ) ) . derivative f ( x ) derivative u ( x ) respect v ( x ) times derivative v ( x ) respect x : d f dx = du dv · dv dx ( 7.18 ) chain rule extends two functions . computing derivative composite function f ( x ) = u ( v ( w ( x ) ) ) , derivative f ( x ) : d f dx = du dv · dv dw · dw dx ( 7.19 ) compute 3 derivatives need . computation graph L = ce , directly compute derivative ∂ L ∂ c : ∂ L ∂ c = e ( 7.20 ) two , need chain rule : ∂ L ∂ = ∂ L ∂ e ∂ e ∂ ∂ L ∂ b = ∂ L ∂ e ∂ e ∂ d ∂ d ∂ b ( 7.21 ) Eq . 7.21 thus requires five intermediate derivatives : ∂ L ∂ e , ∂ L ∂ c , ∂ e ∂ , ∂ e ∂ d , ∂ d ∂ b , follows ( making fact derivative sum sum derivatives ) : L = ce : ∂ L ∂ e = c , ∂ L ∂ c = e e = + d : ∂ e ∂ = 1 , ∂ e ∂ d = 1 d = 2b : ∂ d ∂ b = 2 backward pass , compute partials edge graph right left , multiplying necessary partials result final derivative need . Thus begin annotating final node ∂ L ∂ L = 1 . Moving left , compute ∂ L ∂ c ∂ L ∂ e , , annotated graph way input variables . forward pass conveniently already computed values forward intermediate variables need ( like d e ) compute derivatives . Fig . 7.10 shows backward pass . node need compute local partial derivative respect parent , multiply partial derivative passed down parent , pass child . Backward differentiation neural network course computation graphs real neural networks complex . Fig . 7.11 shows sample computation graph 2-layer neural network n0 = 14 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS e = d + d = 2b L = ce = 3 b = 1 e = 5 d = 2 L = - 10 ∂ L = 1 ∂ L ∂ L = - 4 ∂ b ∂ L = - 2 ∂ d b c ∂ L = - 2 ∂ ∂ L = 5 ∂ c ∂ L = - 2 ∂ e ∂ L = - 2 ∂ e ∂ e = 1 ∂ d ∂ L = 5 ∂ c ∂ d = 2 ∂ b ∂ e = 1 ∂ backward pass c = - 2 Figure 7.10 Computation graph function L ( , b , c ) = c ( + 2b ) , showing back - ward pass computation ∂ L ∂ , ∂ L ∂ b , ∂ L ∂ c . 2 , n1 = 2 , n2 = 1 , assuming binary classification hence sigmoid output unit simplicity . function computation graph computing : z [ 1 ] = W [ 1 ] x + b [ 1 ] [ 1 ] = ReLU ( z [ 1 ] ) z [ 2 ] = W [ 2 ] [ 1 ] + b [ 2 ] [ 2 ] = σ ( z [ 2 ] ) ŷ = [ 2 ] ( 7.22 ) z [ 2 ] = + [ 2 ] = σ [ 1 ] = ReLU z [ 1 ] = + b [ 1 ] * * * * x1 x2 [ 1 ] = ReLU z [ 1 ] = + b [ 1 ] * * w [ 2 ] 11 w [ 1 ] 11 w [ 1 ] 21 w [ 1 ] 12 w [ 1 ] 22 b [ 2 ] w [ 2 ] 21 L ( [ 2 ] , y ) Figure 7.11 Sample computation graph simple 2-layer neural net (= 1 hidden layer ) two input dimensions 2 hidden dimensions . weights need updating ( need know partial derivative loss function ) shown orange . order backward pass , need know derivatives functions graph . already saw Section ? ? derivative sigmoid σ : dσ ( z ) dz = σ ( z ) ( 1 − σ ( z ) ) ( 7.23 ) 7.5 • NEURAL LANGUAGE MODELS 15 need derivatives activation functions . derivative tanh : d tanh ( z ) dz = 1 − tanh2 ( z ) ( 7.24 ) derivative ReLU d ReLU ( z ) dz = { 0 f x < 0 1 f x ≥ 0 ( 7.25 ) 7.4.5 details learning Optimization neural networks non-convex optimization problem , com - plex logistic regression , reasons many best practices successful learning . logistic regression initialize gradient descent weights biases value 0 . neural networks , contrast , need initialize weights small random numbers . helpful normalize input values 0 mean unit variance . Various forms regularization prevent overfitting . important dropout : randomly dropping units connections fromdropout network training ( Hinton et al . 2012 , Srivastava et al . 2014 ) . Tuning hyperparameters important . parameters neural network thehyperparameter weights W biases b ; learned gradient descent . hyperparameters things chosen algorithm designer ; optimal values tuned devset rather gradient descent learning training set . Hyperparameters include learning rate η , mini-batch size , model architecture ( number layers , number hidden nodes per layer , choice activation functions ) , regularize , . Gradient descent itself many architectural variants Adam ( Kingma Ba , 2015 ) . Finally , modern neural networks built computation graph - malisms make easy natural gradient computation parallelization onto vector-based GPUs ( Graphic Processing Units ) . Pytorch ( Paszke et al . , 2017 ) TensorFlow ( Abadi et al . , 2015 ) two popular . interested reader consult neural network textbook further details ; sugges - tions end chapter . 7.5 Neural Language Models first application neural networks , consider language modeling : pre - dicting upcoming words prior word context . Neural net-based language models turn many advantages n - gram language models Chapter 3 . Among neural language models need smoothing , handle longer histories , general - ize contexts similar words . training set size , neural lan - guage model higher predictive accuracy n-gram language model . Furthermore , neural language models underlie many models introduce tasks like machine translation , dialog , language generation . 16 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS hand , cost improved performance : neural net language models strikingly slower train traditional language models , many tasks n-gram language model still right tool . chapter describe simple feedforward neural language models , first introduced Bengio et al . ( 2003 ) . Modern neural language models generally feedforward recurrent , technology introduce Chap - ter 9 . feedforward neural LM standard feedforward network takes input time t representation number previous words ( wt − 1 , wt − 2 , etc . ) outputs probability distribution possible next words . Thus — like n-gram LM — feedforward neural LM approximates probability word entire prior context P ( wt | wt − 11 ) approximating based N previous words : P ( wt | wt − 11 ) ≈ P ( wt | w t − 1 t − N + 1 ) ( 7.26 ) following examples 4-gram example , show net estimate probability P ( wt = | wt − 1 , wt − 2 , wt − 3 ) . 7.5.1 Embeddings neural language models , prior context represented embeddings previous words . Representing prior context embeddings , rather ex - act words n-gram language models , allows neural language models generalize unseen data better n-gram language models . example , suppose seen sentence training : make sure get home feed cat . never seen word “ dog ” words “ feed ” . test set trying predict comes prefix “ forgot got home feed ” . n-gram language model predict “ cat ” , “ dog ” . neural LM , make fact “ cat ” “ dog ” similar embeddings , able assign reasonably high probability “ dog ” well “ cat ” , merely similar vectors . works practice . assume embedding dic - tionary E gives , word vocabulary V , embedding word , perhaps precomputed algorithm like word2vec Chapter 6 . Fig . 7.12 shows sketch simplified feedforward neural language model N = 3 ; moving window time t embedding vector represent - ing 3 previous words ( words wt − 1 , wt − 2 , wt − 3 ) . 3 vectors concatenated together produce x , input layer neural network whose output softmax probability distribution words . Thus y42 , value output node 42 probability next word wt V42 , vocabulary word index 42 . model shown Fig . 7.12 quite sufficient , assuming learn embed - dings separately method like word2vec methods Chapter 6 . method another algorithm learn embedding representations input words called pretraining . pretrained embeddings sufficient yourpretraining purposes , need . , often like learn embeddings simultaneously training network . true whatever task network designed ( sentiment 7.5 • NEURAL LANGUAGE MODELS 17 h1 h2 y1 h3 hdh … … U W y42 y | V | Projection layer 1 ⨉ 3d concatenated embeddings context words Hidden layer Output layer P ( w | u ) … thehole . . . . . . ground lived word 42 embedding word 35 embedding word 9925 embedding word 45180 wt-1wt-2 wtwt-3 dh ⨉ 3d 1 ⨉ dh | V | ⨉ dh P ( wt = V42 | wt-3 , wt-2 , wt-3 ) 1 ⨉ | V | Figure 7.12 simplified view feedforward neural language model moving text . timestep t network takes 3 context words , converts d-dimensional embedding , concatenates 3 embeddings together get 1 × Nd unit input layer x network . units multiplied weight matrix W bias vector b activation function produce hidden layer h , multiplied another weight matrix U . ( graphic simplicity show b future pictures . ) Finally , softmax output layer predicts node probability next word wt vocabulary word Vi . ( picture simplified assumes just look up embedding dictionary E d-dimensional embedding vector word , precomputed algorithm like word2vec . ) classification , translation , parsing ) places strong constraints makes good representation . show architecture allows embeddings learned . , add extra layer network , propagate error way back embedding vectors , starting embeddings random values slowly moving toward sensible representations . work input layer , pre-trained embeddings , going represent N previous words one-hot vector length | V | , i.e . , dimension word vocabulary . one-hot vector vectorone-hot vector element equal 1 — dimension corresponding word’s index vocabulary — elements set zero . Thus one-hot representation word “ toothpaste ” , supposing happens index 5 vocabulary , x5 xi = 0 ∀ 6 = 5 , shown : [ 0 0 0 0 1 0 0 . . . 0 0 0 0 ] 1 2 3 4 5 6 7 . . . . . . | V | Fig . 7.13 shows additional layers needed learn embeddings LM training . N = 3 context words represented 3 one-hot vectors , fully connected embedding layer via 3 instantiations embedding matrix E . Note learn separate weight matrices mapping 3 previous words projection layer , single embedding dictionary E that’s shared among three . That’s time , many different words appear wt − 2 wt − 1 , like just represent word vector , whichever context position appears . embedding weight matrix E thus 18 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS h1 h2 y1 h3 hdh … … U W y42 y | V | Projection layer 1 ⨉ 3d Hidden layer Output layer P ( w | context ) … thehole . . . . . . ground lived word 42 wt-1wt-2 wtwt-3 dh ⨉ 3d 1 ⨉ dh | V | ⨉ dh P ( wt = V42 | wt-3 , wt-2 , wt-3 ) 1 ⨉ | V | Input layer one-hot vectors index word 35 0 0 1 00 1 | V | 35 0 0 1 00 1 | V | 45180 0 0 1 00 1 | V | 9925 0 0 index word 9925 index word 45180 E 1 ⨉ | V | d ⨉ | V | E shared words Figure 7.13 Learning way back embeddings . Notice embedding matrix E shared among 3 context words . row word , vector d dimensions , hence dimensionality V × d . walk forward pass Fig . 7.13 . 1 . Select three embeddings E : three previous words , look up indices , create 3 one-hot vectors , multiply em - bedding matrix E . Consider wt − 3 . one-hot vector ‘ ’ ( index 35 ) multiplied embedding matrix E , give first part first hidden layer , called projection layer . row input matrix E justprojection layer embedding word , input one-hot column vector xi word Vi , projection layer input w Exi = ei , embedding word . concatenate three embeddings context words . 2 . Multiply W : multiply W ( add b ) pass rectified linear ( ) activation function get hidden layer h . 3 . Multiply U : h multiplied U 4 . Apply softmax : softmax , node output layer estimates probability P ( wt = | wt − 1 , wt − 2 , wt − 3 ) summary , e represent projection layer , formed concate - nating 3 embeddings three context vectors , equations neural language model become : e = ( Ex1 , Ex2 , . . . , Ex ) ( 7.27 ) h = σ ( + b ) ( 7.28 ) z = Uh ( 7.29 ) y = softmax ( z ) ( 7.30 ) 7.6 • SUMMARY 19 7.5.2 Training neural language model train model , i.e . set parameters θ = E , W , U , b , gradient descent ( Fig . ? ? ) , error backpropagation computation graph compute gradient . Training thus sets weights W U network , predicting upcoming words , learning embeddings E words best predict upcoming words . Generally training proceeds taking input long text , concatenating sentences , starting random weights , iteratively moving text predicting word wt . word wt , cross-entropy ( negative log likelihood ) loss : L = − log p ( wt | wt − 1 , . . . , wt − n + 1 ) ( 7.31 ) gradient loss : θt + 1 = θt − η ∂ − log p ( wt | wt − 1 , . . . , wt − n + 1 ) ∂ θ ( 7.32 ) gradient computed standard neural network framework backpropagate U , W , b , E . Training parameters minimize loss result algorithm language modeling ( word predictor ) new set embeddings E word representations tasks . 7.6 Summary • Neural networks built neural units , originally inspired human neurons simply abstract computational device . • neural unit multiplies input values weight vector , adds bias , applies non-linear activation function like sigmoid , tanh , rectified linear . • fully-connected , feedforward network , unit layer connected unit layer + 1 , cycles . • power neural networks comes ability early layers learn representations utilized later layers network . • Neural networks trained optimization algorithms like gradient de - scent . • Error backpropagation , backward differentiation computation graph , compute gradients loss function network . • Neural language models neural network probabilistic classifier , compute probability next word previous n words . • Neural language models pretrained embeddings , learn embed - dings scratch process language modeling . Bibliographical Historical Notes origins neural networks lie 1940s McCulloch-Pitts neuron ( McCul - loch Pitts , 1943 ) , simplified model human neuron kind com - 20 CHAPTER 7 • NEURAL NETWORKS NEURAL LANGUAGE MODELS puting element described terms propositional logic . late 1950s early 1960s , number labs ( including Frank Rosenblatt Cornell Bernard Widrow Stanford ) developed research neural networks ; phase saw development perceptron ( Rosenblatt , 1958 ) , transformation threshold bias , notation still ( Widrow Hoff , 1960 ) . field neural networks declined shown single percep - tron unit unable model functions simple XOR ( Minsky Papert , 1969 ) . small amount work continued next two decades , major revival field come 1980s , practical tools building deeper networks like error backpropagation became widespread ( Rumel - hart et al . , 1986 ) . 1980s wide variety neural network related architectures developed , particularly applications psychology cog - nitive science ( Rumelhart McClelland 1986b , McClelland Elman 1986 , Rumelhart McClelland 1986a , Elman 1990 ) , term connection - ist parallel distributed processing often ( Feldman Ballard 1982 , connectionist Smolensky 1988 ) . Many principles techniques developed period foundational modern work , including ideas distributed representations ( Hinton , 1986 ) , recurrent networks ( Elman , 1990 ) , tensors com - positionality ( Smolensky , 1990 ) . 1990s larger neural networks began applied many practical lan - guage processing tasks well , like handwriting recognition ( LeCun et al . 1989 , LeCun et al . 1990 ) speech recognition ( Morgan Bourlard 1989 , Morgan Bourlard 1990 ) . early 2000s , improvements computer hardware advances optimization training techniques made possible train even larger deeper networks , leading modern term deep learning ( Hinton et al . 2006 , Bengio et al . 2007 ) . cover related history Chapter 9 . number excellent books subject . Goldberg ( 2017 ) superb comprehensive coverage neural networks natural language pro - cessing . neural networks general Goodfellow et al . ( 2016 ) Nielsen ( 2015 ) . Bibliographical Historical Notes 21 Abadi , M . , Agarwal , . , Barham , P . , Brevdo , E . , Chen , Z . , Citro , C . , Corrado , G . S . , Davis , . , Dean , J . , Devin , M . , Ghemawat , S . , Goodfellow , . , Harp , . , Irving , G . , Isard , M . , Jia , Y . , Jozefowicz , R . , Kaiser , L . , Kudlur , M . , Lev - enberg , J . , Mané , D . , Monga , R . , Moore , S . , Murray , D . , Olah , C . , Schuster , M . , Shlens , J . , Steiner , B . , Sutskever , . , Talwar , K . , Tucker , P . , Vanhoucke , V . , Vasudevan , V . , Viégas , F . , Vinyals , O . , Warden , P . , Wattenberg , M . , Wicke , M . , Yu , Y . , Zheng , X . ( 2015 ) . TensorFlow : Large - scale machine learning heterogeneous systems . . Soft - ware available tensorflow.org . Bengio , Y . , Ducharme , R . , Vincent , P . , Jauvin , C . ( 2003 ) . neural probabilistic language model . Journal machine learning research , 3 ( Feb ) , 1137 – 1155 . Bengio , Y . , Lamblin , P . , Popovici , D . , Larochelle , H . ( 2007 ) . Greedy layer-wise training deep networks . NIPS 2007 , 153 – 160 . Elman , J . L . ( 1990 ) . Finding structure time . Cognitive science , 14 ( 2 ) , 179 – 211 . Feldman , J . . Ballard , D . H . ( 1982 ) . Connectionist models properties . Cognitive Science , 6 , 205 – 254 . Goldberg , Y . ( 2017 ) . Neural Network Methods Natural Language Processing , Vol . 10 Synthesis Lectures Hu - man Language Technologies . Morgan & Claypool . Goodfellow , . , Bengio , Y . , Courville , . ( 2016 ) . Deep Learning . MIT Press . Hinton , G . E . ( 1986 ) . Learning distributed representations concepts . COGSCI-86 , 1 – 12 . Hinton , G . E . , Osindero , S . , Teh , Y . - W . ( 2006 ) . fast learning algorithm deep belief nets . Neural computa - tion , 18 ( 7 ) , 1527 – 1554 . Hinton , G . E . , Srivastava , N . , Krizhevsky , . , Sutskever , . , Salakhutdinov , R . R . ( 2012 ) . Improving neural networks preventing co-adaptation feature detectors . arXiv preprint arXiv : 1207.0580 . Kingma , D . Ba , J . ( 2015 ) . Adam : method stochas - tic optimization . ICLR 2015 . LeCun , Y . , Boser , B . , Denker , J . S . , Henderson , D . , Howard , R . E . , Hubbard , W . , Jackel , L . D . ( 1989 ) . Backpropa - gation applied handwritten zip code recognition . Neural computation , 1 ( 4 ) , 541 – 551 . LeCun , Y . , Boser , B . E . , Denker , J . S . , Henderson , D . , Howard , R . E . , Hubbard , W . E . , Jackel , L . D . ( 1990 ) . Handwritten digit recognition back-propagation net - work . NIPS 1990 , 396 – 404 . McClelland , J . L . Elman , J . L . ( 1986 ) . TRACE model speech perception . Cognitive Psychology , 18 , 1 – 86 . McCulloch , W . S . Pitts , W . ( 1943 ) . logical calculus ideas immanent nervous activity . Bulletin Mathemat - ical Biophysics , 5 , 115 – 133 . Reprinted Neurocomput - ing : Foundations Research , ed . J . . Anderson E Rosenfeld . MIT Press 1988 . Minsky , M . Papert , S . ( 1969 ) . Perceptrons . MIT Press . Morgan , N . Bourlard , H . ( 1989 ) . Generalization parameter estimation feedforward nets : experi - ments . Advances neural information processing sys - tems , 630 – 637 . Morgan , N . Bourlard , H . ( 1990 ) . Continuous speech recognition multilayer perceptrons hidden markov models . ICASSP-90 , 413 – 416 . Nielsen , M . . ( 2015 ) . Neural networks Deep learning . Determination Press USA . Paszke , . , Gross , S . , Chintala , S . , Chanan , G . , Yang , E . , De - Vito , Z . , Lin , Z . , Desmaison , . , Antiga , L . , Lerer , . ( 2017 ) . Automatic differentiation pytorch . NIPS-W . Rosenblatt , F . ( 1958 ) . perceptron : probabilistic model information storage organization brain . . Psy - chological review , 65 ( 6 ) , 386 – 408 . Rumelhart , D . E . , Hinton , G . E . , Williams , R . J . ( 1986 ) . Learning internal representations error propagation . Rumelhart , D . E . McClelland , J . L . ( Eds . ) , Parallel Distributed Processing , Vol . 2 , 318 – 362 . MIT Press . Rumelhart , D . E . McClelland , J . L . ( 1986a ) . learn - ing past tense English verbs . Rumelhart , D . E . McClelland , J . L . ( Eds . ) , Parallel Distributed Processing , Vol . 2 , 216 – 271 . MIT Press . Rumelhart , D . E . McClelland , J . L . ( Eds . ) . ( 1986b ) . Par - allel Distributed Processing . MIT Press . Russell , S . Norvig , P . ( 2002 ) . Artificial Intelligence : Modern Approach ( 2nd Ed . ) . Prentice Hall . Smolensky , P . ( 1988 ) . proper treatment connec - tionism . Behavioral brain sciences , 11 ( 1 ) , 1 – 23 . Smolensky , P . ( 1990 ) . Tensor product variable binding representation symbolic structures connectionist systems . Artificial intelligence , 46 ( 1-2 ) , 159 – 216 . Srivastava , N . , Hinton , G . E . , Krizhevsky , . , Sutskever , . , Salakhutdinov , R . R . ( 2014 ) . Dropout : simple way prevent neural networks overfitting . . JMLR , 15 ( 1 ) , 1929 – 1958 . Widrow , B . Hoff , M . E . ( 1960 ) . Adaptive switching cir - cuits . IRE WESCON Convention Record , Vol . 4 , 96 – 104 .