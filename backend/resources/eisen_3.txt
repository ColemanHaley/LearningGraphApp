Chapter 3 Nonlinear classification Linear classification may seem like all we need for natural language processing . The bag - of-words representation is inherently high dimensional , and the number of features is often larger than the number of labeled training instances . This means that it is usually possible to find a linear classifier that perfectly fits the training data , or even to fit any ar - bitrary labeling of the training instances ! Moving to nonlinear classification may therefore only increase the risk of overfitting . Furthermore , for many tasks , lexical features ( words ) are meaningful in isolation , and can offer independent evidence about the instance label — unlike computer vision , where individual pixels are rarely informative , and must be evaluated holistically to make sense of an image . For these reasons , natural language processing has historically focused on linear classification . But in recent years , nonlinear classifiers have swept through natural language pro - cessing , and are now the default approach for many tasks ( Manning , 2015 ) . There are at least three reasons for this change . • There have been rapid advances in deep learning , a family of nonlinear meth - ods that learn complex functions of the input through multiple layers of compu - tation ( Goodfellow et al . , 2016 ) . • Deep learning facilitates the incorporation of word embeddings , which are dense vector representations of words . Word embeddings can be learned from large amounts of unlabeled data , and enable generalization to words that do not appear in the an - notated training data ( word embeddings are discussed in detail in chapter 14 ) . • While CPU speeds have plateaued , there have been rapid advances in specialized hardware called graphics processing units ( GPUs ) , which have become faster , cheaper , and easier to program . Many deep learning models can be implemented efficiently on GPUs , offering substantial performance improvements over CPU-based comput - ing . 47 48 CHAPTER 3 . NONLINEAR CLASSIFICATION This chapter focuses on neural networks , which are the dominant approach for non - linear classification in natural language processing today . 1 Historically , a few other non - linear learning methods have been applied to language data . • Kernel methods are generalizations of the nearest-neighbor classification rule , which classifies each instance by the label of the most similar example in the training set . The application of the kernel support vector machine to information extraction is described in chapter 17 . • Decision trees classify instances by checking a set of conditions . Scaling decision trees to bag-of-words inputs is difficult , but decision trees have been successful in problems such as coreference resolution ( chapter 15 ) , where more compact feature sets can be constructed ( Soon et al . , 2001 ) . • Boosting and related ensemble methods work by combining the predictions of sev - eral “ weak ” classifiers , each of which may consider only a small subset of features . Boosting has been successfully applied to text classification ( Schapire and Singer , 2000 ) and syntactic analysis ( Abney et al . , 1999 ) , and remains one of the most suc - cessful methods on machine learning competition sites such as Kaggle ( Chen and Guestrin , 2016 ) . Hastie et al . ( 2009 ) provide an excellent overview of these techniques . 3.1 Feedforward neural networks Consider the problem of building a classifier for movie reviews . The goal is to predict a label y ∈ { GOOD , BAD , OKAY } from a representation of the text of each document , x . But what makes a good movie ? The story , acting , cinematography , editing , soundtrack , and so on . Now suppose the training set contains labels for each of these additional features , z = [ z1 , z2 , . . . , zKz ] > . With a training set of such information , we could build a two-step classifier : 1 . Use the text x to predict the features z . Specifically , train a logistic regression clas - sifier to compute p ( zk | x ) , for each k ∈ { 1 , 2 , . . . , Kz } . 2 . Use the features z to predict the label y . Again , train a logistic regression classifier to compute p ( y | z ) . On test data , z is unknown , so we will use the probabilities p ( z | x ) from the first layer as the features . This setup is shown in Figure 3.1 , which describes the proposed classifier in a computa - tion graph : the text features x are connected to the middle layer z , which is connected to the label y . 1I will use “ deep learning ” and “ neural networks ” interchangeably . Jacob Eisenstein . Draft of October 15 , 2018 . 3.1 . FEEDFORWARD NEURAL NETWORKS 49 . . . . . . x z y Figure 3.1 : A feedforward neural network . Shaded circles indicate observed features , usually words ; squares indicate nodes in the computation graph , which are computed from the information carried over the incoming arrows . If we assume that each zk is binary , zk ∈ { 0 , 1 } , then the probability p ( zk | x ) can be modeled using binary logistic regression : Pr ( zk = 1 | x ; Θ ( x → z ) ) = σ ( θ ( x → z ) k · x ) = ( 1 + exp ( − θ ( x → z ) k · x ) ) − 1 , [ 3.1 ] where σ is the sigmoid function ( shown in Figure 3.2 ) , and the matrix Θ ( x → z ) ∈ RKz × V is constructed by stacking the weight vectors for each zk , Θ ( x → z ) = [ θ ( x → z ) 1 , θ ( x → z ) 2 , . . . , θ ( x → z ) Kz ] > . [ 3.2 ] We will assume that x contains a term with a constant value of 1 , so that a corresponding offset parameter is included in each θ ( x → z ) k . The output layer is computed by the multi-class logistic regression probability , Pr ( y = j | z ; Θ ( z → y ) , b ) = exp ( θ ( z → y ) j · z + bj ) ∑ j ′ ∈ Y exp ( θ ( z → y ) j ′ · z + bj ′ ) , [ 3.3 ] where bj is an offset for label j , and the output weight matrix Θ ( z → y ) ∈ RKy × Kz is again constructed by concatenation , Θ ( z → y ) = [ θ ( z → y ) 1 , θ ( z → y ) 2 , . . . , θ ( z → y ) Ky ] > . [ 3.4 ] The vector of probabilities over each possible value of y is denoted , p ( y | z ; Θ ( z → y ) , b ) = SoftMax ( Θ ( z → y ) z + b ) , [ 3.5 ] where element j in the output of the SoftMax function is computed as in Equation 3.3 . This set of equations defines a multilayer classifier , which can be summarized as , p ( z | x ; Θ ( x → z ) ) = σ ( Θ ( x → z ) x ) [ 3.6 ] p ( y | z ; Θ ( z → y ) , b ) = SoftMax ( Θ ( z → y ) z + b ) , [ 3.7 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 50 CHAPTER 3 . NONLINEAR CLASSIFICATION 3 2 1 0 1 2 3 1 0 1 2 3 values sigmoid tanh ReLU 3 2 1 0 1 2 3 0.0 0.2 0.4 0.6 0.8 1.0 derivatives Figure 3.2 : The sigmoid , tanh , and ReLU activation functions where the function σ is now applied elementwise to the vector of inner products , σ ( Θ ( x → z ) x ) = [ σ ( θ ( x → z ) 1 · x ) , σ ( θ ( x → z ) 2 · x ) , . . . , σ ( θ ( x → z ) Kz · x ) ] > . [ 3.8 ] Now suppose that the hidden features z are never observed , even in the training data . We can still construct the architecture in Figure 3.1 . Instead of predicting y from a discrete vector of predicted values z , we use the probabilities σ ( θk · x ) . The resulting classifier is barely changed : z = σ ( Θ ( x → z ) x ) [ 3.9 ] p ( y | x ; Θ ( z → y ) , b ) = SoftMax ( Θ ( z → y ) z + b ) . [ 3.10 ] This defines a classification model that predicts the label y ∈ Y from the base features x , through a “ hidden layer ” z . This is a feedforward neural network . 2 3.2 Designing neural networks There several ways to generalize the feedforward neural network . 3.2.1 Activation functions If the hidden layer is viewed as a set of latent features , then the sigmoid function in Equa - tion 3.9 represents the extent to which each of these features is “ activated ” by a given input . However , the hidden layer can be regarded more generally as a nonlinear trans - formation of the input . This opens the door to many other activation functions , some of which are shown in Figure 3.2 . At the moment , the choice of activation functions is more art than science , but a few points can be made about the most popular varieties : 2The architecture is sometimes called a multilayer perceptron , but this is misleading , because each layer is not a perceptron as defined in the previous chapter . Jacob Eisenstein . Draft of October 15 , 2018 . 3.2 . DESIGNING NEURAL NETWORKS 51 • The range of the sigmoid function is ( 0 , 1 ) . The bounded range ensures that a cas - cade of sigmoid functions will not “ blow up ” to a huge output , and this is impor - tant for deep networks with several hidden layers . The derivative of the sigmoid is ∂ ∂ aσ ( a ) = σ ( a ) ( 1 − σ ( a ) ) . This derivative becomes small at the extremes , which can make learning slow ; this is called the vanishing gradient problem . • The range of the tanh activation function is ( − 1 , 1 ) : like the sigmoid , the range is bounded , but unlike the sigmoid , it includes negative values . The derivative is ∂ ∂ a tanh ( a ) = 1 − tanh ( a ) 2 , which is steeper than the logistic function near the ori - gin ( LeCun et al . , 1998 ) . The tanh function can also suffer from vanishing gradients at extreme values . • The rectified linear unit ( ReLU ) is zero for negative inputs , and linear for positive inputs ( Glorot et al . , 2011 ) , ReLU ( a ) = { a , a ≥ 0 0 , otherwise . [ 3.11 ] The derivative is a step function , which is 1 if the input is positive , and zero other - wise . Once the activation is zero , the gradient is also zero . This can lead to the prob - lem of “ dead neurons ” , where some ReLU nodes are zero for all inputs , throughout learning . A solution is the leaky ReLU , which has a small positive slope for negative inputs ( Maas et al . , 2013 ) , Leaky-ReLU ( a ) = { a , a ≥ 0 . 0001a , otherwise . [ 3.12 ] Sigmoid and tanh are sometimes described as squashing functions , because they squash an unbounded input into a bounded range . Glorot and Bengio ( 2010 ) recommend against the use of the sigmoid activation in deep networks , because its mean value of 12 can cause the next layer of the network to be saturated , leading to small gradients on its own pa - rameters . Several other activation functions are reviewed in the textbook by Goodfellow et al . ( 2016 ) , who recommend ReLU as the “ default option . ” 3.2.2 Network structure Deep networks stack up several hidden layers , with each z ( d ) acting as the input to the next layer , z ( d + 1 ) . As the total number of nodes in the network increases , so does its capacity to learn complex functions of the input . Given a fixed number of nodes , one must decide whether to emphasize width ( large Kz at each layer ) or depth ( many layers ) . At present , this tradeoff is not well understood . 3 3With even a single hidden layer , a neural network can approximate any continuous function on a closed and bounded subset of RN to an arbitrarily small non-zero error ; see section 6.4.1 of Goodfellow et al . ( 2016 ) Under contract with MIT Press , shared under CC-BY-NC-ND license . 52 CHAPTER 3 . NONLINEAR CLASSIFICATION It is also possible to “ short circuit ” a hidden layer , by propagating information directly from the input to the next higher level of the network . This is the idea behind residual net - works , which propagate information directly from the input to the subsequent layer ( He et al . , 2016 ) , z = f ( Θ ( x → z ) x ) + x , [ 3.13 ] where f is any nonlinearity , such as sigmoid or ReLU . A more complex architecture is the highway network ( Srivastava et al . , 2015 ; Kim et al . , 2016 ) , in which an addition gate controls an interpolation between f ( Θ ( x → z ) x ) and x , t = σ ( Θ ( t ) x + b ( t ) ) [ 3.14 ] z = t � f ( Θ ( x → z ) x ) + ( 1 − t ) � x , [ 3.15 ] where � refers to an elementwise vector product , and 1 is a column vector of ones . As before , the sigmoid function is applied elementwise to its input ; recall that the output of this function is restricted to the range ( 0 , 1 ) . Gating is also used in the long short-term memory ( LSTM ) , which is discussed in chapter 6 . Residual and highway connections address a problem with deep architectures : repeated application of a nonlinear activation function can make it difficult to learn the parameters of the lower levels of the network , which are too distant from the supervision signal . 3.2.3 Outputs and loss functions In the multi-class classification example , a softmax output produces probabilities over each possible label . This aligns with a negative conditional log-likelihood , − L = − N ∑ i = 1 log p ( y ( i ) | x ( i ) ; Θ ) . [ 3.16 ] where Θ = { Θ ( x → z ) , Θ ( z → y ) , b } is the entire set of parameters . This loss can be written alternatively as follows : ỹj , Pr ( y = j | x ( i ) ; Θ ) [ 3.17 ] − L = − N ∑ i = 1 ey ( i ) · log ỹ [ 3.18 ] for a survey of these theoretical results . However , depending on the function to be approximated , the width of the hidden layer may need to be arbitrarily large . Furthermore , the fact that a network has the capacity to approximate any given function does not imply that it is possible to learn the function using gradient-based optimization . Jacob Eisenstein . Draft of October 15 , 2018 . 3.3 . LEARNING NEURAL NETWORKS 53 where ey ( i ) is a one-hot vector of zeros with a value of 1 at position y ( i ) . The inner product between ey ( i ) and log ỹ is also called the multinomial cross-entropy , and this terminology is preferred in many neural networks papers and software packages . It is also possible to train neural networks from other objectives , such as a margin loss . In this case , it is not necessary to use softmax at the output layer : an affine transformation of the hidden layer is enough : Ψ ( y ; x ( i ) , Θ ) = θ ( z → y ) y · z + by [ 3.19 ] ` MARGIN ( Θ ; x ( i ) , y ( i ) ) = max y 6 = y ( i ) ( 1 + Ψ ( y ; x ( i ) , Θ ) − Ψ ( y ( i ) ; x ( i ) , Θ ) ) + . [ 3.20 ] In regression problems , the output is a scalar or vector ( see subsection 4.1.2 ) . For these problems , a typical loss function is the squared error ( y − ŷ ) 2 or squared norm | | y − ŷ | | 22 . 3.2.4 Inputs and lookup layers In text classification , the input layer x can refer to a bag-of-words vector , where xj is the count of word j . The input to the hidden unit zk is then ∑ V j = 1 θ ( x → z ) j , k xj , and word j is represented by the vector θ ( x → z ) j . This vector is sometimes described as the embedding of word j , and can be learned from unlabeled data , using techniques discussed in chapter 14 . The columns of Θ ( x → z ) are each Kz-dimensional word embeddings . Chapter 2 presented an alternative view of text documents , as a sequence of word tokens , w1 , w2 , . . . , wM . In a neural network , each word token wm is represented with a one-hot vector , ewm , with dimension V . The matrix-vector product Θ ( x → z ) ewm returns the embedding of word wm . The complete document can represented by horizontally concatenating these one-hot vectors , W = [ ew1 , ew2 , . . . , ewM ] , and the bag-of-words rep - resentation can be recovered from the matrix-vector product W [ 1 , 1 , . . . , 1 ] > , which sums each row over the tokens m = { 1 , 2 , . . . , M } . The matrix product Θ ( x → z ) W contains the horizontally concatenated embeddings of each word in the document , which will be useful as the starting point for convolutional neural networks ( see section 3.4 ) . This is sometimes called a lookup layer , because the first step is to lookup the embeddings for each word in the input text . 3.3 Learning neural networks The feedforward network in Figure 3.1 can now be written as , z ← f ( Θ ( x → z ) x ( i ) ) [ 3.21 ] ỹ ← SoftMax ( Θ ( z → y ) z + b ) [ 3.22 ] ` ( i ) ← − ey ( i ) · log ỹ , [ 3.23 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 54 CHAPTER 3 . NONLINEAR CLASSIFICATION where f is an elementwise activation function , such as σ or ReLU , and ` ( i ) is the loss at instance i . The parameters Θ ( x → z ) , Θ ( z → y ) , and b can be estimated using online gradient - based optimization . The simplest such algorithm is stochastic gradient descent , which was discussed in section 2.6 . Each parameter is updated by the gradient of the loss , b ← b − η ( t ) ∇ b ` ( i ) [ 3.24 ] θ ( z → y ) k ← θ ( z → y ) k − η ( t ) ∇ θ ( z → y ) k ` ( i ) [ 3.25 ] θ ( x → z ) n ← θ ( x → z ) n − η ( t ) ∇ θ ( x → z ) n ` ( i ) , [ 3.26 ] where η ( t ) is the learning rate on iteration t , ` ( i ) is the loss on instance ( or minibatch ) i , and θ ( x → z ) n is column n of the matrix Θ ( x → z ) , and θ ( z → y ) k is column k of Θ ( z → y ) . The gradients of the negative log-likelihood on b and θ ( z → y ) k are similar to the gradi - ents in logistic regression . For θ ( z → y ) , the gradient is , ∇ θ ( z → y ) k ` ( i ) =   ∂ ` ( i ) ∂ θ ( z → y ) k , 1 , ∂ ` ( i ) ∂ θ ( z → y ) k , 2 , . . . , ∂ ` ( i ) ∂ θ ( z → y ) k , Ky   > [ 3.27 ] ∂ ` ( i ) ∂ θ ( z → y ) k , j = − ∂ ∂ θ ( z → y ) k , j   θ ( z → y ) y ( i ) · z − log ∑ y ∈ Y expθ ( z → y ) y · z   [ 3.28 ] = ( Pr ( y = j | z ; Θ ( z → y ) , b ) − δ ( j = y ( i ) ) ) zk , [ 3.29 ] where δ ( j = y ( i ) ) is a function that returns one when j = y ( i ) , and zero otherwise . The gradient ∇ b ` ( i ) is similar to Equation 3.29 . The gradients on the input layer weights Θ ( x → z ) are obtained by the chain rule of differentiation : ∂ ` ( i ) ∂ θ ( x → z ) n , k = ∂ ` ( i ) ∂ zk ∂ zk ∂ θ ( x → z ) n , k [ 3.30 ] = ∂ ` ( i ) ∂ zk ∂ f ( θ ( x → z ) k · x ) ∂ θ ( x → z ) n , k [ 3.31 ] = ∂ ` ( i ) ∂ zk × f ′ ( θ ( x → z ) k · x ) × xn , [ 3.32 ] where f ′ ( θ ( x → z ) k · x ) is the derivative of the activation function f , applied at the input Jacob Eisenstein . Draft of October 15 , 2018 . 3.3 . LEARNING NEURAL NETWORKS 55 θ ( x → z ) k · x . For example , if f is the sigmoid function , then the derivative is , ∂ ` ( i ) ∂ θ ( x → z ) n , k = ∂ ` ( i ) ∂ zk × σ ( θ ( x → z ) k · x ) × ( 1 − σ ( θ ( x → z ) k · x ) ) × xn [ 3.33 ] = ∂ ` ( i ) ∂ zk × zk × ( 1 − zk ) × xn . [ 3.34 ] For intuition , consider each of the terms in the product . • If the negative log-likelihood ` ( i ) does not depend much on zk , then ∂ ` ( i ) ∂ zk ≈ 0 . In this case it doesn’t matter how zk is computed , and so ∂ ` ( i ) ∂ θ ( x → z ) n , k ≈ 0 . • If zk is near 1 or 0 , then the curve of the sigmoid function is nearly flat ( Figure 3.2 ) , and changing the inputs will make little local difference . The term zk × ( 1 − zk ) is maximized at zk = 12 , where the slope of the sigmoid function is steepest . • If xn = 0 , then it does not matter how we set the weights θ ( x → z ) n , k , so ∂ ` ( i ) ∂ θ ( x → z ) n , k = 0 . 3.3.1 Backpropagation The equations above rely on the chain rule to compute derivatives of the loss with respect to each parameter of the model . Furthermore , local derivatives are frequently reused : for example , ∂ ` ( i ) ∂ zk is reused in computing the derivatives with respect to each θ ( x → z ) n , k . These terms should therefore be computed once , and then cached . Furthermore , we should only compute any derivative once we have already computed all of the necessary “ inputs ” demanded by the chain rule of differentiation . This combination of sequencing , caching , and differentiation is known as backpropagation . It can be generalized to any directed acyclic computation graph . A computation graph is a declarative representation of a computational process . At each node t , compute a value vt by applying a function ft to a ( possibly empty ) list of parent nodes , πt . Figure 3.3 shows the computation graph for a feedforward network with one hidden layer . There are nodes for the input x ( i ) , the hidden layer z , the predicted output ŷ , and the parameters Θ . During training , there is also a node for the ground truth label y ( i ) and the loss ` ( i ) . The predicted output ŷ is one of the parents of the loss ( the other is the label y ( i ) ) ; its parents include Θ and z , and so on . Computation graphs include three types of nodes : Variables . In the feedforward network of Figure 3.3 , the variables include the inputs x , the hidden nodes z , the outputs y , and the loss function . Inputs are variables that do not have parents . Backpropagation computes the gradients with respect to all Under contract with MIT Press , shared under CC-BY-NC-ND license . 56 CHAPTER 3 . NONLINEAR CLASSIFICATION Algorithm 6 General backpropagation algorithm . In the computation graph G , every node contains a function ft and a set of parent nodes πt ; the inputs to the graph are x ( i ) . 1 : procedure BACKPROP ( G = { ft , πt } Tt = 1 } , x ( i ) ) 2 : vt ( n ) ← x ( i ) n for all n and associated computation nodes t ( n ) . 3 : for t ∈ TOPOLOGICALSORT ( G ) do . Forward pass : compute value at each node 4 : if | πt | > 0 then 5 : vt ← ft ( vπt , 1 , vπt , 2 , . . . , vπt , Nt ) 6 : gobjective = 1 . Backward pass : compute gradients at each node 7 : for t ∈ REVERSE ( TOPOLOGICALSORT ( G ) ) do 8 : gt ← ∑ t ′ : t ∈ πt ′ gt ′ × ∇ vtvt ′ . Sum over all t ′ that are children of t , propagating the gradient gt ′ , scaled by the local gradient ∇ vtvt ′ 9 : return { g1 , g2 , . . . , gT } variables except the inputs , and propagates these gradients backwards to the pa - rameters . Parameters . In a feedforward network , the parameters include the weights and offsets . In Figure 3.3 , the parameters are summarized in the node Θ , but we could have separate nodes for Θ ( x → z ) , Θ ( z → y ) , and any offset parameters . Parameter nodes do not have parents ; they are not computed from other nodes , but rather , are learned by gradient descent . Loss . The loss ` ( i ) is the quantity that is to be minimized during training . The node rep - resenting the loss in the computation graph is not the parent of any other node ; its parents are typically the predicted label ŷ and the true label y ( i ) . Backpropagation begins by computing the gradient of the loss , and then propagating this gradient backwards to its immediate parents . If the computation graph is a directed acyclic graph , then it is possible to order the nodes with a topological sort , so that if node t is a parent of node t ′ , then t < t ′ . This means that the values { vt } Tt = 1 can be computed in a single forward pass . The topolog - ical sort is reversed when computing gradients : each gradient gt is computed from the gradients of the children of t , implementing the chain rule of differentiation . The general backpropagation algorithm for computation graphs is shown in Algorithm 6 . While the gradients with respect to each parameter may be complex , they are com - posed of products of simple parts . For many networks , all gradients can be computed through automatic differentiation . This means that you need only specify the feedfor - ward computation , and the gradients necessary for learning can be obtained automati - cally . There are many software libraries that perform automatic differentiation on compu - Jacob Eisenstein . Draft of October 15 , 2018 . 3.3 . LEARNING NEURAL NETWORKS 57 x ( i ) z ŷ ` ( i ) y ( i ) Θ vx vz vŷ vΘ gŷ g ` gz gz vy vΘ g ` gŷ Figure 3.3 : A computation graph for the feedforward neural network shown in Figure 3.1 . tation graphs , such as TORCH ( Collobert et al . , 2011 ) , TENSORFLOW ( Abadi et al . , 2016 ) , and DYNET ( Neubig et al . , 2017 ) . One important distinction between these libraries is whether they support dynamic computation graphs , in which the structure of the compu - tation graph varies across instances . Static computation graphs are compiled in advance , and can be applied to fixed-dimensional data , such as bag-of-words vectors . In many nat - ural language processing problems , each input has a distinct structure , requiring a unique computation graph . A simple case occurs in recurrent neural network language models ( see chapter 6 ) , in which there is one node for each word in a sentence . More complex cases include recursive neural networks ( see chapter 14 ) , in which the network is a tree structure matching the syntactic organization of the input . 3.3.2 Regularization and dropout In linear classification , overfitting was addressed by augmenting the objective with a reg - ularization term , λ | | θ | | 22 . This same approach can be applied to feedforward neural net - works , penalizing each matrix of weights : L = N ∑ i = 1 ` ( i ) + λz → y | | Θ ( z → y ) | | 2F + λx → z | | Θ ( x → z ) | | 2F , [ 3.35 ] where | | Θ | | 2F = ∑ i , j θ 2 i , j is the squared Frobenius norm , which generalizes the L2 norm to matrices . The bias parameters b are not regularized , as they do not contribute to the sensitivity of the classifier to the inputs . In gradient-based optimization , the practical effect of Frobenius norm regularization is that the weights “ decay ” towards zero at each update , motivating the alternative name weight decay . Another approach to controlling model complexity is dropout , which involves ran - domly setting some computation nodes to zero during training ( Srivastava et al . , 2014 ) . For example , in the feedforward network , on each training instance , with probability ρwe Under contract with MIT Press , shared under CC-BY-NC-ND license . 58 CHAPTER 3 . NONLINEAR CLASSIFICATION set each input xn and each hidden layer node zk to zero . Srivastava et al . ( 2014 ) recom - mend ρ = 0.5 for hidden units , and ρ = 0.2 for input units . Dropout is also incorporated in the gradient computation , so if node zk is dropped , then none of the weights θ ( x → z ) k will be updated for this instance . Dropout prevents the network from learning to depend too much on any one feature or hidden node , and prevents feature co-adaptation , in which a hidden unit is only useful in combination with one or more other hidden units . Dropout is a special case of feature noising , which can also involve adding Gaussian noise to inputs or hidden units ( Holmstrom and Koistinen , 1992 ) . Wager et al . ( 2013 ) show that dropout is approximately equivalent to “ adaptive ” L2 regularization , with a separate regularization penalty for each feature . 3.3.3 * Learning theory Chapter 2 emphasized the importance of convexity for learning : for convex objectives , the global optimum can be found efficiently . The negative log-likelihood and hinge loss are convex functions of the parameters of the output layer . However , the output of a feed - forward network is generally not a convex function of the parameters of the input layer , Θ ( x → z ) . Feedforward networks can be viewed as function composition , where each layer is a function that is applied to the output of the previous layer . Convexity is generally not preserved in the composition of two convex functions — and furthermore , “ squashing ” activation functions like tanh and sigmoid are not convex . The non-convexity of hidden layer neural networks can also be seen by permuting the elements of the hidden layer , from z = [ z1 , z2 , . . . , zKz ] to z̃ = [ zπ ( 1 ) , zπ ( 2 ) , . . . , zπ ( Kz ) ] . This corresponds to applying π to the rows of Θ ( x → z ) and the columns of Θ ( z → y ) , resulting in permuted parameter matrices Θ ( x → z ) π and Θ ( z → y ) π . As long as this permutation is applied consistently , the loss will be identical , L ( Θ ) = L ( Θπ ) : it is invariant to this permutation . However , the loss of the linear combination L ( αΘ + ( 1 − α ) Θπ ) will generally not be identical to the loss under Θ or its permutations . If L ( Θ ) is better than the loss at any points in the immediate vicinity , and if L ( Θ ) = L ( Θπ ) , then the loss function does not satisfy the definition of convexity ( see section 2.4 ) . One of the exercises asks you to prove this more rigorously . In practice , the existence of multiple optima is not necessary problematic , if all such optima are permutations of the sort described in the previous paragraph . In contrast , “ bad ” local optima are better than their neighbors , but much worse than the global op - timum . Fortunately , in large feedforward neural networks , most local optima are nearly as good as the global optimum ( Choromanska et al . , 2015 ) . More generally , a critical point is one at which the gradient is zero . Critical points may be local optima , but they may also be saddle points , which are local minima in some directions , but local maxima in other directions . For example , the equation x21 − x22 has a saddle point at x = ( 0 , 0 ) . In large networks , the overwhelming majority of critical points are saddle points , rather Jacob Eisenstein . Draft of October 15 , 2018 . 3.3 . LEARNING NEURAL NETWORKS 59 than local minima or maxima ( Dauphin et al . , 2014 ) . Saddle points can pose problems for gradient-based optimization , since learning will slow to a crawl as the gradient goes to zero . However , the noise introduced by stochastic gradient descent , and by feature noising techniques such as dropout , can help online optimization to escape saddle points and find high-quality optima ( Ge et al . , 2015 ) . Other techniques address saddle points directly , using local reconstructions of the Hessian matrix ( Dauphin et al . , 2014 ) or higher - order derivatives ( Anandkumar and Ge , 2016 ) . Another theoretical puzzle about neural networks is how they are able to generalize to unseen data . Given enough parameters , a two-layer feedforward network can “ memo - rize ” its training data , attaining perfect accuracy on any training set . A particularly salient demonstration was provided by Zhang et al . ( 2017 ) , who showed that neural networks can learn to perfectly classify a training set of images , even when the labels are replaced with random values ! Of course , this network attains only chance accuracy when applied to heldout data . The concern is that when such a powerful learner is applied to real training data , it may learn a pathological classification function , which exploits irrelevant details of the training data and fails to generalize . Yet this extreme overfitting is rarely en - countered in practice , and can usually be prevented by regularization , dropout , and early stopping ( see subsection 3.3.4 ) . Recent papers have derived generalization guarantees for specific classes of neural networks ( e.g . , Kawaguchi et al . , 2017 ; Brutzkus et al . , 2018 ) , but theoretical work in this area is ongoing . 3.3.4 Tricks Getting neural networks to work sometimes requires heuristic “ tricks ” ( Bottou , 2012 ; Goodfellow et al . , 2016 ; Goldberg , 2017b ) . This section presents some tricks that are espe - cially important . Initialization Initialization is not especially important for linear classifiers , since con - vexity ensures that the global optimum can usually be found quickly . But for multilayer neural networks , it is helpful to have a good starting point . One reason is that if the mag - nitude of the initial weights is too large , a sigmoid or tanh nonlinearity will be saturated , leading to a small gradient , and slow learning . Large gradients can cause training to di - verge , with the parameters taking increasingly extreme values until reaching the limits of the floating point representation . Initialization can help avoid these problems by ensuring that the variance over the initial gradients is constant and bounded throughout the network . For networks with tanh activation functions , this can be achieved by sampling the initial weights from the Under contract with MIT Press , shared under CC-BY-NC-ND license . 60 CHAPTER 3 . NONLINEAR CLASSIFICATION following uniform distribution ( Glorot and Bengio , 2010 ) , θi , j ∼ U [ − √ 6 √ din ( n ) + dout ( n ) , √ 6 √ din ( n ) + dout ( n ) ] , [ 3.36 ] [ 3.37 ] For the weights leading to a ReLU activation function , He et al . ( 2015 ) use similar argu - mentation to justify sampling from a zero-mean Gaussian distribution , θi , j ∼ N ( 0 , √ 2 / din ( n ) ) [ 3.38 ] Rather than initializing the weights independently , it can be beneficial to initialize each layer jointly as an orthonormal matrix , ensuring that Θ > Θ = I ( Saxe et al . , 2014 ) . Or - thonormal matrices preserve the norm of the input , so that | | Θx | | = | | x | | , which prevents the gradients from exploding or vanishing . Orthogonality ensures that the hidden units are uncorrelated , so that they correspond to different features of the input . Orthonormal initialization can be performed by applying singular value decomposition to a matrix of values sampled from a standard normal distribution : ai , j ∼ N ( 0 , 1 ) [ 3.39 ] A ={ ai , j } din ( j ) , dout ( j ) i = 1 , j = 1 [ 3.40 ] U , S , V > = SVD ( A ) [ 3.41 ] Θ ( j ) ← U . [ 3.42 ] The matrix U contains the singular vectors of A , and is guaranteed to be orthonormal . For more on singular value decomposition , see chapter 14 . Even with careful initialization , there can still be significant variance in the final re - sults . It can be useful to make multiple training runs , and select the one with the best performance on a heldout development set . Clipping and normalization Learning can be sensitive to the magnitude of the gradient : too large , and learning can diverge , with successive updates thrashing between increas - ingly extreme values ; too small , and learning can grind to a halt . Several heuristics have been proposed to address this issue . • In gradient clipping ( Pascanu et al . , 2013 ) , an upper limit is placed on the norm of the gradient , and the gradient is rescaled when this limit is exceeded , CLIP ( g̃ ) = { g | | ĝ | | < τ τ | | g | | g otherwise . [ 3.43 ] Jacob Eisenstein . Draft of October 15 , 2018 . 3.3 . LEARNING NEURAL NETWORKS 61 • In batch normalization ( Ioffe and Szegedy , 2015 ) , the inputs to each computation node are recentered by their mean and variance across all of the instances in the minibatch B ( see subsection 2.6.2 ) . For example , in a feedforward network with one hidden layer , batch normalization would tranform the inputs to the hidden layer as follows : µ ( B ) = 1 | B | ∑ i ∈ B x ( i ) [ 3.44 ] s ( B ) = 1 | B | ∑ i ∈ B ( x ( i ) − µ ( B ) ) 2 [ 3.45 ] x ( i ) =( x ( i ) − µ ( B ) ) / √ s ( B ) . [ 3.46 ] Empirically , this speeds convergence of deep architectures . One explanation is that it helps to correct for changes in the distribution of activations during training . • In layer normalization ( Ba et al . , 2016 ) , the inputs to each nonlinear activation func - tion are recentered across the layer : a = Θ ( x → z ) x [ 3.47 ] µ = 1 Kz Kz ∑ k = 1 ak [ 3.48 ] s = 1 Kz Kz ∑ k = 1 ( ak − µ ) 2 [ 3.49 ] z =( a − µ ) / √ s . [ 3.50 ] Layer normalization has similar motivations to batch normalization , but it can be applied across a wider range of architectures and training conditions . Online optimization There is a cottage industry of online optimization algorithms that attempt to improve on stochastic gradient descent . AdaGrad was reviewed in subsec - tion 2.6.2 ; its main innovation is to set adaptive learning rates for each parameter by stor - ing the sum of squared gradients . Rather than using the sum over the entire training history , we can keep a running estimate , v ( t ) j = βv ( t − 1 ) j + ( 1 − β ) g2t , j , [ 3.51 ] where gt , j is the gradient with respect to parameter j at time t , and β ∈ [ 0 , 1 ] . This term places more emphasis on recent gradients , and is employed in the AdaDelta ( Zeiler , 2012 ) and Adam ( Kingma and Ba , 2014 ) optimizers . Online optimization and its theoretical background are reviewed by Bottou et al . ( 2016 ) . Early stopping , mentioned in subsec - tion 2.3.2 , can help to avoid overfitting by terminating training after reaching a plateau in the performance on a heldout validation set . Under contract with MIT Press , shared under CC-BY-NC-ND license . 62 CHAPTER 3 . NONLINEAR CLASSIFICATION Practical advice The bag of tricks for training neural networks continues to grow , and it is likely that there will be several new ones by the time you read this . Today , it is standard practice to use gradient clipping , early stopping , and a sensible initialization of parameters to small random values . More bells and whistles can be added as solutions to specific problems — for example , if it is difficult to find a good learning rate for stochastic gradient descent , then it may help to try a fancier optimizer with an adaptive learning rate . Alternatively , if a method such as layer normalization is used by related models in the research literature , you should probably consider it , especially if you are having trouble matching published results . As with linear classifiers , it is important to evaluate these decisions on a held-out development set , and not on the test set that will be used to provide the final measure of the model’s performance ( see subsection 2.2.5 ) . 3.4 Convolutional neural networks A basic weakness of the bag-of-words model is its inability to account for the ways in which words combine to create meaning , including even simple reversals such as not pleasant , hardly a generous offer , and I wouldn’t mind missing the flight . Computer vision faces the related challenge of identifying the semantics of images from pixel features that are uninformative in isolation . An earlier generation of computer vision research focused on designing filters to aggregate local pixel-level features into more meaningful representations , such as edges and corners ( e.g . , Canny , 1987 ) . Similarly , earlier NLP re - search attempted to capture multiword linguistic phenomena by hand-designed lexical patterns ( Hobbs et al . , 1997 ) . In both cases , the output of the filters and patterns could then act as base features in a linear classifier . But rather than designing these feature ex - tractors by hand , a better approach is to learn them , using the magic of backpropagation . This is the idea behind convolutional neural networks . Following subsection 3.2.4 , define the base layer of a neural network as , X ( 0 ) = Θ ( x → z ) [ ew1 , ew2 , . . . , ewM ] , [ 3.52 ] where ewm is a column vector of zeros , with a 1 at position wm . The base layer has dimen - sion X ( 0 ) ∈ RKe × M , where Ke is the size of the word embeddings . To merge information across adjacent words , we convolve X ( 0 ) with a set of filter matrices C ( k ) ∈ RKe × h . Convo - lution is indicated by the symbol ∗ , and is defined , X ( 1 ) = f ( b + C ∗ X ( 0 ) ) = ⇒ x ( 1 ) k , m = f ( bk + Ke ∑ k ′ = 1 h ∑ n = 1 c ( k ) k ′ , n × x ( 0 ) k ′ , m + n − 1 ) , [ 3.53 ] where f is an activation function such as tanh or ReLU , and b is a vector of offsets . The convolution operation slides the matrix C ( k ) across the columns of X ( 0 ) . At each position m , we compute the elementwise product C ( k ) � X ( 0 ) m : m + h − 1 , and take the sum . Jacob Eisenstein . Draft of October 15 , 2018 . 3.4 . CONVOLUTIONAL NEURAL NETWORKS 63 X ( 0 ) CC * X ( 1 ) z convolution pooling prediction M Ke Kf Kf Figure 3.4 : A convolutional neural network for text classification A simple filter might compute a weighted average over nearby words , C ( k ) =     0.5 1 0.5 0.5 1 0.5 . . . . . . . . . 0.5 1 0.5     , [ 3.54 ] thereby representing trigram units like not so unpleasant . In one-dimensional convolu - tion , each filter matrix C ( k ) is constrained to have non-zero values only at row k ( Kalch - brenner et al . , 2014 ) . This means that each dimension of the word embedding is processed by a separate filter , and it implies that Kf = Ke . To deal with the beginning and end of the input , the base matrix X ( 0 ) may be padded with h column vectors of zeros at the beginning and end ; this is known as wide convolu - tion . If padding is not applied , then the output from each layer will be h − 1 units smaller than the input ; this is known as narrow convolution . The filter matrices need not have identical filter widths , so more generally we could write hk to indicate to width of filter C ( k ) . As suggested by the notation X ( 0 ) , multiple layers of convolution may be applied , so that X ( d ) is the input to X ( d + 1 ) . AfterD convolutional layers , we obtain a matrix representation of the document X ( D ) ∈ RKz × M . If the instances have variable lengths , it is necessary to aggregate over allM word positions to obtain a fixed-length representation . This can be done by a pooling operation , Under contract with MIT Press , shared under CC-BY-NC-ND license . 64 CHAPTER 3 . NONLINEAR CLASSIFICATION Figure 3.5 : A dilated convolutional neural network captures progressively larger context through recursive application of the convolutional operator such as max-pooling ( Collobert et al . , 2011 ) or average-pooling , z = MaxPool ( X ( D ) ) = ⇒ zk = max ( x ( D ) k , 1 , x ( D ) k , 2 , . . . x ( D ) k , M ) [ 3.55 ] z = AvgPool ( X ( D ) ) = ⇒ zk = 1 M M ∑ m = 1 x ( D ) k , m . [ 3.56 ] The vector z can now act as a layer in a feedforward network , culminating in a prediction ŷ and a loss ` ( i ) . The setup is shown in Figure 3.4 . Just as in feedforward networks , the parameters ( C ( k ) , b , Θ ) can be learned by back - propagating from the classification loss . This requires backpropagating through the max - pooling operation , which is a discontinuous function of the input . But because we need only a local gradient , backpropagation flows only through the argmax m : ∂ zk ∂ x ( D ) k , m = { 1 , x ( D ) k , m = max ( x ( D ) k , 1 , x ( D ) k , 2 , . . . x ( D ) k , M ) 0 , otherwise . [ 3.57 ] The computer vision literature has produced a huge variety of convolutional archi - tectures , and many of these innovations can be applied to text data . One avenue for improvement is more complex pooling operations , such as k-max pooling ( Kalchbrenner et al . , 2014 ) , which returns a matrix of the k largest values for each filter . Another innova - tion is the use of dilated convolution to build multiscale representations ( Yu and Koltun , 2016 ) . At each layer , the convolutional operator applied in strides , skipping ahead by s steps after each feature . As we move up the hierarchy , each layer is s times smaller than the layer below it , effectively summarizing the input ( Kalchbrenner et al . , 2016 ; Strubell et al . , 2017 ) . This idea is shown in Figure 3.5 . Multi-layer convolutional networks can also be augmented with “ shortcut ” connections , as in the residual network from subsec - tion 3.2.2 ( Johnson and Zhang , 2017 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 3.4 . CONVOLUTIONAL NEURAL NETWORKS 65 Additional resources The deep learning textbook by Goodfellow et al . ( 2016 ) covers many of the topics in this chapter in more detail . For a comprehensive review of neural networks in natural lan - guage processing , see Goldberg ( 2017b ) . A seminal work on deep learning in natural language processing is the aggressively titled “ Natural Language Processing ( Almost ) from Scratch ” , which uses convolutional neural networks to perform a range of language processing tasks ( Collobert et al . , 2011 ) , although there is earlier work ( e.g . , Henderson , 2004 ) . This chapter focuses on feedforward and convolutional neural networks , but recur - rent neural networks are one of the most important deep learning architectures for natural language processing . They are covered extensively in chapters 6 and 7 . The role of deep learning in natural language processing research has caused angst in some parts of the natural language processing research community ( e.g . , Goldberg , 2017a ) , especially as some of the more zealous deep learning advocates have argued that end-to-end learning from “ raw ” text can eliminate the need for linguistic constructs such as sentences , phrases , and even words ( Zhang et al . , 2015 , originally titled “ Text under - standing from scratch ” ) . These developments were surveyed by Manning ( 2015 ) . While reports of the demise of linguistics in natural language processing remain controversial at best , deep learning and backpropagation have become ubiquitous in both research and applications . Exercises 1 . Figure 3.3 shows the computation graph for a feedforward neural network with one layer . a ) Update the computation graph to include a residual connection between x and z . b ) Update the computation graph to include a highway connection between x and z . 2 . Prove that the softmax and sigmoid functions are equivalent when the number of possible labels is two . Specifically , for any Θ ( z → y ) ( omitting the offset b for simplic - ity ) , show how to construct a vector of weights θ such that , SoftMax ( Θ ( z → y ) z ) [ 0 ] = σ ( θ · z ) . [ 3.58 ] 3 . Convolutional neural networks often aggregate across words by using max-pooling ( Equation 3.55 in section 3.4 ) . A potential concern is that there is zero gradient with respect to the parts of the input that are not included in the maximum . The following Under contract with MIT Press , shared under CC-BY-NC-ND license . 66 CHAPTER 3 . NONLINEAR CLASSIFICATION questions consider the gradient with respect to an element of the input , x ( 0 ) m , k , and they assume that all parameters are independently distributed . a ) First consider a minimal network , with z = MaxPool ( X ( 0 ) ) . What is the prob - ability that the gradient ∂ ` ∂ x ( 0 ) m , k is non-zero ? b ) Now consider a two-level network , with X ( 1 ) = f ( b + C ∗ X ( 0 ) ) . Express the probability that the gradient ∂ ` ∂ x ( 0 ) m , k is non-zero , in terms of the input length M , the filter size n , and the number of filters Kf . c ) Using a calculator , work out the probability for the case M = 128 , n = 4 , Kf = 32 . d ) Now consider a three-level network , X ( 2 ) = f ( b + C ∗ X ( 1 ) ) . Give the general equation for the probability that ∂ ` ∂ x ( 0 ) m , k is non-zero , and compute the numerical probability for the scenario in the previous part , assuming Kf = 32 and n = 4 at both levels . 4 . Design a feedforward network to compute the XOR function : f ( x1 , x2 ) =            − 1 , x1 = 1 , x2 = 1 1 , x1 = 1 , x2 = 0 1 , x1 = 0 , x2 = 1 − 1 , x1 = 0 , x2 = 0 . [ 3.59 ] Your network should have a single output node which uses the Sign activation func - tion , f ( x ) = { 1 , x > 0 − 1 , x ≤ 0 . . Use a single hidden layer , with ReLU activation func - tions . Describe all weights and offsets . 5 . Consider the same network as above ( with ReLU activations for the hidden layer ) , with an arbitrary differentiable loss function ` ( y ( i ) , ỹ ) , where ỹ is the activation of the output node . Suppose all weights and offsets are initialized to zero . Show that gradient descent will not learn the desired function from this initialization . 6 . The simplest solution to the previous problem relies on the use of the ReLU activa - tion function at the hidden layer . Now consider a network with arbitrary activations on the hidden layer . Show that if the initial weights are any uniform constant , then gradient descent will not learn the desired function from this initialization . 7 . Consider a network in which : the base features are all binary , x ∈ { 0 , 1 } M ; the hidden layer activation function is sigmoid , zk = σ ( θk · x ) ; and the initial weights are sampled independently from a standard normal distribution , θj , k ∼ N ( 0 , 1 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 3.4 . CONVOLUTIONAL NEURAL NETWORKS 67 • Show how the probability of a small initial gradient on any weight , ∂ zk ∂ θj , k < α , depends on the size of the input M . Hint : use the lower bound , Pr ( σ ( θk · x ) × ( 1 − σ ( θk · x ) ) < α ) ≥ 2 Pr ( σ ( θk · x ) < α ) , [ 3.60 ] and relate this probability to the variance V [ θk · x ] . • Design an alternative initialization that removes this dependence . 8 . The ReLU activation function can lead to “ dead neurons ” , which can never be acti - vated on any input . Consider the following two-layer feedforward network with a scalar output y : zi = ReLU ( θ ( x → z ) i · x + bi ) [ 3.61 ] y = θ ( z → y ) · z . [ 3.62 ] Suppose that the input is a binary vector of observations , x ∈ { 0 , 1 } D . a ) Under what condition is node zi “ dead ” ? Your answer should be expressed in terms of the parameters θ ( x → z ) i and bi . b ) Suppose that the gradient of the loss on a given instance is ∂ ` ∂ y = 1 . Derive the gradients ∂ ` ∂ bi and ∂ ` ∂ θ ( x → z ) j , i for such an instance . c ) Using your answers to the previous two parts , explain why a dead neuron can never be brought back to life during gradient-based learning . 9 . Suppose that the parameters Θ = { Θ ( x → z ) , Θ ( z → y ) , b } are a local optimum of a feedforward network in the following sense : there exists some � > 0 such that , ( | | Θ̃ ( x → z ) − Θ ( x → z ) | | 2F + | | Θ̃ ( z → y ) − Θ ( z → y ) | | 2F + | | b̃ − b | | 22 < � ) ⇒ ( L ( Θ̃ ) > L ( Θ ) ) [ 3.63 ] Define the function π as a permutation on the hidden units , as described in subsec - tion 3.3.3 , so that for any Θ , L ( Θ ) = L ( Θπ ) . Prove that if a feedforward network has a local optimum in the sense of Equation 3.63 , then its loss is not a convex function of the parameters Θ , using the definition of convexity from section 2.4 10 . Consider a network with a single hidden layer , and a single output , y = θ ( z → y ) · g ( Θ ( x → z ) x ) . [ 3.64 ] Assume that g is the ReLU function . Show that for any matrix of weights Θ ( x → z ) , it is permissible to rescale each row to have a norm of one , because an identical output can be obtained by finding a corresponding rescaling of θ ( z → y ) . Under contract with MIT Press , shared under CC-BY-NC-ND license .