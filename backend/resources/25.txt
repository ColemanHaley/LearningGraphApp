Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 25 Question Answering quest knowledge deeply human , surprising practi - cally soon computers asking questions . early 1960s , systems two major paradigms question answering — information - retrieval-based knowledge-based — answer questions baseball statis - tics scientific facts . Even imaginary computers got act . Deep Thought , computer Douglas Adams invented Hitchhiker’s Guide Galaxy , managed answer “ Ultimate Question Life , Universe , Everything ” . 1 2011 , IBM’s Watson question-answering system won TV game-show Jeop - ardy ! hybrid architecture surpassed humans answering questions like WILLIAM WILKINSON’S “ ACCOUNT PRINCIPAL - ITIES WALLACHIA MOLDOVIA ” INSPIRED AU - THOR’S FAMOUS NOVEL2 question answering systems focus factoid questions , questions answered simple facts expressed short texts . answers questions below expressed personal name , temporal expression , location : ( 25.1 ) founded Virgin Airlines ? ( 25.2 ) average age onset autism ? ( 25.3 ) Apple Computer based ? chapter describe two major paradigms factoid question - swering . Information-retrieval IR-based question answering relies vast quantities textual information web collections like PubMed . user question , information retrieval techniques first find relevant documents passages . systems ( feature-based , neural , ) reading comprehen - sion algorithms read retrieved documents passages draw answer directly spans text . second paradigm , knowledge-based question answering , system - stead builds semantic representation query , mapping states border Texas ? logical representation : λx . state ( x ) ∧ borders ( x , texas ) , Ada Lovelace born ? gapped relation : birth-year ( Ada Lovelace , ? x ) . meaning representations query databases facts . Finally , large industrial systems like DeepQA system IBM’s Watson often hybrids , text datasets structured knowledge bases answer questions . DeepQA finds many candidate answers knowledge bases textual sources , scores candidate answer knowledge sources like geospatial databases , taxonomical classification , textual sources . describe IR-based approaches ( including neural reading comprehension sys - tems ) next section , followed sections knowledge-based systems , Watson Deep QA , discussion evaluation . 1 answer 42 , unfortunately details question never revealed . 2 answer , course , ‘ Bram Stoker ’ , novel Dracula . 2 CHAPTER 25 • QUESTION ANSWERING 25.1 IR-based Factoid Question Answering goal information retrieval based question answering answer user’s question finding short text segments web collection doc - uments . Figure 25.1 shows sample factoid questions answers . Question Answer Louvre Museum located ? Paris , France abbreviation limited partnership ? L.P . names Odin’s ravens ? Huginn Muninn currency China ? yuan kind nuts marzipan ? almonds instrument Max Roach play ? drums official language Algeria ? Arabic many pounds stone ? 14 Figure 25.1 sample factoid questions answers . Figure 25.2 shows three phases IR-based factoid question-answering system : question processing , passage retrieval ranking , answer extraction . Document DocumentDocument Docume ntDocumentDocumentDocumentDocument Question Processing Document Passage Retrieval Query Formulation Answer Type Detection Question Passage Retrieval Document Retrieval Answer Extraction Answer passages Indexing Relevant Docs DocumentDocumentDocument Figure 25.2 IR-based factoid question answering three stages : question processing , passage retrieval , answer processing . 25.1.1 Question Processing main goal question-processing phase extract query : keywords passed IR system match potential documents . systems additionally extract further information : • answer type : entity type ( person , location , time , etc . ) answer . • focus : string words question likely replaced answer answer string found . • question type : definition question , math question , list question ? example , question state capital largest population ? query processing might produce : query : “ state capital largest population ” answer type : city focus : state capital next two sections summarize two commonly tasks , query formulation answer type detection . 25.1 • IR-BASED FACTOID QUESTION ANSWERING 3 25.1.2 Query Formulation Query formulation task creating query — list tokens — send information retrieval system retrieve documents might contain answer strings . question answering web , simply pass entire question web search engine , perhaps leaving question word ( , , etc . ) . question answering smaller sets documents like corporate information pages Wikipedia , still IR engine index search documents , generally standard tf-idf cosine matching , might need processing . example , searching Wikipedia , helps compute tf-idf bigrams rather unigrams query document ( Chen et al . , 2017 ) . might need query expansion , web answer question might appear many different forms , probably match question , smaller document sets answer might appear once . Query expansion methods add query terms hopes matching particular form answer appears , like adding morphological variants content words question , synonyms thesaurus . query formulation approach sometimes questioning web apply query reformulation rules query . rules rephrase question toqueryreformulation make look like substring possible declarative answers . question “ laser invented ? ” might reformulated “ laser invented ” ; question “ Valley Kings ? ” “ Valley Kings located ” . sample handwritten reformulation rules Lin ( 2007 ) : ( 25.4 ) wh-word verb B → . . . verb + ed B ( 25.5 ) → located 25.1.3 Answer Types systems make question classification , task finding answerquestionclassification type , named-entity categorizing answer . question like “ founded Vir-answer type gin Airlines ? ” expects answer type PERSON . question like “ Canadian city largest population ? ” expects answer type CITY . know answer type question person , avoid examining every sentence document collection , focusing sentences mentioning people . answer types might just named entities like PERSON , LOCATION , ORGANIZATION described Chapter 18 , larger hierarchical set answer types called answer type taxonomy . taxonomies builtanswer typetaxonomy automatically , resources like WordNet ( Harabagiu et al . 2000 , Pasca 2003 ) , designed hand . Figure 25.4 shows hand-built ontology , Li Roth ( 2005 ) tagset ; subset shown Fig . 25.3 . hierarchical tagset , question labeled coarse-grained tag like HUMAN fine - grained tag like HUMAN : DESCRIPTION , HUMAN : GROUP , HUMAN : IND , . HUMAN : DESCRIPTION type often called BIOGRAPHY question answer required give brief biography person rather just name . Question classifiers built hand-writing rules like following rule ( Hovy et al . , 2002 ) detecting answer type BIOGRAPHY : ( 25.6 ) { | | | } PERSON question classifiers , , based supervised learning , trained databases questions hand-labeled answer type ( Li Roth , 2002 ) . feature-based neural methods . Feature based 4 CHAPTER 25 • QUESTION ANSWERING NUMERIC ABBREVIATION ENTITY DESCRIPTION LOCATION HUMAN Li & Roth Taxonomy country city state reason definition food currency animal date distance percent size money individual title group expression abbreviation Figure 25.3 subset Li Roth ( 2005 ) answer types . methods rely words questions embeddings , part-of-speech word , named entities questions . Often , single word question gives extra information answer type , identity feature . word sometimes called answer type word question headword , defined headword first NP question’s wh-word ; head - words indicated boldface following examples : ( 25.7 ) city China largest number foreign financial companies ? ( 25.8 ) state flower California ? general , question classification accuracies relatively high easy ques - tion types like PERSON , LOCATION , TIME questions ; detecting REASON DESCRIPTION questions harder . 25.1.4 Document Passage Retrieval IR query produced question processing stage sent IR engine , resulting set documents ranked relevance query . answer-extraction methods designed apply smaller regions paragraphs , QA systems next divide top n documents smaller passages suchpassages sections , paragraphs , sentences . might already segmented source document might need run paragraph segmentation algorithm . simplest form passage retrieval simply pass every pas-passageretrieval sage answer extraction stage . sophisticated variant filter passages running named entity answer type classification retrieved passages , discarding passages contain answer type question . possible supervised learning fully rank remaining passages , features like : • number named entities right type passage • number question keywords passage • longest exact sequence question keywords occurs passage • rank document passage extracted • proximity keywords original query ( Pasca 2003 , Monz 2004 ) . • number n-grams overlap passage question ( Brill et al . , 2002 ) . question answering web take snippets Websnippets search engine passages . 25.1 • IR-BASED FACTOID QUESTION ANSWERING 5 Tag Example ABBREVIATION abb abbreviation limited partnership ? exp “ c ” stand equation E = mc2 ? DESCRIPTION definition tannins ? description words Canadian National anthem ? manner get rust stains clothing ? reason caused Titanic sink ? ENTITY animal names Odin’s ravens ? body part body contains corpus callosum ? color colors make up rainbow ? creative book find story Aladdin ? currency currency China ? disease / medicine Salk vaccine prevent ? event war involved battle Chapultepec ? food kind nuts marzipan ? instrument instrument Max Roach play ? lang official language Algeria ? letter letter appears cold-water tap Spain ? name King Arthur’s sword ? plant fragrant white climbing roses ? product fastest computer ? religion religion members ? sport name ball game played Mayans ? substance fuel airplanes ? symbol chemical symbol nitrogen ? technique best way remove wallpaper ? term say “ Grandma ” Irish ? vehicle name Captain Bligh’s ship ? word singular dice ? HUMAN description Confucius ? group major companies part Dow Jones ? ind first Russian astronaut spacewalk ? title Queen Victoria’s title regarding India ? LOCATION city oldest capital city Americas ? country country borders others ? mountain highest peak Africa ? river runs Liverpool ? state states state income tax ? NUMERIC code telephone number University Colorado ? count many soldiers died World War II ? date date Boxing Day ? distance long Mao’s 1930s Long March ? money McDonald’s hamburger cost 1963 ? order Shanghai rank among world cities population ? population Mexico ? period average life expectancy Stone Age ? percent fraction beaver’s life spent swimming ? temp hot oven making Peachy Oat Muffins ? speed fast spacecraft travel escape Earth’s gravity ? size size Argentina ? weight many pounds stone ? Figure 25.4 Question typology Li Roth ( 2002 ) , ( 2005 ) . Example sentences corpus 5500 labeled questions . question labeled coarse - grained tag like HUMAN NUMERIC fine-grained tag like HUMAN : DESCRIPTION , HUMAN : GROUP , HUMAN : IND , . 6 CHAPTER 25 • QUESTION ANSWERING 25.1.5 Answer Extraction final stage question answering extract specific answer passage , example responding 29,029 feet question like “ tall Mt . Everest ? ” . task commonly modeled span labeling : passage , identifying span text constitutes answer . span simple baseline algorithm answer extraction run named entity tagger candidate passage return whatever span passage correct - swer type . Thus , following examples , underlined named entities extracted passages answer HUMAN DISTANCE-QUANTITY questions : “ prime minister India ? ” Manmohan Singh , Prime Minister India , told left leaders deal renegotiated . “ tall Mt . Everest ? ” official height Mount Everest 29029 feet Unfortunately , answers many questions , DEFINITION questions , tend particular named entity type . reason modern work answer extraction sophisticated algorithms , generally based supervised learning . next section introduces simple feature-based classifier , turn modern neural algorithms . 25.1.6 Feature-based Answer Extraction Supervised learning approaches answer extraction train classifiers decide span sentence contains answer . obviously useful feature answer type feature baseline algorithm . Hand-written regular expression pat - terns play role , sample patterns definition questions Fig . 25.5 . Pattern Question Answer < AP > < QP > autism ? “ , developmental disorders autism ” < QP > , < AP > caldera ? “ Long Valley caldera , volcanic crater 19 miles long ” Figure 25.5 answer-extraction patterns answer phrase ( AP ) question phrase ( QP ) definition questions ( Pasca , 2003 ) . features classifiers include : Answer type match : True candidate answer contains phrase cor - rect answer type . Pattern match : identity pattern matches candidate answer . Number matched question keywords : many question keywords con - tained candidate answer . Keyword distance : distance candidate answer query key - words . Novelty factor : True least word candidate answer novel , , query . Apposition features : True candidate answer appositive phrase con - taining many question terms . approximated number question terms separated candidate answer three words comma ( Pasca , 2003 ) . 25.1 • IR-BASED FACTOID QUESTION ANSWERING 7 Punctuation location : True candidate answer immediately followed comma , period , quotation marks , semicolon , exclamation mark . Sequences question terms : length longest sequence question terms occurs candidate answer . 25.1.7 N-gram tiling answer extraction alternative approach answer extraction , solely Web search , based n-gram tiling , approach relies redundancy web ( Brilln-gram tiling et al . 2002 , Lin 2007 ) . simplified method begins snippets returned Web search engine , produced reformulated query . first step , n-gram mining , every unigram , bigram , trigram occurring snippet ex - tracted weighted . weight function number snippets n-gram occurred , weight query reformulation pattern returned . n-gram filtering step , n-grams scored well match predicted answer type . scores computed handwritten filters built answer type . Finally , n-gram tiling algorithm concatenates overlapping n - gram fragments longer answers . standard greedy method start highest-scoring candidate try tile candidate candidate . best-scoring concatenation added set candidates , lower-scoring candidate removed , process continues single answer built . 25.1.8 Neural Answer Extraction Neural network approaches answer extraction draw intuition question answer semantically similar appropriate way . , intuition fleshed computing embedding question embedding token passage , selecting passage spans whose embeddings closest question embedding . Reading Comprehension Neural answer extractors often designed context reading compre - hension task . Hirschman et al . ( 1999 ) first proposed take children’sreadingcomprehension reading comprehension tests — pedagogical instruments child passage read answer questions — evaluate ma - chine text comprehension algorithm . acquired corpus 120 passages 5 questions designed 3rd-6th grade children , built answer extraction sys - tem , measured well answers system corresponded answer key test’s publisher . reading comprehension become task itself , useful way measure natural language understanding performance , ( sometimes called reader component question answerers ) . Reading Comprehension Datasets . Modern reading comprehension systems tend collections questions designed specifically NLP , large enough training supervised learning systems . example Stanford Question Answering Dataset ( SQuAD ) consists passages Wikipedia associatedSQuAD questions whose answers spans passage , well questions designed unanswerable ( Rajpurkar et al . 2016 , Rajpurkar et al . 2018 ) ; total just 150,000 questions . Fig . 25.6 shows ( shortened ) excerpt SQUAD 2.0 passage together three questions answer spans . 8 CHAPTER 25 • QUESTION ANSWERING Beyoncé Giselle Knowles-Carter ( born September 4 , 1981 ) American singer , songwriter , record producer actress . Born raised Houston , Texas , performed various singing dancing competitions child , rose fame late 1990s lead singer R & B girl-group Destiny’s Child . Managed father , Mathew Knowles , group became world’s best-selling girl groups time . hiatus saw release Beyoncé’s debut album , Dangerously Love ( 2003 ) , established solo artist worldwide , earned five Grammy Awards featured Billboard Hot 100 number-one singles “ Crazy Love ” “ Baby Boy ” . Q : “ city state Beyoncé grow up ? ” : “ Houston , Texas ” Q : “ areas Beyoncé compete growing up ? ” : “ singing dancing ” Q : “ Beyoncé release Dangerously Love ? ” : “ 2003 ” Figure 25.6 ( Wikipedia ) passage SQuAD 2.0 dataset ( Rajpurkar et al . , 2018 ) 3 sample questions labeled answer spans . SQuAD built humans write questions Wikipedia passage choose answer span . datasets similar techniques ; NewsQA dataset consists 100,000 question-answer pairs CNN news arti - cles . datasets like WikiQA span entire sentence containing answer ( Yang et al . , 2015 ) ; task choosing sentence rather smaller answer span sometimes called sentence selection task . sentenceselection 25.1.9 bi-LSTM-based Reading Comprehension Algorithm Neural algorithms reading comprehension question q l tokens q1 , . . . , ql passage p m tokens p1 , . . . , pm . goal compute , token pi probability pstart ( ) pi start answer span , probability pend ( ) pi end answer span . Fig . 25.7 shows architecture Document Reader component DrQA system Chen et al . ( 2017 ) . Like systems , DrQA builds embedding question , builds embedding token passage , computes similarity function question passage word con - text , question-passage similarity scores decide answer span starts ends . consider algorithm detail , following closely description Chen et al . ( 2017 ) . question represented single embedding q , weighted sum representations question word qi . computed passing series embeddings PE ( q1 ) , . . . , E ( ql ) question words RNN ( bi-LSTM shown Fig . 25.7 ) . resulting hidden representations { q1 , . . . , ql } combined weighted sum q = ∑ j b jq j ( 25.9 ) weight b j measure relevance question word , relies learned weight vector w : b j = exp ( w · q j ) ∑ j ′ exp ( w · q ′ j ) ( 25.10 ) 25.1 • IR-BASED FACTOID QUESTION ANSWERING 9 Beyonce’s debut album LSTM1 LSTM1 LSTM1 LSTM2 LSTM2 LSTM2 GloVe PER NNP Beyonce PassageQuestion LSTM1 LSTM1 LSTM1 LSTM2 LSTM2 LSTM2 GloVe GloVe GloVe … Attention Weighted sum similarity q p2 p3 similarity q q similarity … q-align1 GloVeGloVe pstart ( 1 ) pend ( 1 ) pstart ( 3 ) pend ( 3 ) … … … O NN GloVeGloVe q-align2 1 0 O NN 0 q-align3 GloVeGloVe Att Att p1 p1 p2 p3 ~ p1 p2 p3 ~ ~ q1 q2 q3 Figure 25.7 question answering system Chen et al . ( 2017 ) , considering part question Beyoncé release Dangerously Love ? passage starting Beyoncé’s debut album , Dangerously Love ( 2003 ) . compute passage embedding { p1 , . . . , pm } first form input represen - tation p̃ = { p̃1 , . . . , p̃m } concatenating four components : • embedding word E ( pi ) GLoVE ( Pennington et al . , 2014 ) . • Token features like part speech pi , named entity tag pi , running POS NER taggers . • Exact match features representing passage word pi occurred question : 1 ( pi ∈ q ) . Separate exact match features might lemmatized lower-cased versions tokens . • Aligned question embedding : addition exact match features , many QA systems attention mechanism give sophisticated model similarity passage question words , similar non - identical words like release singles . example weighted similarity ∑ j ai , jE ( q j ) , attention weight ai , j encodes simi - larity pi question word q j . attention weight computed dot product functions α word embeddings question passage : qi , j = exp ( α ( E ( pi ) ) · α ( E ( q j ) ) ) ∑ j ′ exp ( α ( E ( pi ) ) · α ( E ( q ′ j ) ) ) ( 25.11 ) α ( · ) simple feed forward network . pass p̃ biLSTM : { p1 , . . . , pm } ) = RNN ( { p̃1 , . . . , p̃m } ( 25.12 ) result previous two steps single question embedding q representation word passage { p1 , . . . , pm } . order find answer 10 CHAPTER 25 • QUESTION ANSWERING span , train two separate classifiers , compute pi probability pstart ( ) pi start answer span , compute probability pend ( ) . classifiers just take dot product passage question embeddings input , turns work better learn sophis - ticated similarity function , like bilinear attention layer W : pstart ( ) ∝ exp ( piWsq ) pend ( ) ∝ exp ( piWeq ) ( 25.13 ) neural answer extractors trained end-to-end datasets like SQuAD . 25.1.10 BERT-based Question Answering power contextual embeddings allow question answering models based BERT contextual embeddings transformer architecture achieve even higher accuracy ( Fig . 25.8 ) . BERT [ CLS ] Tok 1 Tok N … [ SEP ] Tok 1 Tok M … Question Paragraph ECLS E1 EN ESEP E1 ’ EM ’ C T1 TN TSEP T1 ’ TM ’ … … … … S E Pstart1 Pend1 . . PstartM … PendM … Figure 25.8 BERT model span-based question answering reading - comprehension-based question answering tasks . Figure Devlin et al . ( 2019 ) . Recall Chapter 10 BERT represents two input strings sequence wordpiece tokens separated [ SEP ] token . pre-trained BERT model produce output token embedding T ′ every paragraph token ′ . span-based question answering , represent question first sequence , para - graph second sequence . need add structure output head trained fine-tuning phase . add two new embeddings : span-start embedding S span-end embedding E . get span-start probability output token T ′ , compute dot product S T ′ normalize tokens T ′ paragraph : Pstarti = eS·Ti ∑ j e S·Tj ( 25.14 ) 25.2 • KNOWLEDGE-BASED QUESTION ANSWERING 11 analogous thing compute span-end probability : Pendi = eE·Ti ∑ j e E·Tj ( 25.15 ) score candidate span position j S · Ti + E · Tj , highest scoring span j ≥ chosen model prediction . training objective fine-tuning sum log-likelihoods correct start end positions observation . 25.2 Knowledge-based Question Answering enormous amount information encoded vast amount text web , information obviously exists structured forms . term knowledge-based question answering idea answering natural language question mapping query structured database . Like text - based paradigm question answering , approach dates back earliest days natural language processing , systems like BASEBALL ( Green et al . , 1961 ) answered questions structured database baseball games stats . Systems mapping text string logical form called seman - tic parsers . Semantic parsers question answering usually map version predicate calculus query language like SQL SPARQL , examples Fig . 25.9 . Question Logical form Ada Lovelace born ? birth-year ( Ada Lovelace , ? x ) states border Texas ? λ x . state ( x ) ∧ borders ( x , texas ) largest state argmax ( λx . state ( x ) , λx . size ( x ) ) many people survived sinking Titanic ( count ( ! fb : event . disaster . survivors fb : en . sinking titanic ) ) Figure 25.9 Sample logical forms produced semantic parser question answering . range simple relations like birth-year , relations normalized databases like Freebase , full predicate calculus . logical form question thus form query easily converted . database full relational database , simpler structured databases like sets RDF triples . Recall Chapter 18 RDF triple 3-tuple , predicate two arguments , expressing simple relation proposition . Popular ontologies like Freebase ( Bollacker et al . , 2008 ) DBpedia ( Bizer et al . , 2009 ) large numbers triples derived Wikipedia infoboxes , structured tables associated certain Wikipedia articles . simplest formation knowledge-based question answering task answer factoid questions ask missing arguments triple . Consider RDF triple like following : subject predicate object Ada Lovelace birth-year 1815 triple answer text questions like ‘ Ada Lovelace born ? ’ ‘ born 1815 ? ’ . Question answering paradigm requires mapping textual strings like ” . . . born ” canonical relations knowledge base like birth-year . might sketch task : 12 CHAPTER 25 • QUESTION ANSWERING “ Ada Lovelace born ? ” → birth-year ( Ada Lovelace , ? x ) “ capital England ? ” → capital-city ( ? x , England ) 25.2.1 Rule-based Methods relations frequent , worthwhile write handwritten rules extract relations question , just saw Section ? ? . example , extract birth-year relation , write patterns search question word , main verb like born , extract named entity argument verb . 25.2.2 Supervised Methods cases supervised data , consisting set questions paired correct logical form like examples Fig . 25.9 . task take pairs training tuples produce system maps new questions logical forms . supervised algorithms learning answer simple questions relations first parse questions align parse trees logical form . Generally systems bootstrap small set rules building mapping , initial lexicon well . example , system might built - strings entities system ( Texas , Ada Lovelace ) , simple default rules mapping fragments question parse tree particular relations : V ENTITY → relation ( ? x , entity ) nsubj dobj V ENTITY → relation ( ? x , entity ) tmod nsubj rules lexicon , training tuple like following : “ Ada Lovelace born ? ” → birth-year ( Ada Lovelace , ? x ) first parsed , resulting following mapping . Ada Lovelace born → birth-year ( Ada Lovelace , ? x ) tmod nsubj many pairs like , induce mappings pieces parse fragment , mapping parse fragment left rela - tion right : · born → birth-year ( , ? x ) tmod nsubj supervised system thus parse tuple training set induce bigger set specific rules , allowing map unseen examples “ 25.2 • KNOWLEDGE-BASED QUESTION ANSWERING 13 X born ? ” questions birth-year relation . Rules furthermore associ - ated counts based number times rule parse training data . Like rule counts probabilistic grammars , normalized prob - abilities . probabilities choose highest probability parse sentences multiple semantic interpretations . supervised approach extended deal complex questions just single relations . Consider question biggest state bordering Texas ? — taken GeoQuery database questions U.S . Geography ( Zelle Mooney , 1996 ) — semantic form : argmax ( λx . state ( x ) ∧ borders ( x , texas ) , λx . size ( x ) ) question complex structures simple single-relation questions considered , argmax func - tion , mapping word biggest size . Zettlemoyer Collins ( 2005 ) shows complex default rules ( richer syntactic struc - tures ) learn map text sentences complex logical forms . rules take training set’s pairings sentence meaning complex rules break training example down smaller tuples recombined parse new sentences . 25.2.3 Dealing Variation : Semi-Supervised Methods difficult create training sets questions labeled mean - ing representation , supervised datasets cover wide variety forms even simple factoid questions take . reason techniques mapping factoid questions canonical relations structures knowledge bases find way make textual redundancy . common source redundancy , course , web , contains vast numbers textual variants expressing relation . reason , meth - ods make web text , via semi-supervised methods like distant supervision unsupervised methods like open information extraction , intro - duced Chapter 18 . example REVERB open information extractor ( Fader et al . , 2011 ) extracts billions ( subject , relation , object ) triples strings web , ( “ Ada Lovelace ” , “ born ” , “ 1815 ” ) . aligning strings canonical knowledge source like Wikipedia , create new relations queried simultaneously learning map words question canonical relations . align REVERB triple canonical knowledge source first align arguments predicate . Recall Chapter 22 linking string like “ Ada Lovelace ” Wikipedia page called entity linking ; thus rep - resent concept ‘ Ada Lovelace ’ unique identifier Wikipedia page . subject string associated unique page Wikipedia , dis - ambiguate page sought , example cosine distance triple string ( ‘ Ada Lovelace born 1815 ’ ) candidate Wikipedia page . Date strings like ‘ 1815 ’ turned normalized form standard tools temporal normalization like SUTime ( Chang Manning , 2012 ) . Once aligned arguments , align predicates . Freebase relation people . person . birthdate ( ada lovelace , 1815 ) string ‘ Ada Lovelace born 1815 ’ , linked Ada Lovelace normalized 1815 , learn mapping string ‘ born ’ relation people . person . birthdate . simplest case , aligning relation string words arguments ; complex alignment algorithms like IBM Model 1 ( Chapter 11 ) . phrase aligns predicate many 14 CHAPTER 25 • QUESTION ANSWERING entities , extracted lexicon mapping questions relations . examples resulting lexicon , produced Berant et al . ( 2013 ) , giving many variants phrases align Freebase relation country . capital country capital city : capital capital city become capital capitol national capital official capital political capital administrative capital beautiful capital capitol city remain capital make capital political center bustling capital capital city cosmopolitan capital move capital modern capital federal capital beautiful capital city administrative capital city Figure 25.10 phrases align Freebase relation country . capital Berant et al . ( 2013 ) . useful sources linguistic redundancy paraphrase databases . ex - ample site wikianswers.com contains millions pairs questions users tagged same meaning , 18 million collected PARALEX corpus ( Fader et al . , 2013 ) . Here’s example : Q : green blobs plant cells ? Lemmatized synonyms PARALEX : green blob plant cell ? green part plant cell ? green part plant cell ? green substance plant cell ? part plant cell give green color ? cell part plant enable plant give green color ? part plant cell turn green ? part plant cell cell get green color ? green part plant call ? part plant cell make plant green call ? resulting millions pairs question paraphrases aligned MT alignment approaches create MT-style phrase table trans - lating question phrases synonymous phrases . question answering algorithms generate paraphrases question part process finding answer ( Fader et al . 2013 , Berant Liang 2014 ) . 25.3 multiple information sources : IBM’s Watson course reason limit ourselves just text-based knowledge-based resources question answering . Watson system IBM won Jeop - ardy ! challenge 2011 example system relies wide variety resources answer questions . Figure 25.11 shows 4 stages DeepQA system question - swering component Watson . first stage question processing . DeepQA system runs parsing , named entity tagging , relation extraction question . , like text-based systems Section 25.1 , DeepQA system extracts focus , answer type 25.3 • MULTIPLE INFORMATION SOURCES : IBM’S WATSON 15 Document DocumentDocument ( 1 ) Question Processing Text Resources Focus Detection Lexical Answer Type Detection Question Document Passsage Retrieval passages DocumentDocumentDocument Question Classification Parsing Named Entity Tagging Relation Extraction Coreference Structured Data Relation Retrieval DBPedia Freebase ( 2 ) Candidate Answer Generation Candidate Answer Candidate Answer Candidate AnswerCandidate AnswerCandidateAnswer Candidate AnswerCandidate AnswerCandidate AnswerCandidate Answer Candidate Answer Candidate Answer Candidate Answer ( 3 ) Candidate Answer Scoring Evidence Retrieval scoring Answer Extraction Document titles Anchor text Text Evidence Sources ( 4 ) Confidence Merging Ranking Text Evidence Sources Time DBPedia Space Facebook Answer Type Answer Confidence Candidate Answer + Confidence Candidate Answer + Confidence Candidate Answer + ConfidenceCandidate Answer + ConfidenceCandidate Answer + Confidence Logistic Regression Answer Ranker Merge Equivalent Answers Figure 25.11 4 broad stages Watson QA : ( 1 ) Question Processing , ( 2 ) Candidate Answer Generation , ( 3 ) Candidate Answer Scoring , ( 4 ) Answer Merging Confidence Scoring . ( called lexical answer type LAT ) , performs question classification question sectioning . Consider Jeopardy ! examples , category followed question : Poets Poetry : bank clerk Yukon published “ Songs Sourdough ” 1907 . THEATRE : new play based Sir Arthur Conan Doyle canine classic opened London stage 2007 . questions parsed , named entities extracted ( Sir Arthur Conan Doyle identified PERSON , Yukon GEOPOLITICAL ENTITY , “ Songs Sour - dough ” COMPOSITION ) , coreference run ( linked clerk ) rela - tions like following extracted : authorof ( focus , “ Songs sourdough ” ) publish ( e1 , , “ Songs sourdough ” ) ( e2 , e1 , 1907 ) temporallink ( publish ( . . . ) , 1907 ) Next DeepQA extracts question focus , shown bold examples . Thefocus focus part question co-refers answer , example align supporting passage . focus extracted handwritten rules — made possible relatively stylized syntax Jeopardy ! questions — rule extracting noun phrase determiner “ ” Conan Doyle example , rules extracting pronouns like , , , , poet example . lexical answer type ( shown blue ) word words telllexical answertype something semantic type answer . wide variety questions Jeopardy ! , Jeopardy ! far larger set answer types sets standard factoid algorithms like shown Fig . 25.4 . Even large set named entity tags insufficient define set answer types . DeepQA team investigated set 20,000 questions found named entity tagger 100 named entity types covered less half types questions . Thus DeepQA extracts wide variety words answer types ; roughly 5,000 lexical answer types occurred 20,000 questions investigated , often multiple answer types question . 16 CHAPTER 25 • QUESTION ANSWERING lexical answer types again extracted rules : default rule choose syntactic headword focus . rules improve default choice . example additional lexical answer types words question coreferent particular syntactic relation focus , head - words appositives predicative nominatives focus . cases even Jeopardy ! category act lexical answer type , refers type entity compatible lexical answer types . Thus first case , , poet , clerk lexical answer types . addition rules directly classifier , features logistic regression classifier return probability well lexical answer type . Note answer types function quite differently DeepQA purely IR - based factoid question answerers . algorithm described Section 25.1 , determine answer type , strict filtering algorithm considering text strings exactly type . DeepQA , contrast , extract lots answers , unconstrained answer type , set answer types , later ‘ candidate answer scoring ’ phase , simply score well answer fits answer types many sources evidence . Finally question classified type ( definition question , multiple-choice , puzzle , fill-in-the-blank ) . generally writing pattern-matching regular expressions words parse trees . second candidate answer generation stage , combine processed question external documents knowledge sources suggest many candidate answers . candidate answers extracted text docu - ments structured knowledge bases . structured resources like DBpedia , IMDB , triples produced Open Information Extraction , just query stores relation known entity , just saw Section 25.2 . Thus extracted rela - tion authorof ( focus , " Songs sourdough " ) , query triple store authorof ( ? x , " Songs sourdough " ) return correct author . method extracting answers text depends type text docu - ments . extract answers normal text documents passage search just Section 25.1 . section , need generate query question ; DeepQA generally eliminating stop words , upweighting terms occur relation focus . example query : MOVIE - “ ING ” : Robert Redford Paul Newman starred depression - era grifter flick . ( Answer : “ Sting ” ) following weighted query might extracted : ( 2.0 Robert Redford ) ( 2.0 Paul Newman ) star depression era grifter ( 1.5 flick ) query passed standard IR system . DeepQA makes convenient fact vast majority Jeopardy ! answers title Wikipedia document . find titles , second text retrieval pass specifically Wikipedia documents . extracting passages retrieved Wikipedia document , directly return titles highly ranked retrieved documents possible answers . Once set passages , need extract candidate answers . document happens Wikipedia page , just take title , texts , like news documents , need approaches . Two common approaches extract anchor texts document ( anchor text text < > anchor texts 25.3 • MULTIPLE INFORMATION SOURCES : IBM’S WATSON 17 < / > point URL HTML page ) , extract noun phrases passage Wikipedia document titles . third candidate answer scoring stage many sources evidence score candidates . important lexical answer type . DeepQA includes system takes candidate answer lexical answer type returns score indicating candidate answer interpreted subclass instance answer type . Consider candidate “ difficulty swallowing ” lexical answer type “ manifestation ” . DeepQA first matches words possible entities ontologies like DBpedia WordNet . Thus candidate “ difficulty swallowing ” matched DBpedia entity “ Dysphagia ” , instance mapped WordNet type “ Symptom ” . answer type “ man - ifestation ” mapped WordNet type “ Condition ” . system looks link hyponymy , instance-of synonymy two types ; case hyponymy relation found “ Symptom ” “ Condition ” . scorers based time space relations extracted DBpe - dia structured databases . example , extract temporal properties entity ( person born , died ) compare time expres - sions question . time expression question occurs chronologically person born , evidence against person - swer question . Finally , text retrieval help retrieve evidence supporting candidate answer . retrieve passages terms matching question , replace focus question candidate answer measure overlapping words ordering passage modified question . output stage set candidate answers , vector scoring features . final answer merging scoring step first merges candidate answers equivalent . Thus extracted two candidate answers J.F.K . John F . Kennedy , stage merge two single candidate . Synonym dictio - naries useful resource created listing anchor text strings point same Wikipedia page ; dictionaries give large numbers synonyms Wikipedia title — e.g . , JFK , John F . Kennedy , John Fitzgerald Kennedy , Sena - tor John F . Kennedy , President Kennedy , Jack Kennedy , etc . ( Spitkovsky Chang , 2012 ) . common nouns , morphological parsing merge candidates morphological variants . merge evidence variant , combining scoring feature vectors merged candidates single vector . set candidates , feature vector . classifier takes feature vector assigns confidence value candidate answer . classifier trained thousands candidate answers , labeled correct incorrect , together feature vectors , learns predict probability correct answer . , training , far incorrect answers correct answers , need standard techniques dealing imbalanced data . DeepQA instance weighting , assigning instance weight . 5 incorrect answer example training . candidate answers sorted confidence value , resulting single best answer . 3 summary , seen four stages DeepQA draws intu - 3 merging ranking actually run iteratively ; first candidates ranked classifier , giving rough first value candidate answer , value decide variants name select merged answer , merged answers re-ranked . 18 CHAPTER 25 • QUESTION ANSWERING itions IR-based knowledge-based paradigms . Indeed , Watson’s ar - chitectural innovation reliance proposing large number candidate answers text-based knowledge-based sources developing wide variety evidence features scoring candidates — again text - based knowledge-based . papers mentioned end chapter details . 25.4 Evaluation Factoid Answers common evaluation metric factoid question answering , introduced TREC Q / track 1999 , mean reciprocal rank , MRR . MRR assumes test set ofmeanreciprocal rank MRR questions human-labeled correct answers . MRR assumes systems returning short ranked list answers passages containing - swers . question scored according reciprocal rank first correct answer . example system returned five answers first three wrong hence highest-ranked correct answer ranked fourth , reciprocal rank score question 14 . Questions return sets contain correct answers assigned zero . score system average score question set . formally , evaluation system returning set ranked answers test set consisting N questions , MRR defined MRR = 1 N N ∑ = 1 s.t . ranki 6 = 0 1 ranki ( 25.16 ) Reading comprehension systems datasets like SQuAD often evaluated two metrics , ignoring punctuation articles ( , , ) ( Rajpurkar et al . , 2016 ) : • Exact match : percentage predicted answers match gold answer exactly . • F1 score : average overlap predicted gold answers . Treat prediction gold bag tokens , compute F1 , averaging F1 questions . number test sets available question answering . Early systems TREC QA dataset ; questions handwritten answers TREC competitions 1999 2004 publicly available . TriviaQA ( Joshi et al . , 2017 ) 650K question-answer evidence triples , 95K hand-created question-answer pairs - gether average six supporting evidence documents collected retrospectively Wikipedia Web . Another family datasets starts WEBQUESTIONS ( Berant et al . , 2013 ) , contains 5,810 questions asked web users , beginning wh - word containing exactly entity . Questions paired handwritten - swers drawn Freebase page question’s entity . WEBQUESTIONSSP ( Yih et al . , 2016 ) augments WEBQUESTIONS human-created semantic parses ( SPARQL queries ) questions answerable Freebase . COMPLEXWEB - QUESTIONS augments dataset compositional kinds complex questions , resulting 34,689 questions , answers , web snippets , SPARQL queries . ( Talmor Berant , 2018 ) . BIBLIOGRAPHICAL HISTORICAL NOTES 19 wide variety datasets training testing reading comprehen - sion / answer extraction addition SQuAD ( Rajpurkar et al . , 2016 ) Wik - iQA ( Yang et al . , 2015 ) datasets discussed page 8 . NarrativeQA ( Kočiský et al . , 2018 ) dataset , example , questions based entire long documents like books movie scripts , Question Answering Context ( QuAC ) dataset ( Choi et al . , 2018 ) 100K questions created two crowd workers asking answering questions hidden Wikipedia text . Others take structure fact reading comprehension tasks de - signed children tend multiple choice , task choose among answers . MCTest dataset structure , 500 fictional short stories created crowd workers questions multiple choice answers ( Richard - son et al . , 2013 ) . AI2 Reasoning Challenge ( ARC ) ( Clark et al . , 2018 ) , questions designed hard answer simple lexical methods : property mineral determined just looking ? ( ) luster [ correct ] ( B ) mass ( C ) weight ( D ) hardness ARC example difficult correct answer luster unlikely cooc - cur frequently web phrases like looking , word mineral highly associated incorrect answer hardness . Bibliographical Historical Notes Question answering earliest NLP tasks , early versions text - based knowledge-based paradigms developed early 1960s . text-based algorithms generally relied simple parsing question sentences document , looking matches . approach early ( Phillips , 1960 ) perhaps complete early system , strikingly prefigures modern relation-based systems , Protosynthex sys - tem Simmons et al . ( 1964 ) . question , Protosynthex first formed query content words question , retrieved candidate answer sen - tences document , ranked frequency-weighted term overlap question . query retrieved sentence parsed dependency parsers , sentence whose structure best matches question structure se - lected . Thus question worms eat ? match worms eat grass : subject worms dependent eat , version dependency grammar time , birds eat worms birds subject : worms eat Worms eat grass Birds eat worms alternative knowledge-based paradigm implemented BASEBALL system ( Green et al . , 1961 ) . system answered questions baseball games like “ Red Sox play July 7 ” querying structured database game information . database stored kind attribute-value matrix values attributes game : Month = July Place = Boston 20 CHAPTER 25 • QUESTION ANSWERING Day = 7 Game Serial . = 96 ( Team = Red Sox , Score = 5 ) ( Team = Yankees , Score = 3 ) question constituency-parsed algorithm Zellig Harris’s TDAP project University Pennsylvania , essentially cascade finite - state transducers ( historical discussion Joshi Hopely 1999 Kart - tunen 1999 ) . content analysis phase word phrase associated program computed parts meaning . Thus phrase ‘ ’ code assign semantics Place = ? , result question “ Red Sox play July 7 ” assigned meaning Place = ? Team = Red Sox Month = July Day = 7 question matched against database return answer . Simmons ( 1965 ) summarizes early QA systems . Another important progenitor knowledge-based paradigm question - answering work predicate calculus meaning representation lan - guage . LUNAR system ( Woods et al . 1972 , Woods 1978 ) designed beLUNAR natural language interface database chemical facts lunar geology . answer questions like samples greater 13 percent aluminum parsing logical form ( TEST ( X16 / ( SEQ SAMPLES ) : T ; ( CONTAIN ’ X16 ( NPR * X17 / ( QUOTE AL203 ) ) ( GREATERTHAN 13 PCT ) ) ) ) rise web brought information-retrieval paradigm question - swering forefront TREC QA track beginning 1999 , leading wide variety factoid non-factoid systems competing annual evaluations . same time , Hirschman et al . ( 1999 ) introduced idea chil - dren’s reading comprehension tests evaluate machine text comprehension algo - rithms . acquired corpus 120 passages 5 questions designed 3rd-6th grade children , built answer extraction system , measured well answers system corresponded answer key test’s publisher . algorithm focused word overlap feature ; later algorithms added named entity features complex similarity question answer span ( Riloff Thelen 2000 , Ng et al . 2000 ) . Neural reading comprehension systems drew insight early sys - tems answer finding focus question-passage similarity . Many architectural outlines modern systems laid AttentiveReader ( - mann et al . , 2015 ) . idea passage-aligned question embeddings passage computation introduced Lee et al . ( 2017 ) . Seo et al . ( 2017 ) achieves high performance introducing bi-directional attention flow . Chen et al . ( 2017 ) Clark Gardner ( 2018 ) show extract answers entire documents . DeepQA component Watson system won Jeopardy ! challenge described series papers volume 56 IBM Journal Research Development ; example Ferrucci ( 2012 ) , Lally et al . ( 2012 ) , Chu-Carroll et al . ( 2012 ) , Murdock et al . ( 2012b ) , Murdock et al . ( 2012a ) , Kalyanpur et al . ( 2012 ) , Gondek et al . ( 2012 ) . EXERCISES 21 question-answering tasks include Quiz Bowl , timing consid - erations question interrupted ( Boyd-Graber et al . , 2018 ) . Question answering important function modern personal assistant dialog systems ; Chapter 26 . Exercises 22 Chapter 25 • Question Answering Berant , J . , Chou , . , Frostig , R . , Liang , P . ( 2013 ) . Se - mantic parsing freebase question-answer pairs . EMNLP 2013 . Berant , J . Liang , P . ( 2014 ) . Semantic parsing via para - phrasing . ACL 2014 . Bizer , C . , Lehmann , J . , Kobilarov , G . , Auer , S . , Becker , C . , Cyganiak , R . , Hellmann , S . ( 2009 ) . DBpedia — crys - tallization point Web Data . Web Semantics : sci - ence , services agents world wide web , 7 ( 3 ) , 154 – 165 . Bollacker , K . , Evans , C . , Paritosh , P . , Sturge , T . , Tay - lor , J . ( 2008 ) . Freebase : collaboratively created graph database structuring human knowledge . SIGMOD 2008 , 1247 – 1250 . Boyd-Graber , J . , Feng , S . , Rodriguez , P . ( 2018 ) . Human - computer question answering : case quizbowl . Escalera , S . Weimer , M . ( Eds . ) , NIPS ’ 17 Compe - tition : Building Intelligent Systems . Springer . Brill , E . , Dumais , S . T . , Banko , M . ( 2002 ) . analy - sis AskMSR question-answering system . EMNLP 2002 , 257 – 264 . Chang , . X . Manning , C . D . ( 2012 ) . SUTime : li - brary recognizing normalizing time expressions . . LREC-12 , 3735 – 3740 . Chen , D . , Fisch , . , Weston , J . , Bordes , . ( 2017 ) . Reading wikipedia answer open-domain questions . ACL 2017 . Choi , E . , , H . , Iyyer , M . , Yatskar , M . , Yih , W . - t . , Choi , Y . , Liang , P . , Zettlemoyer , L . ( 2018 ) . Quac : Question answering context . EMNLP 2018 . Chu-Carroll , J . , Fan , J . , Boguraev , B . K . , Carmel , D . , Shein - wald , D . , Welty , C . ( 2012 ) . Finding needles haystack : Search candidate generation . IBM Journal Research Development , 56 ( 3/4 ) , 6:1 – 6:12 . Clark , C . Gardner , M . ( 2018 ) . Simple effective multi-paragraph reading comprehension . ACL 2018 . Clark , P . , Cowhey , . , Etzioni , O . , Khot , T . , Sabharwal , . , Schoenick , C . , Tafjord , O . ( 2018 ) . Think solved question answering ? Try ARC , AI2 reasoning challenge . . arXiv preprint arXiv : 1803.05457 . Devlin , J . , Chang , M . - W . , Lee , K . , Toutanova , K . ( 2019 ) . BERT : Pre-training deep bidirectional transformers language understanding . NAACL HLT 2019 , 4171 – 4186 . Fader , . , Soderland , S . , Etzioni , O . ( 2011 ) . Identifying relations open information extraction . EMNLP-11 , 1535 – 1545 . Fader , . , Zettlemoyer , L . , Etzioni , O . ( 2013 ) . Paraphrase-driven learning open question answering . ACL 2013 , 1608 – 1618 . Ferrucci , D . . ( 2012 ) . Introduction “ Watson ” . IBM Journal Research Development , 56 ( 3/4 ) , 1:1 – 1:15 . Gondek , D . C . , Lally , . , Kalyanpur , . , Murdock , J . W . , Duboué , P . . , Zhang , L . , Pan , Y . , Qiu , Z . M . , Welty , C . ( 2012 ) . framework merging ranking - swers deepqa . IBM Journal Research Develop - ment , 56 ( 3/4 ) , 14:1 – 14:12 . Green , B . F . , Wolf , . K . , Chomsky , C . , Laughery , K . ( 1961 ) . Baseball : automatic question answerer . Proceedings Western Joint Computer Conference 19 , 219 – 224 . Reprinted Grosz et al . ( 1986 ) . Harabagiu , S . , Pasca , M . , Maiorano , S . ( 2000 ) . Exper - iments open-domain textual question answering . COLING-00 . Hermann , K . M . , Kocisky , T . , Grefenstette , E . , Espeholt , L . , Kay , W . , Suleyman , M . , Blunsom , P . ( 2015 ) . Teaching machines read comprehend . Advances Neural Information Processing Systems , 1693 – 1701 . Hirschman , L . , Light , M . , Breck , E . , Burger , J . D . ( 1999 ) . Deep Read : reading comprehension system . ACL-99 , 325 – 332 . Hovy , E . H . , Hermjakob , U . , Ravichandran , D . ( 2002 ) . question / answer typology surface text patterns . HLT-01 . Joshi , . K . Hopely , P . ( 1999 ) . parser antiq - uity . Kornai , . ( Ed . ) , Extended Finite State Models Language , 6 – 15 . Cambridge University Press . Joshi , M . , Choi , E . , Weld , D . S . , Zettlemoyer , L . ( 2017 ) . Triviaqa : large scale distantly supervised chal - lenge dataset reading comprehension . ACL 2017 . Kalyanpur , . , Boguraev , B . K . , Patwardhan , S . , Murdock , J . W . , Lally , . , Welty , C . , Prager , J . M . , Coppola , B . , Fokoue-Nkoutche , . , Zhang , L . , Pan , Y . , Qiu , Z . M . ( 2012 ) . Structured data inference deepqa . IBM Jour - nal Research Development , 56 ( 3/4 ) , 10:1 – 10:14 . Karttunen , L . ( 1999 ) . Comments Joshi . Kornai , . ( Ed . ) , Extended Finite State Models Language , 16 – 18 . Cambridge University Press . Kočiský , T . , Schwarz , J . , Blunsom , P . , Dyer , C . , Hermann , K . M . , Melis , G . , Grefenstette , E . ( 2018 ) . Narra - tiveQA reading comprehension challenge . TACL , 6 , 317 – 328 . Lally , . , Prager , J . M . , McCord , M . C . , Boguraev , B . K . , Patwardhan , S . , Fan , J . , Fodor , P . , Chu-Carroll , J . ( 2012 ) . Question analysis : Watson reads clue . IBM Journal Research Development , 56 ( 3/4 ) , 2:1 – 2:14 . Lee , K . , Salant , S . , Kwiatkowski , T . , Parikh , . , Das , D . , Berant , J . ( 2017 ) . Learning recurrent span representations extractive question answering . arXiv 1611.01436 . Li , X . Roth , D . ( 2002 ) . Learning question classifiers . COLING-02 , 556 – 562 . Li , X . Roth , D . ( 2005 ) . Learning question classifiers : role semantic information . Journal Natural Lan - guage Engineering , 11 ( 4 ) . Lin , J . ( 2007 ) . exploration principles underlying redundancy-based factoid question answering . ACM Trans - actions Information Systems , 25 ( 2 ) . Monz , C . ( 2004 ) . Minimal span weighting retrieval ques - tion answering . SIGIR Workshop Information Re - trieval Question Answering , 23 – 30 . Murdock , J . W . , Fan , J . , Lally , . , Shima , H . , Boguraev , B . K . ( 2012a ) . Textual evidence gathering analysis . IBM Journal Research Development , 56 ( 3/4 ) , 8:1 – 8:14 . Exercises 23 Murdock , J . W . , Kalyanpur , . , Welty , C . , Fan , J . , Fer - rucci , D . . , Gondek , D . C . , Zhang , L . , Kanayama , H . ( 2012b ) . Typing candidate answers type coercion . IBM Journal Research Development , 56 ( 3/4 ) , 7:1 – 7:13 . Ng , H . T . , Teo , L . H . , Kwan , J . L . P . ( 2000 ) . machine learning approach answering questions reading com - prehension tests . EMNLP 2000 , 124 – 132 . Pasca , M . ( 2003 ) . Open-Domain Question Answering Large Text Collections . CSLI . Pennington , J . , Socher , R . , Manning , C . D . ( 2014 ) . Glove : Global vectors word representation . EMNLP 2014 , 1532 – 1543 . Phillips , . V . ( 1960 ) . question-answering routine . Tech . rep . 16 , MIT AI Lab . Rajpurkar , P . , Jia , R . , Liang , P . ( 2018 ) . Know know : Unanswerable questions SQuAD . ACL 2018 . Rajpurkar , P . , Zhang , J . , Lopyrev , K . , Liang , P . ( 2016 ) . SQuAD : 100,000 + questions machine comprehension text . EMNLP 2016 . Richardson , M . , Burges , C . J . C . , Renshaw , E . ( 2013 ) . MCTest : challenge dataset open-domain machine comprehension text . EMNLP 2013 , 193 – 203 . Riloff , E . Thelen , M . ( 2000 ) . rule-based question - swering system reading comprehension tests . Pro - ceedings ANLP / NAACL workshop reading compre - hension tests , 13 – 19 . Seo , M . , Kembhavi , . , Farhadi , . , Hajishirzi , H . ( 2017 ) . Bidirectional attention flow machine compre - hension . ICLR 2017 . Simmons , R . F . ( 1965 ) . Answering English questions computer : survey . CACM , 8 ( 1 ) , 53 – 70 . Simmons , R . F . , Klein , S . , McConlogue , K . ( 1964 ) . - dexing dependency logic answering english ques - tions . American Documentation , 15 ( 3 ) , 196 – 204 . Spitkovsky , V . . Chang , . X . ( 2012 ) . cross-lingual dictionary English Wikipedia concepts . LREC-12 . Talmor , . Berant , J . ( 2018 ) . web knowledge - base answering complex questions . NAACL HLT 2018 . Woods , W . . ( 1978 ) . Semantics quantification natu - ral language question answering . Yovits , M . ( Ed . ) , Ad - vances Computers , 2 – 64 . Academic . Woods , W . . , Kaplan , R . M . , Nash-Webber , B . L . ( 1972 ) . lunar sciences natural language information system : Final report . Tech . rep . 2378 , BBN . Yang , Y . , Yih , W . - t . , Meek , C . ( 2015 ) . Wikiqa : challenge dataset open-domain question answering . EMNLP 2015 . Yih , W . - t . , Richardson , M . , Meek , C . , Chang , M . - W . , Suh , J . ( 2016 ) . value semantic parse labeling knowledge base question answering . ACL 2016 , 201 – 206 . Zelle , J . M . Mooney , R . J . ( 1996 ) . Learning parse database queries inductive logic programming . AAAI-96 , 1050 – 1055 . Zettlemoyer , L . Collins , M . ( 2005 ) . Learning map sentences logical form : Structured classification probabilistic categorial grammars . Uncertainty Ar - tificial Intelligence , UAI ’ 05 , 658 – 666 .