Chapter 19 Text generation In many of the most interesting problems in natural language processing , language is the output . The previous chapter described the specific case of machine translation , but there are many other applications , from summarization of research articles , to automated journalism , to dialogue systems . This chapter emphasizes three main scenarios : data-to - text , in which text is generated to explain or describe a structured record or unstructured perceptual input ; text-to-text , which typically involves fusing information from multiple linguistic sources into a single coherent summary ; and dialogue , in which text is generated as part of an interactive conversation with one or more human participants . 19.1 Data-to-text generation In data-to-text generation , the input ranges from structured records , such as the descrip - tion of an weather forecast ( as shown in Figure 19.1 ) , to unstructured perceptual data , such as a raw image or video ; the output may be a single sentence , such as an image cap - tion , or a multi-paragraph argument . Despite this diversity of conditions , all data-to-text systems share some of the same challenges ( Reiter and Dale , 2000 ) : • determining what parts of the data to describe ; • planning a presentation of this information ; • lexicalizing the data into words and phrases ; • organizing words and phrases into well-formed sentences and paragraphs . The earlier stages of this process are sometimes called content selection and text plan - ning ; the later stages are often called surface realization . Early systems for data-to-text generation were modular , with separate software com - ponents for each task . Artificial intelligence planning algorithms can be applied to both 457 458 CHAPTER 19 . TEXT GENERATION Temperature time min mean max 06:00-21:00 9 15 21 Cloud sky cover time percent ( % ) 06:00-09:00 25-50 09:00-12:00 50-75 Wind speed time min mean max 06:00-21:00 15 20 30 Wind direction time mode 06:00-21:00 S Cloudy , with temperatures between 10 and 20 degrees . South wind around 20 mph . Figure 19.1 : An example input-output pair for the task of generating text descriptions of weather forecasts ( adapted from Konstas and Lapata , 2013 ) . the high-level information structure and the organization of individual sentences , ensur - ing that communicative goals are met ( McKeown , 1992 ; Moore and Paris , 1993 ) . Surface realization can be performed by grammars or templates , which link specific types of data to candidate words and phrases . A simple example template is offered by Wiseman et al . ( 2017 ) , for generating descriptions of basketball games : ( 19.1 ) The < team1 > ( < wins1 > - losses1 ) defeated the < team2 > ( < wins2 > - < losses2 > ) , < pts1 > - < pts2 > . The New York Knicks ( 45-5 ) defeated the Boston Celtics ( 11-38 ) , 115-79 . For more complex cases , it may be necessary to apply morphological inflections such as pluralization and tense marking — even in the simple example above , languages such as Russian would require case marking suffixes for the team names . Such inflections can be applied as a postprocessing step . Another difficult challenge for surface realization is the generation of varied referring expressions ( e.g . , The Knicks , New York , they ) , which is critical to avoid repetition . As discussed in subsection 16.2.1 , the form of referring expressions is constrained by the discourse and information structure . An example at the intersection of rule-based and statistical techniques is the NITRO - GEN system ( Langkilde and Knight , 1998 ) . The input to NITROGEN is an abstract meaning representation ( AMR ; see section 13.3 ) of semantic content to be expressed in a single sen - tence . In data-to-text scenarios , the abstract meaning representation is the output of a higher-level text planning stage . A set of rules then converts the abstract meaning repre - sentation into various sentence plans , which may differ in both the high-level structure ( e.g . , active versus passive voice ) as well as the low-level details ( e.g . , word and phrase choice ) . Some examples are shown in Figure 19.2 . To control the combinatorial explosion in the number of possible realizations for any given meaning , the sentence plans are uni - fied into a single finite-state acceptor , in which word tokens are represented by arcs ( see Jacob Eisenstein . Draft of October 15 , 2018 . 19.1 . DATA-TO-TEXT GENERATION 459 ( a / admire-01 : ARG0 ( v / visitor : ARG1-of ( c / arrive-01 : ARG4 ( j / Japan ) ) ) : ARG1 ( m / " Mount Fuji " ) ) • Visitors who came to Japan admire Mount Fuji . • Visitors who came in Japan admire Mount Fuji . • Mount Fuji is admired by the visitor who came in Japan . Figure 19.2 : Abstract meaning representation and candidate surface realizations from the NITROGEN system . Example adapted from Langkilde and Knight ( 1998 ) . subsection 9.1.1 ) . A bigram language model is then used to compute weights on the arcs , so that the shortest path is also the surface realization with the highest bigram language model probability . More recent systems are unified models that are trained end-to-end using backpropa - gation . Data-to-text generation shares many properties with machine translation , includ - ing a problem of alignment : labeled examples provide the data and the text , but they do not specify which parts of the text correspond to which parts of the data . For example , to learn from Figure 19.1 , the system must align the word cloudy to records in CLOUD SKY COVER , the phrases 10 and 20 degrees to the MIN and MAX fields in TEMPERATURE , and so on . As in machine translation , both latent variables and neural attention have been proposed as solutions . 19.1.1 Latent data-to-text alignment Given a dataset of texts and associated records { ( w ( i ) , y ( i ) ) } Ni = 1 , our goal is to learn a model Ψ , so that ŵ = argmax w ∈ V ∗ Ψ ( w , y ; θ ) , [ 19.1 ] where V ∗ is the set of strings over a discrete vocabulary , and θ is a vector of parameters . The relationship between w and y is complex : the data y may contain dozens of records , and w may extend to several sentences . To facilitate learning and inference , it would be helpful to decompose the scoring function Ψ into subcomponents . This would be possi - ble if given an alignment , specifying which element of y is expressed in each part of w . Specifically , let zm indicates the record aligned to word m . For example , in Figure 19.1 , z1 might specify that the word cloudy is aligned to the record cloud-sky-cover : percent . The score for this alignment would then be given by the weight on features such as ( cloudy , cloud-sky-cover : percent ) . [ 19.2 ] In general , given an observed set of alignments , the score for a generation can be Under contract with MIT Press , shared under CC-BY-NC-ND license . 460 CHAPTER 19 . TEXT GENERATION written as sum of local scores ( Angeli et al . , 2010 ) : Ψ ( w , y ; θ ) = M ∑ m = 1 ψw , y ( wm , yzm ) + ψw ( wm , wm − 1 ) + ψz ( zm , zm − 1 ) , [ 19.3 ] where ψw can represent a bigram language model , and ψz can be tuned to reward coher - ence , such as the use of related records in nearby words . 1 The parameters of this model could be learned from labeled data { ( w ( i ) , y ( i ) , z ( i ) ) } Ni = 1 . However , while several datasets include structured records and natural language text ( Barzilay and McKeown , 2005 ; Chen and Mooney , 2008 ; Liang and Klein , 2009 ) , the alignments between text and records are usually not available . 2 One solution is to model the problem probabilistically , treating the alignment as a latent variable ( Liang et al . , 2009 ; Konstas and Lapata , 2013 ) . The model can then be estimated using expectation maximization or sampling ( see chapter 5 ) . 19.1.2 Neural data-to-text generation The encoder-decoder model and neural attention were introduced in section 18.3 as methods for neural machine translation . They can also be applied to data-to-text gen - eration , with the data acting as the source language ( Mei et al . , 2016 ) . In neural machine translation , the attention mechanism linked words in the source to words in the target ; in data-to-text generation , the attention mechanism can link each part of the generated text back to a record in the data . The biggest departure from translation is in the encoder , which depends on the form of the data . Data encoders In some types of structured records , all values are drawn from discrete sets . For example , the birthplace of an individual is drawn from a discrete set of possible locations ; the diag - nosis and treatment of a patient are drawn from an exhaustive list of clinical codes ( John - son et al . , 2016 ) . In such cases , vector embeddings can be estimated for each field and possible value : for example , a vector embedding for the field BIRTHPLACE , and another embedding for the value BERKELEY CALIFORNIA ( Bordes et al . , 2011 ) . The table of such embeddings serves as the encoding of a structured record ( He et al . , 2017 ) . It is also possi - ble to compress the entire table into a single vector representation , by pooling across the embeddings of each field and value ( Lebret et al . , 2016 ) . 1More expressive decompositions of Ψ are possible . For example , Wong and Mooney ( 2007 ) use a syn - chronous context-free grammar ( see subsection 18.2.4 ) to “ translate ” between a meaning representation and natural language text . 2An exception is a dataset of records and summaries from American football games , containing annota - tions of alignments between sentences and records ( Snyder and Barzilay , 2007 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 19.1 . DATA-TO-TEXT GENERATION 461 Figure 19.3 : Examples of the image captioning task , with attention masks shown for each of the underlined words ( Xu et al . , 2015 ) . Sequences Some types of structured records have a natural ordering , such as events in a game ( Chen and Mooney , 2008 ) and steps in a recipe ( Tutin and Kittredge , 1992 ) . For example , the following records describe a sequence of events in a robot soccer match ( Mei et al . , 2016 ) : PASS ( arg1 = PURPLE6 , arg2 = PURPLE3 ) KICK ( arg1 = PURPLE3 ) BADPASS ( arg1 = PURPLE3 , arg2 = PINK9 ) . Each event is a single record , and can be encoded by a concatenation of vector represen - tations for the event type ( e.g . , PASS ) , the field ( e.g . , arg1 ) , and the values ( e.g . , PURPLE3 ) , e.g . , X = [ uPASS , uarg1 , uPURPLE6 , uarg2 , uPURPLE3 ] . [ 19.4 ] This encoding can then act as the input layer for a recurrent neural network , yielding a sequence of vector representations { zr } Rr = 1 , where r indexes over records . Interestingly , this sequence-based approach can work even in cases where there is no natural ordering over the records , such as the weather data in Figure 19.1 ( Mei et al . , 2016 ) . Images Another flavor of data-to-text generation is the generation of text captions for images . Examples from this task are shown in Figure 19.3 . Images are naturally repre - sented as tensors : a color image of 320 × 240 pixels would be stored as a tensor with 320 × 240 × 3 intensity values . The dominant approach to image classification is to en - code images as vectors using a combination of convolution and pooling ( Krizhevsky et al . , Under contract with MIT Press , shared under CC-BY-NC-ND license . 462 CHAPTER 19 . TEXT GENERATION a 20 % ch an ce of sh ow er s an d th un de rs to rm s af te r no on . m os tly clo ud y wi th a hi gh ne ar 71 . id-0 : temperature ( min = 52 , max = 71 , mean = 63 ) id-2 : windSpeed ( min = 8 , mean = 17 , max = 23 ) id-5 : skyCover ( mode = 50-75 ) id-10 : precipChance ( min = 19 , mean = 32 , max = 73 ) id-15 : thunderChance ( mode = SChc ) Figure 19.4 : Neural attention in text generation . Figure adapted from Mei et al . ( 2016 ) . 2012 ) . Chapter 3 explains how to use convolutional networks for text ; for images , convo - lution is applied across the vertical , horizontal , and color dimensions . By pooling the re - sults of successive convolutions , the image is converted to a vector representation , which can then be fed directly into the decoder as the initial state ( Vinyals et al . , 2015 ) , just as in the sequence-to-sequence translation model ( see section 18.3 ) . Alternatively , one can apply a set of convolutional networks , yielding vector representations for different parts of the image , which can then be combined using neural attention ( Xu et al . , 2015 ) . Attention Given a set of embeddings of the data { zr } Rr = 1 and a decoder state hm , an attention vector over the data can be computed using the same techniques as in machine translation ( see subsection 18.3.1 ) . When generating word m of the output , attention is computed over the records , ψα ( m , r ) = βα · f ( Θα [ hm ; zr ] ) [ 19.5 ] αm = g ( [ ψα ( m , 1 ) , ψα ( m , 2 ) , . . . , ψα ( m , R ) ] ) [ 19.6 ] cm = R ∑ r = 1 αm → rzr , [ 19.7 ] where f is an elementwise nonlinearity such as tanh or ReLU , and g is a either softmax or elementwise sigmoid . The weighted sum cm can then be included in the recurrent update to the decoder state , or in the emission probabilities , as described in subsection 18.3.1 . Figure 19.4 shows the attention to components of a weather record , while generating the text shown on the x-axis . Adapting this architecture to image captioning is straightforward . A convolutional neural networks is applied to a set of image locations , and the output at each location ` is Jacob Eisenstein . Draft of October 15 , 2018 . 19.1 . DATA-TO-TEXT GENERATION 463 represented with a vector z ` . Attention can then be computed over the image locations , as shown in the right panels of each pair of images in Figure 19.3 . Various modifications to this basic mechanism have been proposed . In coarse-to-fine attention ( Mei et al . , 2016 ) , each record receives a global attention ar ∈ [ 0 , 1 ] , which is in - dependent of the decoder state . This global attention , which represents the overall impor - tance of the record , is multiplied with the decoder-based attention scores , before comput - ing the final normalized attentions . In structured attention , the attention vectorαm → · can include structural biases , which can favor assigning higher attention values to contiguous segments or to dependency subtrees ( Kim et al . , 2017 ) . Structured attention vectors can be computed by running the forward-backward algorithm to obtain marginal attention probabilities ( see section 7.5.3 ) . Because each step in the forward-backward algorithm is differentiable , it can be encoded in a computation graph , and end-to-end learning can be performed by backpropagation . Decoder Given the encoding , the decoder can function just as in neural machine translation ( see subsection 18.3.1 ) , using the attention-weighted encoder representation in the decoder recurrence and / or output computation . As in machine translation , beam search can help to avoid search errors ( Lebret et al . , 2016 ) . Many applications require generating words that do not appear in the training vocab - ulary . For example , a weather record may contain a previously unseen city name ; a sports record may contain a previously unseen player name . Such tokens can be generated in the text by copying them over from the input ( e.g . , Gulcehre et al . , 2016 ) . 3 First introduce an additional variable sm ∈ { gen , copy } , indicating whether token w ( t ) m should be generated or copied . The decoder probability is then , p ( w ( t ) | w ( t ) 1 : m − 1 , Z , sm ) = { SoftMax ( βw ( t ) · h ( t ) m − 1 ) , sm = gen ∑ R r = 1 δ ( w ( s ) r = w ( t ) ) × αm → r , sm = copy , [ 19.8 ] where δ ( w ( s ) r = w ( t ) ) is an indicator function , taking the value 1 iff the text of the record w ( s ) r is identical to the target wordw ( t ) . The probability of copying record r from the source is δ ( sm = copy ) × αm → r , the product of the copy probability by the local attention . Note that in this model , the attention weights αm are computed from the previous decoder state hm − 1 . The computation graph therefore remains a feedforward network , with recurrent paths such as h ( t ) m − 1 → αm → w ( t ) m → h ( t ) m . To facilitate end-to-end training , the switching variable sm can be represented by a gate πm , which is computed from a two-layer feedforward network , whose input consists 3A number of variants of this strategy have been proposed ( e.g . , Gu et al . , 2016 ; Merity et al . , 2017 ) . See Wiseman et al . ( 2017 ) for an overview . Under contract with MIT Press , shared under CC-BY-NC-ND license . 464 CHAPTER 19 . TEXT GENERATION of the concatenation of the decoder state h ( t ) m − 1 and the attention-weighted representation of the data , cm = ∑ R r = 1 αm → rzr , πm = σ ( Θ ( 2 ) f ( Θ ( 1 ) [ h ( t ) m − 1 ; cm ] ) ) . [ 19.9 ] The full generative probability at token m is then , p ( w ( t ) | w ( t ) 1 : m , Z ) = πm × expβw ( t ) · h ( t ) m − 1 ∑ V j = 1 expβj · h ( t ) m − 1 ︸ ︷ ︷ ︸ generate + ( 1 − πm ) × R ∑ r = 1 δ ( w ( s ) r = w ( t ) ) × αm → r ︸ ︷ ︷ ︸ copy . [ 19.10 ] 19.2 Text-to-text generation Text-to-text generation includes problems of summarization and simplification : • reading a novel and outputting a paragraph-long summary of the plot ; 4 • reading a set of blog posts about politics , and outputting a bullet list of the various issues and perspectives ; • reading a technical research article about the long-term health consequences of drink - ing kombucha , and outputting a summary of the article in language that non-experts can understand . These problems can be approached in two ways : through the encoder-decoder architec - ture discussed in the previous section , or by operating directly on the input text . 19.2.1 Neural abstractive summarization Sentence summarization is the task of shortening a sentence while preserving its mean - ing , as in the following examples ( Knight and Marcu , 2000 ; Rush et al . , 2015 ) : ( 19.2 ) a . The documentation is typical of Epson quality : excellent . Documentation is excellent . 4In section 16.3.4 , we encountered a special case of single-document summarization , which involved extracting the most important sentences or discourse units . We now consider the more challenging problem of abstractive summarization , in which the summary can include words that do not appear in the original text . Jacob Eisenstein . Draft of October 15 , 2018 . 19.2 . TEXT-TO-TEXT GENERATION 465 b . Russian defense minister Ivanov called sunday for the creation of a joint front for combating global terrorism . Russia calls for joint front against terrorism . Sentence summarization is closely related to sentence compression , in which the sum - mary is produced by deleting words or phrases from the original ( Clarke and Lapata , 2008 ) . But as shown in ( 19.2b ) , a sentence summary can also introduce new words , such as against , which replaces the phrase for combatting . Sentence summarization can be treated as a machine translation problem , using the attentional encoder-decoder translation model discussed in subsection 18.3.1 ( Rush et al . , 2015 ) . The longer sentence is encoded into a sequence of vectors , one for each token . The decoder then computes attention over these vectors when updating its own recurrent state . As with data-to-text generation , it can be useful to augment the encoder-decoder model with the ability to copy words directly from the source . Rush et al . ( 2015 ) train this model by building four million sentence pairs from news articles . In each pair , the longer sentence is the first sentence of the article , and the summary is the article headline . Sentence summarization can also be trained in a semi-supervised fashion , using a proba - bilistic formulation of the encoder-decoder model called a variational autoencoder ( Miao and Blunsom , 2016 , also see subsection 14.8.2 ) . When summarizing longer documents , an additional concern is that the summary not be repetitive : each part of the summary should cover new ground . This can be addressed by maintaining a vector of the sum total of all attention values thus far , tm = ∑ m n = 1αn . This total can be used as an additional input to the computation of the attention weights , αm → n ∝ exp ( vα · tanh ( Θα [ h ( t ) m ; h ( s ) n ; tm ] ) ) , [ 19.11 ] which enables the model to learn to prefer parts of the source which have not been at - tended to yet ( Tu et al . , 2016 ) . To further encourage diversity in the generated summary , See et al . ( 2017 ) introduce a coverage loss to the objective function , ` m = M ( s ) ∑ n = 1 min ( αm → n , tm → n ) . [ 19.12 ] This loss will be low ifαmassigns little attention to words that already have large values in tm . Coverage loss is similar to the concept of marginal relevance , in which the reward for adding new content is proportional to the extent to which it increases the overall amount of information conveyed by the summary ( Carbonell and Goldstein , 1998 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 466 CHAPTER 19 . TEXT GENERATION 19.2.2 Sentence fusion for multi-document summarization In multi-document summarization , the goal is to produce a summary that covers the content of several documents ( McKeown et al . , 2002 ) . One approach to this challenging problem is to identify sentences across multiple documents that relate to a single theme , and then to fuse them into a single sentence ( Barzilay and McKeown , 2005 ) . As an exam - ple , consider the following two sentences ( McKeown et al . , 2010 ) : ( 19.3 ) a . Palin actually turned against the bridge project only after it became a national symbol of wasteful spending . b . Ms . Palin supported the bridge project while running for governor , and abandoned it after it became a national scandal . An intersection preserves only the content that is present in both sentences : ( 19.4 ) Palin turned against the bridge project after it became a national scandal . A union includes information from both sentences : ( 19.5 ) Ms . Palin supported the bridge project while running for governor , but turned against it when it became a national scandal and a symbol of wasteful spending . Dependency parsing is often used as a technique for sentence fusion . After parsing each sentence , the resulting dependency trees can be aggregated into a lattice ( Barzilay and McKeown , 2005 ) or a graph structure ( Filippova and Strube , 2008 ) , in which identical or closely related words ( e.g . , Palin , bridge , national ) are fused into a single node . The resulting graph can then be pruned back to a tree by solving an integer linear program ( see subsection 13.2.2 ) , max y ∑ i , j , r ψ ( i r − → j , w ; θ ) × yi , j , r [ 19.13 ] s.t . y ∈ C , [ 19.14 ] where the variable yi , j , r ∈ { 0 , 1 } indicates whether there is an edge from i to j of type r , the score of this edge is ψ ( i r − → j , w ; θ ) , and C is a set of constraints , which ensures that y forms a valid dependency graph . As usual , w is the list of words in the graph , and θ is a vector of parameters . The score ψ ( i r − → j , w ; θ ) reflects the “ importance ” of the modifier j to the overall meaning : in intersective fusion , this score indicates the extent to which the content in this edge is expressed in all sentences ; in union fusion , the score indicates whether the content in the edge is expressed in any sentence . The constraint set C can impose additional linguistic constraints : for example , ensuring that coordinated nouns are sufficiently similar . The resulting tree must then be linearized into a sentence . Lin - earization is like the inverse of dependency parsing : instead of parsing from a sequence Jacob Eisenstein . Draft of October 15 , 2018 . 19.3 . DIALOGUE 467 of tokens into a tree , we must convert the tree back into a sequence of tokens . This is typically done by generating a set of candidate linearizations , and choosing the one with the highest score under a language model ( Langkilde and Knight , 1998 ; Song et al . , 2016 ) . 19.3 Dialogue Dialogue systems are capable of conversing with a human interlocutor , often to per - form some task ( Grosz , 1979 ) , but sometimes just to chat ( Weizenbaum , 1966 ) . While re - search on dialogue systems goes back several decades ( Carbonell , 1970 ; Winograd , 1972 ) , commercial systems such as Alexa and Siri have recently brought this technology into widespread use . Nonetheless , there is a significant gap between research and practice : many practical dialogue systems remain scripted and inflexible , while research systems emphasize abstractive text generation , “ on-the-fly ” decision making , and probabilistic reasoning about the user’s intentions . 19.3.1 Finite-state and agenda-based dialogue systems Finite-state automata were introduced in chapter 9 as a formal model of computation , in which string inputs and outputs are linked to transitions between a finite number of discrete states . This model naturally fits simple task-oriented dialogues , such as the one shown in the left panel of Figure 19.5 . This ( somewhat frustrating ) dialogue can be repre - sented with a finite-state transducer , as shown in the right panel of the figure . The accept - ing state is reached only when the two needed pieces of information are provided , and the human user confirms that the order is correct . In this simple scenario , the TOPPING and ADDRESS are the two slots associated with the activity of ordering a pizza , which is called a frame . Frame representations can be hierarchical : for example , an ADDRESS could have slots of its own , such as STREET and CITY . In the example dialogue in Figure 19.5 , the user provides the precise inputs that are needed in each turn ( e.g . , anchovies ; the College of Computing building ) . Some users may prefer to communicate more naturally , with phrases like I’d , uh , like some anchovies please . One approach to handling such utterances is to design a custom grammar , with non - terminals for slots such as TOPPING and LOCATION . However , context-free parsing of unconstrained speech input is challenging . A more lightweight alternative is BIO-style sequence labeling ( see section 8.3 ) , e.g . : ( 19.7 ) I’d O like O anchovies B-TOPPING , O and O please O bring O it O to O the B-ADDR College I-ADDR of I-ADDR Computing I-ADDR Building I-ADDR . O Under contract with MIT Press , shared under CC-BY-NC-ND license . 468 CHAPTER 19 . TEXT GENERATION ( 19.6 ) A : I want to order a pizza . B : What toppings ? A : Anchovies . B : Ok , what address ? A : The College of Computing building . B : Please confirm : one pizza with artichokes , to be delivered to the College of Computing building . A : No . B : What toppings ? . . . q0start q1 q2 q3 q4 q5 q6 What toppings ? Topping What address ? Address Confirm ? No Yes Figure 19.5 : An example dialogue and the associated finite-state model . In the finite-state model , SMALL CAPS indicates that the user must provide information of this type in their answer . The tagger can be driven by a bi-directional recurrent neural network , similar to recurrent approaches to semantic role labeling described in subsection 13.2.3 . The input in ( 19.7 ) could not be handled by the finite-state system from Figure 19.5 , which forces the user to provide the topping first , and then the location . In this sense , the “ initiative ” is driven completely by the system . Agenda-based dialogue systems extend finite-state architectures by attempting to recognize all slots that are filled by the user’s re - ply , thereby handling these more complex examples . Agenda-based systems dynamically pose additional questions until the frame is complete ( Bobrow et al . , 1977 ; Allen et al . , 1995 ; Rudnicky and Xu , 1999 ) . Such systems are said to be mixed-initiative , because both the user and the system can drive the direction of the dialogue . 19.3.2 Markov decision processes The task of dynamically selecting the next move in a conversation is known as dialogue management . This problem can be framed as a Markov decision process , which is a theoretical model that includes a discrete set of states , a discrete set of actions , a function that computes the probability of transitions between states , and a function that computes the cost or reward of action-state pairs . Let’s see how each of these elements pertains to the pizza ordering dialogue system . • Each state is a tuple of information about whether the topping and address are Jacob Eisenstein . Draft of October 15 , 2018 . 19.3 . DIALOGUE 469 known , and whether the order has been confirmed . For example , ( KNOWN TOPPING , UNKNOWN ADDRESS , NOT CONFIRMED ) [ 19.15 ] is a possible state . Any state in which the pizza order is confirmed is a terminal state , and the Markov decision process stops after entering such a state . • The set of actions includes querying for the topping , querying for the address , and requesting confirmation . Each action induces a probability distribution over states , p ( st | at , st − 1 ) . For example , requesting confirmation of the order is not likely to result in a transition to the terminal state if the topping is not yet known . This probability distribution over state transitions may be learned from data , or it may be specified in advance . • Each state-action-state tuple earns a reward , ra ( st , st + 1 ) . In the context of the pizza ordering system , a simple reward function would be , ra ( st , st − 1 ) =      0 , a = CONFIRM , st = ( * , * , CONFIRMED ) − 10 , a = CONFIRM , st = ( * , * , NOT CONFIRMED ) − 1 , a 6 = CONFIRM [ 19.16 ] This function assigns zero reward for successful transitions to the terminal state , a large negative reward to a rejected request for confirmation , and a small negative re - ward for every other type of action . The system is therefore rewarded for reaching the terminal state in few steps , and penalized for prematurely requesting confirma - tion . In a Markov decision process , a policy is a function π : S → A that maps from states to actions ( see section 15.2.4 ) . The value of a policy is the expected sum of discounted rewards , Eπ [ ∑ T t = 1 γ trat ( st , st + 1 ) ] , where γ is the discount factor , γ ∈ [ 0 , 1 ) . Discounting has the effect of emphasizing rewards that can be obtained immediately over less certain rewards in the distant future . An optimal policy can be obtained by dynamic programming , by iteratively updating the value function V ( s ) , which is the expectation of the cumulative reward from s under the optimal action a , V ( s ) ← max a ∈ A ∑ s ′ ∈ S p ( s ′ | s , a ) [ ra ( s , s ′ ) + γV ( s ′ ) ] . [ 19.17 ] The value function V ( s ) is computed in terms of V ( s ′ ) for all states s ′ ∈ S . A series of iterative updates to the value function will eventually converge to a stationary point . This algorithm is known as value iteration . Given the converged value function V ( s ) , the Under contract with MIT Press , shared under CC-BY-NC-ND license . 470 CHAPTER 19 . TEXT GENERATION optimal action at each state is the argmax , π ( s ) = argmax a ∈ A ∑ s ′ ∈ S p ( s ′ | s , a ) [ ra ( s , s ′ ) + γV ( s ′ ) ] . [ 19.18 ] Value iteration and related algorithms are described in detail by Sutton and Barto ( 1998 ) . For applications to dialogue systems , see Levin et al . ( 1998 ) and Walker ( 2000 ) . The Markov decision process framework assumes that the current state of the dialogue is known . In reality , the system may misinterpret the user’s statements — for example , believing that a specification of the delivery location ( PEACHTREE ) is in fact a specification of the topping ( PEACHES ) . In a partially observable Markov decision process ( POMDP ) , the system receives an observation o , which is probabilistically conditioned on the state , p ( o | s ) . It must therefore maintain a distribution of beliefs about which state it is in , with qt ( s ) indicating the degree of belief that the dialogue is in state s at time t . The POMDP formulation can help to make dialogue systems more robust to errors , particularly in the context of spoken language dialogues , where the speech itself may be misrecognized ( Roy et al . , 2000 ; Williams and Young , 2007 ) . However , finding the optimal policy in a POMDP is computationally intractable , requiring additional approximations . 19.3.3 Neural chatbots It’s easier to talk when you don’t need to get anything done . Chatbots are systems that parry the user’s input with a response that keeps the conversation going . They can be built from the encoder-decoder architecture discussed in section 18.3 and subsec - tion 19.1.2 : the encoder converts the user’s input into a vector , and the decoder produces a sequence of words as a response . For example , Shang et al . ( 2015 ) apply the attentional encoder-decoder translation model , training on a dataset of posts and responses from the Chinese microblogging platform Sina Weibo . 5 This approach is capable of generating replies that relate thematically to the input , as shown in the following examples ( trans - lated from Chinese by Shang et al . , 2015 ) . ( 19.8 ) a . A : High fever attacks me every New Year’s day . B : Get well soon and stay healthy ! b . A : I gain one more year . Grateful to my group , so happy . B : Getting old now . Time has no mercy . While encoder-decoder models can generate responses that make sense in the con - text of the immediately preceding turn , they struggle to maintain coherence over longer 5Twitter is also frequently used for construction of dialogue datasets ( Ritter et al . , 2011 ; Sordoni et al . , 2015 ) . Another source is technical support chat logs from the Ubuntu linux distribution ( Uthus and Aha , 2013 ; Lowe et al . , 2015 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 19.3 . DIALOGUE 471 conversations . One solution is to model the dialogue context recurrently . This creates a hierarchical recurrent network , including both word-level and turn-level recurrences . The turn-level hidden state is then used as additional context in the decoder ( Serban et al . , 2016 ) . An open question is how to integrate the encoder-decoder architecture into task-oriented dialogue systems . Neural chatbots can be trained end-to-end : the user’s turn is analyzed by the encoder , and the system output is generated by the decoder . This architecture can be trained by log-likelihood using backpropagation ( e.g . , Sordoni et al . , 2015 ; Serban et al . , 2016 ) , or by more elaborate objectives , using reinforcement learning ( Li et al . , 2016 ) . In contrast , the task-oriented dialogue systems described in subsection 19.3.1 typically involve a set of specialized modules : one for recognizing the user input , another for de - ciding what action to take , and a third for arranging the text of the system output . Recurrent neural network decoders can be integrated into Markov Decision Process dialogue systems , by conditioning the decoder on a representation of the information that is to be expressed in each turn ( Wen et al . , 2015 ) . Specifically , the long short-term memory ( LSTM ; section 6.3 ) architecture is augmented so that the memory cell at turn m takes an additional input dm , which is a representation of the slots and values to be expressed in the next turn . However , this approach still relies on additional modules to recognize the user’s utterance and to plan the overall arc of the dialogue . Another promising direction is to create embeddings for the elements in the domain : for example , the slots in a record and the entities that can fill them . The encoder then encodes not only the words of the user’s input , but the embeddings of the elements that the user mentions . Similarly , the decoder is endowed with the ability to refer to specific elements in the knowledge base . He et al . ( 2017 ) show that such a method can learn to play a collaborative dialogue game , in which both players are given a list of entities and their properties , and the goal is to find an entity that is on both players ’ lists . Additional resources Gatt and Krahmer ( 2018 ) provide a comprehensive recent survey on text generation . For a book-length treatment of earlier work , see Reiter and Dale ( 2000 ) . For a survey on image captioning , see Bernardi et al . ( 2016 ) ; for a survey of pre-neural approaches to dialogue systems , see Rieser and Lemon ( 2011 ) . Dialogue acts were introduced in section 8.6 as a labeling scheme for human-human dialogues ; they also play a critical in task-based dia - logue systems ( e.g . , Allen et al . , 1996 ) . The incorporation of theoretical models of dialogue into computational systems is reviewed by Jurafsky and Martin ( 2009 , chapter 24 ) . While this chapter has focused on the informative dimension of text generation , an - other line of research aims to generate text with configurable stylistic properties ( Walker et al . , 1997 ; Mairesse and Walker , 2011 ; Ficler and Goldberg , 2017 ; Hu et al . , 2017 ) . This Under contract with MIT Press , shared under CC-BY-NC-ND license . 472 CHAPTER 19 . TEXT GENERATION chapter also does not address the generation of creative text such as narratives ( Riedl and Young , 2010 ) , jokes ( Ritchie , 2001 ) , poems ( Colton et al . , 2012 ) , and song lyrics ( Gonçalo Oliveira et al . , 2007 ) . Exercises 1 . Find an article about a professional basketball game , with an associated “ box score ” of statistics . Which are the first three elements in the box score that are expressed in the article ? Can you identify template-based patterns that express these elements of the record ? Now find a second article about a different basketball game . Does it mention the same first three elements of the box score ? Do your templates capture how these elements are expressed in the text ? 2 . This exercise is to be done by a pair of students . One student should choose an article from the news or from Wikipedia , and manually perform semantic role labeling ( SRL ) on three short sentences or clauses . ( See chapter 13 for a review of SRL . ) Identify the main the semantic relation and its arguments and adjuncts . Pass this structured record — but not the original sentence — to the other student , whose job is to generate a sentence expressing the semantics . Then reverse roles , and try to regenerate three sentences from another article , based on the predicate-argument semantics . 3 . Compute the BLEU scores ( see subsection 18.1.1 ) for the generated sentences in the previous problem , using the original article text as the reference . 4 . Align each token in the text of Figure 19.1 to a specific single record in the database , or to the null record ∅ . For example , the tokens south wind would align to the record wind direction : 06:00-21:00 : mode = S . How often is each token aligned to the same record as the previous token ? How many transitions are there ? How might a system learn to output 10 degrees for the record min = 9 ? 5 . In sentence compression and fusion , we may wish to preserve contiguous sequences of tokens ( n-grams ) and / or dependency edges . Find five short news articles with headlines . For each headline , compute the fraction of bigrams that appear in the main text of the article . Then do a manual depenency parse of the headline . For each dependency edge , count how often it appears as a dependency edge in the main text . You may use an automatic dependency parser to assist with this exercise , but check the output , and focus on UD 2.0 dependency grammar , as described in chapter 11 . 6 . subsection 19.2.2 presents the idea of generating text from dependency trees , which requires linearization . Sometimes there are multiple ways that a dependency tree can be linearized . For example : Jacob Eisenstein . Draft of October 15 , 2018 . 19.3 . DIALOGUE 473 ( 19.9 ) a . The sick kids stayed at home in bed . b . The sick kids stayed in bed at home . Both sentences have an identical dependency parse : both home and bed are ( oblique ) dependents of stayed . Identify two more English dependency trees that can each be linearized in more than one way , and try to use a different pattern of variation in each tree . As usual , specify your trees in the Universal Dependencies 2 style , which is described in chapter 11 . 7 . In subsection 19.3.2 , we considered a pizza delivery service . Let’s simplify the prob - lem to take-out , where it is only necessary to determine the topping and confirm the order . The state is a tuple in which the first element is T if the topping is specified and ? otherwise , and the second element is either YES or NO , depending on whether the order has been confirmed . The actions are TOPPING ? ( request information about the topping ) and CONFIRM ? ( request confirmation ) . The state transition function is : p ( st | st − 1 = ( ? , NO ) , a = TOPPING ? ) = { 0.9 , st = ( T , NO ) 0.1 , st = ( ? , NO ) . [ 19.19 ] p ( st | st − 1 = ( ? , NO ) , a = CONFIRM ? ) = { 1 , st = ( ? , NO ) . [ 19.20 ] p ( st | st − 1 = ( T , NO ) , a = TOPPING ? ) = { 1 , st = ( T , NO ) . [ 19.21 ] p ( st | st − 1 = ( T , NO ) , a = CONFIRM ? ) = { 0.9 , st = ( T , YES ) 0.1 , st = ( T , NO ) . [ 19.22 ] Using the reward function defined in Equation 19.16 , the discount γ = 0.9 , and the initialization V ( s ) = 0 , execute three iterations of Equation 19.17 . After these three iterations , compute the optimal action in each state . You can assume that for the terminal states , V ( * , YES ) = 0 , so you only need to compute the values for non - terminal states , V ( ? , NO ) and V ( T , NO ) . 8 . There are several toolkits that allow you to train encoder-decoder translation models “ out of the box ” , such as FAIRSEQ ( Gehring et al . , 2017 ) , XNMT ( Neubig et al . , 2018 ) , TENSOR2TENSOR ( Vaswani et al . , 2018 ) , and OPENNMT ( Klein et al . , 2017 ) . 6 Use one of these toolkits to train a chatbot dialogue system , using either the NPS dialogue corpus that comes with NLTK ( Forsyth and Martell , 2007 ) , or , if you are feeling more ambitious , the Ubuntu dialogue corpus ( Lowe et al . , 2015 ) . 6https :// github.com/facebookresearch/fairseq ; https://github.com/neulab/xnmt ; https://github.com/tensorflow/tensor2tensor ; http://opennmt.net/ Under contract with MIT Press , shared under CC-BY-NC-ND license .