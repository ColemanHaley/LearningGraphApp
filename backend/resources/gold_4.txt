6 . Neural Network Training Neural network training is done by trying to minimize a loss function over a training set , using a gradient-based method . Roughly speaking , all training methods work by repeatedly computing an estimate of the error over the dataset , computing the gradient with respect to the error , and then moving the parameters in the direction of the gradient . Models differ in how the error estimate is computed , and how “ moving in the direction of the gradient ” is defined . We describe the basic algorithm , stochastic gradient descent ( SGD ) , and then briefly mention the other approaches with pointers for further reading . Gradient calculation is central to the approach . Gradients can be efficiently and automatically computed using reverse mode differentiation on a computation graph – a general algorithmic framework for automatically computing the gradient of any network and loss function . 6.1 Stochastic Gradient Training The common approach for training neural networks is using the stochastic gradient descent ( SGD ) algorithm ( Bottou , 2012 ; LeCun , Bottou , Orr , & Muller , 1998a ) or a variant of it . SGD is a general optimization algorithm . It receives a function f parameterized by θ , a loss function , and desired input and output pairs . It then attempts to set the parameters θ such that the loss of f with respect to the training examples is small . The algorithm works as follows : Algorithm 1 Online Stochastic Gradient Descent Training 1 : Input : Function f ( x ; θ ) parameterized with parameters θ . 2 : Input : Training set of inputs x1 , . . . , xn and outputs y1 , . . . , yn . 3 : Input : Loss function L . 4 : while stopping criteria not met do 5 : Sample a training example xi , yi 6 : Compute the loss L ( f ( xi ; θ ) , yi ) 7 : ĝ ← gradients of L ( f ( xi ; θ ) , yi ) w.r.t θ 8 : θ ← θ + ηkĝ 9 : return θ The goal of the algorithm is to set the parameters θ so as to minimize the total loss ∑ n i = 1 L ( f ( xi ; θ ) , yi ) over the training set . It works by repeatedly sampling a training exam - ple and computing the gradient of the error on the example with respect to the parameters θ ( line 7 ) – the input and expected output are assumed to be fixed , and the loss is treated as a function of the parameters θ . The parameters θ are then updated in the direction of the gradient , scaled by a learning rate ηk ( line 8 ) . For further discussion on setting the learning rate , see Section 6.3 . Note that the error calculated in line 6 is based on a single training example , and is thus just a rough estimate of the corpus-wide loss that we are aiming to minimize . The noise in the loss computation may result in inaccurate gradients . A common way of reducing this noise is to estimate the error and the gradients based on a sample of m examples . This gives rise to the minibatch SGD algorithm : 26 Algorithm 2 Minibatch Stochastic Gradient Descent Training 1 : Input : Function f ( x ; θ ) parameterized with parameters θ . 2 : Input : Training set of inputs x1 , . . . , xn and outputs y1 , . . . , yn . 3 : Input : Loss function L . 4 : while stopping criteria not met do 5 : Sample a minibatch of m examples { ( x1 , y1 ) , . . . , ( xm , ym ) } 6 : ĝ ← 0 7 : for i = 1 to m do 8 : Compute the loss L ( f ( xi ; θ ) , yi ) 9 : ĝ ← ĝ + gradients of 1mL ( f ( xi ; θ ) , yi ) w.r.t θ 10 : θ ← θ + ηkĝ 11 : return θ In lines 6 – 9 the algorithm estimates the gradient of the corpus loss based on the minibatch . After the loop , ĝ contains the gradient estimate , and the parameters θ are updated toward ĝ . The minibatch size can vary in size from m = 1 to m = n . Higher values provide better estimates of the corpus-wide gradients , while smaller values allow more updates and in turn faster convergence . Besides the improved accuracy of the gradients estimation , the minibatch algorithm provides opportunities for improved training efficiency . For modest sizes of m , some computing architectures ( i.e . GPUs ) allow an efficient parallel implementation of the computation in lines 6 – 9 . With a small enough learning rate , SGD is guaranteed to converge to a global optimum if the function is convex . However , it can also be used to optimize non-convex functions such as neural-network . While there are no longer guarantees of finding a global optimum , the algorithm proved to be robust and performs well in practice . When training a neural network , the parameterized function f is the neural network , and the parameters θ are the layer-transfer matrices , bias terms , embedding matrices and so on . The gradient computation is a key step in the SGD algorithm , as well as in all other neural network training algorithms . The question is , then , how to compute the gradients of the network’s error with respect to the parameters . Fortunately , there is an easy solution in the form of the backpropagation algorithm ( Rumelhart , Hinton , & Williams , 1986 ; Lecun , Bottou , Bengio , & Haffner , 1998b ) . The backpropagation algorithm is a fancy name for methodologically computing the derivatives of a complex expression using the chain - rule , while caching intermediary results . More generally , the backpropagation algorithm is a special case of the reverse-mode automatic differentiation algorithm ( Neidinger , 2010 , Section 7 ) , ( Baydin , Pearlmutter , Radul , & Siskind , 2015 ; Bengio , 2012 ) . The following section describes reverse mode automatic differentiation in the context of the computation graph abstraction . Beyond SGD While the SGD algorithm can and often does produce good results , more advanced algorithms are also available . The SGD + Momentum ( Polyak , 1964 ) and Nesterov Momentum ( Sutskever , Martens , Dahl , & Hinton , 2013 ) algorithms are variants of SGD in which previous gradients are accumulated and affect the current update . Adaptive learning rate algorithms including AdaGrad ( Duchi , Hazan , & Singer , 2011 ) , AdaDelta ( Zeiler , 2012 ) , 27 RMSProp ( Tieleman & Hinton , 2012 ) and Adam ( Kingma & Ba , 2014 ) are designed to select the learning rate for each minibatch , sometimes on a per-coordinate basis , potentially alleviating the need of fiddling with learning rate scheduling . For details of these algorithms , see the original papers or ( Bengio et al . , 2015 , Sections 8.3 , 8.4 ) . As many neural-network software frameworks provide implementations of these algorithms , it is easy and sometimes worthwhile to try out different variants . 6.2 The Computation Graph Abstraction While one can compute the gradients of the various parameters of a network by hand and implement them in code , this procedure is cumbersome and error prone . For most pur - poses , it is preferable to use automatic tools for gradient computation ( Bengio , 2012 ) . The computation-graph abstraction allows us to easily construct arbitrary networks , evaluate their predictions for given inputs ( forward pass ) , and compute gradients for their parameters with respect to arbitrary scalar losses ( backward pass ) . A computation graph is a representation of an arbitrary mathematical computation as a graph . It is a directed acyclic graph ( DAG ) in which nodes correspond to mathematical operations or ( bound ) variables and edges correspond to the flow of intermediary values between the nodes . The graph structure defines the order of the computation in terms of the dependencies between the different components . The graph is a DAG and not a tree , as the result of one operation can be the input of several continuations . Consider for example a graph for the computation of ( a ∗ b + 1 ) ∗ ( a ∗ b + 2 ) : a b1 2 * + + * The computation of a ∗ b is shared . We restrict ourselves to the case where the computation graph is connected . Since a neural network is essentially a mathematical expression , it can be represented as a computation graph . For example , Figure 3a presents the computation graph for a 1-layer MLP with a soft - max output transformation . In our notation , oval nodes represent mathematical operations or functions , and shaded rectangle nodes represent parameters ( bound variables ) . Network inputs are treated as constants , and drawn without a surrounding node . Input and param - eter nodes have no incoming arcs , and output nodes have no outgoing arcs . The output of each node is a matrix , the dimensionality of which is indicated above the node . This graph is incomplete : without specifying the inputs , we cannot compute an output . Figure 3b shows a complete graph for an MLP that takes three words as inputs , and predicts the distribution over part-of-speech tags for the third word . This graph can be used for prediction , but not for training , as the output is a vector ( not a scalar ) and the graph does not take into account the correct answer or the loss term . Finally , the graph in 3c shows the computation graph for a specific training example , in which the inputs are the ( embeddings 28 x 1 × 150 W1 150 × 20 MUL 1 × 20 ADD 1 × 20 b1 1 × 20 tanh 1 × 20 W2 20 × 17 b2 1 × 17 MUL 1 × 17 ADD 1 × 17 softmax 1 × 17 ( a ) concat 1 × 150 lookup 1 × 50 lookup 1 × 50 lookup 1 × 50 “ the ” “ black ” “ dog ” E | V | × 50 W1 150 × 20 MUL 1 × 20 ADD 1 × 20 b1 1 × 20 tanh 1 × 20 W2 20 × 17 b2 1 × 17 MUL 1 × 17 ADD 1 × 17 softmax 1 × 17 ( b ) concat 1 × 150 lookup 1 × 50 lookup 1 × 50 lookup 1 × 50 “ the ” “ black ” “ dog ” E | V | × 50 W1 150 × 20 MUL 1 × 20 ADD 1 × 20 b1 1 × 20 tanh 1 × 20 W2 20 × 17 b2 1 × 17 MUL 1 × 17 ADD 1 × 17 softmax 1 × 17 pick 1 × 1 5 log 1 × 1 neg 1 × 1 ( c ) Figure 3 : Computation Graph for MLP1 . ( a ) Graph with unbound input . ( b ) Graph with concrete input . ( c ) Graph with concrete input , expected output , and loss node . of ) the words “ the ” , “ black ” , “ dog ” , and the expected output is “ NOUN ” ( whose index is 5 ) . Once the graph is built , it is straightforward to run either a forward computation ( com - pute the result of the computation ) or a backward computation ( computing the gradients ) , as we show below . Constructing the graphs may look daunting , but is actually very easy using dedicated software libraries and APIs . Forward Computation The forward pass computes the outputs of the nodes in the graph . Since each node’s output depends only on itself and on its incoming edges , it is trivial to compute the outputs of all nodes by traversing the nodes in a topological order and computing the output of each node given the already computed outputs of its predecessors . More formally , in a graph of N nodes , we associate each node with an index i according to their topological ordering . Let fi be the function computed by node i ( e.g . multiplication . addition , . . . ) . Let π ( i ) be the parent nodes of node i , and π − 1 ( i ) = { j | i ∈ π ( j ) } the children nodes of node i ( these are the arguments of fi ) . Denote by v ( i ) the output of node 29 i , that is , the application of fi to the output values of its arguments π − 1 ( i ) . For variable and input nodes , fi is a constant function and π − 1 ( i ) is empty . The Forward algorithm computes the values v ( i ) for all i ∈ [ 1 , N ] . Algorithm 3 Computation Graph Forward Pass 1 : for i = 1 to N do 2 : Let a1 , . . . , am = π − 1 ( i ) 3 : v ( i ) ← fi ( v ( a1 ) , . . . , v ( am ) ) Backward Computation ( Derivatives , Backprop ) The backward pass begins by des - ignating a node N with scalar ( 1 × 1 ) output as a loss-node , and running forward computa - tion up to that node . The backward computation will computes the gradients with respect to that node’s value . Denote by d ( i ) the quantity ∂ N ∂ i . The backpropagation algorithm is used to compute the values d ( i ) for all nodes i . The backward pass fills a table d ( i ) as follows : Algorithm 4 Computation Graph Backward Pass ( Backpropagation ) 1 : d ( N ) ← 1 2 : for i = N-1 to 1 do 3 : d ( i ) ← ∑ j ∈ π ( i ) d ( j ) · ∂ fj ∂ i The quantity ∂ fj ∂ i is the partial derivative of fj ( π − 1 ( j ) ) w.r.t the argument i ∈ π − 1 ( j ) . This value depends on the function fj and the values v ( a1 ) , . . . , v ( am ) ( where a1 , . . . , am = π − 1 ( j ) ) of its arguments , which were computed in the forward pass . Thus , in order to define a new kind of node , one need to define two methods : one for calculating the forward value v ( i ) based on the nodes inputs , and the another for calculating ∂ fi ∂ x for each x ∈ π − 1 ( i ) . For further information on automatic differentiation see ( Neidinger , 2010 , Section 7 ) , ( Baydin et al . , 2015 ) . For more in depth discussion of the backpropagation algorithm and computation graphs ( also called flow graphs ) see ( Bengio et al . , 2015 , Section 6.4 ) , ( Lecun et al . , 1998b ; Bengio , 2012 ) . For a popular yet technical presentation , see Chris Olah’s description at http://colah.github.io/posts/2015-08-Backprop/ . Software Several software packages implement the computation-graph model , including Theano18 , Chainer19 , penne20 and CNN / pyCNN21 . All these packages support all the es - sential components ( node types ) for defining a wide range of neural network architectures , covering the structures described in this tutorial and more . Graph creation is made almost transparent by use of operator overloading . The framework defines a type for representing graph nodes ( commonly called expressions ) , methods for constructing nodes for inputs and 18 . http://deeplearning.net/software/theano/ 19 . http://chainer.org 20 . https://bitbucket.org/ndnlp/penne 21 . https://github.com/clab/cnn 30 parameters , and a set of functions and mathematical operations that take expressions as input and result in more complex expressions . For example , the python code for creating the computation graph from Figure ( 3c ) using the pyCNN framework is : from pycnn import * # model initialization . model = Model ( ) model . add_parameters ( " W1 " , ( 20,150 ) ) model . add_parameters ( " b1 " , 20 ) model . add_parameters ( " W2 " , ( 17,20 ) ) model . add_parameters ( " b2 " , 17 ) model . add_lookup_parameters ( " words " , ( 100 , 50 ) ) # Building the computation graph : renew_cg ( ) # create a new graph . # Wrap the model parameters as graph-nodes . W1 = parameter ( model [ " W1 " ] ) b1 = parameter ( model [ " b1 " ] ) W2 = parameter ( model [ " W2 " ] ) b2 = parameter ( model [ " b2 " ] ) def get_index ( x ) : return 1 # Generate the embeddings layer . vthe = lookup ( model [ " words " ] , get_index ( " the " ) ) vblack = lookup ( model [ " words " ] , get_index ( " black " ) ) vdog = lookup ( model [ " words " ] , get_index ( " dog " ) ) # Connect the leaf nodes into a complete graph . x = concatenate ( [ vthe , vblack , vdog ] ) output = softmax ( W2 * ( tanh ( W1 * x ) + b1 ) + b2 ) loss = - log ( pick ( output , 5 ) ) loss_value = loss . forward ( ) loss . backward ( ) # the gradient is computed # and stored in the corresponding # parameters . Most of the code involves various initializations : the first block defines model parameters that are be shared between different computation graphs ( recall that each graph corresponds to a specific training example ) . The second block turns the model parameters into the graph - node ( Expression ) types . The third block retrieves the Expressions for the embeddings of the input words . Finally , the fourth block is where the graph is created . Note how transparent the graph creation is – there is an almost a one-to-one correspondence between creating the graph and describing it mathematically . The last block shows a forward and backward pass . The other software frameworks follow similar patterns . Theano involves an optimizing compiler for computation graphs , which is both a blessing and a curse . On the one hand , once compiled , large graphs can be run efficiently on either the CPU or a GPU , making it ideal for large graphs with a fixed structure , where only the inputs change between instances . However , the compilation step itself can be costly , and it makes the interface a bit cumbersome to work with . In contrast , the other packages focus on building large and dynamic computation graphs and executing them “ on the fly ” without a compilation step . While the execution speed may suffer with respect to Theano’s optimized version , these packages are especially convenient when working with the recurrent and 31 recursive networks described in Sections 10 , 12 as well as in structured prediction settings as described in Section 8 . Implementation Recipe Using the computation graph abstraction , the pseudo-code for a network training algorithm is given in Algorithm 5 . Algorithm 5 Neural Network Training with Computation Graph Abstraction ( using mini - batches of size 1 ) 1 : Define network parameters . 2 : for iteration = 1 to N do 3 : for Training example xi , yi in dataset do 4 : loss node ← build computation graph ( xi , yi , parameters ) 5 : loss node . forward ( ) 6 : gradients ← loss node ( ) . backward ( ) 7 : parameters ← update parameters ( parameters , gradients ) 8 : return parameters . Here , build computation graph is a user-defined function that builds the computation graph for the given input , output and network structure , returning a single loss node . update parameters is an optimizer specific update rule . The recipe specifies that a new graph is created for each training example . This accommodates cases in which the network structure varies between training example , such as recurrent and recursive neural networks , to be discussed in Sections 10 – 12 . For networks with fixed structures , such as an MLPs , it may be more efficient to create one base computation graph and vary only the inputs and expected outputs between examples . Network Composition As long as the network’s output is a vector ( 1 × k matrix ) , it is trivial to compose networks by making the output of one network the input of another , creating arbitrary networks . The computation graph abstractions makes this ability explicit : a node in the computation graph can itself be a computation graph with a designated output node . One can then design arbitrarily deep and complex networks , and be able to easily evaluate and train them thanks to automatic forward and gradient computation . This makes it easy to define and train networks for structured outputs and multi-objective training , as we discuss in Section 7 , as well as complex recurrent and recursive networks , as discussed in Sections 10 – 12 . 6.3 Optimization Issues Once the gradient computation is taken care of , the network is trained using SGD or another gradient-based optimization algorithm . The function being optimized is not convex , and for a long time training of neural networks was considered a “ black art ” which can only be done by selected few . Indeed , many parameters affect the optimization process , and care has to be taken to tune these parameters . While this tutorial is not intended as a comprehensive guide to successfully training neural networks , we do list here a few of the prominent issues . For further discussion on optimization techniques and algorithms for neural networks , refer to ( Bengio et al . , 2015 , Chapter 8 ) . For some theoretical discussion and analysis , refer 32 to ( Glorot & Bengio , 2010 ) . For various practical tips and recommendations , see ( LeCun et al . , 1998a ; Bottou , 2012 ) . Initialization The non-convexity of the loss function means the optimization procedure may get stuck in a local minimum or a saddle point , and that starting from different initial points ( e.g . different random values for the parameters ) may result in different results . Thus , it is advised to run several restarts of the training starting at different random initializations , and choosing the best one based on a development set . 22 The amount of variance in the results is different for different network formulations and datasets , and cannot be predicted in advance . The magnitude of the random values has an important effect on the success of training . An effective scheme due to Glorot and Bengio ( 2010 ) , called xavier initialization after Glorot’s first name , suggests initializing a weight matrix W ∈ Rdin × dout as : W ∼ U [ − √ 6 √ din + dout , + √ 6 √ din + dout ] where U [ a , b ] is a uniformly sampled random value in the range [ a , b ] . This advice works well on many occasions , and is the preferred default initialization method by many . Analysis by He et al ( 2015 ) suggests that when using ReLU non-linearities , the weights should be initialized by sampling from a zero-mean Gaussian distribution whose standard deviation is √ 2 din . This initialization was found by He et al to work better than xavier initialization in an image classification task , especially when deep networks were involved . Vanishing and Exploding Gradients In deep networks , it is common for the error gradients to either vanish ( become exceedingly close to 0 ) or explode ( become exceedingly high ) as they propagate back through the computation graph . The problem becomes more severe in deeper networks , and especially so in recursive and recurrent networks ( Pascanu , Mikolov , & Bengio , 2012 ) . Dealing with the vanishing gradients problem is still an open research question . Solutions include making the networks shallower , step-wise training ( first train the first layers based on some auxiliary output signal , then fix them and train the upper layers of the complete network based on the real task signal ) , or specialized architectures that are designed to assist in gradient flow ( e.g . , the LSTM and GRU architectures for recurrent networks , discussed in Section 11 ) . Dealing with the exploding gradients has a simple but very effective solution : clipping the gradients if their norm exceeds a given threshold . Let ĝ be the gradients of all parameters in the network , and ‖ ĝ ‖ be their L2 norm . Pascanu et al ( 2012 ) suggest to set : ĝ ← threshold ‖ ĝ ‖ ĝ if ‖ ĝ ‖ > threshold . Saturation and Dead Neurons Layers with tanh and sigmoid activations can become saturated – resulting in output values for that layer that are all close to one , the upper - limit of the activation function . Saturated neurons have very small gradients , and should be avoided . Layers with the ReLU activation cannot be saturated , but can “ die ” – most or all values are negative and thus clipped at zero for all inputs , resulting in a gradient of zero for that layer . If your network does not train well , it is advisable to monitor the network for layers with many saturated or dead neurons . Saturated neurons are caused by too large 22 . When debugging , and for reproducibility of results , it is advised to used a fixed random seed . 33 values entering the layer . This may be controlled for by changing the initialization , scaling the range of the input values , or changing the learning rate . Dead neurons are caused by all weights entering the layer being negative ( for example this can happen after a large gradient update ) . Reducing the learning rate will help in this situation . For saturated layers , another option is to normalize the values in the saturated layer after the activation , i.e . instead of g ( h ) = tanh ( h ) using g ( h ) = tanh ( h ) ‖ tanh ( h ) ‖ . Layer normalization is an effective measure for countering saturation , but is also expensive in terms of gradient computation . Shuffling The order in which the training examples are presented to the network is im - portant . The SGD formulation above specifies selecting a random example in each turn . In practice , most implementations go over the training example in order . It is advised to shuffle the training examples before each pass through the data . Learning Rate Selection of the learning rate is important . Too large learning rates will prevent the network from converging on an effective solution . Too small learning rates will take very long time to converge . As a rule of thumb , one should experiment with a range of initial learning rates in range [ 0 , 1 ] , e.g . 0.001 , 0.01 , 0.1 , 1 . Monitor the network’s loss over time , and decrease the learning rate once the network seem to be stuck in a fixed region . Learning rate scheduling decrease the rate as a function of the number of observed minibatches . A common schedule is dividing the initial learning rate by the iteration number . Léon Bottou ( 2012 ) recommends using a learning rate of the form ηt = η0 ( 1 + η0λt ) − 1 where η0 is the initial learning rate , ηt is the learning rate to use on the tth training example , and λ is an additional hyperparameter . He further recommends determining a good value of η0 based on a small sample of the data prior to running on the entire dataset . Minibatches Parameter updates occur either every training example ( minibatches of size 1 ) or every k training examples . Some problems benefit from training with larger minibatch sizes . In terms of the computation graph abstraction , one can create a computation graph for each of the k training examples , and then connecting the k loss nodes under an averaging node , whose output will be the loss of the minibatch . Large minibatched training can also be beneficial in terms of computation efficiency on specialized computing architectures such as GPUs . This is beyond the scope of this tutorial . 6.4 Regularization Neural network models have many parameters , and overfitting can easily occur . Overfitting can be alleviated to some extent by regularization . A common regularization method is L2 regularization , placing a squared penalty on parameters with large values by adding an additive λ2 ‖ θ ‖ 2 term to the objective function to be minimized , where θ is the set of model parameters , ‖ · ‖ 2 is the squared L2 norm ( sum of squares of the values ) , and λ is a hyperparameter controlling the amount of regularization . A recently proposed alternative regularization method is dropout ( Hinton , Srivastava , Krizhevsky , Sutskever , & Salakhutdinov , 2012 ) . The dropout method is designed to prevent the network from learning to rely on specific weights . It works by randomly dropping ( set - ting to 0 ) half of the neurons in the network ( or in a specific layer ) in each training example . Work by Wager et al ( 2013 ) establishes a strong connection between the dropout method 34 and L2 regularization . Gal and Gharamani ( 2015 ) show that a multi-layer perceptron with dropout applied at every layer can be interpreted as Bayesian model averaging . The dropout technique is one of the key factors contributing to very strong results of neural-network methods on image classification tasks ( Krizhevsky , Sutskever , & Hinton , 2012 ) , especially when combined with ReLU activation units ( Dahl , Sainath , & Hinton , 2013 ) . The dropout technique is effective also in NLP applications of neural networks . 35