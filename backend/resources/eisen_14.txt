Chapter 14 Distributional and distributed semantics A recurring theme in natural language processing is the complexity of the mapping from words to meaning . In chapter 4 , we saw that a single word form , like bank , can have mul - tiple meanings ; conversely , a single meaning may be created by multiple surface forms , a lexical semantic relationship known as synonymy . Despite this complex mapping be - tween words and meaning , natural language processing systems usually rely on words as the basic unit of analysis . This is especially true in semantics : the logical and frame semantic methods from the previous two chapters rely on hand-crafted lexicons that map from words to semantic predicates . But how can we analyze texts that contain words that we haven’t seen before ? This chapter describes methods that learn representations of word meaning by analyzing unlabeled data , vastly improving the generalizability of natural language processing systems . The theory that makes it possible to acquire mean - ingful representations from unlabeled data is the distributional hypothesis . 14.1 The distributional hypothesis Here’s a word you may not know : tezgüino ( the example is from Lin , 1998 ) . If you do not know the meaning of tezgüino , then you are in the same situation as a natural language processing system when it encounters a word that did not appear in its training data . Now suppose you see that tezgüino is used in the following contexts : ( 14.1 ) A bottle of is on the table . ( 14.2 ) Everybody likes . ( 14.3 ) Don’t have before you drive . ( 14.4 ) We make out of corn . 325 326 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS ( 14.1 ) ( 14.2 ) ( 14.3 ) ( 14.4 ) . . . tezgüino 1 1 1 1 loud 0 0 0 0 motor oil 1 0 0 1 tortillas 0 1 0 1 choices 0 1 0 0 wine 1 1 1 0 Table 14.1 : Distributional statistics for tezgüino and five related terms 4 3 2 1 0 1 2 3 4 4 3 2 1 0 1 2 3 4 sister brother niece nephew aunt uncle woman man heiress heir madam sir countess earl duchess dukequeen king empress emperor 4 2 0 2 4 6 4 3 2 1 0 1 2 3 4 slow slower slowest short shorter shortest strong stronger strongest loudlouder loudest clear clearer clearest soft softer softest dark darker darkest Figure 14.1 : Lexical semantic relationships have regular linear structures in two dimen - sional projections of distributional statistics ( Pennington et al . , 2014 ) . What other words fit into these contexts ? How about : loud , motor oil , tortillas , choices , wine ? Each row of Table 14.1 is a vector that summarizes the contextual properties for each word , with a value of one for contexts in which the word can appear , and a value of zero for contexts in which it cannot . Based on these vectors , we can conclude : wine is very similar to tezgüino ; motor oil and tortillas are fairly similar to tezgüino ; loud is completely different . These vectors , which we will call word representations , describe the distributional properties of each word . Does vector similarity imply semantic similarity ? This is the dis - tributional hypothesis , stated by Firth ( 1957 ) as : “ You shall know a word by the company it keeps . ” The distributional hypothesis has stood the test of time : distributional statistics are a core part of language technology today , because they make it possible to leverage large amounts of unlabeled data to learn about rare words that do not appear in labeled training data . Distributional statistics have a striking ability to capture lexical semantic relationships Jacob Eisenstein . Draft of October 15 , 2018 . 14.2 . DESIGN DECISIONS FOR WORD REPRESENTATIONS 327 such as analogies . Figure 14.1 shows two examples , based on two-dimensional projections of distributional word embeddings , discussed later in this chapter . In each case , word - pair relationships correspond to regular linear patterns in this two dimensional space . No labeled data about the nature of these relationships was required to identify this underly - ing structure . Distributional semantics are computed from context statistics . Distributed seman - tics are a related but distinct idea : that meaning can be represented by numerical vectors rather than symbolic structures . Distributed representations are often estimated from dis - tributional statistics , as in latent semantic analysis and WORD2VEC , described later in this chapter . However , distributed representations can also be learned in a supervised fashion from labeled data , as in the neural classification models encountered in chapter 3 . 14.2 Design decisions for word representations There are many approaches for computing word representations , but most can be distin - guished on three main dimensions : the nature of the representation , the source of contex - tual information , and the estimation procedure . 14.2.1 Representation Today , the dominant word representations are k-dimensional vectors of real numbers , known as word embeddings . ( The name is due to the fact that each discrete word is em - bedded in a continuous vector space . ) This representation dates back at least to the late 1980s ( Deerwester et al . , 1990 ) , and is used in popular techniques such as WORD2VEC ( Mikolov et al . , 2013 ) . Word embeddings are well suited for neural networks , where they can be plugged in as inputs . They can also be applied in linear classifiers and structure prediction mod - els ( Turian et al . , 2010 ) , although it can be difficult to learn linear models that employ real-valued features ( Kummerfeld et al . , 2015 ) . A popular alternative is bit-string repre - sentations , such as Brown clusters ( section 14.4 ) , in which each word is represented by a variable-length sequence of zeros and ones ( Brown et al . , 1992 ) . Another representational question is whether to estimate one embedding per surface form ( e.g . , bank ) , or to estimate distinct embeddings for each word sense or synset . In - tuitively , if word representations are to capture the meaning of individual words , then words with multiple meanings should have multiple embeddings . This can be achieved by integrating unsupervised clustering with word embedding estimation ( Huang and Yates , 2012 ; Li and Jurafsky , 2015 ) . However , Arora et al . ( 2018 ) argue that it is unnec - essary to model distinct word senses explicitly , because the embeddings for each surface form are a linear combination of the embeddings of the underlying senses . Under contract with MIT Press , shared under CC-BY-NC-ND license . 328 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS The moment one learns English , complications set in ( Alfau , 1999 ) Brown Clusters { one } WORD2VEC , h = 2 { moment , one , English , complications } Structured WORD2VEC , h = 2 { ( moment , − 2 ) , ( one , − 1 ) , ( English , + 1 ) , ( complications , + 2 ) } Dependency contexts , { ( one , NSUBJ ) , ( English , DOBJ ) , ( moment , ACL − 1 ) } Table 14.2 : Contexts for the word learns , according to various word representations . For dependency context , ( one , NSUBJ ) means that there is a relation of type NSUBJ ( nominal subject ) to the word one , and ( moment , ACL − 1 ) means that there is a relation of type ACL ( adjectival clause ) from the word moment . 14.2.2 Context The distributional hypothesis says that word meaning is related to the “ contexts ” in which the word appears , but context can be defined in many ways . In the tezgüino example , con - texts are entire sentences , but in practice there are far too many sentences . At the oppo - site extreme , the context could be defined as the immediately preceding word ; this is the context considered in Brown clusters . WORD2VEC takes an intermediate approach , using local neighborhoods of words ( e.g . , h = 5 ) as contexts ( Mikolov et al . , 2013 ) . Contexts can also be much larger : for example , in latent semantic analysis , each word’s context vector includes an entry per document , with a value of one if the word appears in the document ( Deerwester et al . , 1990 ) ; in explicit semantic analysis , these documents are Wikipedia pages ( Gabrilovich and Markovitch , 2007 ) . In structured WORD2VEC , context words are labeled by their position with respect to the target word wm ( e.g . , two words before , one word after ) , which makes the result - ing word representations more sensitive to syntactic differences ( Ling et al . , 2015 ) . An - other way to incorporate syntax is to perform parsing as a preprocessing step , and then form context vectors from the dependency edges ( Levy and Goldberg , 2014 ) or predicate - argument relations ( Lin , 1998 ) . The resulting context vectors for several of these methods are shown in Table 14.2 . The choice of context has a profound effect on the resulting representations , which can be viewed in terms of word similarity . Applying latent semantic analysis ( section 14.3 ) to contexts of size h = 2 and h = 30 yields the following nearest-neighbors for the word dog : 1 • ( h = 2 ) : cat , horse , fox , pet , rabbit , pig , animal , mongrel , sheep , pigeon 1The example is from lecture slides by Marco Baroni , Alessandro Lenci , and Stefan Evert , who applied latent semantic analysis to the British National Corpus . You can find an online demo here : http://clic . cimec.unitn.it/infomap-query/ Jacob Eisenstein . Draft of October 15 , 2018 . 14.3 . LATENT SEMANTIC ANALYSIS 329 • ( h = 30 ) : kennel , puppy , pet , bitch , terrier , rottweiler , canine , cat , to bark , Alsatian Which word list is better ? Each word in the h = 2 list is an animal , reflecting the fact that locally , the word dog tends to appear in the same contexts as other animal types ( e.g . , pet the dog , feed the dog ) . In the h = 30 list , nearly everything is dog-related , including specific breeds such as rottweiler and Alsatian . The list also includes words that are not animals ( kennel ) , and in one case ( to bark ) , is not a noun at all . The 2-word context window is more sensitive to syntax , while the 30-word window is more sensitive to topic . 14.2.3 Estimation Word embeddings are estimated by optimizing some objective : the likelihood of a set of unlabeled data ( or a closely related quantity ) , or the reconstruction of a matrix of context counts , similar to Table 14.1 . Maximum likelihood estimation Likelihood-based optimization is derived from the objective log p ( w ; U ) , where U ∈ RK × V is matrix of word embeddings , and w = { wm } Mm = 1 is a corpus , represented as a list of M tokens . Recurrent neural network lan - guage models ( section 6.3 ) optimize this objective directly , backpropagating to the input word embeddings through the recurrent structure . However , state-of-the-art word em - beddings employ huge corpora with hundreds of billions of tokens , and recurrent archi - tectures are difficult to scale to such data . As a result , likelihood-based word embeddings are usually based on simplified likelihoods or heuristic approximations . Matrix factorization The matrix C = { count ( i , j ) } stores the co-occurrence counts of word i and context j . Word representations can be obtained by approximately factoring this matrix , so that count ( i , j ) is approximated by a function of a word embedding ui and a context embedding vj . These embeddings can be obtained by minimizing the norm of the reconstruction error , min u , v | | C − C̃ ( u , v ) | | F , [ 14.1 ] where C̃ ( u , v ) is the approximate reconstruction resulting from the embeddings u and v , and | | X | | F indicates the Frobenius norm , ∑ i , j x 2 i , j . Rather than factoring the matrix of word-context counts directly , it is often helpful to transform these counts using information - theoretic metrics such as pointwise mutual information ( PMI ) , described in the next sec - tion . 14.3 Latent semantic analysis Latent semantic analysis ( LSA ) is one of the oldest approaches to distributed seman - tics ( Deerwester et al . , 1990 ) . It induces continuous vector representations of words by Under contract with MIT Press , shared under CC-BY-NC-ND license . 330 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS factoring a matrix of word and context counts , using truncated singular value decompo - sition ( SVD ) , min U ∈ RV × K , S ∈ RK × K , V ∈ R | C | × K | | C − USV > | | F [ 14.2 ] s.t . U > U = I [ 14.3 ] V > V = I [ 14.4 ] ∀ i 6 = j , Si , j = 0 , [ 14.5 ] where V is the size of the vocabulary , | C | is the number of contexts , and K is size of the resulting embeddings , which are set equal to the rows of the matrix U . The matrix S is constrained to be diagonal ( these diagonal elements are called the singular values ) , and the columns of the product SV > provide descriptions of the contexts . Each element ci , j is then reconstructed as a bilinear product , ci , j ≈ K ∑ k = 1 ui , kskvj , k . [ 14.6 ] The objective is to minimize the sum of squared approximation errors . The orthonormal - ity constraints U > U = V > V = I ensure that all pairs of dimensions in U and V are uncorrelated , so that each dimension conveys unique information . Efficient implemen - tations of truncated singular value decomposition are available in numerical computing packages such as SCIPY and MATLAB . 2 Latent semantic analysis is most effective when the count matrix is transformed before the application of SVD . One such transformation is pointwise mutual information ( PMI ; Church and Hanks , 1990 ) , which captures the degree of association between word i and context j , PMI ( i , j ) = log p ( i , j ) p ( i ) p ( j ) = log p ( i | j ) p ( j ) p ( i ) p ( j ) = log p ( i | j ) p ( i ) [ 14.7 ] = log count ( i , j ) − log V ∑ i ′ = 1 count ( i ′ , j ) [ 14.8 ] − log ∑ j ′ ∈ C count ( i , j ′ ) + log V ∑ i ′ = 1 ∑ j ′ ∈ C count ( i ′ , j ′ ) . [ 14.9 ] The pointwise mutual information can be viewed as the logarithm of the ratio of the con - ditional probability of word i in context j to the marginal probability of word i in all 2An important implementation detail is to represent C as a sparse matrix , so that the storage cost is equal to the number of non-zero entries , rather than the size V × | C | . Jacob Eisenstein . Draft of October 15 , 2018 . 14.4 . BROWN CLUSTERS 331 evaluation assessment analysis understanding opinion conversation discussion reps representatives representative rep day year week month quarter half accounts people customers individuals employees students Figure 14.2 : Subtrees produced by bottom-up Brown clustering on news text ( Miller et al . , 2004 ) . contexts . When word i is statistically associated with context j , the ratio will be greater than one , so PMI ( i , j ) > 0 . The PMI transformation focuses latent semantic analysis on re - constructing strong word-context associations , rather than on reconstructing large counts . The PMI is negative when a word and context occur together less often than if they were independent , but such negative correlations are unreliable because counts of rare events have high variance . Furthermore , the PMI is undefined when count ( i , j ) = 0 . One solution to these problems is to use the Positive PMI ( PPMI ) , PPMI ( i , j ) = { PMI ( i , j ) , p ( i | j ) > p ( i ) 0 , otherwise . [ 14.10 ] Bullinaria and Levy ( 2007 ) compare a range of matrix transformations for latent se - mantic analysis , using a battery of tasks related to word meaning and word similarity ( for more on evaluation , see section 14.6 ) . They find that PPMI-based latent semantic analysis yields strong performance on a battery of tasks related to word meaning : for example , PPMI-based LSA vectors can be used to solve multiple-choice word similarity questions from the Test of English as a Foreign Language ( TOEFL ) , obtaining 85 % accuracy . 14.4 Brown clusters Learning algorithms like perceptron and conditional random fields often perform better with discrete feature vectors . A simple way to obtain discrete representations from dis - Under contract with MIT Press , shared under CC-BY-NC-ND license . 332 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS bitstring ten most frequent words 011110100111 excited thankful grateful stoked pumped anxious hyped psyched exited geeked 01111010100 talking talkin complaining talkn bitching tlkn tlkin bragging rav - ing + k 011110101010 thinking thinkin dreaming worrying thinkn speakin reminiscing dreamin daydreaming fantasizing 011110101011 saying sayin suggesting stating sayn jokin talmbout implying insisting 5’2 011110101100 wonder dunno wondered duno donno dno dono wonda wounder dunnoe 011110101101 wondering wonders debating deciding pondering unsure won - derin debatin woundering wondern 011110101110 sure suree suuure suure sure - surre sures shuree Table 14.3 : Fragment of a Brown clustering of Twitter data ( Owoputi et al . , 2013 ) . Each row is a leaf in the tree , showing the ten most frequent words . This part of the tree emphasizes verbs of communicating and knowing , especially in the present partici - ple . Each leaf node includes orthographic variants ( thinking , thinkin , thinkn ) , semanti - cally related terms ( excited , thankful , grateful ) , and some outliers ( 5’2 , + k ) . See http : / / www.cs.cmu.edu/ ˜ ark / TweetNLP / cluster_viewer . html for more . tributional statistics is by clustering ( subsection 5.1.1 ) , so that words in the same cluster have similar distributional statistics . This can help in downstream tasks , by sharing fea - tures between all words in the same cluster . However , there is an obvious tradeoff : if the number of clusters is too small , the words in each cluster will not have much in common ; if the number of clusters is too large , then the learner will not see enough examples from each cluster to generalize . A solution to this problem is hierarchical clustering : using the distributional statistics to induce a tree-structured representation . Fragments of Brown cluster trees are shown in Figure 14.2 and Table 14.3 . Each word’s representation consists of a binary string describ - ing a path through the tree : 0 for taking the left branch , and 1 for taking the right branch . In the subtree in the upper right of the figure , the representation of the word conversation is 10 ; the representation of the word assessment is 0001 . Bitstring prefixes capture similar - ity at varying levels of specificity , and it is common to use the first eight , twelve , sixteen , and twenty bits as features in tasks such as named entity recognition ( Miller et al . , 2004 ) and dependency parsing ( Koo et al . , 2008 ) . Hierarchical trees can be induced from a likelihood-based objective , using a discrete Jacob Eisenstein . Draft of October 15 , 2018 . 14.4 . BROWN CLUSTERS 333 latent variable ki ∈ { 1 , 2 , . . . , K } to represent the cluster of word i : log p ( w ; k ) ≈ M ∑ m = 1 log p ( wm | wm − 1 ; k ) [ 14.11 ] , M ∑ m = 1 log p ( wm | kwm ) + log p ( kwm | kwm − 1 ) . [ 14.12 ] This is similar to a hidden Markov model , with the crucial difference that each word can be emitted from only a single cluster : ∀ k 6 = kwm , p ( wm | k ) = 0 . Using the objective in Equation 14.12 , the Brown clustering tree can be constructed from the bottom up : begin with each word in its own cluster , and incrementally merge clusters until only a single cluster remains . At each step , we merge the pair of clusters such that the objective in Equation 14.12 is maximized . Although the objective seems to involve a sum over the entire corpus , the score for each merger can be computed from the cluster-to-cluster co-occurrence counts . These counts can be updated incrementally as the clustering proceeds . The optimal merge at each step can be shown to maximize the average mutual information , I ( k ) = K ∑ k1 = 1 K ∑ k2 = 1 p ( k1 , k2 ) × PMI ( k1 , k2 ) [ 14.13 ] p ( k1 , k2 ) = count ( k1 , k2 ) ∑ K k1 ′ = 1 ∑ K k2 ′ = 1 count ( k1 ′ , k2 ′ ) , where p ( k1 , k2 ) is the joint probability of a bigram involving a word in cluster k1 followed by a word in k2 . This probability and the PMI are both computed from the co-occurrence counts between clusters . After each merger , the co-occurrence vectors for the merged clusters are simply added up , so that the next optimal merger can be found efficiently . This bottom-up procedure requires iterating over the entire vocabulary , and evaluat - ing K2t possible mergers at each step , where Kt is the current number of clusters at step t of the algorithm . Furthermore , computing the score for each merger involves a sum over K2t clusters . The maximum number of clusters isK0 = V , which occurs when every word is in its own cluster at the beginning of the algorithm . The time complexity is thusO ( V 5 ) . To avoid this complexity , practical implementations use a heuristic approximation called exchange clustering . The K most common words are placed in clusters of their own at the beginning of the process . We then consider the next most common word , and merge it with one of the existing clusters . This continues until the entire vocabulary has been incorporated , at which point the K clusters are merged down to a single cluster , forming a tree . The algorithm never considers more than K + 1 clusters at any step , and the complexity is O ( V K + V log V ) , with the second term representing the cost of sorting Under contract with MIT Press , shared under CC-BY-NC-ND license . 334 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS vwm − 2 vwm − 1 vwm vwm + 1 vwm + 2 wm − 2 wm − 1 wm wm + 1 wm + 2 U ( a ) Continuous bag-of-words ( CBOW ) vwm wmwm − 1wm − 2 wm + 1 wm + 2 U ( b ) Skipgram Figure 14.3 : The CBOW and skipgram variants of WORD2VEC . The parameter U is the matrix of word embeddings , and each vm is the context embedding for word wm . the words at the beginning of the algorithm . For more details on the algorithm , see Liang ( 2005 ) . 14.5 Neural word embeddings Neural word embeddings combine aspects of the previous two methods : like latent se - mantic analysis , they are a continuous vector representation ; like Brown clusters , they are trained from a likelihood-based objective . Let the vector ui represent the K-dimensional embedding for word i , and let vj represent the K-dimensional embedding for context j . The inner product ui · vj represents the compatibility between word i and context j . By incorporating this inner product into an approximation to the log-likelihood of a cor - pus , it is possible to estimate both parameters by backpropagation . WORD2VEC ( Mikolov et al . , 2013 ) includes two such approximations : continuous bag-of-words ( CBOW ) and skipgrams . 14.5.1 Continuous bag-of-words ( CBOW ) In recurrent neural network language models , each wordwm is conditioned on a recurrently - updated state vector , which is based on word representations going all the way back to the beginning of the text . The continuous bag-of-words ( CBOW ) model is a simplification : the local context is computed as an average of embeddings for words in the immediate neighborhood m − h , m − h + 1 , . . . , m + h − 1 , m + h , vm = 1 2h h ∑ n = 1 vwm + n + vwm − n . [ 14.14 ] Thus , CBOW is a bag-of-words model , because the order of the context words does not matter ; it is continuous , because rather than conditioning on the words themselves , we condition on a continuous vector constructed from the word embeddings . The parameter h determines the neighborhood size , which Mikolov et al . ( 2013 ) set to h = 4 . Jacob Eisenstein . Draft of October 15 , 2018 . 14.5 . NEURAL WORD EMBEDDINGS 335 The CBOW model optimizes an approximation to the corpus log-likelihood , log p ( w ) ≈ M ∑ m = 1 log p ( wm | wm − h , wm − h + 1 , . . . , wm + h − 1 , wm + h ) [ 14.15 ] = M ∑ m = 1 log exp ( uwm · vm ) ∑ V j = 1 exp ( uj · vm ) [ 14.16 ] = M ∑ m = 1 uwm · vm − log V ∑ j = 1 exp ( uj · vm ) . [ 14.17 ] 14.5.2 Skipgrams In the CBOW model , words are predicted from their context . In the skipgram model , the context is predicted from the word , yielding the objective : log p ( w ) ≈ M ∑ m = 1 hm ∑ n = 1 log p ( wm − n | wm ) + log p ( wm + n | wm ) [ 14.18 ] = M ∑ m = 1 hm ∑ n = 1 log exp ( uwm − n · vwm ) ∑ V j = 1 exp ( uj · vwm ) + log exp ( uwm + n · vwm ) ∑ V j = 1 exp ( uj · vwm ) [ 14.19 ] = M ∑ m = 1 hm ∑ n = 1 uwm − n · vwm + uwm + n · vwm − 2 log V ∑ j = 1 exp ( uj · vwm ) . [ 14.20 ] In the skipgram approximation , each word is generated multiple times ; each time it is conditioned only on a single word . This makes it possible to avoid averaging the word vectors , as in the CBOW model . The local neighborhood size hm is randomly sampled from a uniform categorical distribution over the range { 1 , 2 , . . . , hmax } ; Mikolov et al . ( 2013 ) set hmax = 10 . Because the neighborhood grows outward with h , this approach has the effect of weighting near neighbors more than distant ones . Skipgram performs better on most evaluations than CBOW ( see section 14.6 for details of how to evaluate word representations ) , but CBOW is faster to train ( Mikolov et al . , 2013 ) . 14.5.3 Computational complexity The WORD2VEC models can be viewed as an efficient alternative to recurrent neural net - work language models , which involve a recurrent state update whose time complexity is quadratic in the size of the recurrent state vector . CBOW and skipgram avoid this computation , and incur only a linear time complexity in the size of the word and con - text representations . However , all three models compute a normalized probability over word tokens ; a naı̈ve implementation of this probability requires summing over the entire Under contract with MIT Press , shared under CC-BY-NC-ND license . 336 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS 0 1 2 3 4Ahab σ ( u0 · vc ) whale σ ( − u0 · vc ) × σ ( u2 · vc ) blubber σ ( − u0 · vc ) × σ ( − u2 · vc ) Figure 14.4 : A fragment of a hierarchical softmax tree . The probability of each word is computed as a product of probabilities of local branching decisions in the tree . vocabulary . The time complexity of this sum isO ( V × K ) , which dominates all other com - putational costs . There are two solutions : hierarchical softmax , a tree-based computation that reduces the cost to a logarithm of the size of the vocabulary ; and negative sampling , an approximation that eliminates the dependence on vocabulary size . Both methods are also applicable to RNN language models . Hierarchical softmax In Brown clustering , the vocabulary is organized into a binary tree . Mnih and Hin - ton ( 2008 ) show that the normalized probability over words in the vocabulary can be reparametrized as a probability over paths through such a tree . This hierarchical softmax probability is computed as a product of binary decisions over whether to move left or right through the tree , with each binary decision represented as a sigmoid function of the inner product between the context embedding vc and an output embedding associated with the node un , Pr ( left at n | c ) = σ ( un · vc ) [ 14.21 ] Pr ( right at n | c ) = 1 − σ ( un · vc ) = σ ( − un · vc ) , [ 14.22 ] where σ refers to the sigmoid function , σ ( x ) = 11 + exp ( − x ) . The range of the sigmoid is the interval ( 0 , 1 ) , and 1 − σ ( x ) = σ ( − x ) . As shown in Figure 14.4 , the probability of generating each word is redefined as the product of the probabilities across its path . The sum of all such path probabilities is guar - anteed to be one , for any context vector vc ∈ RK . In a balanced binary tree , the depth is logarithmic in the number of leaf nodes , and thus the number of multiplications is equal to O ( log V ) . The number of non-leaf nodes is equal to O ( 2V − 1 ) , so the number of pa - rameters to be estimated increases by only a small multiple . The tree can be constructed using an incremental clustering procedure similar to hierarchical Brown clusters ( Mnih Jacob Eisenstein . Draft of October 15 , 2018 . 14.5 . NEURAL WORD EMBEDDINGS 337 and Hinton , 2008 ) , or by using the Huffman ( 1952 ) encoding algorithm for lossless com - pression . Negative sampling Likelihood-based methods are computationally intensive because each probability must be normalized over the vocabulary . These probabilities are based on scores for each word in each context , and it is possible to design an alternative objective that is based on these scores more directly : we seek word embeddings that maximize the score for the word that was really observed in each context , while minimizing the scores for a set of randomly selected negative samples : ψ ( i , j ) = log σ ( ui · vj ) + ∑ i ′ ∈ Wneg log ( 1 − σ ( ui ′ · vj ) ) , [ 14.23 ] where ψ ( i , j ) is the score for word i in context j , andWneg is the set of negative samples . The objective is to maximize the sum over the corpus , ∑ M m = 1 ψ ( wm , cm ) , where wm is token m and cm is the associated context . The set of negative samples Wneg is obtained by sampling from a unigram language model . Mikolov et al . ( 2013 ) construct this unigram language model by exponentiating the empirical word probabilities , setting p̂ ( i ) ∝ ( count ( i ) ) 34 . This has the effect of redis - tributing probability mass from common to rare words . The number of negative samples increases the time complexity of training by a constant factor . Mikolov et al . ( 2013 ) report that 5-20 negative samples works for small training sets , and that two to five samples suffice for larger corpora . 14.5.4 Word embeddings as matrix factorization The negative sampling objective in Equation 14.23 can be justified as an efficient approx - imation to the log-likelihood , but it is also closely linked to the matrix factorization ob - jective employed in latent semantic analysis . For a matrix of word-context pairs in which all counts are non-zero , negative sampling is equivalent to factorization of the matrix M , where Mij = PMI ( i , j ) − log k : each cell in the matrix is equal to the pointwise mutual information of the word and context , shifted by log k , with k equal to the number of neg - ative samples ( Levy and Goldberg , 2014 ) . For word-context pairs that are not observed in the data , the pointwise mutual information is − ∞ , but this can be addressed by consid - ering only PMI values that are greater than log k , resulting in a matrix of shifted positive pointwise mutual information , Mij = max ( 0 , PMI ( i , j ) − log k ) . [ 14.24 ] Word embeddings are obtained by factoring this matrix with truncated singular value decomposition . Under contract with MIT Press , shared under CC-BY-NC-ND license . 338 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS word 1 word 2 similarity love sex 6.77 stock jaguar 0.92 money cash 9.15 development issue 3.97 lad brother 4.46 Table 14.4 : Subset of the WS-353 ( Finkelstein et al . , 2002 ) dataset of word similarity ratings ( examples from Faruqui et al . ( 2016 ) ) . GloVe ( “ global vectors ” ) are a closely related approach ( Pennington et al . , 2014 ) , in which the matrix to be factored is constructed from log co-occurrence counts , Mij = log count ( i , j ) . The word embeddings are estimated by minimizing the sum of squares , min u , v , b , b̃ V ∑ j = 1 ∑ j ∈ C f ( Mij ) ( logMij ∧ − logMij ) 2 s.t . logMij ∧ = ui · vj + bi + b̃j , [ 14.25 ] where bi and b̃j are offsets for word i and context j , which are estimated jointly with the embeddings u and v . The weighting function f ( Mij ) is set to be zero at Mij = 0 , thus avoiding the problem of taking the logarithm of zero counts ; it saturates at Mij = mmax , thus avoiding the problem of overcounting common word-context pairs . This heuristic turns out to be critical to the method’s performance . The time complexity of sparse matrix reconstruction is determined by the number of non-zero word-context counts . Pennington et al . ( 2014 ) show that this number grows sublinearly with the size of the dataset : roughly O ( N0.8 ) for typical English corpora . In contrast , the time complexity of WORD2VEC is linear in the corpus size . Computing the co - occurrence counts also requires linear time in the size of the corpus , but this operation can easily be parallelized using MapReduce-style algorithms ( Dean and Ghemawat , 2008 ) . 14.6 Evaluating word embeddings Distributed word representations can be evaluated in two main ways . Intrinsic evalu - ations test whether the representations cohere with our intuitions about word meaning . Extrinsic evaluations test whether they are useful for downstream tasks , such as sequence labeling . Jacob Eisenstein . Draft of October 15 , 2018 . 14.6 . EVALUATING WORD EMBEDDINGS 339 14.6.1 Intrinsic evaluations A basic question for word embeddings is whether the similarity of words i and j is re - flected in the similarity of the vectors ui and uj . Cosine similarity is typically used to compare two word embeddings , cos ( ui , uj ) = ui · uj | | ui | | 2 × | | uj | | 2 . [ 14.26 ] For any embedding method , we can evaluate whether the cosine similarity of word em - beddings is correlated with human judgments of word similarity . The WS-353 dataset ( Finkel - stein et al . , 2002 ) includes similarity scores for 353 word pairs ( Table 14.4 ) . To test the accuracy of embeddings for rare and morphologically complex words , Luong et al . ( 2013 ) introduce a dataset of “ rare words . ” Outside of English , word similarity resources are lim - ited , mainly consisting of translations of WS-353 and the related SimLex-999 dataset ( Hill et al . , 2015 ) . Word analogies ( e.g . , king : queen : : man : woman ) have also been used to evaluate word embeddings ( Mikolov et al . , 2013 ) . In this evaluation , the system is provided with the first three parts of the analogy ( i1 : j1 : : i2 : ? ) , and the final element is predicted by finding the word embedding most similar to ui1 − uj1 + ui2 . Another evaluation tests whether word embeddings are related to broad lexical semantic categories called supersenses ( Ciaramita and Johnson , 2003 ) : verbs of motion , nouns that describe animals , nouns that describe body parts , and so on . These supersenses are annotated for English synsets in Word - Net ( Fellbaum , 2010 ) . This evaluation is implemented in the QVEC metric , which tests whether the matrix of supersenses can be reconstructed from the matrix of word embed - dings ( Tsvetkov et al . , 2015 ) . Levy et al . ( 2015 ) compared several dense word representations for English — includ - ing latent semantic analysis , WORD2VEC , and GloVe — using six word similarity metrics and two analogy tasks . None of the embeddings outperformed the others on every task , but skipgrams were the most broadly competitive . Hyperparameter tuning played a key role : any method will perform badly if the wrong hyperparameters are used . Relevant hyperparameters include the embedding size , as well as algorithm-specific details such as the neighborhood size and the number of negative samples . 14.6.2 Extrinsic evaluations Word representations contribute to downstream tasks like sequence labeling and docu - ment classification by enabling generalization across words . The use of distributed repre - sentations as features is a form of semi-supervised learning , in which performance on a supervised learning problem is augmented by learning distributed representations from unlabeled data ( Miller et al . , 2004 ; Koo et al . , 2008 ; Turian et al . , 2010 ) . These pre-trained word representations can be used as features in a linear prediction model , or as the input Under contract with MIT Press , shared under CC-BY-NC-ND license . 340 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS layer in a neural network , such as a Bi-LSTM tagging model ( section 7.6 ) . Word repre - sentations can be evaluated by the performance of the downstream systems that consume them : for example , GloVe embeddings are convincingly better than Latent Semantic Anal - ysis as features in the downstream task of named entity recognition ( Pennington et al . , 2014 ) . Unfortunately , extrinsic and intrinsic evaluations do not always point in the same direction , and the best word representations for one downstream task may perform poorly on another task ( Schnabel et al . , 2015 ) . When word representations are updated from labeled data in the downstream task , they are said to be fine-tuned . When labeled data is plentiful , pre-training may be un - necessary ; when labeled data is scare , fine-tuning may lead to overfitting . Various com - binations of pre-training and fine-tuning can be employed . Pre-trained embeddings can be used as initialization before fine-tuning , and this can substantially improve perfor - mance ( Lample et al . , 2016 ) . Alternatively , both fine-tuned and pre-trained embeddings can be used as inputs in a single model ( Kim , 2014 ) . In semi-supervised scenarios , pretrained word embeddings can be replaced by “ con - textualized ” word representations ( Peters et al . , 2018 ) . These contextualized represen - tations are set to the hidden states of a deep bi-directional LSTM , which is trained as a bi-directional language model , motivating the name ELMo ( embeddings from language models ) . By running the language model , we obtain contextualized word representa - tions , which can then be used as the base layer in a supervised neural network for any task . This approach yields significant gains over pretrained word embeddings on several tasks , presumably because the contextualized embeddings use unlabeled data to learn how to integrate linguistic context into the base layer of the supervised neural network . 14.6.3 Fairness and bias Figure 14.1 shows how word embeddings can capture analogies such as man : woman : : king : queen . While king and queen are gender-specific by definition , other professions or titles are associated with genders and other groups merely by statistical tendency . This statistical tendency may be a fact about the world ( e.g . , professional baseball players are usually men ) , or a fact about the text corpus ( e.g . , there are professional basketball leagues for both women and men , but the men’s basketball is written about far more often ) . There is now considerable evidence that word embeddings do indeed encode such bi - ases . Bolukbasi et al . ( 2016 ) show that the words most aligned with the vector difference she − he are stereotypically female professions homemaker , nurse , receptionist ; in the other direction are maestro , skipper , protege . Caliskan et al . ( 2017 ) systematize this observation by showing that biases in word embeddings align with well-validated gender stereotypes . Garg et al . ( 2018 ) extend these results to ethnic stereotypes of Asian Americans , and pro - vide a historical perspective on how stereotypes evolve over 100 years of text data . Because word embeddings are the input layer for many other natural language pro - Jacob Eisenstein . Draft of October 15 , 2018 . 14.7 . DISTRIBUTED REPRESENTATIONS BEYOND DISTRIBUTIONAL STATISTICS341 cessing systems , these findings highlight the risk that natural language processing will replicate and amplify biases in the world , as well as in text . If , for example , word em - beddings encode the belief that women are as unlikely to be programmers as they are to be nephews , then software is unlikely to successfully parse , translate , index , and generate texts in which women do indeed program computers . For example , contemporary NLP systems often fail to properly resolve pronoun references in texts that cut against gender stereotypes ( Rudinger et al . , 2018 ; Zhao et al . , 2018 ) . ( The task of pronoun resolution is described in depth in chapter 15 . ) Such biases can have profound consequences : for exam - ple , search engines are more likely to yield personalized advertisements for public arrest records when queried with names that are statistically associated with African Ameri - cans ( Sweeney , 2013 ) . There is now an active research literature on “ debiasing ” machine learning and natural language processing , as evidenced by the growth of annual meet - ings such as Fairness , Accountability , and Transparency in Machine Learning ( FAT / ML ) . However , given that the ultimate source of these biases is the text itself , it may be too much to hope for a purely algorithmic solution . There is no substitute for critical thought about the inputs to natural language processing systems – and the uses of their outputs . 14.7 Distributed representations beyond distributional statistics Distributional word representations can be estimated from huge unlabeled datasets , thereby covering many words that do not appear in labeled data : for example , GloVe embeddings are estimated from 800 billion tokens of web data , 3 while the largest labeled datasets for NLP tasks are on the order of millions of tokens . Nonetheless , even a dataset of hundreds of billions of tokens will not cover every word that may be encountered in the future . Furthermore , many words will appear only a few times , making their embeddings un - reliable . Many languages exceed English in morphological complexity , and thus have lower token-to-type ratios . When this problem is coupled with small training corpora , it becomes especially important to leverage other sources of information beyond distribu - tional statistics . 14.7.1 Word-internal structure One solution is to incorporate word-internal structure into word embeddings . Purely distributional approaches consider words as atomic units , but in fact , many words have internal structure , so that their meaning can be composed from the representations of sub-word units . Consider the following terms , all of which are missing from Google’s pre-trained WORD2VEC embeddings : 4 3http :// commoncrawl.org/ 4https :// code.google.com/archive/p/word2vec/ , accessed September 20 , 2017 Under contract with MIT Press , shared under CC-BY-NC-ND license . 342 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS umillicuries ũmillicuries u ( M ) milli + u ( M ) curie u ( M ) + s umillicuries umillicurie u ( M ) milli + u ( M ) curie u ( M ) + s Figure 14.5 : Two architectures for building word embeddings from subword units . On the left , morpheme embeddings u ( m ) are combined by addition with the non-compositional word embedding ũ ( Botha and Blunsom , 2014 ) . On the right , morpheme embeddings are combined in a recursive neural network ( Luong et al . , 2013 ) . millicuries This word has morphological structure ( see subsection 9.1.2 for more on mor - phology ) : the prefix milli - indicates an amount , and the suffix - s indicates a plural . ( A millicurie is an unit of radioactivity . ) caesium This word is a single morpheme , but the characters - ium are often associated with chemical elements . ( Caesium is the British spelling of a chemical element , spelled cesium in American English . ) IAEA This term is an acronym , as suggested by the use of capitalization . The prefix I - fre - quently refers to international organizations , and the suffix - A often refers to agen - cies or associations . ( IAEA is the International Atomic Energy Agency . ) Zhezhgan This term is in title case , suggesting the name of a person or place , and the character bigram zh indicates that it is likely a transliteration . ( Zhezhgan is a mining facility in Kazakhstan . ) How can word-internal structure be incorporated into word representations ? One approach is to construct word representations from embeddings of the characters or mor - phemes . For example , if word i has morphological segmentsMi , then its embedding can be constructed by addition ( Botha and Blunsom , 2014 ) , ui = ũi + ∑ j ∈ Mi u ( M ) j , [ 14.27 ] where u ( M ) m is a morpheme embedding and ũi is a non-compositional embedding of the whole word , which is an additional free parameter of the model ( Figure 14.5 , left side ) . All embeddings are estimated from a log-bilinear language model ( Mnih and Hinton , 2007 ) , which is similar to the CBOW model ( section 14.5 ) , but includes only contextual information from preceding words . The morphological segments are obtained using an unsupervised segmenter ( Creutz and Lagus , 2007 ) . For words that do not appear in the Jacob Eisenstein . Draft of October 15 , 2018 . 14.7 . DISTRIBUTED REPRESENTATIONS BEYOND DISTRIBUTIONAL STATISTICS343 training data , the embedding can be constructed directly from the morphemes , assuming that each morpheme appears in some other word in the training data . The free param - eter ũ adds flexibility : words with similar morphemes are encouraged to have similar embeddings , but this parameter makes it possible for them to be different . Word-internal structure can be incorporated into word representations in various other ways . Here are some of the main parameters . Subword units . Examples like IAEA and Zhezhgan are not based on morphological com - position , and a morphological segmenter is unlikely to identify meaningful sub - word units for these terms . Rather than using morphemes for subword embeddings , one can use characters ( Santos and Zadrozny , 2014 ; Ling et al . , 2015 ; Kim et al . , 2016 ) , character n-grams ( Wieting et al . , 2016a ; Bojanowski et al . , 2017 ) , and byte-pair en - codings , a compression technique which captures frequent substrings ( Gage , 1994 ; Sennrich et al . , 2016 ) . Composition . Combining the subword embeddings by addition does not differentiate between orderings , nor does it identify any particular morpheme as the root . A range of more flexible compositional models have been considered , including re - currence ( Ling et al . , 2015 ) , convolution ( Santos and Zadrozny , 2014 ; Kim et al . , 2016 ) , and recursive neural networks ( Luong et al . , 2013 ) , in which representa - tions of progressively larger units are constructed over a morphological parse , e.g . ( ( milli + curie ) + s ) , ( ( in + flam ) + able ) , ( in + ( vis + ible ) ) . A recursive embedding model is shown in the right panel of Figure 14.5 . Estimation . Estimating subword embeddings from a full dataset is computationally ex - pensive . An alternative approach is to train a subword model to match pre-trained word embeddings ( Cotterell et al . , 2016 ; Pinter et al . , 2017 ) . To train such a model , it is only necessary to iterate over the vocabulary , and the not the corpus . 14.7.2 Lexical semantic resources Resources such as WordNet provide another source of information about word meaning : if we know that caesium is a synonym of cesium , or that a millicurie is a type of measurement unit , then this should help to provide embeddings for the unknown words , and to smooth embeddings of rare words . One way to do this is to retrofit pre-trained word embeddings across a network of lexical semantic relationships ( Faruqui et al . , 2015 ) by minimizing the following objective , min U V ∑ j = 1 | | ui − ûi | | 2 + ∑ ( i , j ) ∈ L βij | | ui − uj | | 2 , [ 14.28 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 344 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS where ûi is the pretrained embedding of word i , and L = { ( i , j ) } is a lexicon of word relations . The hyperparameter βij controls the importance of adjacent words having similar embeddings ; Faruqui et al . ( 2015 ) set it to the inverse of the degree of word i , βij = | { j : ( i , j ) ∈ L } | − 1 . Retrofitting improves performance on a range of intrinsic evalu - ations , and gives small improvements on an extrinsic document classification task . 14.8 Distributed representations of multiword units Can distributed representations extend to phrases , sentences , paragraphs , and beyond ? Before exploring this possibility , recall the distinction between distributed and distri - butional representations . Neural embeddings such as WORD2VEC are both distributed ( vector-based ) and distributional ( derived from counts of words in context ) . As we con - sider larger units of text , the counts decrease : in the limit , a multi-paragraph span of text would never appear twice , except by plagiarism . Thus , the meaning of a large span of text cannot be determined from distributional statistics alone ; it must be computed com - positionally from smaller spans . But these considerations are orthogonal to the question of whether distributed representations — dense numerical vectors — are sufficiently ex - pressive to capture the meaning of phrases , sentences , and paragraphs . 14.8.1 Purely distributional methods Some multiword phrases are non-compositional : the meaning of such phrases is not de - rived from the meaning of the individual words using typical compositional semantics . This includes proper nouns like San Francisco as well as idiomatic expressions like kick the bucket ( Baldwin and Kim , 2010 ) . For these cases , purely distributional approaches can work . A simple approach is to identify multiword units that appear together fre - quently , and then treat these units as words , learning embeddings using a technique such as WORD2VEC . The problem of identifying multiword units is sometimes called collocation extrac - tion . A good collocation has high pointwise mutual information ( PMI ; see section 14.3 ) . For example , Naı̈ve Bayes is a good collocation because p ( wt = Bayes | wt − 1 = naı̈ve ) is much larger than p ( wt = Bayes ) . Collocations of more than two words can be identified by a greedy incremental search : for example , mutual information might first be extracted as a collocation and grouped into a single word type mutual information ; then pointwise mutual information can be extracted later . After identifying such units , they can be treated as words when estimating skipgram embeddings . Mikolov et al . ( 2013 ) show that the re - sulting embeddings perform reasonably well on a task of solving phrasal analogies , e.g . New York : New York Times : : Baltimore : Baltimore Sun . Jacob Eisenstein . Draft of October 15 , 2018 . 14.8 . DISTRIBUTED REPRESENTATIONS OF MULTIWORD UNITS 345 this was the only way it was the only way it was her turn to blink it was hard to tell it was time to move on he had to do it again they all looked at each other they all turned to look back they both turned to face him they both turned and walked away Figure 14.6 : By interpolating between the distributed representations of two sentences ( in bold ) , it is possible to generate grammatical sentences that combine aspects of both ( Bow - man et al . , 2016 ) 14.8.2 Distributional-compositional hybrids To move beyond short multiword phrases , composition is necessary . A simple but sur - prisingly powerful approach is to represent a sentence with the average of its word em - beddings ( Mitchell and Lapata , 2010 ) . This can be considered a hybrid of the distribu - tional and compositional approaches to semantics : the word embeddings are computed distributionally , and then the sentence representation is computed by composition . The WORD2VEC approach can be stretched considerably further , embedding entire sentences using a model similar to skipgrams , in the “ skip-thought ” model of Kiros et al . ( 2015 ) . Each sentence is encoded into a vector using a recurrent neural network : the encod - ing of sentence t is set to the RNN hidden state at its final token , h ( t ) Mt . This vector is then a parameter in a decoder model that is used to generate the previous and subsequent sen - tences : the decoder is another recurrent neural network , which takes the encoding of the neighboring sentence as an additional parameter in its recurrent update . ( This encoder - decoder model is discussed at length in chapter 18 . ) The encoder and decoder are trained simultaneously from a likelihood-based objective , and the trained encoder can be used to compute a distributed representation of any sentence . Skip-thought can also be viewed as a hybrid of distributional and compositional approaches : the vector representation of each sentence is computed compositionally from the representations of the individual words , but the training objective is distributional , based on sentence co-occurrence across a corpus . Autoencoders are a variant of encoder-decoder models in which the decoder is trained to produce the same text that was originally encoded , using only the distributed encod - ing vector ( Li et al . , 2015 ) . The encoding acts as a bottleneck , so that generalization is necessary if the model is to successfully fit the training data . In denoising autoencoders , Under contract with MIT Press , shared under CC-BY-NC-ND license . 346 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS the input is a corrupted version of the original sentence , and the auto-encoder must re - construct the uncorrupted original ( Vincent et al . , 2010 ; Hill et al . , 2016 ) . By interpolating between distributed representations of two sentences , αui + ( 1 − α ) uj , it is possible to gen - erate sentences that combine aspects of the two inputs , as shown in Figure 14.6 ( Bowman et al . , 2016 ) . Autoencoders can also be applied to longer texts , such as paragraphs and documents . This enables applications such as question answering , which can be performed by match - ing the encoding of the question with encodings of candidate answers ( Miao et al . , 2016 ) . 14.8.3 Supervised compositional methods Given a supervision signal , such as a label describing the sentiment or meaning of a sen - tence , a wide range of compositional methods can be applied to compute a distributed representation that then predicts the label . The simplest is to average the embeddings of each word in the sentence , and pass this average through a feedforward neural net - work ( Iyyer et al . , 2015 ) . Convolutional and recurrent neural networks go further , with the ability to effectively capturing multiword phenomena such as negation ( Kalchbrenner et al . , 2014 ; Kim , 2014 ; Li et al . , 2015 ; Tang et al . , 2015 ) . Another approach is to incorpo - rate the syntactic structure of the sentence into a recursive neural network , in which the representation for each syntactic constituent is computed from the representations of its children ( Socher et al . , 2012 ) . However , in many cases , recurrent neural networks perform as well or better than recursive networks ( Li et al . , 2015 ) . Whether convolutional , recurrent , or recursive , a key question is whether supervised sentence representations are task-specific , or whether a single supervised sentence repre - sentation model can yield useful performance on other tasks . Wieting et al . ( 2016b ) train a variety of sentence embedding models for the task of labeling pairs of sentences as para - phrases . They show that the resulting sentence embeddings give good performance for sentiment analysis . The Stanford Natural Language Inference corpus classifies sentence pairs as entailments ( the truth of sentence i implies the truth of sentence j ) , contradictions ( the truth of sentence i implies the falsity of sentence j ) , and neutral ( i neither entails nor contradicts j ) . Sentence embeddings trained on this dataset transfer to a wide range of classification tasks ( Conneau et al . , 2017 ) . 14.8.4 Hybrid distributed-symbolic representations The power of distributed representations is in their generality : the distributed represen - tation of a unit of text can serve as a summary of its meaning , and therefore as the input for downstream tasks such as classification , matching , and retrieval . For example , dis - tributed sentence representations can be used to recognize the paraphrase relationship between closely related sentences like the following : Jacob Eisenstein . Draft of October 15 , 2018 . 14.8 . DISTRIBUTED REPRESENTATIONS OF MULTIWORD UNITS 347 ( 14.5 ) a . Donald thanked Vlad profusely . b . Donald conveyed to Vlad his profound appreciation . c . Vlad was showered with gratitude by Donald . Symbolic representations are relatively brittle to this sort of variation , but are better suited to describe individual entities , the things that they do , and the things that are done to them . In examples ( 14.5a ) - ( 14.5c ) , we not only know that somebody thanked someone else , but we can make a range of inferences about what has happened between the entities named Donald and Vlad . Because distributed representations do not treat entities symbol - ically , they lack the ability to reason about the roles played by entities across a sentence or larger discourse . 5 A hybrid between distributed and symbolic representations might give the best of both worlds : robustness to the many different ways of describing the same event , plus the expressiveness to support inferences about entities and the roles that they play . A “ top-down ” hybrid approach is to begin with logical semantics ( of the sort de - scribed in the previous two chapters ) , and but replace the predefined lexicon with a set of distributional word clusters ( Poon and Domingos , 2009 ; Lewis and Steedman , 2013 ) . A “ bottom-up ” approach is to add minimal symbolic structure to existing distributed repre - sentations , such as vector representations for each entity ( Ji and Eisenstein , 2015 ; Wiseman et al . , 2016 ) . This has been shown to improve performance on two problems that we will encounter in the following chapters : classification of discourse relations between adja - cent sentences ( chapter 16 ; Ji and Eisenstein , 2015 ) , and coreference resolution of entity mentions ( chapter 15 ; Wiseman et al . , 2016 ; Ji et al . , 2017 ) . Research on hybrid seman - tic representations is still in an early stage , and future representations may deviate more boldly from existing symbolic and distributional approaches . Additional resources Turney and Pantel ( 2010 ) survey a number of facets of vector word representations , fo - cusing on matrix factorization methods . Schnabel et al . ( 2015 ) highlight problems with similarity-based evaluations of word embeddings , and present a novel evaluation that controls for word frequency . Baroni et al . ( 2014 ) address linguistic issues that arise in attempts to combine distributed and compositional representations . In bilingual and multilingual distributed representations , embeddings are estimated for translation pairs or tuples , such as ( dog , perro , chien ) . These embeddings can improve machine translation ( Zou et al . , 2013 ; Klementiev et al . , 2012 ) , transfer natural language 5At a 2014 workshop on semantic parsing , this critique of distributed representations was expressed by Ray Mooney — a leading researcher in computational semantics — in a now well-known quote , “ you can’t cram the meaning of a whole sentence into a single vector ! ” Under contract with MIT Press , shared under CC-BY-NC-ND license . 348 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS processing models across languages ( Täckström et al . , 2012 ) , and make monolingual word embeddings more accurate ( Faruqui and Dyer , 2014 ) . A typical approach is to learn a pro - jection that maximizes the correlation of the distributed representations of each element in a translation pair , which can be obtained from a bilingual dictionary . Distributed rep - resentations can also be linked to perceptual information , such as image features . Bruni et al . ( 2014 ) use textual descriptions of images to obtain visual contextual information for various words , which supplements traditional distributional context . Image features can also be inserted as contextual information in log bilinear language models ( Kiros et al . , 2014 ) , making it possible to automatically generate text descriptions of images . Exercises 1 . Prove that the sum of probabilities of paths through a hierarchical softmax tree is equal to one . 2 . In skipgram word embeddings , the negative sampling objective can be written as , L = ∑ i ∈ V ∑ j ∈ C count ( i , j ) ψ ( i , j ) , [ 14.29 ] with ψ ( i , j ) is defined in Equation 14.23 . Suppose we draw the negative samples from the empirical unigram distribution p̂ ( i ) = punigram ( i ) . First , compute the expectation of L with respect the negative samples , using this probability . Next , take the derivative of this expectation with respect to the score of a single word context pair σ ( ui · vj ) , and solve for the pointwise mutual information PMI ( i , j ) . You should be able to show that at the optimum , the PMI is a simple function of σ ( ui · vj ) and the number of negative samples . ( This exercise is part of a proof that shows that skipgram with negative sampling is closely related to PMI-weighted matrix factorization . ) 3 . * In Brown clustering , prove that the cluster merge that maximizes the average mu - tual information ( Equation 14.13 ) also maximizes the log-likelihood objective ( Equa - tion 14.12 ) . 4 . A simple way to compute a distributed phrase representation is to add up the dis - tributed representations of the words in the phrase . Consider a sentiment analysis model in which the predicted sentiment is , ψ ( w ) = θ · ( ∑ Mm = 1 xm ) , where xm is the vector representation of word m . Prove that in such a model , the following two Jacob Eisenstein . Draft of October 15 , 2018 . 14.8 . DISTRIBUTED REPRESENTATIONS OF MULTIWORD UNITS 349 inequalities cannot both hold : ψ ( good ) > ψ ( not good ) [ 14.30 ] ψ ( bad ) < ψ ( not bad ) . [ 14.31 ] Then construct a similar example pair for the case in which phrase representations are the average of the word representations . 5 . Now let’s consider a slight modification to the prediction model in the previous problem : ψ ( w ) = θ · ReLU ( M ∑ m = 1 xm ) [ 14.32 ] Show that in this case , it is possible to achieve the inequalities above . Your solution should provide the weights θ and the embeddings xgood , xbad , and xnot . For the next two problems , download a set of pre-trained word embeddings , such as the WORD2VEC or polyglot embeddings . 6 . Use cosine similarity to find the most similar words to : dog , whale , before , however , fabricate . 7 . Use vector addition and subtraction to compute target vectors for the analogies be - low . After computing each target vector , find the top three candidates by cosine similarity . • dog : puppy : : cat : ? • speak : speaker : : sing : ? • France : French : : England : ? • France : wine : : England : ? The remaining problems will require you to build a classifier and test its properties . Pick a text classification dataset , such as the Cornell Movie Review data . 6 Divide your data into training ( 60 % ) , development ( 20 % ) , and test sets ( 20 % ) , if no such division already exists . 8 . Train a convolutional neural network , with inputs set to pre-trained word embed - dings from the previous two problems . Use an additional , fine-tuned embedding for out-of-vocabulary words . Train until performance on the development set does not improve . You can also use the development set to tune the model architecture , such as the convolution width and depth . Report F - MEASURE and accuracy , as well as training time . 6http :// www.cs.cornell.edu/people/pabo/movie-review-data/ Under contract with MIT Press , shared under CC-BY-NC-ND license . 350 CHAPTER 14 . DISTRIBUTIONAL AND DISTRIBUTED SEMANTICS 9 . Now modify your model from the previous problem to fine-tune the word embed - dings . Report F - MEASURE , accuracy , and training time . 10 . Try a simpler approach , in which word embeddings in the document are averaged , and then this average is passed through a feed-forward neural network . Again , use the development data to tune the model architecture . How close is the accuracy to the convolutional networks from the previous problems ? Jacob Eisenstein . Draft of October 15 , 2018 .