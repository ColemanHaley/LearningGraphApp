Chapter 7 Sequence labeling The goal of sequence labeling is to assign tags to words , or more generally , to assign discrete labels to discrete elements in a sequence . There are many applications of se - quence labeling in natural language processing , and chapter 8 presents an overview . For now , we’ll focus on the classic problem of part-of-speech tagging , which requires tagging each word by its grammatical category . Coarse-grained grammatical categories include NOUNs , which describe things , properties , or ideas , and VERBs , which describe actions and events . Consider a simple input : ( 7.1 ) They can fish . A dictionary of coarse-grained part-of-speech tags might include NOUN as the only valid tag for they , but both NOUN and VERB as potential tags for can and fish . A accurate se - quence labeling algorithm should select the verb tag for both can and fish in ( 7.1 ) , but it should select noun for the same two words in the phrase can of fish . 7.1 Sequence labeling as classification One way to solve a tagging problem is to turn it into a classification problem . Let f ( ( w , m ) , y ) indicate the feature function for tag y at position m in the sequencew = ( w1 , w2 , . . . , wM ) . A simple tagging model would have a single base feature , the word itself : f ( ( w = they can fish , m = 1 ) , N ) =( they , N ) [ 7.1 ] f ( ( w = they can fish , m = 2 ) , V ) =( can , V ) [ 7.2 ] f ( ( w = they can fish , m = 3 ) , V ) =( fish , V ) . [ 7.3 ] Here the feature function takes three arguments as input : the sentence to be tagged ( e.g . , they can fish ) , the proposed tag ( e.g . , N or V ) , and the index of the token to which this tag 145 146 CHAPTER 7 . SEQUENCE LABELING is applied . This simple feature function then returns a single feature : a tuple including the word to be tagged and the tag that has been proposed . If the vocabulary size is V and the number of tags is K , then there are V × K features . Each of these features must be assigned a weight . These weights can be learned from a labeled dataset using a clas - sification algorithm such as perceptron , but this isn’t necessary in this case : it would be equivalent to define the classification weights directly , with θw , y = 1 for the tag y most frequently associated with word w , and θw , y = 0 for all other tags . However , it is easy to see that this simple classification approach cannot correctly tag both they can fish and can of fish , because can and fish are grammatically ambiguous . To han - dle both of these cases , the tagger must rely on context , such as the surrounding words . We can build context into the feature set by incorporating the surrounding words as ad - ditional features : f ( ( w = they can fish , 1 ) , N ) = { ( wm = they , ym = N ) , ( wm − 1 = � , ym = N ) , ( wm + 1 = can , ym = N ) } [ 7.4 ] f ( ( w = they can fish , 2 ) , V ) = { ( wm = can , ym = V ) , ( wm − 1 = they , ym = V ) , ( wm + 1 = fish , ym = V ) } [ 7.5 ] f ( ( w = they can fish , 3 ) , V ) = { ( wm = fish , ym = V ) , ( wm − 1 = can , ym = V ) , ( wm + 1 = � , ym = V ) } . [ 7.6 ] These features contain enough information that a tagger should be able to choose the right tag for the word fish : words that come after can are likely to be verbs , so the feature ( wm − 1 = can , ym = V ) should have a large positive weight . However , even with this enhanced feature set , it may be difficult to tag some se - quences correctly . One reason is that there are often relationships between the tags them - selves . For example , in English it is relatively rare for a verb to follow another verb — particularly if we differentiate MODAL verbs like can and should from more typical verbs , like give , transcend , and befuddle . We would like to incorporate preferences against tag se - quences like VERB-VERB , and in favor of tag sequences like NOUN-VERB . The need for such preferences is best illustrated by a garden path sentence : ( 7.2 ) The old man the boat . Grammatically , the word the is a DETERMINER . When you read the sentence , what part of speech did you first assign to old ? Typically , this word is an ADJECTIVE — abbrevi - ated as J — which is a class of words that modify nouns . Similarly , man is usually a noun . The resulting sequence of tags is D J N D N . But this is a mistaken “ garden path ” inter - pretation , which ends up leading nowhere . It is unlikely that a determiner would directly Jacob Eisenstein . Draft of October 15 , 2018 . 7.2 . SEQUENCE LABELING AS STRUCTURE PREDICTION 147 follow a noun , 1 and it is particularly unlikely that the entire sentence would lack a verb . The only possible verb in ( 7.2 ) is the word man , which can refer to the act of maintaining and piloting something — often boats . But if man is tagged as a verb , then old is seated between a determiner and a verb , and must be a noun . And indeed , adjectives often have a second interpretation as nouns when used in this way ( e.g . , the young , the restless ) . This reasoning , in which the labeling decisions are intertwined , cannot be applied in a setting where each tag is produced by an independent classification decision . 7.2 Sequence labeling as structure prediction As an alternative , think of the entire sequence of tags as a label itself . For a given sequence of words w = ( w1 , w2 , . . . , wM ) , there is a set of possible taggings Y ( w ) = YM , where Y = { N , V , D , . . . } refers to the set of individual tags , and YM refers to the set of tag sequences of lengthM . We can then treat the sequence labeling problem as a classification problem in the label space Y ( w ) , ŷ = argmax y ∈ Y ( w ) Ψ ( w , y ) , [ 7.7 ] where y = ( y1 , y2 , . . . , yM ) is a sequence of M tags , and Ψ is a scoring function on pairs of sequences , VM × YM → R . Such a function can include features that capture the rela - tionships between tagging decisions , such as the preference that determiners not follow nouns , or that all sentences have verbs . Given that the label space is exponentially large in the length of the sequence M , can it ever be practical to perform tagging in this way ? The problem of making a series of in - terconnected labeling decisions is known as inference . Because natural language is full of interrelated grammatical structures , inference is a crucial aspect of natural language pro - cessing . In English , it is not unusual to have sentences of length M = 20 ; part-of-speech tag sets vary in size from 10 to several hundred . Taking the low end of this range , we have | Y ( w1 : M ) | ≈ 1020 , one hundred billion billion possible tag sequences . Enumerating and scoring each of these sequences would require an amount of work that is exponential in the sequence length , so inference is intractable . However , the situation changes when we restrict the scoring function . Suppose we choose a function that decomposes into a sum of local parts , Ψ ( w , y ) = M + 1 ∑ m = 1 ψ ( w , ym , ym − 1 , m ) , [ 7.8 ] where each ψ ( · ) scores a local part of the tag sequence . Note that the sum goes up toM + 1 , so that we can include a score for a special end-of-sequence tag , ψ ( w1 : M , � , yM , M + 1 ) . We also define a special the tag to begin the sequence , y0 , ♦ . 1The main exception occurs with ditransitive verbs , such as They gave the winner a trophy . Under contract with MIT Press , shared under CC-BY-NC-ND license . 148 CHAPTER 7 . SEQUENCE LABELING In a linear model , local scoring function can be defined as a dot product of weights and features , ψ ( w1 : M , ym , ym − 1 , m ) = θ · f ( w , ym , ym − 1 , m ) . [ 7.9 ] The feature vector f can consider the entire input w , and can look at pairs of adjacent tags . This is a step up from per-token classification : the weights can assign low scores to infelicitous tag pairs , such as noun-determiner , and high scores for frequent tag pairs , such as determiner-noun and noun-verb . In the example they can fish , a minimal feature function would include features for word-tag pairs ( sometimes called emission features ) and tag-tag pairs ( sometimes called transition features ) : f ( w = they can fish , y = N V V ) = M + 1 ∑ m = 1 f ( w , ym , ym − 1 , m ) [ 7.10 ] = f ( w , N , ♦ , 1 ) + f ( w , V , N , 2 ) + f ( w , V , V , 3 ) + f ( w , � , V , 4 ) [ 7.11 ] =( wm = they , ym = N ) + ( ym = N , ym − 1 = ♦ ) + ( wm = can , ym = V ) + ( ym = V , ym − 1 = N ) + ( wm = fish , ym = V ) + ( ym = V , ym − 1 = V ) + ( ym = � , ym − 1 = V ) . [ 7.12 ] There are seven active features for this example : one for each word-tag pair , and one for each tag-tag pair , including a final tag yM + 1 = � . These features capture the two main sources of information for part-of-speech tagging in English : which tags are appropriate for each word , and which tags tend to follow each other in sequence . Given appropriate weights for these features , taggers can achieve high accuracy , even for difficult cases like the old man the boat . We will now discuss how this restricted scoring function enables efficient inference , through the Viterbi algorithm ( Viterbi , 1967 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 7.3 . THE VITERBI ALGORITHM 149 7.3 The Viterbi algorithm By decomposing the scoring function into a sum of local parts , it is possible to rewrite the tagging problem as follows : ŷ = argmax y ∈ Y ( w ) Ψ ( w , y ) [ 7.13 ] = argmax y1 : M M + 1 ∑ m = 1 ψ ( w , ym , ym − 1 , m ) [ 7.14 ] = argmax y1 : M M + 1 ∑ m = 1 sm ( ym , ym − 1 ) , [ 7.15 ] where the final line simplifies the notation with the shorthand , sm ( ym , ym − 1 ) , ψ ( w1 : M , ym , ym − 1 , m ) . [ 7.16 ] This inference problem can be solved efficiently using dynamic programming , an al - gorithmic technique for reusing work in recurrent computations . We begin by solving an auxiliary problem : rather than finding the best tag sequence , we compute the score of the best tag sequence , max y1 : M Ψ ( w , y1 : M ) = max y1 : M M + 1 ∑ m = 1 sm ( ym , ym − 1 ) . [ 7.17 ] This score involves a maximization over all tag sequences of length M , written maxy1 : M . This maximization can be broken into two pieces , max y1 : M Ψ ( w , y1 : M ) = max yM max y1 : M − 1 M + 1 ∑ m = 1 sm ( ym , ym − 1 ) . [ 7.18 ] Within the sum , only the final term sM + 1 ( � , yM ) depends on yM , so we can pull this term out of the second maximization , max y1 : M Ψ ( w , y1 : M ) = ( max yM sM + 1 ( � , yM ) ) + ( max y1 : M − 1 M ∑ m = 1 sm ( ym , ym − 1 ) ) . [ 7.19 ] The second term in Equation 7.19 has the same form as our original problem , with M replaced byM − 1 . This indicates that the problem can be reformulated as a recurrence . We do this by defining an auxiliary variable called the Viterbi variable vm ( k ) , representing Under contract with MIT Press , shared under CC-BY-NC-ND license . 150 CHAPTER 7 . SEQUENCE LABELING Algorithm 11 The Viterbi algorithm . Each sm ( k , k ′ ) is a local score for tag ym = k and ym − 1 = k ′ . for k ∈ { 0 , . . . K } do v1 ( k ) = s1 ( k , ♦ ) for m ∈ { 2 , . . . , M } do for k ∈ { 0 , . . . , K } do vm ( k ) = maxk ′ sm ( k , k ′ ) + vm − 1 ( k ′ ) bm ( k ) = argmaxk ′ sm ( k , k ′ ) + vm − 1 ( k ′ ) yM = argmaxk sM + 1 ( � , k ) + vM ( k ) for m ∈ { M − 1 , . . . 1 } do ym = bm ( ym + 1 ) return y1 : M the score of the best sequence terminating in the tag k : vm ( ym ) , max y1 : m − 1 m ∑ n = 1 sn ( yn , yn − 1 ) [ 7.20 ] = max ym − 1 sm ( ym , ym − 1 ) + max y1 : m − 2 m − 1 ∑ n = 1 sn ( yn , yn − 1 ) [ 7.21 ] = max ym − 1 sm ( ym , ym − 1 ) + vm − 1 ( ym − 1 ) . [ 7.22 ] Each set of Viterbi variables is computed from the local score sm ( ym , ym − 1 ) , and from the previous set of Viterbi variables . The initial condition of the recurrence is simply the score for the first tag , v1 ( y1 ) , s1 ( y1 , ♦ ) . [ 7.23 ] The maximum overall score for the sequence is then the final Viterbi variable , max y1 : M Ψ ( w1 : M , y1 : M ) = vM + 1 ( � ) . [ 7.24 ] Thus , the score of the best labeling for the sequence can be computed in a single forward sweep : first compute all variables v1 ( · ) from Equation 7.23 , and then compute all variables v2 ( · ) from the recurrence in Equation 7.22 , continuing until the final variable vM + 1 ( � ) . The Viterbi variables can be arranged in a structure known as a trellis , shown in Fig - ure 7.1 . Each column indexes a token m in the sequence , and each row indexes a tag in Y ; every vm − 1 ( k ) is connected to every vm ( k ′ ) , indicating that vm ( k ′ ) is computed from vm − 1 ( k ) . Special nodes are set aside for the start and end states . Jacob Eisenstein . Draft of October 15 , 2018 . 7.3 . THE VITERBI ALGORITHM 151 0 they can fish - 10 N - 3 - 9 - 9 V - 12 - 5 - 11 Figure 7.1 : The trellis representation of the Viterbi variables , for the example they can fish , using the weights shown in Table 7.1 . The original goal was to find the best scoring sequence , not simply to compute its score . But by solving the auxiliary problem , we are almost there . Recall that each vm ( k ) represents the score of the best tag sequence ending in that tag k in positionm . To compute this , we maximize over possible values of ym − 1 . By keeping track of the “ argmax ” tag that maximizes this choice at each step , we can walk backwards from the final tag , and recover the optimal tag sequence . This is indicated in Figure 7.1 by the thick lines , which we trace back from the final position . These backward pointers are written bm ( k ) , indicating the optimal tag ym − 1 on the path to Ym = k . The complete Viterbi algorithm is shown in Algorithm 11 . When computing the initial Viterbi variables v1 ( · ) , the special tag ♦ indicates the start of the sequence . When comput - ing the final tag YM , another special tag , � indicates the end of the sequence . These special tags enable the use of transition features for the tags that begin and end the sequence : for example , conjunctions are unlikely to end sentences in English , so we would like a low score for sM + 1 ( � , CC ) ; nouns are relatively likely to appear at the beginning of sentences , so we would like a high score for s1 ( N , ♦ ) , assuming the noun tag is compatible with the first word token w1 . Complexity If there are K tags and M positions in the sequence , then there are M × K Viterbi variables to compute . Computing each variable requires finding a maximum over K possible predecessor tags . The total time complexity of populating the trellis is there - foreO ( MK2 ) , with an additional factor for the number of active features at each position . After completing the trellis , we simply trace the backwards pointers to the beginning of the sequence , which takes O ( M ) operations . Under contract with MIT Press , shared under CC-BY-NC-ND license . 152 CHAPTER 7 . SEQUENCE LABELING they can fish N − 2 − 3 − 3 V − 10 − 1 − 3 ( a ) Weights for emission features . N V � ♦ − 1 − 2 − ∞ N − 3 − 1 − 1 V − 1 − 3 − 1 ( b ) Weights for transition features . The “ from ” tags are on the columns , and the “ to ” tags are on the rows . Table 7.1 : Feature weights for the example trellis shown in Figure 7.1 . Emission weights from ♦ and � are implicitly set to − ∞ . 7.3.1 Example Consider the minimal tagset { N , V } , corresponding to nouns and verbs . Even in this tagset , there is considerable ambiguity : for example , the words can and fish can each take both tags . Of the 2 × 2 × 2 = 8 possible taggings for the sentence they can fish , four are possible given these possible tags , and two are grammatical . 2 The values in the trellis in Figure 7.1 are computed from the feature weights defined in Table 7.1 . We begin with v1 ( N ) , which has only one possible predecessor , the start tag ♦ . This score is therefore equal to s1 ( N , ♦ ) = − 2 − 1 = − 3 , which is the sum of the scores for the emission and transition features respectively ; the backpointer is b1 ( N ) = ♦ . The score for v1 ( V ) is computed in the same way : s1 ( V , ♦ ) = − 10 − 2 = − 12 , and again b1 ( V ) = ♦ . The backpointers are represented in the figure by thick lines . Things get more interesting at m = 2 . The score v2 ( N ) is computed by maximizing over the two possible predecessors , v2 ( N ) = max ( v1 ( N ) + s2 ( N , N ) , v1 ( V ) + s2 ( N , V ) ) [ 7.25 ] = max ( − 3 − 3 − 3 , − 12 − 3 − 1 ) = − 9 [ 7.26 ] b2 ( N ) = N . [ 7.27 ] This continues until reaching v4 ( � ) , which is computed as , v4 ( � ) = max ( v3 ( N ) + s4 ( � , N ) , v3 ( V ) + s4 ( � , V ) ) [ 7.28 ] = max ( − 9 + 0 − 1 , − 11 + 0 − 1 ) [ 7.29 ] = − 10 , [ 7.30 ] so b4 ( � ) = N . As there is no emission w4 , the emission features have scores of zero . 2The tagging they / N can / V fish / N corresponds to the scenario of putting fish into cans , or perhaps of firing them . Jacob Eisenstein . Draft of October 15 , 2018 . 7.4 . HIDDEN MARKOV MODELS 153 To compute the optimal tag sequence , we walk backwards from here , next checking b3 ( N ) = V , and then b2 ( V ) = N , and finally b1 ( N ) = ♦ . This yields y = ( N , V , N ) , which corresponds to the linguistic interpretation of the fishes being put into cans . 7.3.2 Higher-order features The Viterbi algorithm was made possible by a restriction of the scoring function to local parts that consider only pairs of adjacent tags . We can think of this as a bigram language model over tags . A natural question is how to generalize Viterbi to tag trigrams , which would involve the following decomposition : Ψ ( w , y ) = M + 2 ∑ m = 1 f ( w , ym , ym − 1 , ym − 2 , m ) , [ 7.31 ] where y − 1 = ♦ and yM + 2 = � . One solution is to create a new tagset Y ( 2 ) from the Cartesian product of the original tagset with itself , Y ( 2 ) = Y × Y . The tags in this product space are ordered pairs , rep - resenting adjacent tags at the token level : for example , the tag ( N , V ) would represent a noun followed by a verb . Transitions between such tags must be consistent : we can have a transition from ( N , V ) to ( V , N ) ( corresponding to the tag sequence N V N ) , but not from ( N , V ) to ( N , N ) , which would not correspond to any coherent tag sequence . This con - straint can be enforced in feature weights , with θ ( ( a , b ) , ( c , d ) ) = − ∞ if b 6 = c . The remaining feature weights can encode preferences for and against various tag trigrams . In the Cartesian product tag space , there are K2 tags , suggesting that the time com - plexity will increase to O ( MK4 ) . However , it is unnecessary to max over predecessor tag bigrams that are incompatible with the current tag bigram . By exploiting this constraint , it is possible to limit the time complexity to O ( MK3 ) . The space complexity grows to O ( MK2 ) , since the trellis must store all possible predecessors of each tag . In general , the time and space complexity of higher-order Viterbi grows exponentially with the order of the tag n-grams that are considered in the feature decomposition . 7.4 Hidden Markov Models The Viterbi sequence labeling algorithm is built on the scores sm ( y , y ′ ) . We will now dis - cuss how these scores can be estimated probabilistically . Recall from section 2.2 that the probabilistic Naı̈ve Bayes classifier selects the label y to maximize p ( y | x ) ∝ p ( y , x ) . In probabilistic sequence labeling , our goal is similar : select the tag sequence that maximizes p ( y | w ) ∝ p ( y , w ) . The locality restriction in Equation 7.8 can be viewed as a conditional independence assumption on the random variables y . Under contract with MIT Press , shared under CC-BY-NC-ND license . 154 CHAPTER 7 . SEQUENCE LABELING Algorithm 12 Generative process for the hidden Markov model y0 ← ♦ , m ← 1 repeat ym ∼ Categorical ( λym − 1 ) . sample the current tag wm ∼ Categorical ( φym ) . sample the current word until ym = � . terminate when the stop symbol is generated Naı̈ve Bayes was introduced as a generative model — a probabilistic story that ex - plains the observed data as well as the hidden label . A similar story can be constructed for probabilistic sequence labeling : first , the tags are drawn from a prior distribution ; next , the tokens are drawn from a conditional likelihood . However , for inference to be tractable , additional independence assumptions are required . First , the probability of each token depends only on its tag , and not on any other element in the sequence : p ( w | y ) = M ∏ m = 1 p ( wm | ym ) . [ 7.32 ] Second , each tag ym depends only on its predecessor , p ( y ) = M ∏ m = 1 p ( ym | ym − 1 ) , [ 7.33 ] where y0 = ♦ in all cases . Due to this Markov assumption , probabilistic sequence labeling models are known as hidden Markov models ( HMMs ) . The generative process for the hidden Markov model is shown in Algorithm 12 . Given the parameters λ and φ , we can compute p ( w , y ) for any token sequence w and tag se - quence y . The HMM is often represented as a graphical model ( Wainwright and Jordan , 2008 ) , as shown in Figure 7.2 . This representation makes the independence assumptions explicit : if a variable v1 is probabilistically conditioned on another variable v2 , then there is an arrow v2 → v1 in the diagram . If there are no arrows between v1 and v2 , they are conditionally independent , given each variable’s Markov blanket . In the hidden Markov model , the Markov blanket for each tag ym includes the “ parent ” ym − 1 , and the “ children ” ym + 1 and wm . 3 It is important to reflect on the implications of the HMM independence assumptions . A non-adjacent pair of tags ym and yn are conditionally independent ; if m < n and we are given yn − 1 , then ym offers no additional information about yn . However , if we are not given any information about the tags in a sequence , then all tags are probabilistically coupled . 3In general graphical models , a variable’s Markov blanket includes its parents , children , and its children’s other parents ( Murphy , 2012 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 7.4 . HIDDEN MARKOV MODELS 155 y1 y2 · · · yM w1 w2 · · · wM Figure 7.2 : Graphical representation of the hidden Markov model . Arrows indicate prob - abilistic dependencies . 7.4.1 Estimation The hidden Markov model has two groups of parameters : Emission probabilities . The probability pe ( wm | ym ; φ ) is the emission probability , since the words are treated as probabilistically “ emitted ” , conditioned on the tags . Transition probabilities . The probability pt ( ym | ym − 1 ; λ ) is the transition probability , since it assigns probability to each possible tag-to-tag transition . Both of these groups of parameters are typically computed from smoothed relative frequency estimation on a labeled corpus ( see section 6.2 for a review of smoothing ) . The unsmoothed probabilities are , φk , i , Pr ( Wm = i | Ym = k ) = count ( Wm = i , Ym = k ) count ( Ym = k ) λk , k ′ , Pr ( Ym = k ′ | Ym − 1 = k ) = count ( Ym = k ′ , Ym − 1 = k ) count ( Ym − 1 = k ) . Smoothing is more important for the emission probability than the transition probability , because the vocabulary is much larger than the number of tags . 7.4.2 Inference The goal of inference in the hidden Markov model is to find the highest probability tag sequence , ŷ = argmax y p ( y | w ) . [ 7.34 ] As in Naı̈ve Bayes , it is equivalent to find the tag sequence with the highest log-probability , since the logarithm is a monotonically increasing function . It is furthermore equivalent to maximize the joint probability p ( y , w ) = p ( y | w ) × p ( w ) ∝ p ( y | w ) , which is pro - portional to the conditional probability . Putting these observations together , the inference Under contract with MIT Press , shared under CC-BY-NC-ND license . 156 CHAPTER 7 . SEQUENCE LABELING problem can be reformulated as , ŷ = argmax y log p ( y , w ) . [ 7.35 ] We can now apply the HMM independence assumptions : log p ( y , w ) = log p ( y ) + log p ( w | y ) [ 7.36 ] = M + 1 ∑ m = 1 log pY ( ym | ym − 1 ) + log pW | Y ( wm | ym ) [ 7.37 ] = M + 1 ∑ m = 1 log λym , ym − 1 + log φym , wm [ 7.38 ] = M + 1 ∑ m = 1 sm ( ym , ym − 1 ) , [ 7.39 ] where , sm ( ym , ym − 1 ) , log λym , ym − 1 + log φym , wm , [ 7.40 ] and , φ � , w = { 1 , w = � 0 , otherwise , [ 7.41 ] which ensures that the stop tag � can only be applied to the final token � . This derivation shows that HMM inference can be viewed as an application of the Viterbi decoding algorithm , given an appropriately defined scoring function . The local score sm ( ym , ym − 1 ) can be interpreted probabilistically , sm ( ym , ym − 1 ) = log py ( ym | ym − 1 ) + log pw | y ( wm | ym ) [ 7.42 ] = log p ( ym , wm | ym − 1 ) . [ 7.43 ] Now recall the definition of the Viterbi variables , vm ( ym ) = max ym − 1 sm ( ym , ym − 1 ) + vm − 1 ( ym − 1 ) [ 7.44 ] = max ym − 1 log p ( ym , wm | ym − 1 ) + vm − 1 ( ym − 1 ) . [ 7.45 ] By setting vm − 1 ( ym − 1 ) = maxy1 : m − 2 log p ( y1 : m − 1 , w1 : m − 1 ) , we obtain the recurrence , vm ( ym ) = max ym − 1 log p ( ym , wm | ym − 1 ) + max y1 : m − 2 log p ( y1 : m − 1 , w1 : m − 1 ) [ 7.46 ] = max y1 : m − 1 log p ( ym , wm | ym − 1 ) + log p ( y1 : m − 1 , w1 : m − 1 ) [ 7.47 ] = max y1 : m − 1 log p ( y1 : m , w1 : m ) . [ 7.48 ] Jacob Eisenstein . Draft of October 15 , 2018 . 7.5 . DISCRIMINATIVE SEQUENCE LABELING WITH FEATURES 157 In words , the Viterbi variable vm ( ym ) is the log probability of the best tag sequence ending in ym , joint with the word sequence w1 : m . The log probability of the best complete tag sequence is therefore , max y1 : M log p ( y1 : M + 1 , w1 : M + 1 ) = vM + 1 ( � ) [ 7.49 ] * Viterbi as an example of the max-product algorithm The Viterbi algorithm can also be implemented using probabilities , rather than log-probabilities . In this case , each vm ( ym ) is equal to , vm ( ym ) = max y1 : m − 1 p ( y1 : m − 1 , ym , w1 : m ) [ 7.50 ] = max ym − 1 p ( ym , wm | ym − 1 ) × max y1 : m − 2 p ( y1 : m − 2 , ym − 1 , w1 : m − 1 ) [ 7.51 ] = max ym − 1 p ( ym , wm | ym − 1 ) × vm − 1 ( ym − 1 ) [ 7.52 ] = pw | y ( wm | ym ) × maxym − 1 py ( ym | ym − 1 ) × vm − 1 ( ym − 1 ) . [ 7.53 ] Each Viterbi variable is computed by maximizing over a set of products . Thus , the Viterbi algorithm is a special case of the max-product algorithm for inference in graphical mod - els ( Wainwright and Jordan , 2008 ) . However , the product of probabilities tends towards zero over long sequences , so the log-probability version of Viterbi is recommended in practical implementations . 7.5 Discriminative sequence labeling with features Today , hidden Markov models are rarely used for supervised sequence labeling . This is because HMMs are limited to only two phenomena : • word-tag compatibility , via the emission probability pW | Y ( wm | ym ) ; • local context , via the transition probability pY ( ym | ym − 1 ) . The Viterbi algorithm permits the inclusion of richer information in the local scoring func - tion ψ ( w1 : M , ym , ym − 1 , m ) , which can be defined as a weighted sum of arbitrary local fea - tures , ψ ( w , ym , ym − 1 , m ) = θ · f ( w , ym , ym − 1 , m ) , [ 7.54 ] where f is a locally-defined feature function , and θ is a vector of weights . Under contract with MIT Press , shared under CC-BY-NC-ND license . 158 CHAPTER 7 . SEQUENCE LABELING The local decomposition of the scoring function Ψ is reflected in a corresponding de - composition of the feature function : Ψ ( w , y ) = M + 1 ∑ m = 1 ψ ( w , ym , ym − 1 , m ) [ 7.55 ] = M + 1 ∑ m = 1 θ · f ( w , ym , ym − 1 , m ) [ 7.56 ] = θ · M + 1 ∑ m = 1 f ( w , ym , ym − 1 , m ) [ 7.57 ] = θ · f ( global ) ( w , y1 : M ) , [ 7.58 ] where f ( global ) ( w , y ) is a global feature vector , which is a sum of local feature vectors , f ( global ) ( w , y ) = M + 1 ∑ m = 1 f ( w1 : M , ym , ym − 1 , m ) , [ 7.59 ] with yM + 1 = � and y0 = ♦ by construction . Let’s now consider what additional information these features might encode . Word affix features . Consider the problem of part-of-speech tagging on the first four lines of the poem Jabberwocky ( Carroll , 1917 ) : ( 7.3 ) ’ Twas brillig , and the slithy toves Did gyre and gimble in the wabe : All mimsy were the borogoves , And the mome raths outgrabe . Many of these words were made up by the author of the poem , so a corpus would offer no information about their probabilities of being associated with any particular part of speech . Yet it is not so hard to see what their grammatical roles might be in this passage . Context helps : for example , the word slithy follows the determiner the , so it is probably a noun or adjective . Which do you think is more likely ? The suffix - thy is found in a number of adjectives , like frothy , healthy , pithy , worthy . It is also found in a handful of nouns — e.g . , apathy , sympathy — but nearly all of these have the longer coda - pathy , unlike slithy . So the suffix gives some evidence that slithy is an adjective , and indeed it is : later in the text we find that it is a combination of the adjectives lithe and slimy . 4 4Morphology is the study of how words are formed from smaller linguistic units . chapter 9 touches on computational approaches to morphological analysis . See Bender ( 2013 ) for an overview of the underlying linguistic principles , and Haspelmath and Sims ( 2013 ) or Lieber ( 2015 ) for a full treatment . Jacob Eisenstein . Draft of October 15 , 2018 . 7.5 . DISCRIMINATIVE SEQUENCE LABELING WITH FEATURES 159 Fine-grained context . The hidden Markov model captures contextual information in the form of part-of-speech tag bigrams . But sometimes , the necessary contextual information is more specific . Consider the noun phrases this fish and these fish . Many part-of-speech tagsets distinguish between singular and plural nouns , but do not distinguish between singular and plural determiners ; for example , the well known Penn Treebank tagset fol - lows these conventions . A hidden Markov model would be unable to correctly label fish as singular or plural in both of these cases , because it only has access to two features : the pre - ceding tag ( determiner in both cases ) and the word ( fish in both cases ) . The classification - based tagger discussed in section 7.1 had the ability to use preceding and succeeding words as features , and it can also be incorporated into a Viterbi-based sequence labeler as a local feature . Example . Consider the tagging D J N ( determiner , adjective , noun ) for the sequence the slithy toves , so that w = the slithy toves y =D J N . Let’s create the feature vector for this example , assuming that we have word-tag features ( indicated by W ) , tag-tag features ( indicated by T ) , and suffix features ( indicated by M ) . You can assume that you have access to a method for extracting the suffix - thy from slithy , - es from toves , and ∅ from the , indicating that this word has no suffix . 5 The resulting feature vector is , f ( the slithy toves , D J N ) = f ( the slithy toves , D , ♦ , 1 ) + f ( the slithy toves , J , D , 2 ) + f ( the slithy toves , N , J , 3 ) + f ( the slithy toves , � , N , 4 ) ={ ( T : ♦ , D ) , ( W : the , D ) , ( M : ∅ , D ) , ( T : D , J ) , ( W : slithy , J ) , ( M : - thy , J ) , ( T : J , N ) , ( W : toves , N ) , ( M : - es , N ) ( T : N , � ) } . These examples show that local features can incorporate information that lies beyond the scope of a hidden Markov model . Because the features are local , it is possible to apply the Viterbi algorithm to identify the optimal sequence of tags . The remaining question 5Such a system is called a morphological segmenter . The task of morphological segmentation is briefly described in section 9.1.4 ; a well known segmenter is MORFESSOR ( Creutz and Lagus , 2007 ) . In real applica - tions , a typical approach is to include features for all orthographic suffixes up to some maximum number of characters : for slithy , we would have suffix features for - y , - hy , and - thy . Under contract with MIT Press , shared under CC-BY-NC-ND license . 160 CHAPTER 7 . SEQUENCE LABELING is how to estimate the weights on these features . section 2.3 presented three main types of discriminative classifiers : perceptron , support vector machine , and logistic regression . Each of these classifiers has a structured equivalent , enabling it to be trained from labeled sequences rather than individual tokens . 7.5.1 Structured perceptron The perceptron classifier is trained by increasing the weights for features that are asso - ciated with the correct label , and decreasing the weights for features that are associated with incorrectly predicted labels : ŷ = argmax y ∈ Y θ · f ( x , y ) [ 7.60 ] θ ( t + 1 ) ← θ ( t ) + f ( x , y ) − f ( x , ŷ ) . [ 7.61 ] We can apply exactly the same update in the case of structure prediction , ŷ = argmax y ∈ Y ( w ) θ · f ( w , y ) [ 7.62 ] θ ( t + 1 ) ← θ ( t ) + f ( w , y ) − f ( w , ŷ ) . [ 7.63 ] This learning algorithm is called structured perceptron , because it learns to predict the structured output y . The only difference is that instead of computing ŷ by enumerating the entire set Y , the Viterbi algorithm is used to efficiently search the set of possible tag - gings , YM . Structured perceptron can be applied to other structured outputs as long as efficient inference is possible . As in perceptron classification , weight averaging is crucial to get good performance ( see subsection 2.3.2 ) . Example For the example they can fish , suppose that the reference tag sequence is y ( i ) = N V V , but the tagger incorrectly returns the tag sequence ŷ = N V N . Assuming a model with features for emissions ( wm , ym ) and transitions ( ym − 1 , ym ) , the corresponding struc - tured perceptron update is : θ ( fish , V ) ← θ ( fish , V ) + 1 , θ ( fish , N ) ← θ ( fish , N ) − 1 [ 7.64 ] θ ( V , V ) ← θ ( V , V ) + 1 , θ ( V , N ) ← θ ( V , N ) − 1 [ 7.65 ] θ ( V , � ) ← θ ( V , � ) + 1 , θ ( N , � ) ← θ ( N , � ) − 1 . [ 7.66 ] 7.5.2 Structured support vector machines Large-margin classifiers such as the support vector machine improve on the perceptron by pushing the classification boundary away from the training instances . The same idea can Jacob Eisenstein . Draft of October 15 , 2018 . 7.5 . DISCRIMINATIVE SEQUENCE LABELING WITH FEATURES 161 be applied to sequence labeling . A support vector machine in which the output is a struc - tured object , such as a sequence , is called a structured support vector machine ( Tsochan - taridis et al . , 2004 ) . 6 In classification , we formalized the large-margin constraint as , ∀ y 6 = y ( i ) , θ · f ( x , y ( i ) ) − θ · f ( x , y ) ≥ 1 , [ 7.67 ] requiring a margin of at least 1 between the scores for all labels y that are not equal to the correct label y ( i ) . The weights θ are then learned by constrained optimization ( see subsection 2.4.2 ) . This idea can be applied to sequence labeling by formulating an equivalent set of con - straints for all possible labelings Y ( w ) for an input w . However , there are two problems . First , in sequence labeling , some predictions are more wrong than others : we may miss only one tag out of fifty , or we may get all fifty wrong . We would like our learning algo - rithm to be sensitive to this difference . Second , the number of constraints is equal to the number of possible labelings , which is exponentially large in the length of the sequence . The first problem can be addressed by adjusting the constraint to require larger mar - gins for more serious errors . Let c ( y ( i ) , ŷ ) ≥ 0 represent the cost of predicting label ŷwhen the true label is y ( i ) . We can then generalize the margin constraint , ∀ y , θ · f ( w ( i ) , y ( i ) ) − θ · f ( w ( i ) , y ) ≥ c ( y ( i ) , y ) . [ 7.68 ] This cost-augmented margin constraint specializes to the constraint in Equation 7.67 if we choose the delta function c ( y ( i ) , y ) = δ ( ( ) y ( i ) 6 = y ) . A more expressive cost function is the Hamming cost , c ( y ( i ) , y ) = M ∑ m = 1 δ ( y ( i ) m 6 = ym ) , [ 7.69 ] which computes the number of errors in y . By incorporating the cost function as the margin constraint , we require that the true labeling be seperated from the alternatives by a margin that is proportional to the number of incorrect tags in each alternative labeling . The second problem is that the number of constraints is exponential in the length of the sequence . This can be addressed by focusing on the prediction ŷ that maximally violates the margin constraint . This prediction can be identified by solving the following cost-augmented decoding problem : ŷ = argmax y 6 = y ( i ) θ · f ( w ( i ) , y ) − θ · f ( w ( i ) , y ( i ) ) + c ( y ( i ) , y ) [ 7.70 ] = argmax y 6 = y ( i ) θ · f ( w ( i ) , y ) + c ( y ( i ) , y ) , [ 7.71 ] 6This model is also known as a max-margin Markov network ( Taskar et al . , 2003 ) , emphasizing that the scoring function is constructed from a sum of components , which are Markov independent . Under contract with MIT Press , shared under CC-BY-NC-ND license . 162 CHAPTER 7 . SEQUENCE LABELING where in the second line we drop the term θ · f ( w ( i ) , y ( i ) ) , which is constant in y . We can now reformulate the margin constraint for sequence labeling , θ · f ( w ( i ) , y ( i ) ) − max y ∈ Y ( w ) ( θ · f ( w ( i ) , y ) + c ( y ( i ) , y ) ) ≥ 0 . [ 7.72 ] If the score for θ · f ( w ( i ) , y ( i ) ) is greater than the cost-augmented score for all alternatives , then the constraint will be met . The name “ cost-augmented decoding ” is due to the fact that the objective includes the standard decoding problem , maxŷ ∈ Y ( w ) θ · f ( w , ŷ ) , plus an additional term for the cost . Essentially , we want to train against predictions that are strong and wrong : they should score highly according to the model , yet incur a large loss with respect to the ground truth . Training adjusts the weights to reduce the score of these predictions . For cost-augmented decoding to be tractable , the cost function must decompose into local parts , just as the feature function f ( · ) does . The Hamming cost , defined above , obeys this property . To perform cost-augmented decoding using the Hamming cost , we need only to add features fm ( ym ) = δ ( ym 6 = y ( i ) m ) , and assign a constant weight of 1 to these features . Decoding can then be performed using the Viterbi algorithm . 7 As with large-margin classifiers , it is possible to formulate the learning problem in an unconstrained form , by combining a regularization term on the weights and a Lagrangian for the constraints : min θ 1 2 | | θ | | 22 − C ( ∑ i θ · f ( w ( i ) , y ( i ) ) − max y ∈ Y ( w ( i ) ) [ θ · f ( w ( i ) , y ) + c ( y ( i ) , y ) ] ) , [ 7.73 ] In this formulation , C is a parameter that controls the tradeoff between the regulariza - tion term and the margin constraints . A number of optimization algorithms have been proposed for structured support vector machines , some of which are discussed in sub - section 2.4.2 . An empirical comparison by Kummerfeld et al . ( 2015 ) shows that stochastic subgradient descent — which is essentially a cost-augmented version of the structured perceptron — is highly competitive . 7.5.3 Conditional random fields The conditional random field ( CRF ; Lafferty et al . , 2001 ) is a conditional probabilistic model for sequence labeling ; just as structured perceptron is built on the perceptron clas - sifier , conditional random fields are built on the logistic regression classifier . 8 The basic 7Are there cost functions that do not decompose into local parts ? Suppose we want to assign a constant loss c to any prediction ŷ in which k or more predicted tags are incorrect , and zero loss otherwise . This loss function is combinatorial over the predictions , and thus we cannot decompose it into parts . 8The name “ conditional random field ” is derived from Markov random fields , a general class of models in which the probability of a configuration of variables is proportional to a product of scores across pairs ( or Jacob Eisenstein . Draft of October 15 , 2018 . 7.5 . DISCRIMINATIVE SEQUENCE LABELING WITH FEATURES 163 probability model is , p ( y | w ) = exp ( Ψ ( w , y ) ) ∑ y ′ ∈ Y ( w ) exp ( Ψ ( w , y ′ ) ) . [ 7.74 ] This is almost identical to logistic regression ( section 2.5 ) , but because the label space is now sequences of tags , we require efficient algorithms for both decoding ( searching for the best tag sequence given a sequence of wordsw and a model θ ) and for normalization ( summing over all tag sequences ) . These algorithms will be based on the usual locality assumption on the scoring function , Ψ ( w , y ) = ∑ M + 1 m = 1 ψ ( w , ym , ym − 1 , m ) . Decoding in CRFs Decoding — finding the tag sequence ŷ that maximizes p ( y | w ) — is a direct applica - tion of the Viterbi algorithm . The key observation is that the decoding problem does not depend on the denominator of p ( y | w ) , ŷ = argmax y log p ( y | w ) = argmax y Ψ ( y , w ) − log ∑ y ′ ∈ Y ( w ) exp Ψ ( y ′ , w ) = argmax y Ψ ( y , w ) = argmax y M + 1 ∑ m = 1 sm ( ym , ym − 1 ) . This is identical to the decoding problem for structured perceptron , so the same Viterbi recurrence as defined in Equation 7.22 can be used . Learning in CRFs As with logistic regression , the weights θ are learned by minimizing the regularized neg - ative log-probability , ` = λ 2 | | θ | | 2 − N ∑ i = 1 log p ( y ( i ) | w ( i ) ; θ ) [ 7.75 ] = λ 2 | | θ | | 2 − N ∑ i = 1 θ · f ( w ( i ) , y ( i ) ) + log ∑ y ′ ∈ Y ( w ( i ) ) exp ( θ · f ( w ( i ) , y ′ ) ) , [ 7.76 ] more generally , cliques ) of variables in a factor graph . In sequence labeling , the pairs of variables include all adjacent tags ( ym , ym − 1 ) . The probability is conditioned on the words w , which are always observed , motivating the term “ conditional ” in the name . Under contract with MIT Press , shared under CC-BY-NC-ND license . 164 CHAPTER 7 . SEQUENCE LABELING where λ controls the amount of regularization . The final term in Equation 7.76 is a sum over all possible labelings . This term is the log of the denominator in Equation 7.74 , some - times known as the partition function . 9 There are | Y | M possible labelings of an input of size M , so we must again exploit the decomposition of the scoring function to compute this sum efficiently . The sum ∑ y ∈ Yw ( i ) exp Ψ ( y , w ) can be computed efficiently using the forward recur - rence , which is closely related to the Viterbi recurrence . We first define a set of forward variables , αm ( ym ) , which is equal to the sum of the scores of all paths leading to tag ym at position m : αm ( ym ) , ∑ y1 : m − 1 exp m ∑ n = 1 sn ( yn , yn − 1 ) [ 7.77 ] = ∑ y1 : m − 1 m ∏ n = 1 exp sn ( yn , yn − 1 ) . [ 7.78 ] Note the similarity to the definition of the Viterbi variable , vm ( ym ) = maxy1 : m − 1 ∑ m n = 1 sn ( yn , yn − 1 ) . In the hidden Markov model , the Viterbi recurrence had an alternative interpretation as the max-product algorithm ( see Equation 7.53 ) ; analogously , the forward recurrence is known as the sum-product algorithm , because of the form of [ 7.78 ] . The forward variable can also be computed through a recurrence : αm ( ym ) = ∑ y1 : m − 1 m ∏ n = 1 exp sn ( yn , yn − 1 ) [ 7.79 ] = ∑ ym − 1 ( exp sm ( ym , ym − 1 ) ) ∑ y1 : m − 2 m − 1 ∏ n = 1 exp sn ( yn , yn − 1 ) [ 7.80 ] = ∑ ym − 1 ( exp sm ( ym , ym − 1 ) ) × αm − 1 ( ym − 1 ) . [ 7.81 ] Using the forward recurrence , it is possible to compute the denominator of the condi - tional probability , ∑ y ∈ Y ( w ) Ψ ( w , y ) = ∑ y1 : M sM + 1 ( � , yM ) M ∏ m = 1 sm ( ym , ym − 1 ) [ 7.82 ] = αM + 1 ( � ) . [ 7.83 ] 9The terminology of “ potentials ” and “ partition functions ” comes from statistical mechanics ( Bishop , 2006 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 7.5 . DISCRIMINATIVE SEQUENCE LABELING WITH FEATURES 165 The conditional log-likelihood can be rewritten , ` = λ 2 | | θ | | 2 − N ∑ i = 1 θ · f ( w ( i ) , y ( i ) ) + logαM + 1 ( � ) . [ 7.84 ] Probabilistic programming environments , such as TORCH ( Collobert et al . , 2011 ) and DYNET ( Neubig et al . , 2017 ) , can compute the gradient of this objective using automatic differentiation . The programmer need only implement the forward algorithm as a com - putation graph . As in logistic regression , the gradient of the likelihood with respect to the parameters is a difference between observed and expected feature counts : d ` dθj = λθj + N ∑ i = 1 E [ fj ( w ( i ) , y ) ] − fj ( w ( i ) , y ( i ) ) , [ 7.85 ] where fj ( w ( i ) , y ( i ) ) refers to the count of feature j for token sequence w ( i ) and tag se - quence y ( i ) . The expected feature counts are computed “ under the hood ” when automatic differentiation is applied to Equation 7.84 ( Eisner , 2016 ) . Before the widespread use of automatic differentiation , it was common to compute the feature expectations from marginal tag probabilities p ( ym | w ) . These marginal prob - abilities are sometimes useful on their own , and can be computed using the forward - backward algorithm . This algorithm combines the forward recurrence with an equivalent backward recurrence , which traverses the input from wM back to w1 . * Forward-backward algorithm Marginal probabilities over tag bigrams can be written as , 10 Pr ( Ym − 1 = k ′ , Ym = k | w ) = ∑ y : Ym = k , Ym − 1 = k ′ ∏ M n = 1 exp sn ( yn , yn − 1 ) ∑ y ′ ∏ M n = 1 exp sn ( y ′ n , y ′ n − 1 ) . [ 7.86 ] The numerator sums over all tag sequences that include the transition ( Ym − 1 = k ′ ) → ( Ym = k ) . Because we are only interested in sequences that include the tag bigram , this sum can be decomposed into three parts : the prefixes y1 : m − 1 , terminating in Ym − 1 = k ′ ; the 10Recall the notational convention of upper-case letters for random variables , e.g . Ym , and lower case letters for specific values , e.g . , ym , so that Ym = k is interpreted as the event of random variable Ym taking the value k . Under contract with MIT Press , shared under CC-BY-NC-ND license . 166 CHAPTER 7 . SEQUENCE LABELING Ym − 1 = k ′ Ym = k αm − 1 ( k ′ ) exp sm ( k , k ′ ) βm ( k ) Figure 7.3 : A schematic illustration of the computation of the marginal probability Pr ( Ym − 1 = k ′ , Ym = k ) , using the forward score αm − 1 ( k ′ ) and the backward score βm ( k ) . transition ( Ym − 1 = k ′ ) → ( Ym = k ) ; and the suffixes ym : M , beginning with the tag Ym = k : ∑ y : Ym = k , Ym − 1 = k ′ M ∏ n = 1 exp sn ( yn , yn − 1 ) = ∑ y1 : m − 1 : Ym − 1 = k ′ m − 1 ∏ n = 1 exp sn ( yn , yn − 1 ) × exp sm ( k , k ′ ) × ∑ ym : M : Ym = k M + 1 ∏ n = m + 1 exp sn ( yn , yn − 1 ) . [ 7.87 ] The result is product of three terms : a score that sums over all the ways to get to the position ( Ym − 1 = k ′ ) , a score for the transition from k ′ to k , and a score that sums over all the ways of finishing the sequence from ( Ym = k ) . The first term of Equation 7.87 is equal to the forward variable , αm − 1 ( k ′ ) . The third term — the sum over ways to finish the sequence — can also be defined recursively , this time moving over the trellis from right to left , which is known as the backward recurrence : βm ( k ) , ∑ ym : M : Ym = k M + 1 ∏ n = m exp sn ( yn , yn − 1 ) [ 7.88 ] = ∑ k ′ ∈ Y exp sm + 1 ( k ′ , k ) ∑ ym + 1 : M : Ym = k ′ M + 1 ∏ n = m + 1 exp sn ( yn , yn − 1 ) [ 7.89 ] = ∑ k ′ ∈ Y exp sm + 1 ( k ′ , k ) × βm + 1 ( k ′ ) . [ 7.90 ] To understand this computation , compare with the forward recurrence in Equation 7.81 . Jacob Eisenstein . Draft of October 15 , 2018 . 7.6 . NEURAL SEQUENCE LABELING 167 In practice , numerical stability demands that we work in the log domain , logαm ( k ) = log ∑ k ′ ∈ Y exp ( log sm ( k , k ′ ) + logαm − 1 ( k ′ ) ) [ 7.91 ] log βm − 1 ( k ) = log ∑ k ′ ∈ Y exp ( log sm ( k ′ , k ) + log βm ( k ′ ) ) . [ 7.92 ] The application of the forward and backward probabilities is shown in Figure 7.3 . Both the forward and backward recurrences operate on the trellis , which implies a space complexity O ( MK ) . Because both recurrences require computing a sum over K terms at each node in the trellis , their time complexity is O ( MK2 ) . 7.6 Neural sequence labeling In neural network approaches to sequence labeling , we construct a vector representa - tion for each tagging decision , based on the word and its context . Neural networks can perform tagging as a per-token classification decision , or they can be combined with the Viterbi algorithm to tag the entire sequence globally . 7.6.1 Recurrent neural networks Recurrent neural networks ( RNNs ) were introduced in chapter 6 as a language model - ing technique , in which the context at token m is summarized by a recurrently-updated vector , hm = g ( xm , hm − 1 ) , m = 1 , 2 , . . . M , where xm is the vector embedding of the token wm and the function g defines the recur - rence . The starting condition h0 is an additional parameter of the model . The long short - term memory ( LSTM ) is a more complex recurrence , in which a memory cell is through a series of gates , avoiding repeated application of the non-linearity . Despite these bells and whistles , both models share the basic architecture of recurrent updates across a sequence , and both will be referred to as RNNs here . A straightforward application of RNNs to sequence labeling is to score each tag ym as a linear function of hm : ψm ( y ) = βy · hm [ 7.93 ] ŷm = argmax y ψm ( y ) . [ 7.94 ] The score ψm ( y ) can also be converted into a probability distribution using the usual soft - max operation , p ( y | w1 : m ) = expψm ( y ) ∑ y ′ ∈ Y expψm ( y ′ ) . [ 7.95 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 168 CHAPTER 7 . SEQUENCE LABELING Using this transformation , it is possible to train the tagger from the negative log-likelihood of the tags , as in a conditional random field . Alternatively , a hinge loss or margin loss objective can be constructed from the raw scores ψm ( y ) . The hidden state hm accounts for information in the input leading up to position m , but it ignores the subsequent tokens , which may also be relevant to the tag ym . This can be addressed by adding a second RNN , in which the input is reversed , running the recur - rence from wM to w1 . This is known as a bidirectional recurrent neural network ( Graves and Schmidhuber , 2005 ) , and is specified as : ← − hm = g ( xm , ← − hm + 1 ) , m = 1 , 2 , . . . , M . [ 7.96 ] The hidden states of the left-to-right RNN are denoted − → hm . The left-to-right and right-to - left vectors are concatenated , hm = [ ← − hm ; − → hm ] . The scoring function in Equation 7.93 is applied to this concatenated vector . Bidirectional RNN tagging has several attractive properties . Ideally , the representa - tion hm summarizes the useful information from the surrounding context , so that it is not necessary to design explicit features to capture this information . If the vector hm is an ad - equate summary of this context , then it may not even be necessary to perform the tagging jointly : in general , the gains offered by joint tagging of the entire sequence are diminished as the individual tagging model becomes more powerful . Using backpropagation , the word vectors x can be trained “ end-to-end ” , so that they capture word properties that are useful for the tagging task . Alternatively , if limited labeled data is available , we can use word embeddings that are “ pre-trained ” from unlabeled data , using a language modeling objective ( as in section 6.3 ) or a related word embedding technique ( see chapter 14 ) . It is even possible to combine both fine-tuned and pre-trained embeddings in a single model . Neural structure prediction The bidirectional recurrent neural network incorporates in - formation from throughout the input , but each tagging decision is made independently . In some sequence labeling applications , there are very strong dependencies between tags : it may even be impossible for one tag to follow another . In such scenarios , the tagging decision must be made jointly across the entire sequence . Neural sequence labeling can be combined with the Viterbi algorithm by defining the local scores as : sm ( ym , ym − 1 ) = βym · hm + ηym − 1 , ym , [ 7.97 ] where hm is the RNN hidden state , βym is a vector associated with tag ym , and ηym − 1 , ym is a scalar parameter for the tag transition ( ym − 1 , ym ) . These local scores can then be incorporated into the Viterbi algorithm for inference , and into the forward algorithm for training . This model is shown in Figure 7.4 . It can be trained from the conditional log - likelihood objective defined in Equation 7.76 , backpropagating to the tagging parameters Jacob Eisenstein . Draft of October 15 , 2018 . 7.6 . NEURAL SEQUENCE LABELING 169 ym − 1 ym ym + 1 ← − hm − 1 ← − hm ← − hm + 1 − → hm − 1 − → hm − → hm + 1 xm − 1 xm xm + 1 Figure 7.4 : Bidirectional LSTM for sequence labeling . The solid lines indicate computa - tion , the dashed lines indicate probabilistic dependency , and the dotted lines indicate the optional additional probabilistic dependencies between labels in the biLSTM-CRF . β and η , as well as the parameters of the RNN . This model is called the LSTM-CRF , due to its combination of aspects of the long short-term memory and conditional random field models ( Huang et al . , 2015 ) . The LSTM-CRF is especially effective on the task of named entity recognition ( Lam - ple et al . , 2016 ) , a sequence labeling task that is described in detail in section 8.3 . This task has strong dependencies between adjacent tags , so structure prediction is especially important . 7.6.2 Character-level models As in language modeling , rare and unseen words are a challenge : if we encounter a word that was not in the training data , then there is no obvious choice for the word embed - ding xm . One solution is to use a generic unseen word embedding for all such words . However , in many cases , properties of unseen words can be guessed from their spellings . For example , whimsical does not appear in the Universal Dependencies ( UD ) English Tree - bank , yet the suffix - al makes it likely to be adjective ; by the same logic , unflinchingly is likely to be an adverb , and barnacle is likely to be a noun . In feature-based models , these morphological properties were handled by suffix fea - tures ; in a neural network , they can be incorporated by constructing the embeddings of unseen words from their spellings or morphology . One way to do this is to incorporate an additional layer of bidirectional RNNs , one for each word in the vocabulary ( Ling et al . , 2015 ) . For each such character-RNN , the inputs are the characters , and the output is the concatenation of the final states of the left-facing and right-facing passes , φw = Under contract with MIT Press , shared under CC-BY-NC-ND license . 170 CHAPTER 7 . SEQUENCE LABELING [ − → h ( w ) Nw ; ← − h ( w ) 0 ] , where − → h ( w ) Nw is the final state of the right-facing pass for word w , and Nw is the number of characters in the word . The character RNN model is trained by back - propagation from the tagging objective . On the test data , the trained RNN is applied to out-of-vocabulary words ( or all words ) , yielding inputs to the word-level tagging RNN . Other approaches to compositional word embeddings are described in subsection 14.7.1 . 7.6.3 Convolutional Neural Networks for Sequence Labeling One disadvantage of recurrent neural networks is that the architecture requires iterating through the sequence of inputs and predictions : each hidden vector hm must be com - puted from the previous hidden vector hm − 1 , before predicting the tag ym . These iterative computations are difficult to parallelize , and fail to exploit the speedups offered by graph - ics processing units ( GPUs ) on operations such as matrix multiplication . Convolutional neural networks achieve better computational performance by predicting each label ym from a set of matrix operations on the neighboring word embeddings , xm − k : m + k ( Col - lobert et al . , 2011 ) . Because there is no hidden state to update , the predictions for each ym can be computed in parallel . For more on convolutional neural networks , see section 3.4 . Character-based word embeddings can also be computed using convolutional neural net - works ( Santos and Zadrozny , 2014 ) . 7.7 * Unsupervised sequence labeling In unsupervised sequence labeling , the goal is to induce a hidden Markov model from a corpus of unannotated text ( w ( 1 ) , w ( 2 ) , . . . , w ( N ) ) , where each w ( i ) is a sequence of length M ( i ) . This is an example of the general problem of structure induction , which is the unsupervised version of structure prediction . The tags that result from unsupervised se - quence labeling might be useful for some downstream task , or they might help us to better understand the language’s inherent structure . For part-of-speech tagging , it is common to use a tag dictionary that lists the allowed tags for each word , simplifying the prob - lem ( Christodoulopoulos et al . , 2010 ) . Unsupervised learning in hidden Markov models can be performed using the Baum - Welch algorithm , which combines the forward-backward algorithm ( section 7.5.3 ) with expectation-maximization ( EM ; subsection 5.1.2 ) . In the M-step , the HMM parameters from expected counts : Pr ( W = i | Y = k ) = φk , i = E [ count ( W = i , Y = k ) ] E [ count ( Y = k ) ] Pr ( Ym = k | Ym − 1 = k ′ ) = λk ′ , k = E [ count ( Ym = k , Ym − 1 = k ′ ) ] E [ count ( Ym − 1 = k ′ ) ] Jacob Eisenstein . Draft of October 15 , 2018 . 7.7 . * UNSUPERVISED SEQUENCE LABELING 171 The expected counts are computed in the E-step , using the forward and backward recurrences . The local scores follow the usual definition for hidden Markov models , sm ( k , k ′ ) = log pE ( wm | Ym = k ; φ ) + log pT ( Ym = k | Ym − 1 = k ′ ; λ ) . [ 7.98 ] The expected transition counts for a single instance are , E [ count ( Ym = k , Ym − 1 = k ′ ) | w ] = M ∑ m = 1 Pr ( Ym − 1 = k ′ , Ym = k | w ) [ 7.99 ] = ∑ y : Ym = k , Ym − 1 = k ′ ∏ M n = 1 exp sn ( yn , yn − 1 ) ∑ y ′ ∏ M n = 1 exp sn ( y ′ n , y ′ n − 1 ) . [ 7.100 ] As described in section 7.5.3 , these marginal probabilities can be computed from the forward-backward recurrence , Pr ( Ym − 1 = k ′ , Ym = k | w ) = αm − 1 ( k ′ ) × exp sm ( k , k ′ ) × βm ( k ) αM + 1 ( � ) . [ 7.101 ] In a hidden Markov model , each element of the forward-backward computation has a special interpretation : αm − 1 ( k ′ ) =p ( Ym − 1 = k ′ , w1 : m − 1 ) [ 7.102 ] exp sm ( k , k ′ ) =p ( Ym = k , wm | Ym − 1 = k ′ ) [ 7.103 ] βm ( k ) =p ( wm + 1 : M | Ym = k ) . [ 7.104 ] Applying the conditional independence assumptions of the hidden Markov model ( de - fined in Algorithm 12 ) , the product is equal to the joint probability of the tag bigram and the entire input , αm − 1 ( k ′ ) × exp sm ( k , k ′ ) × βm ( k ) =p ( Ym − 1 = k ′ , w1 : m − 1 ) × p ( Ym = k , wm | Ym − 1 = k ′ ) × p ( wm + 1 : M | Ym = k ) =p ( Ym − 1 = k ′ , Ym = k , w1 : M ) . [ 7.105 ] Dividing by αM + 1 ( � ) = p ( w1 : M ) gives the desired probability , αm − 1 ( k ′ ) × sm ( k , k ′ ) × βm ( k ) αM + 1 ( � ) = p ( Ym − 1 = k ′ , Ym = k , w1 : M ) p ( w1 : M ) [ 7.106 ] = Pr ( Ym − 1 = k ′ , Ym = k | w1 : M ) . [ 7.107 ] The expected emission counts can be computed in a similar manner , using the product αm ( k ) × βm ( k ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 172 CHAPTER 7 . SEQUENCE LABELING 7.7.1 Linear dynamical systems The forward-backward algorithm can be viewed as Bayesian state estimation in a discrete state space . In a continuous state space , ym ∈ RK , the equivalent algorithm is the Kalman smoother . It also computes marginals p ( ym | x1 : M ) , using a similar two-step algorithm of forward and backward passes . Instead of computing a trellis of values at each step , the Kalman smoother computes a probability density function qym ( ym ; µm , Σm ) , character - ized by a mean µm and a covariance Σm around the latent state . Connections between the Kalman smoother and the forward-backward algorithm are elucidated by Minka ( 1999 ) and Murphy ( 2012 ) . 7.7.2 Alternative unsupervised learning methods As noted in section 5.5 , expectation-maximization is just one of many techniques for struc - ture induction . One alternative is to use Markov Chain Monte Carlo ( MCMC ) sampling algorithms , which are briefly described in subsection 5.5.1 . For the specific case of se - quence labeling , Gibbs sampling can be applied by iteratively sampling each tag ym con - ditioned on all the others ( Finkel et al . , 2005 ) : p ( ym | y − m , w1 : M ) ∝ p ( wm | ym ) p ( ym | y − m ) . [ 7.108 ] Gibbs Sampling has been applied to unsupervised part-of-speech tagging by Goldwater and Griffiths ( 2007 ) . Beam sampling is a more sophisticated sampling algorithm , which randomly draws entire sequences y1 : M , rather than individual tags ym ; this algorithm was applied to unsupervised part-of-speech tagging by Van Gael et al . ( 2009 ) . Spectral learn - ing ( see subsection 5.5.2 ) can also be applied to sequence labeling . By factoring matrices of co-occurrence counts of word bigrams and trigrams ( Song et al . , 2010 ; Hsu et al . , 2012 ) , it is possible to obtain globally optimal estimates of the transition and emission parameters , under mild assumptions . 7.7.3 Semiring notation and the generalized viterbi algorithm The Viterbi and Forward recurrences can each be performed over probabilities or log probabilities , yielding a total of four closely related recurrences . These four recurrence scan in fact be expressed as a single recurrence in a more general notation , known as semiring algebra . Let the symbols ⊕ and ⊗ represent generalized addition and multipli - cation respectively . 11 Given these operators , a generalized Viterbi recurrence is denoted , vm ( k ) = ⊕ k ′ ∈ Y sm ( k , k ′ ) ⊗ vm − 1 ( k ′ ) . [ 7.109 ] 11In a semiring , the addition and multiplication operators must both obey associativity , and multiplication must distribute across addition ; the addition operator must be commutative ; there must be additive and multiplicative identities 0 and 1 , such that a ⊕ 0 = a and a ⊗ 1 = a ; and there must be a multiplicative annihilator 0 , such that a ⊗ 0 = 0 . Jacob Eisenstein . Draft of October 15 , 2018 . 7.7 . * UNSUPERVISED SEQUENCE LABELING 173 Each recurrence that we have seen so far is a special case of this generalized Viterbi recurrence : • In the max-product Viterbi recurrence over probabilities , the ⊕ operation corre - sponds to maximization , and the ⊗ operation corresponds to multiplication . • In the forward recurrence over probabilities , the ⊕ operation corresponds to addi - tion , and the ⊗ operation corresponds to multiplication . • In the max-product Viterbi recurrence over log-probabilities , the ⊕ operation corre - sponds to maximization , and the ⊗ operation corresponds to addition . 12 • In the forward recurrence over log-probabilities , the ⊕ operation corresponds to log - addition , a ⊕ b = log ( ea + eb ) . The ⊗ operation corresponds to addition . The mathematical abstraction offered by semiring notation can be applied to the soft - ware implementations of these algorithms , yielding concise and modular implementa - tions . For example , in the OPENFST library , generic operations are parametrized by the choice of semiring ( Allauzen et al . , 2007 ) . Exercises 1 . Extend the example in subsection 7.3.1 to the sentence they can can fish , meaning that “ they can put fish into cans . ” Build the trellis for this example using the weights in Table 7.1 , and identify the best-scoring tag sequence . If the scores for noun and verb are tied , then you may assume that the backpointer always goes to noun . 2 . Using the tagsetY = { N , V } , and the feature set f ( w , ym , ym − 1 , m ) = { ( wm , ym ) , ( ym , ym − 1 ) } , show that there is no set of weights that give the correct tagging for both they can fish ( N V V ) and they can can fish ( N V V N ) . 3 . Work out what happens if you train a structured perceptron on the two exam - ples mentioned in the previous problem , using the transition and emission features ( ym , ym − 1 ) and ( ym , wm ) . Initialize all weights at 0 , and assume that the Viterbi algo - rithm always chooses N when the scores for the two tags are tied , so that the initial prediction for they can fish is N N N . 4 . Consider the garden path sentence , The old man the boat . Given word-tag and tag-tag features , what inequality in the weights must hold for the correct tag sequence to outscore the garden path tag sequence for this example ? 12This is sometimes called the tropical semiring , in honor of the Brazilian mathematician Imre Simon . Under contract with MIT Press , shared under CC-BY-NC-ND license . 174 CHAPTER 7 . SEQUENCE LABELING 5 . Using the weights in Table 7.1 , explicitly compute the log-probabilities for all pos - sible taggings of the input fish can . Verify that the forward algorithm recovers the aggregate log probability . 6 . Sketch out an algorithm for a variant of Viterbi that returns the top-n label se - quences . What is the time and space complexity of this algorithm ? 7 . Show how to compute the marginal probability Pr ( ym − 2 = k , ym = k ′ | w1 : M ) , in terms of the forward and backward variables , and the potentials sn ( yn , yn − 1 ) . 8 . Suppose you receive a stream of text , where some of tokens have been replaced at random with NOISE . For example : • Source : I try all things , I achieve what I can • Message received : I try NOISE NOISE , I NOISE what I NOISE Assume you have access to a pre-trained bigram language model , which gives prob - abilities p ( wm | wm − 1 ) . These probabilities can be assumed to be non-zero for all bigrams . Show how to use the Viterbi algorithm to recover the source by maximizing the bigram language model log-probability . Specifically , set the scores sm ( ym , ym − 1 ) so that the Viterbi algorithm selects a sequence of words that maximizes the bigram language model log-probability , while leaving the non-noise tokens intact . Your solution should not modify the logic of the Viterbi algorithm , it should only set the scores sm ( ym , ym − 1 ) . 9 . Let α ( · ) and β ( · ) indicate the forward and backward variables as defined in sec - tion 7.5.3 . Prove that αM + 1 ( � ) = β0 ( ♦ ) = ∑ y αm ( y ) βm ( y ) , ∀ m ∈ { 1 , 2 , . . . , M } . 10 . Consider an RNN tagging model with a tanh activation function on the hidden layer , and a hinge loss on the output . ( The problem also works for the margin loss and negative log-likelihood . ) Suppose you initialize all parameters to zero : this in - cludes the word embeddings that make up x , the transition matrix Θ , the output weights β , and the initial hidden state h0 . a ) Prove that for any data and for any gradient-based learning algorithm , all pa - rameters will be stuck at zero . b ) Would a sigmoid activation function avoid this problem ? Jacob Eisenstein . Draft of October 15 , 2018 .