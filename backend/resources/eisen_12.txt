Chapter 12 Logical semantics The previous few chapters have focused on building systems that reconstruct the syntax of natural language — its structural organization — through tagging and parsing . But some of the most exciting and promising potential applications of language technology involve going beyond syntax to semantics — the underlying meaning of the text : • Answering questions , such as where is the nearest coffeeshop ? or what is the middle name of the mother of the 44th President of the United States ? . • Building a robot that can follow natural language instructions to execute tasks . • Translating a sentence from one language into another , while preserving the under - lying meaning . • Fact-checking an article by searching the web for contradictory evidence . • Logic-checking an argument by identifying contradictions , ambiguity , and unsup - ported assertions . Semantic analysis involves converting natural language into a meaning representa - tion . To be useful , a meaning representation must meet several criteria : • c1 : it should be unambiguous : unlike natural language , there should be exactly one meaning per statement ; • c2 : it should provide a way to link language to external knowledge , observations , and actions ; • c3 : it should support computational inference , so that meanings can be combined to derive additional knowledge ; • c4 : it should be expressive enough to cover the full range of things that people talk about in natural language . 285 286 CHAPTER 12 . LOGICAL SEMANTICS Much more than this can be said about the question of how best to represent knowledge for computation ( e.g . , Sowa , 2000 ) , but this chapter will focus on these four criteria . 12.1 Meaning and denotation The first criterion for a meaning representation is that statements in the representation should be unambiguous — they should have only one possible interpretation . Natural language does not have this property : as we saw in chapter 10 , sentences like cats scratch people with claws have multiple interpretations . But what does it mean for a statement to be unambiguous ? Programming languages provide a useful example : the output of a program is completely specified by the rules of the language and the properties of the environment in which the program is run . For ex - ample , the python code 5 + 3 will have the output 8 , as will the codes ( 4 * 4 ) - ( 3 * 3 ) + 1 and ( ( 8 ) ) . This output is known as the denotation of the program , and can be written as , J5 + 3K = J ( 4 * 4 ) - ( 3 * 3 ) + 1K = J ( ( 8 ) ) K = 8 . [ 12.1 ] The denotations of these arithmetic expressions are determined by the meaning of the constants ( e.g . , 5 , 3 ) and the relations ( e.g . , + , * , ( , ) ) . Now let’s consider another snippet of python code , double ( 4 ) . The denotation of this code could be , Jdouble ( 4 ) K = 8 , or it could be Jdouble ( 4 ) K = 44 — it depends on the meaning of double . This meaning is defined in a world modelM as an infinite set of pairs . We write the denotation with respect to model M as J·KM , e.g . , JdoubleKM = { ( 0,0 ) , ( 1,2 ) , ( 2,4 ) , . . . } . The world model would also define the ( infinite ) list of constants , e.g . , { 0,1,2 , . . . } . As long as the denotation of string φ in modelM can be computed unambiguously , the language can be said to be unambiguous . This approach to meaning is known as model-theoretic semantics , and it addresses not only criterion c1 ( no ambiguity ) , but also c2 ( connecting language to external knowl - edge , observations , and actions ) . For example , we can connect a representation of the meaning of a statement like the capital of Georgia with a world model that includes knowl - edge base of geographical facts , obtaining the denotation Atlanta . We might populate a world model by detecting and analyzing the objects in an image , and then use this world model to evaluate propositions like a man is riding a moose . Another desirable property of model-theoretic semantics is that when the facts change , the denotations change too : the meaning representation of President of the USA would have a different denotation in the modelM2014 as it would inM2022 . Jacob Eisenstein . Draft of October 15 , 2018 . 12.2 . LOGICAL REPRESENTATIONS OF MEANING 287 12.2 Logical representations of meaning Criterion c3 requires that the meaning representation support inference — for example , automatically deducing new facts from known premises . While many representations have been proposed that meet these criteria , the most mature is the language of first-order logic . 1 12.2.1 Propositional logic The bare bones of logical meaning representation are Boolean operations on propositions : Propositional symbols . Greek symbols like φ and ψ will be used to represent proposi - tions , which are statements that are either true or false . For example , φ may corre - spond to the proposition , bagels are delicious . Boolean operators . We can build up more complex propositional formulas from Boolean operators . These include : • Negation ¬ φ , which is true if φ is false . • Conjunction , φ ∧ ψ , which is true if both φ and ψ are true . • Disjunction , φ ∨ ψ , which is true if at least one of φ and ψ is true • Implication , φ ⇒ ψ , which is true unless φ is true and ψ is false . Implication has identical truth conditions to ¬ φ ∨ ψ . • Equivalence , φ ⇔ ψ , which is true if φ and ψ are both true or both false . Equiv - alence has identical truth conditions to ( φ ⇒ ψ ) ∧ ( ψ ⇒ φ ) . It is not strictly necessary to have all five Boolean operators : readers familiar with Boolean logic will know that it is possible to construct all other operators from either the NAND ( not-and ) or NOR ( not-or ) operators . Nonetheless , it is clearest to use all five operators . From the truth conditions for these operators , it is possible to define a number of “ laws ” for these Boolean operators , such as , • Commutativity : φ ∧ ψ = ψ ∧ φ , φ ∨ ψ = ψ ∨ φ • Associativity : φ ∧ ( ψ ∧ χ ) = ( φ ∧ ψ ) ∧ χ , φ ∨ ( ψ ∨ χ ) = ( φ ∨ ψ ) ∨ χ • Complementation : φ ∧ ¬ φ = ⊥ , φ ∨ ¬ φ = > , where > indicates a true proposition and ⊥ indicates a false proposition . 1Alternatives include the “ variable-free ” representation used in semantic parsing of geographical queries ( Zelle and Mooney , 1996 ) and robotic control ( Ge and Mooney , 2005 ) , and dependency-based com - positional semantics ( Liang et al . , 2013 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 288 CHAPTER 12 . LOGICAL SEMANTICS These laws can be combined to derive further equivalences , which can support logical inferences . For example , suppose φ = The music is loud and ψ = Max can’t sleep . Then if we are given , φ ⇒ ψ If the music is loud , Max can’t sleep . φ The music is loud . we can derive ψ ( Max can’t sleep ) by application of modus ponens , which is one of a set of inference rules that can be derived from more basic laws and used to manipulate propositional formulas . Automated theorem provers are capable of applying inference rules to a set of premises to derive desired propositions ( Loveland , 2016 ) . 12.2.2 First-order logic Propositional logic is so named because it treats propositions as its base units . However , the criterion c4 states that our meaning representation should be sufficiently expressive . Now consider the sentence pair , ( 12.1 ) If anyone is making noise , then Max can’t sleep . Abigail is making noise . People are capable of making inferences from this sentence pair , but such inferences re - quire formal tools that are beyond propositional logic . To understand the relationship between the statement anyone is making noise and the statement Abigail is making noise , our meaning representation requires the additional machinery of first-order logic ( FOL ) . In FOL , logical propositions can be constructed from relationships between entities . Specifically , FOL extends propositional logic with the following classes of terms : Constants . These are elements that name individual entities in the model , such as MAX and ABIGAIL . The denotation of each constant in a model M is an element in the model , e.g . , JMAXK = m and JABIGAILK = a . Relations . Relations can be thought of as sets of entities , or sets of tuples . For example , the relation CAN-SLEEP is defined as the set of entities who can sleep , and has the denotation JCAN-SLEEPK = { a , m , . . . } . To test the truth value of the proposition CAN-SLEEP ( MAX ) , we ask whether JMAXK ∈ JCAN-SLEEPK . Logical relations that are defined over sets of entities are sometimes called properties . Relations may also be ordered tuples of entities . For example BROTHER ( MAX , ABIGAIL ) expresses the proposition that MAX is the brother of ABIGAIL . The denotation of such relations is a set of tuples , JBROTHERK = { ( m , a ) , ( x , y ) , . . . } . To test the truth value of the proposition BROTHER ( MAX , ABIGAIL ) , we ask whether the tuple ( JMAXK , JABIGAILK ) is in the denotation JBROTHERK . Jacob Eisenstein . Draft of October 15 , 2018 . 12.2 . LOGICAL REPRESENTATIONS OF MEANING 289 Using constants and relations , it is possible to express statements like Max can’t sleep and Max is Abigail’s brother : ¬ CAN-SLEEP ( MAX ) BROTHER ( MAX , ABIGAIL ) . These statements can also be combined using Boolean operators , such as , ( BROTHER ( MAX , ABIGAIL ) ∨ BROTHER ( MAX , STEVE ) ) ⇒ ¬ CAN-SLEEP ( MAX ) . This fragment of first-order logic permits only statements about specific entities . To support inferences about statements like If anyone is making noise , then Max can’t sleep , two more elements must be added to the meaning representation : Variables . Variables are mechanisms for referring to entities that are not locally specified . We can then write CAN-SLEEP ( x ) or BROTHER ( x , ABIGAIL ) . In these cases , x is a free variable , meaning that we have not committed to any particular assignment . Quantifiers . Variables are bound by quantifiers . There are two quantifiers in first-order logic . 2 • The existential quantifier ∃ , which indicates that there must be at least one en - tity to which the variable can bind . For example , the statement ∃ xMAKES-NOISE ( X ) indicates that there is at least one entity for which MAKES-NOISE is true . • The universal quantifier ∀ , which indicates that the variable must be able to bind to any entity in the model . For example , the statement , MAKES-NOISE ( ABIGAIL ) ⇒ ( ∀ x ¬ CAN-SLEEP ( x ) ) [ 12.3 ] asserts that if Abigail makes noise , no one can sleep . The expressions ∃ x and ∀ x make x into a bound variable . A formula that contains no free variables is a sentence . Functions . Functions map from entities to entities , e.g . , JCAPITAL-OF ( GEORGIA ) K = JATLANTAK . With functions , it is convenient to add an equality operator , supporting statements like , ∀ x ∃ yMOTHER-OF ( x ) = DAUGHTER-OF ( y ) . [ 12.4 ] 2In first-order logic , it is possible to quantify only over entities . In second-order logic , it is possible to quantify over properties . This makes it possible to represent statements like Butch has every property that a good boxer has ( example from Blackburn and Bos , 2005 ) , ∀ P ∀ x ( ( GOOD-BOXER ( x ) ⇒ P ( x ) ) ⇒ P ( BUTCH ) ) . [ 12.2 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 290 CHAPTER 12 . LOGICAL SEMANTICS Note that MOTHER-OF is a functional analogue of the relation MOTHER , so that MOTHER-OF ( x ) = y if MOTHER ( x , y ) . Any logical formula that uses functions can be rewritten using only relations and quantification . For example , MAKES-NOISE ( MOTHER-OF ( ABIGAIL ) ) [ 12.5 ] can be rewritten as ∃ xMAKES-NOISE ( x ) ∧ MOTHER ( x , ABIGAIL ) . An important property of quantifiers is that the order can matter . Unfortunately , natu - ral language is rarely clear about this ! The issue is demonstrated by examples like everyone speaks a language , which has the following interpretations : ∀ x ∃ y SPEAKS ( x , y ) [ 12.6 ] ∃ y ∀ x SPEAKS ( x , y ) . [ 12.7 ] In the first case , y may refer to several different languages , while in the second case , there is a single y that is spoken by everyone . Truth-conditional semantics One way to look at the meaning of an FOL sentence φ is as a set of truth conditions , or models under which φ is satisfied . But how to determine whether a sentence is true or false in a given model ? We will approach this inductively , starting with a predicate applied to a tuple of constants . The truth of such a sentence depends on whether the tuple of denotations of the constants is in the denotation of the predicate . For example , CAPITAL ( GEORGIA , ATLANTA ) is true in modelM iff , ( JGEORGIAKM , JATLANTAKM ) ∈ JCAPITALKM . [ 12.8 ] The Boolean operators ∧ , ∨ , . . . provide ways to construct more complicated sentences , and the truth of such statements can be assessed based on the truth tables associated with these operators . The statement ∃ xφ is true if there is some assignment of the variable x to an entity in the model such that φ is true ; the statement ∀ xφ is true if φ is true under all possible assignments of x . More formally , we would say that φ is satisfied underM , written asM | = φ . Truth conditional semantics allows us to define several other properties of sentences and pairs of sentences . Suppose that in every M under which φ is satisfied , another formula ψ is also satisfied ; then φ entails ψ , which is also written as φ | = ψ . For example , CAPITAL ( GEORGIA , ATLANTA ) | = ∃ xCAPITAL ( GEORGIA , x ) . [ 12.9 ] A statement that is satisfied under any model , such as φ ∨ ¬ φ , is valid , written | = ( φ ∨ ¬ φ ) . A statement that is not satisfied under any model , such as φ ∧ ¬ φ , is unsatisfiable , Jacob Eisenstein . Draft of October 15 , 2018 . 12.3 . SEMANTIC PARSING AND THE LAMBDA CALCULUS 291 or inconsistent . A model checker is a program that determines whether a sentence φ is satisfied in M . A model builder is a program that constructs a model in which φ is satisfied . The problems of checking for consistency and validity in first-order logic are undecidable , meaning that there is no algorithm that can automatically determine whether an FOL formula is valid or inconsistent . Inference in first-order logic Our original goal was to support inferences that combine general statements If anyone is making noise , then Max can’t sleep with specific statements like Abigail is making noise . We can now represent such statements in first-order logic , but how are we to perform the inference that Max can’t sleep ? One approach is to use “ generalized ” versions of propo - sitional inference rules like modus ponens , which can be applied to FOL formulas . By repeatedly applying such inference rules to a knowledge base of facts , it is possible to produce proofs of desired propositions . To find the right sequence of inferences to derive a desired theorem , classical artificial intelligence search algorithms like backward chain - ing can be applied . Such algorithms are implemented in interpreters for the prolog logic programming language ( Pereira and Shieber , 2002 ) . 12.3 Semantic parsing and the lambda calculus The previous section laid out a lot of formal machinery ; the remainder of this chapter links these formalisms back to natural language . Given an English sentence like Alex likes Brit , how can we obtain the desired first-order logical representation , LIKES ( ALEX , BRIT ) ? This is the task of semantic parsing . Just as a syntactic parser is a function from a natu - ral language sentence to a syntactic structure such as a phrase structure tree , a semantic parser is a function from natural language to logical formulas . As in syntactic analysis , semantic parsing is difficult because the space of inputs and outputs is very large , and their interaction is complex . Our best hope is that , like syntactic parsing , semantic parsing can somehow be decomposed into simpler sub-problems . This idea , usually attributed to the German philosopher Gottlob Frege , is called the principle of compositionality : the meaning of a complex expression is a function of the meanings of that expression’s constituent parts . We will define these “ constituent parts ” as syntactic constituents : noun phrases and verb phrases . These constituents are combined using function application : if the syntactic parse contains the production x → y z , then the semantics of x , written x . sem , will be computed as a function of the semantics of the Under contract with MIT Press , shared under CC-BY-NC-ND license . 292 CHAPTER 12 . LOGICAL SEMANTICS S : likes ( alex , brit ) VP : ? NP : brit Brit V : ? likes NP : alex Alex Figure 12.1 : The principle of compositionality requires that we identify meanings for the constituents likes and likes Brit that will make it possible to compute the meaning for the entire sentence . constituents , y . sem and z . sem . 3 4 12.3.1 The lambda calculus Let’s see how this works for a simple sentence like Alex likes Brit , whose syntactic structure is shown in Figure 12.1 . Our goal is the formula , LIKES ( ALEX , BRIT ) , and it is clear that the meaning of the constituents Alex and Brit should be ALEX and BRIT . That leaves two more constituents : the verb likes , and the verb phrase likes Brit . The meanings of these units must be defined in a way that makes it possible to recover the desired meaning for the entire sentence by function application . If the meanings of Alex and Brit are constants , then the meanings of likes and likes Brit must be functional expressions , which can be applied to their siblings to produce the desired analyses . Modeling these partial analyses requires extending the first-order logic meaning rep - resentation . We do this by adding lambda expressions , which are descriptions of anony - mous functions , 5 e.g . , λx . LIKES ( x , BRIT ) . [ 12.10 ] This functional expression is the meaning of the verb phrase likes Brit ; it takes a single argument , and returns the result of substituting that argument for x in the expression 3subsection 9.3.2 briefly discusses Combinatory Categorial Grammar ( CCG ) as an alternative to a phrase - structure analysis of syntax . CCG is argued to be particularly well-suited to semantic parsing ( Hockenmaier and Steedman , 2007 ) , and is used in much of the contemporary work on machine learning for semantic parsing , summarized in section 12.4 . 4The approach of algorithmically building up meaning representations from a series of operations on the syntactic structure of a sentence is generally attributed to the philosopher Richard Montague , who published a series of influential papers on the topic in the early 1970s ( e.g . , Montague , 1973 ) . 5Formally , all first-order logic formulas are lambda expressions ; in addition , if φ is a lambda expression , then λx . φ is also a lambda expression . Readers who are familiar with functional programming will recognize lambda expressions from their use in programming languages such as Lisp and Python . Jacob Eisenstein . Draft of October 15 , 2018 . 12.3 . SEMANTIC PARSING AND THE LAMBDA CALCULUS 293 LIKES ( x , BRIT ) . We write this substitution as , ( λx . LIKES ( x , BRIT ) ) @ALEX = LIKES ( ALEX , BRIT ) , [ 12.11 ] with the symbol “ @ ” indicating function application . Function application in the lambda calculus is sometimes called β-reduction or β-conversion . The expression φ @ ψ indicates a function application to be performed by β-reduction , and φ ( ψ ) indicates a function or predicate in the final logical form . Equation 12.11 shows how to obtain the desired semantics for the sentence Alex likes Brit : by applying the lambda expression λx . LIKES ( x , BRIT ) to the logical constant ALEX . This rule of composition can be specified in a syntactic-semantic grammar , in which syntactic productions are paired with semantic operations . For the syntactic production S → NP VP , we have the semantic rule VP.sem@NP.sem . The meaning of the transitive verb phrase likes Brit can also be obtained by function application on its syntactic constituents . For the syntactic production VP → V NP , we apply the semantic rule , VP . sem =( V . sem ) @NP . sem [ 12.12 ] =( λy . λx . LIKES ( x , y ) ) @ ( BRIT ) [ 12.13 ] = λx . LIKES ( x , BRIT ) . [ 12.14 ] Thus , the meaning of the transitive verb likes is a lambda expression whose output is another lambda expression : it takes y as an argument to fill in one of the slots in the LIKES relation , and returns a lambda expression that is ready to take an argument to fill in the other slot . 6 Table 12.1 shows a minimal syntactic-semantic grammar fragment , G1 . The complete derivation of Alex likes Brit in G1 is shown in Figure 12.2 . In addition to the transitive verb likes , the grammar also includes the intransitive verb sleeps ; it should be clear how to derive the meaning of sentences like Alex sleeps . For verbs that can be either transitive or intransitive , such as eats , we would have two terminal productions , one for each sense ( terminal productions are also called the lexical entries ) . Indeed , most of the grammar is in the lexicon ( the terminal productions ) , since these productions select the basic units of the semantic interpretation . 12.3.2 Quantification Things get more complicated when we move from sentences about named entities to sen - tences that involve more general noun phrases . Let’s consider the example , A dog sleeps , 6This can be written in a few different ways . The notation λy , x . LIKES ( x , y ) is a somewhat informal way to indicate a lambda expression that takes two arguments ; this would be acceptable in functional programming . Logicians ( e.g . , Carpenter , 1997 ) often prefer the more formal notation λy . λx . LIKES ( x ) ( y ) , indicating that each lambda expression takes exactly one argument . Under contract with MIT Press , shared under CC-BY-NC-ND license . 294 CHAPTER 12 . LOGICAL SEMANTICS S : likes ( alex , brit ) VP : λx . likes ( x , brit ) NP : brit Brit Vt : λy . λx . likes ( x , y ) likes NP : alex Alex Figure 12.2 : Derivation of the semantic representation for Alex likes Brit in the grammar G1 . S → NP VP VP.sem@NP.sem VP → Vt NP Vt.sem@NP.sem VP → Vi Vi . sem Vt → likes λy . λx . LIKES ( x , y ) Vi → sleeps λx . SLEEPS ( x ) NP → Alex ALEX NP → Brit BRIT Table 12.1 : G1 , a minimal syntactic-semantic context-free grammar which has the meaning ∃ xDOG ( x ) ∧ SLEEPS ( x ) . Clearly , the DOG relation will be intro - duced by the word dog , and the SLEEP relation will be introduced by the word sleeps . The existential quantifier ∃ must be introduced by the lexical entry for the determiner a . 7 However , this seems problematic for the compositional approach taken in the grammar G1 : if the semantics of the noun phrase a dog is an existentially quantified expression , how can it be the argument to the semantics of the verb sleeps , which expects an entity ? And where does the logical conjunction come from ? There are a few different approaches to handling these issues . 8 We will begin by re - versing the semantic relationship between subject NPs and VPs , so that the production S → NP VP has the semantics NP.sem@VP.sem : the meaning of the sentence is now the semantics of the noun phrase applied to the verb phrase . The implications of this change are best illustrated by exploring the derivation of the example , shown in Figure 12.3 . Let’s 7Conversely , the sentence Every dog sleeps would involve a universal quantifier , ∀ xDOG ( x ) ⇒ SLEEPS ( x ) . The definite article the requires more consideration , since the dog must refer to some dog which is uniquely identifiable , perhaps from contextual information external to the sentence . Carpenter ( 1997 , pp . 96-100 ) summarizes recent approaches to handling definite descriptions . 8Carpenter ( 1997 ) offers an alternative treatment based on combinatory categorial grammar . Jacob Eisenstein . Draft of October 15 , 2018 . 12.3 . SEMANTIC PARSING AND THE LAMBDA CALCULUS 295 S : ∃ xdog ( x ) ∧ sleeps ( x ) VP : λx . sleeps ( x ) Vi : λx . sleeps ( x ) sleeps NP : λP . ∃ xP ( x ) ∧ dog ( x ) NN : dog dog DT : λQ . λP . ∃ x.P ( x ) ∧ Q ( x ) A Figure 12.3 : Derivation of the semantic representation for A dog sleeps , in grammar G2 start with the indefinite article a , to which we assign the rather intimidating semantics , λP . λQ . ∃ xP ( x ) ∧ Q ( x ) . [ 12.15 ] This is a lambda expression that takes two relations as arguments , P and Q . The relation P is scoped to the outer lambda expression , so it will be provided by the immediately adjacent noun , which in this case is DOG . Thus , the noun phrase a dog has the semantics , NP . sem =DET.sem@NN.sem [ 12.16 ] =( λP . λQ . ∃ xP ( x ) ∧ Q ( x ) ) @ ( DOG ) [ 12.17 ] = λQ . ∃ xDOG ( x ) ∧ Q ( x ) . [ 12.18 ] This is a lambda expression that is expecting another relation , Q , which will be provided by the verb phrase , SLEEPS . This gives the desired analysis , ∃ xDOG ( x ) ∧ SLEEPS ( x ) . 9 If noun phrases like a dog are interpreted as lambda expressions , then proper nouns like Alex must be treated in the same way . This is achieved by type-raising from con - stants to lambda expressions , x ⇒ λP . P ( x ) . After type-raising , the semantics of Alex is λP . P ( ALEX ) — a lambda expression that expects a relation to tell us something about ALEX . 10 Again , make sure you see how the analysis in Figure 12.3 can be applied to the sentence Alex sleeps . 9When applying β-reduction to arguments that are themselves lambda expressions , be sure to use unique variable names to avoid confusion . For example , it is important to distinguish the x in the semantics for a from the x in the semantics for likes . Variable names are abstractions , and can always be changed — this is known as α-conversion . For example , λx . P ( x ) can be converted to λy . P ( y ) , etc . 10Compositional semantic analysis is often supported by type systems , which make it possible to check whether a given function application is valid . The base types are entities e and truth values t . A property , such as DOG , is a function from entities to truth values , so its type is written 〈 e , t 〉 . A transitive verb has type 〈 e , 〈 e , t 〉 〉 : after receiving the first entity ( the direct object ) , it returns a function from entities to truth values , which will be applied to the subject of the sentence . The type-raising operation x ⇒ λP . P ( x ) corresponds to a change in type from e to 〈 〈 e , t 〉 , t 〉 : it expects a function from entities to truth values , and returns a truth value . Under contract with MIT Press , shared under CC-BY-NC-ND license . 296 CHAPTER 12 . LOGICAL SEMANTICS S : ∃ xdog ( x ) ∧ likes ( x , alex ) VP : λx . likes ( x , alex ) NP : λP . P ( alex ) NNP : alex Alex Vt : λP . λx . P ( λy . likes ( x , y ) ) likes NP : λQ . ∃ xdog ( x ) ∧ Q ( x ) NN : dog dog DT : λP . λQ . ∃ xP ( x ) ∧ Q ( x ) A Figure 12.4 : Derivation of the semantic representation for A dog likes Alex . Direct objects are handled by applying the same type-raising operation to transitive verbs : the meaning of verbs such as likes is raised to , λP . λx . P ( λy . LIKES ( x , y ) ) [ 12.19 ] As a result , we can keep the verb phrase production VP . sem = V.sem@NP.sem , knowing that the direct object will provide the function P in Equation 12.19 . To see how this works , let’s analyze the verb phrase likes a dog . After uniquely relabeling each lambda variable , VP . sem =V.sem@NP.sem =( λP . λx . P ( λy . LIKES ( x , y ) ) ) @ ( λQ . ∃ zDOG ( z ) ∧ Q ( z ) ) = λx . ( λQ . ∃ zDOG ( z ) ∧ Q ( z ) ) @ ( λy . LIKES ( x , y ) ) = λx . ∃ zDOG ( z ) ∧ ( λy . LIKES ( x , y ) ) @z = λx . ∃ zDOG ( z ) ∧ LIKES ( x , z ) . These changes are summarized in the revised grammar G2 , shown in Table 12.2 . Fig - ure 12.4 shows a derivation that involves a transitive verb , an indefinite noun phrase , and a proper noun . 12.4 Learning semantic parsers As with syntactic parsing , any syntactic-semantic grammar with sufficient coverage risks producing many possible analyses for any given sentence . Machine learning is the dom - inant approach to selecting a single analysis . We will focus on algorithms that learn to score logical forms by attaching weights to features of their derivations ( Zettlemoyer and Collins , 2005 ) . Alternative approaches include transition-based parsing ( Zelle and Mooney , 1996 ; Misra and Artzi , 2016 ) and methods inspired by machine translation ( Wong and Mooney , 2006 ) . Methods also differ in the form of supervision used for learning , which can range from complete derivations to much more limited training signals . We will begin with the case of complete supervision , and then consider how learning is still possible even when seemingly key information is missing . Jacob Eisenstein . Draft of October 15 , 2018 . 12.4 . LEARNING SEMANTIC PARSERS 297 S → NP VP NP.sem@VP.sem VP → Vt NP Vt.sem@NP.sem VP → Vi Vi . sem NP → DET NN DET.sem@NN.sem NP → NNP λP . P ( NNP . sem ) DET → a λP . λQ . ∃ xP ( x ) ∧ Q ( x ) DET → every λP . λQ . ∀ x ( P ( x ) ⇒ Q ( x ) ) Vt → likes λP . λx . P ( λy . LIKES ( x , y ) ) Vi → sleeps λx . SLEEPS ( x ) NN → dog DOG NNP → Alex ALEX NNP → Brit BRIT Table 12.2 : G2 , a syntactic-semantic context-free grammar fragment , which supports quantified noun phrases Datasets Early work on semantic parsing focused on natural language expressions of geographical database queries , such as What states border Texas . The GeoQuery dataset of Zelle and Mooney ( 1996 ) was originally coded in prolog , but has subsequently been expanded and converted into the SQL database query language by Popescu et al . ( 2003 ) and into first-order logic with lambda calculus by Zettlemoyer and Collins ( 2005 ) , pro - viding logical forms like λx . STATE ( x ) ∧ BORDERS ( x , TEXAS ) . Another early dataset con - sists of instructions for RoboCup robot soccer teams ( Kate et al . , 2005 ) . More recent work has focused on broader domains , such as the Freebase database ( Bollacker et al . , 2008 ) , for which queries have been annotated by Krishnamurthy and Mitchell ( 2012 ) and Cai and Yates ( 2013 ) . Other recent datasets include child-directed speech ( Kwiatkowski et al . , 2012 ) and elementary school science exams ( Krishnamurthy , 2016 ) . 12.4.1 Learning from derivations Let w ( i ) indicate a sequence of text , and let y ( i ) indicate the desired logical form . For example : w ( i ) = Alex eats shoots and leaves y ( i ) = EATS ( ALEX , SHOOTS ) ∧ EATS ( ALEX , LEAVES ) In the standard supervised learning paradigm that was introduced in section 2.3 , we first define a feature function , f ( w , y ) , and then learn weights on these features , so that y ( i ) = argmaxy θ · f ( w , y ) . The weight vector θ is learned by comparing the features of the true label f ( w ( i ) , y ( i ) ) against either the features of the predicted label f ( w ( i ) , ŷ ) ( per - Under contract with MIT Press , shared under CC-BY-NC-ND license . 298 CHAPTER 12 . LOGICAL SEMANTICS S : eats ( alex , shoots ) ∧ eats ( alex , leavesn ) VP : λx . eats ( x , shoots ) ∧ eats ( x , leavesn ) NP : λP . P ( shoots ) ∧ P ( leavesn ) NP : λP . P ( leavesn ) leaves CC : λP . λQ . λx . P ( x ) ∧ Q ( x ) and NP : λP . P ( shoots ) shoots Vt : λP . λx . P ( λy . eats ( x , y ) ) eats NP : λP . P ( alex ) Alex Figure 12.5 : Derivation for gold semantic analysis of Alex eats shoots and leaves ceptron , support vector machine ) or the expected feature vector Ey | w [ f ( w ( i ) , y ) ] ( logistic regression ) . While this basic framework seems similar to discriminative syntactic parsing , there is a crucial difference . In ( context-free ) syntactic parsing , the annotation y ( i ) contains all of the syntactic productions ; indeed , the task of identifying the correct set of productions is identical to the task of identifying the syntactic structure . In semantic parsing , this is not the case : the logical form EATS ( ALEX , SHOOTS ) ∧ EATS ( ALEX , LEAVES ) does not reveal the syntactic-semantic productions that were used to obtain it . Indeed , there may be spu - rious ambiguity , so that a single logical form can be reached by multiple derivations . ( We previously encountered spurious ambiguity in transition-based dependency parsing , subsection 11.3.2 . ) These ideas can be formalized by introducing an additional variable z , representing the derivation of the logical form y from the text w . Assume that the feature function de - composes across the productions in the derivation , f ( w , z , y ) = ∑ T t = 1 f ( w , zt , y ) , where zt indicates a single syntactic-semantic production . For example , we might have a feature for the production S → NP VP : NP.sem@VP.sem , as well as for terminal productions like NNP → Alex : ALEX . Under this decomposition , it is possible to compute scores for each semantically-annotated subtree in the analysis ofw , so that bottom-up parsing algo - rithms like CKY ( section 10.1 ) can be applied to find the best-scoring semantic analysis . Figure 12.5 shows a derivation of the correct semantic analysis of the sentence Alex eats shoots and leaves , in a simplified grammar in which the plural noun phrases shoots and leaves are interpreted as logical constants SHOOTS and LEAVESn . Figure 12.6 shows a derivation of an incorrect analysis . Assuming one feature per production , the perceptron update is shown in Table 12.3 . From this update , the parser would learn to prefer the noun interpretation of leaves over the verb interpretation . It would also learn to prefer noun phrase coordination over verb phrase coordination . While the update is explained in terms of the perceptron , it would be easy to replace the perceptron with a conditional random field . In this case , the online updates would be Jacob Eisenstein . Draft of October 15 , 2018 . 12.4 . LEARNING SEMANTIC PARSERS 299 S : eats ( alex , shoots ) ∧ leavesv ( alex ) VP : λx . eats ( x , shoots ) ∧ leavesv ( x ) VP : λx . leavesv ( x ) Vi : λx . leavesv ( x ) leaves CC : λP . λQ . λx . P ( x ) ∧ Q ( x ) and VP : λx . eats ( x , shoots ) NP : λP . P ( shoots ) shoots Vt : λP . λx . P ( λy . eats ( x , y ) ) eats NP : λP . P ( alex ) Alex Figure 12.6 : Derivation for incorrect semantic analysis of Alex eats shoots and leaves NP1 → NP2 CC NP3 ( CC . sem @ ( NP2 . sem ) ) @ ( NP3 . sem ) + 1 VP1 → VP2 CC VP3 ( CC . sem @ ( VP2 . sem ) ) @ ( VP3 . sem ) - 1 NP → leaves LEAVESn + 1 VP → Vi Vi . sem - 1 Vi → leaves λx . LEAVESv - 1 Table 12.3 : Perceptron update for analysis in Figure 12.5 ( gold ) and Figure 12.6 ( predicted ) based on feature expectations , which can be computed using the inside-outside algorithm ( section 10.6 ) . 12.4.2 Learning from logical forms Complete derivations are expensive to annotate , and are rarely available . 11 One solution is to focus on learning from logical forms directly , while treating the derivations as la - tent variables ( Zettlemoyer and Collins , 2005 ) . In a conditional probabilistic model over logical forms y and derivations z , we have , p ( y , z | w ) = exp ( θ · f ( w , z , y ) ) ∑ y ′ , z ′ exp ( θ · f ( w , z ′ , y ′ ) ) , [ 12.20 ] which is the standard log-linear model , applied to the logical form y and the derivation z . Since the derivation z unambiguously determines the logical form y , it may seem silly to model the joint probability over y and z . However , since z is unknown , it can be marginalized out , p ( y | w ) = ∑ z p ( y , z | w ) . [ 12.21 ] 11An exception is the work of Ge and Mooney ( 2005 ) , who annotate the meaning of each syntactic con - stituents for several hundred sentences . Under contract with MIT Press , shared under CC-BY-NC-ND license . 300 CHAPTER 12 . LOGICAL SEMANTICS The semantic parser can then select the logical form with the maximum log marginal probability , log ∑ z p ( y , z | w ) = log ∑ z exp ( θ · f ( w , z , y ) ) ∑ y ′ , z ′ exp ( θ · f ( w , z ′ , y ′ ) ) [ 12.22 ] ∝ log ∑ z exp ( θ · f ( w , z ′ , y ′ ) ) [ 12.23 ] ≥ max z θ · f ( w , z , y ) . [ 12.24 ] It is impossible to push the log term inside the sum over z , so our usual linear scoring function does not apply . We can recover this scoring function only in approximation , by taking the max ( rather than the sum ) over derivations z , which provides a lower bound . Learning can be performed by maximizing the log marginal likelihood , ` ( θ ) = N ∑ i = 1 log p ( y ( i ) | w ( i ) ; θ ) [ 12.25 ] = N ∑ i = 1 log ∑ z p ( y ( i ) , z ( i ) | w ( i ) ; θ ) . [ 12.26 ] This log-likelihood is not convex in θ , unlike the log-likelihood of a fully-observed condi - tional random field . This means that learning can give different results depending on the initialization . The derivative of Equation 12.26 is , ∂ ` i ∂ θ = ∑ z p ( z | y , w ; θ ) f ( w , z , y ) − ∑ y ′ , z ′ p ( y ′ , z ′ | w ; θ ) f ( w , z ′ , y ′ ) [ 12.27 ] = Ez | y , wf ( w , z , y ) − Ey , z | wf ( w , z , y ) [ 12.28 ] Both expectations can be computed via bottom-up algorithms like inside-outside . Al - ternatively , we can again maximize rather than marginalize over derivations for an ap - proximate solution . In either case , the first term of the gradient requires us to identify derivations z that are compatible with the logical form y . This can be done in a bottom - up dynamic programming algorithm , by having each cell in the table t [ i , j , X ] include the set of all possible logical forms forX wi + 1 : j . The resulting table may therefore be much larger than in syntactic parsing . This can be controlled by using pruning to eliminate in - termediate analyses that are incompatible with the final logical form y ( Zettlemoyer and Collins , 2005 ) , or by using beam search and restricting the size of each cell to some fixed constant ( Liang et al . , 2013 ) . If we replace each expectation in Equation 12.28 with argmax and then apply stochastic gradient descent to learn the weights , we obtain the latent variable perceptron , a simple Jacob Eisenstein . Draft of October 15 , 2018 . 12.4 . LEARNING SEMANTIC PARSERS 301 Algorithm 16 Latent variable perceptron 1 : procedure LATENTVARIABLEPERCEPTRON ( w ( 1 : N ) , y ( 1 : N ) ) 2 : θ ← 0 3 : repeat 4 : Select an instance i 5 : z ( i ) ← argmaxz θ · f ( w ( i ) , z , y ( i ) ) 6 : ŷ , ẑ ← argmaxy ′ , z ′ θ · f ( w ( i ) , z ′ , y ′ ) 7 : θ ← θ + f ( w ( i ) , z ( i ) , y ( i ) ) − f ( w ( i ) , ẑ , ŷ ) 8 : until tired 9 : return θ and general algorithm for learning with missing data . The algorithm is shown in its most basic form in Algorithm 16 , but the usual tricks such as averaging and margin loss can be applied ( Yu and Joachims , 2009 ) . Aside from semantic parsing , the latent variable perceptron has been used in tasks such as machine translation ( Liang et al . , 2006 ) and named entity recognition ( Sun et al . , 2009 ) . In latent conditional random fields , we use the full expectations rather than maximizing over the hidden variable . This model has also been employed in a range of problems beyond semantic parsing , including parse reranking ( Koo and Collins , 2005 ) and gesture recognition ( Quattoni et al . , 2007 ) . 12.4.3 Learning from denotations Logical forms are easier to obtain than complete derivations , but the annotation of logical forms still requires considerable expertise . However , it is relatively easy to obtain deno - tations for many natural language sentences . For example , in the geography domain , the denotation of a question would be its answer ( Clarke et al . , 2010 ; Liang et al . , 2013 ) : Text : What states border Georgia ? Logical form : λx . STATE ( x ) ∧ BORDER ( x , GEORGIA ) Denotation :{ Alabama , Florida , North Carolina , South Carolina , Tennessee } Similarly , in a robotic control setting , the denotation of a command would be an action or sequence of actions ( Artzi and Zettlemoyer , 2013 ) . In both cases , the idea is to reward the semantic parser for choosing an analysis whose denotation is correct : the right answer to the question , or the right action . Learning from logical forms was made possible by summing or maxing over deriva - tions . This idea can be carried one step further , summing or maxing over all logical forms with the correct denotation . Let vi ( y ) ∈ { 0 , 1 } be a validation function , which assigns a Under contract with MIT Press , shared under CC-BY-NC-ND license . 302 CHAPTER 12 . LOGICAL SEMANTICS binary score indicating whether the denotation JyK for the textw ( i ) is correct . We can then learn by maximizing a conditional-likelihood objective , ` ( i ) ( θ ) = log ∑ y vi ( y ) × p ( y | w ; θ ) [ 12.29 ] = log ∑ y vi ( y ) × ∑ z p ( y , z | w ; θ ) , [ 12.30 ] which sums over all derivations z of all valid logical forms , { y : vi ( y ) = 1 } . This cor - responds to the log-probability that the semantic parser produces a logical form with a valid denotation . Differentiating with respect to θ , we obtain , ∂ ` ( i ) ∂ θ = ∑ y , z : vi ( y ) = 1 p ( y , z | w ) f ( w , z , y ) − ∑ y ′ , z ′ p ( y ′ , z ′ | w ) f ( w , z ′ , y ′ ) , [ 12.31 ] which is the usual difference in feature expectations . The positive term computes the expected feature expectations conditioned on the denotation being valid , while the second term computes the expected feature expectations according to the current model , without regard to the ground truth . Large-margin learning formulations are also possible for this problem . For example , Artzi and Zettlemoyer ( 2013 ) generate a set of valid and invalid derivations , and then impose a constraint that all valid derivations should score higher than all invalid derivations . This constraint drives a perceptron-like learning rule . Additional resources A key issue not considered here is how to handle semantic underspecification : cases in which there are multiple semantic interpretations for a single syntactic structure . Quanti - fier scope ambiguity is a classic example . Blackburn and Bos ( 2005 ) enumerate a number of approaches to this issue , and also provide links between natural language semantics and computational inference techniques . Much of the contemporary research on semantic parsing uses the framework of combinatory categorial grammar ( CCG ) . Carpenter ( 1997 ) provides a comprehensive treatment of how CCG can support compositional semantic analysis . Another recent area of research is the semantics of multi-sentence texts . This can be handled with models of dynamic semantics , such as dynamic predicate logic ( Groe - nendijk and Stokhof , 1991 ) . Alternative readings on formal semantics include an “ informal ” reading from Levy and Manning ( 2009 ) , and a more involved introduction from Briscoe ( 2011 ) . To learn more about ongoing research on data-driven semantic parsing , readers may consult the survey Jacob Eisenstein . Draft of October 15 , 2018 . 12.4 . LEARNING SEMANTIC PARSERS 303 article by Liang and Potts ( 2015 ) , tutorial slides and videos by Artzi and Zettlemoyer ( 2013 ) , 12 and the source code by Yoav Artzi13 and Percy Liang . 14 Exercises 1 . The modus ponens inference rule states that if we know φ ⇒ ψ and φ , then ψ must be true . Justify this rule , using the definition of the ⇒ operator and some of the laws provided in subsection 12.2.1 , plus one additional identity : ⊥ ∨ φ = φ . 2 . Convert the following examples into first-order logic , using the relations CAN-SLEEP , MAKES-NOISE , and BROTHER . • If Abigail makes noise , no one can sleep . • If Abigail makes noise , someone cannot sleep . • None of Abigail’s brothers can sleep . • If one of Abigail’s brothers makes noise , Abigail cannot sleep . 3 . Extend the grammar fragment G1 to include the ditransitive verb teaches and the proper noun Swahili . Show how to derive the interpretation for the sentence Alex teaches Brit Swahili , which should be TEACHES ( ALEX , BRIT , SWAHILI ) . The grammar need not be in Chomsky Normal Form . For the ditransitive verb , use NP1 and NP2 to indicate the two direct objects . 4 . Derive the semantic interpretation for the sentence Alex likes every dog , using gram - mar fragment G2 . 5 . Extend the grammar fragment G2 to handle adjectives , so that the meaning of an angry dog is λP . ∃ xDOG ( x ) ∧ ANGRY ( x ) ∧ P ( x ) . Specifically , you should supply the lexical entry for the adjective angry , and you should specify the syntactic-semantic productions NP → DET NOM , NOM → JJ NOM , and NOM → NN . 6 . Extend your answer to the previous question to cover copula constructions with predicative adjectives , such as Alex is angry . The interpretation should be ANGRY ( ALEX ) . You should add a verb phrase production VP → Vcop JJ , and a terminal production Vcop → is . Show why your grammar extensions result in the correct interpretation . 7 . In Figure 12.5 and Figure 12.6 , we treat the plurals shoots and leaves as entities . Revise G2 so that the interpretation of Alex eats leaves is ∀ x . ( LEAF ( x ) ⇒ EATS ( ALEX , x ) ) , and show the resulting perceptron update . 12Videos are currently available at http://yoavartzi.com/tutorial/ 13http :// yoavartzi.com/spf 14https :// github.com/percyliang/sempre Under contract with MIT Press , shared under CC-BY-NC-ND license . 304 CHAPTER 12 . LOGICAL SEMANTICS 8 . Statements like every student eats a pizza have two possible interpretations , depend - ing on quantifier scope : ∀ x ∃ yPIZZA ( y ) ∧ ( STUDENT ( x ) ⇒ EATS ( x , y ) ) [ 12.32 ] ∃ y ∀ xPIZZA ( y ) ∧ ( STUDENT ( x ) ⇒ EATS ( x , y ) ) [ 12.33 ] a ) Explain why these interpretations really are different . b ) Which is generated by grammar G2 ? Note that you may have to manipulate the logical form to exactly align with the grammar . 9 . * Modify G2 so that produces the second interpretation in the previous problem . Hint : one possible solution involves changing the semantics of the sentence pro - duction and one other production . 10 . In the GeoQuery domain , give a natural language query that has multiple plausible semantic interpretations with the same denotation . List both interpretaions and the denotation . Hint : There are many ways to do this , but one approach involves using toponyms ( place names ) that could plausibly map to several different entities in the model . Jacob Eisenstein . Draft of October 15 , 2018 .