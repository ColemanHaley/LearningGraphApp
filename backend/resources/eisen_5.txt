Chapter 5 Learning without supervision So far , we have assumed the following setup : • a training set where you get observations x and labels y ; • a test set where you only get observations x . Without labeled data , is it possible to learn anything ? This scenario is known as unsu - pervised learning , and we will see that indeed it is possible to learn about the underlying structure of unlabeled observations . This chapter will also explore some related scenarios : semi-supervised learning , in which only some instances are labeled , and domain adap - tation , in which the training data differs from the data on which the trained system will be deployed . 5.1 Unsupervised learning To motivate unsupervised learning , consider the problem of word sense disambiguation ( section 4.2 ) . The goal is to classify each instance of a word , such as bank into a sense , • bank # 1 : a financial institution • bank # 2 : the land bordering a river It is difficult to obtain sufficient training data for word sense disambiguation , because even a large corpus will contain only a few instances of all but the most common words . Is it possible to learn anything about these different senses without labeled data ? Word sense disambiguation is usually performed using feature vectors constructed from the local context of the word to be disambiguated . For example , for the word 95 96 CHAPTER 5 . LEARNING WITHOUT SUPERVISION 0 10 20 30 40 density of word group 1 0 20 40 d en si ty of w or d gr ou p 2 Figure 5.1 : Counts of words from two different context groups bank , the immediate context might typically include words from one of the following two groups : 1 . financial , deposits , credit , lending , capital , markets , regulated , reserve , liquid , assets 2 . land , water , geography , stream , river , flow , deposits , discharge , channel , ecology Now consider a scatterplot , in which each point is a document containing the word bank . The location of the document on the x-axis is the count of words in group 1 , and the location on the y-axis is the count for group 2 . In such a plot , shown in Figure 5.1 , two “ blobs ” might emerge , and these blobs correspond to the different senses of bank . Here’s a related scenario , from a different problem . Suppose you download thousands of news articles , and make a scatterplot , where each point corresponds to a document : the x-axis is the frequency of the group of words ( hurricane , winds , storm ) ; the y-axis is the frequency of the group ( election , voters , vote ) . This time , three blobs might emerge : one for documents that are largely about a hurricane , another for documents largely about a election , and a third for documents about neither topic . These clumps represent the underlying structure of the data . But the two-dimensional scatter plots are based on groupings of context words , and in real scenarios these word lists are unknown . Unsupervised learning applies the same basic idea , but in a high - dimensional space with one dimension for every context word . This space can’t be di - rectly visualized , but the goal is the same : try to identify the underlying structure of the observed data , such that there are a few clusters of points , each of which is internally coherent . Clustering algorithms are capable of finding such structure automatically . 5.1.1 K-means clustering Clustering algorithms assign each data point to a discrete cluster , zi ∈ 1 , 2 , . . . K . One of the best known clustering algorithms is K-means , an iterative algorithm that maintains Jacob Eisenstein . Draft of October 15 , 2018 . 5.1 . UNSUPERVISED LEARNING 97 Algorithm 8 K-means clustering algorithm 1 : procedure K-MEANS ( x1 : N , K ) 2 : for i ∈ 1 . . . N do . initialize cluster memberships 3 : z ( i ) ← RANDOMINT ( 1 , K ) 4 : repeat 5 : for k ∈ 1 . . . K do . recompute cluster centers 6 : νk ← 1δ ( z ( i ) = k ) ∑ N i = 1 δ ( z ( i ) = k ) x ( i ) 7 : for i ∈ 1 . . . N do . reassign instances to nearest clusters 8 : z ( i ) ← argmink | | x ( i ) − νk | | 2 9 : until converged 10 : return { z ( i ) } . return cluster assignments a cluster assignment for each instance , and a central ( “ mean ” ) location for each cluster . K-means iterates between updates to the assignments and the centers : 1 . each instance is placed in the cluster with the closest center ; 2 . each center is recomputed as the average over points in the cluster . This procedure is formalized in Algorithm 8 . The term | | x ( i ) − ν | | 2 refers to the squared Euclidean norm , ∑ V j = 1 ( x ( i ) j − νj ) 2 . An important property of K-means is that the con - verged solution depends on the initialization , and a better clustering can sometimes be found simply by re-running the algorithm from a different random starting point . Soft K-means is a particularly relevant variant . Instead of directly assigning each point to a specific cluster , soft K-means assigns to each point a distribution over clusters q ( i ) , so that ∑ K k = 1 q ( i ) ( k ) = 1 , and ∀ k , q ( i ) ( k ) ≥ 0 . The soft weight q ( i ) ( k ) is computed from the distance of x ( i ) to the cluster center νk . In turn , the center of each cluster is computed from a weighted average of the points in the cluster , νk = 1 ∑ N i = 1 q ( i ) ( k ) N ∑ i = 1 q ( i ) ( k ) x ( i ) . [ 5.1 ] We will now explore a probablistic version of softK-means clustering , based on expectation - maximization ( EM ) . Because EM clustering can be derived as an approximation to maximum - likelihood estimation , it can be extended in a number of useful ways . Under contract with MIT Press , shared under CC-BY-NC-ND license . 98 CHAPTER 5 . LEARNING WITHOUT SUPERVISION 5.1.2 Expectation-Maximization ( EM ) Expectation-maximization combines the idea of soft K-means with Naı̈ve Bayes classifi - cation . To review , Naı̈ve Bayes defines a probability distribution over the data , log p ( x , y ; φ , µ ) = N ∑ i = 1 log ( p ( x ( i ) | y ( i ) ; φ ) × p ( y ( i ) ; µ ) ) [ 5.2 ] Now suppose that you never observe the labels . To indicate this , we’ll refer to the label of each instance as z ( i ) , rather than y ( i ) , which is usually reserved for observed variables . By marginalizing over the latent variables z , we obtain the marginal probability of the observed instances x : log p ( x ; φ , µ ) = N ∑ i = 1 log p ( x ( i ) ; φ , µ ) [ 5.3 ] = N ∑ i = 1 log K ∑ z = 1 p ( x ( i ) , z ; φ , µ ) [ 5.4 ] = N ∑ i = 1 log K ∑ z = 1 p ( x ( i ) | z ; φ ) × p ( z ; µ ) . [ 5.5 ] The parameters φ and µ can be obtained by maximizing the marginal likelihood in Equation 5.5 . Why is this the right thing to maximize ? Without labels , discriminative learning is impossible — there’s nothing to discriminate . So maximum likelihood is all we have . When the labels are observed , we can estimate the parameters of the Naı̈ve Bayes probability model separately for each label . But marginalizing over the labels couples these parameters , making direct optimization of log p ( x ) intractable . We will approxi - mate the log-likelihood by introducing an auxiliary variable q ( i ) , which is a distribution over the label set Z = { 1 , 2 , . . . , K } . The optimization procedure will alternate between updates to q and updates to the parameters ( φ , µ ) . Thus , q ( i ) plays here as in soft K - means . To derive the updates for this optimization , multiply the right side of Equation 5.5 by Jacob Eisenstein . Draft of October 15 , 2018 . 5.1 . UNSUPERVISED LEARNING 99 the ratio q ( i ) ( z ) q ( i ) ( z ) = 1 , log p ( x ; φ , µ ) = N ∑ i = 1 log K ∑ z = 1 p ( x ( i ) | z ; φ ) × p ( z ; µ ) × q ( i ) ( z ) q ( i ) ( z ) [ 5.6 ] = N ∑ i = 1 log K ∑ z = 1 q ( i ) ( z ) × p ( x ( i ) | z ; φ ) × p ( z ; µ ) × 1 q ( i ) ( z ) [ 5.7 ] = N ∑ i = 1 logEq ( i ) [ p ( x ( i ) | z ; φ ) p ( z ; µ ) q ( i ) ( z ) ] , [ 5.8 ] where Eq ( i ) [ f ( z ) ] = ∑ K z = 1 q ( i ) ( z ) × f ( z ) refers to the expectation of the function f under the distribution z ∼ q ( i ) . Jensen’s inequality says that because log is a concave function , we can push it inside the expectation , and obtain a lower bound . log p ( x ; φ , µ ) ≥ N ∑ i = 1 Eq ( i ) [ log p ( x ( i ) | z ; φ ) p ( z ; µ ) q ( i ) ( z ) ] [ 5.9 ] J , N ∑ i = 1 Eq ( i ) [ log p ( x ( i ) | z ; φ ) + log p ( z ; µ ) − log q ( i ) ( z ) ] [ 5.10 ] = N ∑ i = 1 Eq ( i ) [ log p ( x ( i ) , z ; φ , µ ) ] + H ( q ( i ) ) [ 5.11 ] We will focus on Equation 5.10 , which is the lower bound on the marginal log-likelihood of the observed data , log p ( x ) . Equation 5.11 shows the connection to the information theoretic concept of entropy , H ( q ( i ) ) = − ∑ Kz = 1 q ( i ) ( z ) log q ( i ) ( z ) , which measures the av - erage amount of information produced by a draw from the distribution q ( i ) . The lower bound J is a function of two groups of arguments : • the distributions q ( i ) for each instance ; • the parameters µ and φ . The expectation-maximization ( EM ) algorithm maximizes the bound with respect to each of these arguments in turn , while holding the other fixed . The E-step The step in which we update q ( i ) is known as the E-step , because it updates the distribu - tion under which the expectation is computed . To derive this update , first write out the Under contract with MIT Press , shared under CC-BY-NC-ND license . 100 CHAPTER 5 . LEARNING WITHOUT SUPERVISION expectation in the lower bound as a sum , J = N ∑ i = 1 K ∑ z = 1 q ( i ) ( z ) [ log p ( x ( i ) | z ; φ ) + log p ( z ; µ ) − log q ( i ) ( z ) ] . [ 5.12 ] When optimizing this bound , we must also respect a set of “ sum-to-one ” constraints , ∑ K z = 1 q ( i ) ( z ) = 1 for all i . Just as in Naı̈ve Bayes , this constraint can be incorporated into a Lagrangian : Jq = N ∑ i = 1 K ∑ z = 1 q ( i ) ( z ) ( log p ( x ( i ) | z ; φ ) + log p ( z ; µ ) − log q ( i ) ( z ) ) + λ ( i ) ( 1 − K ∑ z = 1 q ( i ) ( z ) ) , [ 5.13 ] where λ ( i ) is the Lagrange multiplier for instance i . The Lagrangian is maximized by taking the derivative and solving for q ( i ) : ∂ Jq ∂ q ( i ) ( z ) = log p ( x ( i ) | z ; φ ) + log p ( z ; θ ) − log q ( i ) ( z ) − 1 − λ ( i ) [ 5.14 ] log q ( i ) ( z ) = log p ( x ( i ) | z ; φ ) + log p ( z ; µ ) − 1 − λ ( i ) [ 5.15 ] q ( i ) ( z ) ∝ p ( x ( i ) | z ; φ ) × p ( z ; µ ) . [ 5.16 ] Applying the sum-to-one constraint gives an exact solution , q ( i ) ( z ) = p ( x ( i ) | z ; φ ) × p ( z ; µ ) ∑ K z ′ = 1 p ( x ( i ) | z ′ ; φ ) × p ( z ′ ; µ ) [ 5.17 ] =p ( z | x ( i ) ; φ , µ ) . [ 5.18 ] After normalizing , each q ( i ) — which is the soft distribution over clusters for data x ( i ) — is set to the posterior probability p ( z | x ( i ) ; φ , µ ) under the current parameters . Although the Lagrange multipliers λ ( i ) were introduced as additional parameters , they drop out during normalization . The M-step Next , we hold fixed the soft assignments q ( i ) , and maximize with respect to the pa - rameters , φ and µ . Let’s focus on the parameter φ , which parametrizes the likelihood p ( x | z ; φ ) , and leave µ for an exercise . The parameter φ is a distribution over words for each cluster , so it is optimized under the constraint that ∑ V j = 1 φz , j = 1 . To incorporate this Jacob Eisenstein . Draft of October 15 , 2018 . 5.1 . UNSUPERVISED LEARNING 101 constraint , we introduce a set of Lagrange multiplers { λz } Kz = 1 , and from the Lagrangian , Jφ = N ∑ i = 1 K ∑ z = 1 q ( i ) ( z ) ( log p ( x ( i ) | z ; φ ) + log p ( z ; µ ) − log q ( i ) ( z ) ) + K ∑ z = 1 λz ( 1 − V ∑ j = 1 φz , j ) . [ 5.19 ] The term log p ( x ( i ) | z ; φ ) is the conditional log-likelihood for the multinomial , which expands to , log p ( x ( i ) | z , φ ) = C + V ∑ j = 1 xj log φz , j , [ 5.20 ] where C is a constant with respect to φ — see Equation 2.12 in section 2.2 for more dis - cussion of this probability function . Setting the derivative of Jφ equal to zero , ∂ Jφ ∂ φz , j = N ∑ i = 1 q ( i ) ( z ) × x ( i ) j φz , j − λz [ 5.21 ] φz , j ∝ N ∑ i = 1 q ( i ) ( z ) × x ( i ) j . [ 5.22 ] Because φz is constrained to be a probability distribution , the exact solution is computed as , φz , j = ∑ N i = 1 q ( i ) ( z ) × x ( i ) j ∑ V j ′ = 1 ∑ N i = 1 q ( i ) ( z ) × x ( i ) j ′ = Eq [ count ( z , j ) ] ∑ V j ′ = 1Eq [ count ( z , j ′ ) ] , [ 5.23 ] where the counter j ∈ { 1 , 2 , . . . , V } indexes over base features , such as words . This update setsφz equal to the relative frequency estimate of the expected counts under the distribution q . As in supervised Naı̈ve Bayes , we can smooth these counts by adding a constant α . The update for µ is similar : µz ∝ ∑ N i = 1 q ( i ) ( z ) = Eq [ count ( z ) ] , which is the expected frequency of cluster z . These probabilities can also be smoothed . In sum , the M-step is just like Naı̈ve Bayes , but with expected counts rather than observed counts . The multinomial likelihood p ( x | z ) can be replaced with other probability distribu - tions : for example , for continuous observations , a Gaussian distribution can be used . In some cases , there is no closed-form update to the parameters of the likelihood . One ap - proach is to run gradient-based optimization at each M-step ; another is to simply take a single step along the gradient step and then return to the E-step ( Berg-Kirkpatrick et al . , 2010 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 102 CHAPTER 5 . LEARNING WITHOUT SUPERVISION 0 2 4 6 8 iteration 430000 440000 450000 n eg at iv e lo g - lik el ih o o d b ou n d Figure 5.2 : Sensitivity of expectation-maximization to initialization . Each line shows the progress of optimization from a different random initialization . 5.1.3 EM as an optimization algorithm Algorithms that update a global objective by alternating between updates to subsets of the parameters are called coordinate ascent algorithms . The objective J ( the lower bound on the marginal likelihood of the data ) is separately convex in q and ( µ , φ ) , but it is not jointly convex in all terms ; this condition is known as biconvexity . Each step of the expectation - maximization algorithm is guaranteed not to decrease the lower bound J , which means that EM will converge towards a solution at which no nearby points yield further im - provements . This solution is a local optimum — it is as good or better than any of its immediate neighbors , but is not guaranteed to be optimal among all possible configura - tions of ( q , µ , φ ) . The fact that there is no guarantee of global optimality means that initialization is important : where you start can determine where you finish . To illustrate this point , Figure 5.2 shows the objective function for EM with ten different random initializations : while the objective function improves monotonically in each run , it converges to several different values . 1 For the convex objectives that we encountered in chapter 2 , it was not necessary to worry about initialization , because gradient-based optimization guaranteed to reach the global minimum . But in expectation-maximization — as in the deep neural networks from chapter 3 — initialization matters . In hard EM , each q ( i ) distribution assigns probability of 1 to a single label ẑ ( i ) , and zero probability to all others ( Neal and Hinton , 1998 ) . This is similar in spirit to K-means clus - tering , and can outperform standard EM in some cases ( Spitkovsky et al . , 2010 ) . Another variant of expectation-maximization incorporates stochastic gradient descent ( SGD ) : after performing a local E-step at each instance x ( i ) , we immediately make a gradient update to the parameters ( µ , φ ) . This algorithm has been called incremental expectation maxi - mization ( Neal and Hinton , 1998 ) and online expectation maximization ( Sato and Ishii , 1The figure shows the upper bound on the negative log-likelihood , because optimization is typically framed as minimization rather than maximization . Jacob Eisenstein . Draft of October 15 , 2018 . 5.1 . UNSUPERVISED LEARNING 103 2000 ; Cappé and Moulines , 2009 ) , and is especially useful when there is no closed-form optimum for the likelihood p ( x | z ) , and in online settings where new data is constantly streamed in ( see Liang and Klein , 2009 , for a comparison for online EM variants ) . 5.1.4 How many clusters ? So far , we have assumed that the number of clusters K is given . In some cases , this as - sumption is valid . For example , a lexical semantic resource like WORDNET might define the number of senses for a word . In other cases , the number of clusters could be a parame - ter for the user to tune : some readers want a coarse-grained clustering of news stories into three or four clusters , while others want a fine-grained clustering into twenty or more . But many times there is little extrinsic guidance for how to choose K . One solution is to choose the number of clusters to maximize a metric of clustering quality . The other parameters µ and φ are chosen to maximize the log-likelihood bound J , so this might seem a potential candidate for tuning K . However , J will never decrease with K : if it is possible to obtain a bound of JK with K clusters , then it is always possible to do at least as well with K + 1 clusters , by simply ignoring the additional cluster and setting its probability to zero in q and µ . It is therefore necessary to introduce a penalty for model complexity , so that fewer clusters are preferred . For example , the Akaike Infor - mation Crition ( AIC ; Akaike , 1974 ) is the linear combination of the number of parameters and the log-likelihood , AIC = 2M − 2J , [ 5.24 ] where M is the number of parameters . In an expectation-maximization clustering algo - rithm , M = K × V + K . Since the number of parameters increases with the number of clustersK , the AIC may prefer more parsimonious models , even if they do not fit the data quite as well . Another choice is to maximize the predictive likelihood on heldout data . This data is not used to estimate the model parameters φ and µ , and so it is not the case that the likelihood on this data is guaranteed to increase with K . Figure 5.3 shows the negative log-likelihood on training and heldout data , as well as the AIC . * Bayesian nonparametrics An alternative approach is to treat the number of clusters as another latent variable . This requires statistical inference over a set of models with a variable number of clusters . This is not possible within the framework of expectation - maximization , but there are several alternative inference procedures which can be ap - plied , including Markov Chain Monte Carlo ( MCMC ) , which is briefly discussed in sec - tion 5.5 ( for more details , see Chapter 25 of Murphy , 2012 ) . Bayesian nonparametrics have been applied to the problem of unsupervised word sense induction , learning not only the word senses but also the number of senses per word ( Reisinger and Mooney , 2010 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 104 CHAPTER 5 . LEARNING WITHOUT SUPERVISION 10 20 30 40 50 Number of clusters 220000 240000 260000 Negative log-likelihood bound AIC 10 20 30 40 50 Number of clusters 75000 80000 85000 Out-of-sample negative log likelihood Figure 5.3 : The negative log-likelihood and AIC for several runs of expectation - maximization , on synthetic data . Although the data was generated from a model with K = 10 , the optimal number of clusters is K̂ = 15 , according to AIC and the heldout log-likelihood . The training set log-likelihood continues to improve as K increases . 5.2 Applications of expectation-maximization EM is not really an “ algorithm ” like , say , quicksort . Rather , it is a framework for learning with missing data . The recipe for using EM on a problem of interest is : • Introduce latent variables z , such that it is easy to write the probability P ( x , z ) . It should also be easy to estimate the associated parameters , given knowledge of z . • Derive the E-step updates for q ( z ) , which is typically factored as q ( z ) = ∏ Ni = 1 qz ( i ) ( z ( i ) ) , where i is an index over instances . • The M-step updates typically correspond to the soft version of a probabilistic super - vised learning algorithm , like Naı̈ve Bayes . This section discusses a few of the many applications of this general framework . 5.2.1 Word sense induction The chapter began by considering the problem of word sense disambiguation when the senses are not known in advance . Expectation-maximization can be applied to this prob - lem by treating each cluster as a word sense . Each instance represents the use of an ambiguous word , and x ( i ) is a vector of counts for the other words that appear nearby : Schütze ( 1998 ) uses all words within a 50-word window . The probability p ( x ( i ) | z ) can be set to the multinomial distribution , as in Naı̈ve Bayes . The EM algorithm can be applied directly to this data , yielding clusters that ( hopefully ) correspond to the word senses . Better performance can be obtained by first applying singular value decomposition ( SVD ) to the matrix of context-counts Cij = count ( i , j ) , where count ( i , j ) is the count of word j in the context of instance i . Truncated singular value decomposition approximates Jacob Eisenstein . Draft of October 15 , 2018 . 5.2 . APPLICATIONS OF EXPECTATION-MAXIMIZATION 105 the matrix C as a product of three matrices , U , S , V , under the constraint that U and V are orthonormal , and S is diagonal : min U , S , V | | C − USV > | | F [ 5.25 ] s.t.U ∈ RV × K , UU > = I S = Diag ( s1 , s2 , . . . , sK ) V > ∈ RNp × K , VV > = I , where | | · | | F is the Frobenius norm , | | X | | F = √ ∑ i , j X 2 i , j . The matrix U contains the left singular vectors of C , and the rows of this matrix can be used as low-dimensional representations of the count vectors ci . EM clustering can be made more robust by setting the instance descriptions x ( i ) equal to these rows , rather than using raw counts ( Schütze , 1998 ) . However , because the instances are now dense vectors of continuous numbers , the probability p ( x ( i ) | z ) must be defined as a multivariate Gaussian distribution . In truncated singular value decomposition , the hyperparameter K is the truncation limit : when K is equal to the rank of C , the norm of the difference between the original matrix C and its reconstruction USV > will be zero . Lower values ofK increase the recon - struction error , but yield vector representations that are smaller and easier to learn from . Singular value decomposition is discussed in more detail in chapter 14 . 5.2.2 Semi-supervised learning Expectation-maximization can also be applied to the problem of semi-supervised learn - ing : learning from both labeled and unlabeled data in a single model . Semi-supervised learning makes use of annotated examples , ensuring that each label y corresponds to the desired concept . By adding unlabeled examples , it is possible cover a greater fraction of the features than would appear in labeled data alone . Other methods for semi-supervised learning are discussed in section 5.3 , but for now , let’s approach the problem within the framework of expectation-maximization ( Nigam et al . , 2000 ) . Suppose we have labeled data { ( x ( i ) , y ( i ) ) } N ` i = 1 , and unlabeled data { x ( i ) } N ` + Nui = N ` + 1 , where N ` is the number of labeled instances andNu is the number of unlabeled instances . We can learn from the combined data by maximizing a lower bound on the joint log-likelihood , L = N ∑̀ i = 1 log p ( x ( i ) , y ( i ) ; µ , φ ) + N ` + Nu ∑ j = N ` + 1 log p ( x ( j ) ; µ , φ ) [ 5.26 ] = N ∑̀ i = 1 ( log p ( x ( i ) | y ( i ) ; φ ) + log p ( y ( i ) ; µ ) ) + N ` + Nu ∑ j = N ` + 1 log K ∑ y = 1 p ( x ( j ) , y ; µ , φ ) . [ 5.27 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 106 CHAPTER 5 . LEARNING WITHOUT SUPERVISION Algorithm 9 Generative process for the Naı̈ve Bayes classifier with hidden components for Instance i ∈ { 1 , 2 , . . . , N } do : Draw the label y ( i ) ∼ Categorical ( µ ) ; Draw the component z ( i ) ∼ Categorical ( βy ( i ) ) ; Draw the word counts x ( i ) | y ( i ) , z ( i ) ∼ Multinomial ( φz ( i ) ) . The left sum is identical to the objective in Naı̈ve Bayes ; the right sum is the marginal log - likelihood for expectation-maximization clustering , from Equation 5.5 . We can construct a lower bound on this log-likelihood by introducing distributions q ( j ) for all j ∈ { N ` + 1 , . . . , N ` + Nu } . The E-step updates these distributions ; the M-step updates the parameters φ and µ , us - ing the expected counts from the unlabeled data and the observed counts from the labeled data . A critical issue in semi-supervised learning is how to balance the impact of the labeled and unlabeled data on the classifier weights , especially when the unlabeled data is much larger than the labeled dataset . The risk is that the unlabeled data will dominate , caus - ing the parameters to drift towards a “ natural clustering ” of the instances — which may not correspond to a good classifier for the labeled data . One solution is to heuristically reweight the two components of Equation 5.26 , tuning the weight of the two components on a heldout development set ( Nigam et al . , 2000 ) . 5.2.3 Multi-component modeling As a final application , let’s return to fully supervised classification . A classic dataset for text classification is 20 newsgroups , which contains posts to a set of online forums , called newsgroups . One of the newsgroups is comp . sys . mac . hardware , which discusses Ap - ple computing hardware . Suppose that within this newsgroup there are two kinds of posts : reviews of new hardware , and question-answer posts about hardware problems . The language in these components of the mac . hardware class might have little in com - mon ; if so , it would be better to model these components separately , rather than treating their union as a single class . However , the component responsible for each instance is not directly observed . Recall that Naı̈ve Bayes is based on a generative process , which provides a stochastic explanation for the observed data . In Naı̈ve Bayes , each label is drawn from a categorical distribution with parameter µ , and each vector of word counts is drawn from a multi - nomial distribution with parameter φy . For multi-component modeling , we envision a slightly different generative process , incorporating both the observed label y ( i ) and the latent component z ( i ) . This generative process is shown in Algorithm 9 . A new parameter βy ( i ) defines the distribution of components , conditioned on the label y ( i ) . The component , and not the class label , then parametrizes the distribution over words . Jacob Eisenstein . Draft of October 15 , 2018 . 5.3 . SEMI-SUPERVISED LEARNING 107 ( 5.1 ) , Villeneuve a bel et bien réussi son pari de changer de perspectives tout en assurant une cohérence à la franchise . 2 ( 5.2 ) / Il est également trop long et bancal dans sa narration , tiède dans ses intentions , et tiraillé entre deux personnages et directions qui ne parviennent pas à coexister en har - monie . 3 ( 5.3 ) Denis Villeneuve a réussi une suite parfaitement maitrisée4 ( 5.4 ) Long , bavard , hyper design , à peine agité ( le comble de l’action : une bagarre dans la flotte ) , métaphysique et , surtout , ennuyeux jusqu’à la catalepsie . 5 ( 5.5 ) Une suite d’une écrasante puissance , mêlant parfaitement le contemplatif au narratif . 6 ( 5.6 ) Le film impitoyablement bavard finit quand même par se taire quand se lève l’espèce de bouquet final où semble se déchaı̂ner , comme en libre parcours de poulets décapités , l’armée des graphistes numériques griffant nerveusement la palette graphique entre ag - onie et orgasme . 7 Table 5.1 : Labeled and unlabeled reviews of the films Blade Runner 2049 and Transformers : The Last Knight . The labeled data includes ( x ( i ) , y ( i ) ) , but not z ( i ) , so this is another case of missing data . Again , we sum over the missing data , applying Jensen’s inequality to as to obtain a lower bound on the log-likelihood , log p ( x ( i ) , y ( i ) ) = log Kz ∑ z = 1 p ( x ( i ) , y ( i ) , z ; µ , φ , β ) [ 5.28 ] ≥ log p ( y ( i ) ; µ ) + E q ( i ) Z | Y [ log p ( x ( i ) | z ; φ ) + log p ( z | y ( i ) ; β ) − log q ( i ) ( z ) ] . [ 5.29 ] We are now ready to apply expectation-maximization . As usual , the E-step updates the distribution over the missing data , q ( i ) Z | Y . The M-step updates the parameters , βy , z = Eq [ count ( y , z ) ] ∑ Kz z ′ = 1Eq [ count ( y , z ′ ) ] [ 5.30 ] φz , j = Eq [ count ( z , j ) ] ∑ V j ′ = 1Eq [ count ( z , j ′ ) ] . [ 5.31 ] 5.3 Semi-supervised learning In semi-supervised learning , the learner makes use of both labeled and unlabeled data . To see how this could help , suppose you want to do sentiment analysis in French . In Ta - Under contract with MIT Press , shared under CC-BY-NC-ND license . 108 CHAPTER 5 . LEARNING WITHOUT SUPERVISION ble 5.1 , there are two labeled examples , one positive and one negative . From this data , a learner could conclude that réussi is positive and long is negative . This isn’t much ! How - ever , we can propagate this information to the unlabeled data , and potentially learn more . • If we are confident that réussi is positive , then we might guess that ( 5.3 ) is also posi - tive . • That suggests that parfaitement is also positive . • We can then propagate this information to ( 5.5 ) , and learn from this words in this example . • Similarly , we can propagate from the labeled data to ( 5.4 ) , which we guess to be negative because it shares the word long . This suggests that bavard is also negative , which we propagate to ( 5.6 ) . Instances ( 5.3 ) and ( 5.4 ) were “ similar ” to the labeled examples for positivity and negativ - ity , respectively . By using these instances to expand the models for each class , it became possible to correctly label instances ( 5.5 ) and ( 5.6 ) , which didn’t share any important fea - tures with the original labeled data . This requires a key assumption : that similar instances will have similar labels . In subsection 5.2.2 , we discussed how expectation-maximization can be applied to semi-supervised learning . Using the labeled data , the initial parameters φ would assign a high weight for réussi in the positive class , and a high weight for long in the negative class . These weights helped to shape the distributions q for instances ( 5.3 ) and ( 5.4 ) in the E-step . In the next iteration of the M-step , the parameters φ are updated with counts from these instances , making it possible to correctly label the instances ( 5.5 ) and ( 5.6 ) . However , expectation-maximization has an important disadvantage : it requires using a generative classification model , which restricts the features that can be used for clas - sification . In this section , we explore non-probabilistic approaches , which impose fewer restrictions on the classification model . 5.3.1 Multi-view learning EM semi-supervised learning can be viewed as self-training : the labeled data guides the initial estimates of the classification parameters ; these parameters are used to compute a label distribution over the unlabeled instances , q ( i ) ; the label distributions are used to update the parameters . The risk is that self-training drifts away from the original labeled data . This problem can be ameliorated by multi-view learning . Here we take the as - sumption that the features can be decomposed into multiple “ views ” , each of which is conditionally independent , given the label . For example , consider the problem of classi - fying a name as a person or location : one view is the name itself ; another is the context in which it appears . This situation is illustrated in Table 5.2 . Jacob Eisenstein . Draft of October 15 , 2018 . 5.3 . SEMI-SUPERVISED LEARNING 109 x ( 1 ) x ( 2 ) y 1 . Peachtree Street located on LOC 2 . Dr . Walker said PER 3 . Zanzibar located in ? → LOC 4 . Zanzibar flew to ? → LOC 5 . Dr . Robert recommended ? → PER 6 . Oprah recommended ? → PER Table 5.2 : Example of multiview learning for named entity classification Co-training is an iterative multi-view learning algorithm , in which there are separate classifiers for each view ( Blum and Mitchell , 1998 ) . At each iteration of the algorithm , each classifier predicts labels for a subset of the unlabeled instances , using only the features available in its view . These predictions are then used as ground truth to train the classifiers associated with the other views . In the example shown in Table 5.2 , the classifier on x ( 1 ) might correctly label instance # 5 as a person , because of the feature Dr ; this instance would then serve as training data for the classifier on x ( 2 ) , which would then be able to correctly label instance # 6 , thanks to the feature recommended . If the views are truly independent , this procedure is robust to drift . Furthermore , it imposes no restrictions on the classifiers that can be used for each view . Word-sense disambiguation is particularly suited to multi-view learning , thanks to the heuristic of “ one sense per discourse ” : if a polysemous word is used more than once in a given text or conversation , all usages refer to the same sense ( Gale et al . , 1992 ) . This motivates a multi-view learning approach , in which one view corresponds to the local context ( the surrounding words ) , and another view corresponds to the global context at the document level ( Yarowsky , 1995 ) . The local context view is first trained on a small seed dataset . We then identify its most confident predictions on unlabeled instances . The global context view is then used to extend these confident predictions to other instances within the same documents . These new instances are added to the training data to the local context classifier , which is retrained and then applied to the remaining unlabeled data . 5.3.2 Graph-based algorithms Another family of approaches to semi-supervised learning begins by constructing a graph , in which pairs of instances are linked with symmetric weights ωi , j , e.g . , ωi , j = exp ( − α × | | x ( i ) − x ( j ) | | 2 ) . [ 5.32 ] The goal is to use this weighted graph to propagate labels from a small set of labeled instances to larger set of unlabeled instances . Under contract with MIT Press , shared under CC-BY-NC-ND license . 110 CHAPTER 5 . LEARNING WITHOUT SUPERVISION In label propagation , this is done through a series of matrix operations ( Zhu et al . , 2003 ) . Let Q be a matrix of size N × K , in which each row q ( i ) describes the labeling of instance i . When ground truth labels are available , then q ( i ) is an indicator vector , with q ( i ) y ( i ) = 1 and q ( i ) y ′ 6 = y ( i ) = 0 . Let us refer to the submatrix of rows containing labeled instances as QL , and the remaining rows as QU . The rows of QU are initialized to assign equal probabilities to all labels , qi , k = 1K . Now , let Ti , j represent the “ transition ” probability of moving from node j to node i , Ti , j , Pr ( j → i ) = ωi , j ∑ N k = 1 ωk , j . [ 5.33 ] We compute values of Ti , j for all instances j and all unlabeled instances i , forming a matrix of size NU × N . If the dataset is large , this matrix may be expensive to store and manip - ulate ; a solution is to sparsify it , by keeping only the κ largest values in each row , and setting all other values to zero . We can then “ propagate ” the label distributions to the unlabeled instances , Q̃U ← TQ [ 5.34 ] s ← Q̃U1 [ 5.35 ] QU ← Diag ( s ) − 1Q̃U . [ 5.36 ] The expression Q̃U1 indicates multiplication of Q̃U by a column vector of ones , which is equivalent to computing the sum of each row of Q̃U . The matrix Diag ( s ) is a diagonal matrix with the elements of s on the diagonals . The product Diag ( s ) − 1Q̃U has the effect of normalizing the rows of Q̃U , so that each row of QU is a probability distribution over labels . 5.4 Domain adaptation In many practical scenarios , the labeled data differs in some key respect from the data to which the trained model is to be applied . A classic example is in consumer reviews : we may have labeled reviews of movies ( the source domain ) , but we want to predict the reviews of appliances ( the target domain ) . A similar issues arise with genre differences : most linguistically-annotated data is news text , but application domains range from social media to electronic health records . In general , there may be several source and target domains , each with their own properties ; however , for simplicity , this discussion will focus mainly on the case of a single source and target domain . The simplest approach is “ direct transfer ” : train a classifier on the source domain , and apply it directly to the target domain . The accuracy of this approach depends on the extent to which features are shared across domains . In review text , words like outstanding and Jacob Eisenstein . Draft of October 15 , 2018 . 5.4 . DOMAIN ADAPTATION 111 disappointing will apply across both movies and appliances ; but others , like terrifying , may have meanings that are domain-specific . As a result , direct transfer performs poorly : for example , an out-of-domain classifier ( trained on book reviews ) suffers twice the error rate of an in-domain classifier on reviews of kitchen appliances ( Blitzer et al . , 2007 ) . Domain adaptation algorithms attempt to do better than direct transfer by learning from data in both domains . There are two main families of domain adaptation algorithms , depending on whether any labeled data is available in the target domain . 5.4.1 Supervised domain adaptation In supervised domain adaptation , there is a small amount of labeled data in the target domain , and a large amount of data in the source domain . The simplest approach would be to ignore domain differences , and simply merge the training data from the source and target domains . There are several other baseline approaches to dealing with this sce - nario ( Daumé III , 2007 ) : Interpolation . Train a classifier for each domain , and combine their predictions , e.g . , ŷ = argmax y λsΨs ( x , y ) + ( 1 − λs ) Ψt ( x , y ) , [ 5.37 ] where Ψs and Ψt are the scoring functions from the source and target domain clas - sifiers respectively , and λs is the interpolation weight . Prediction . Train a classifier on the source domain data , use its prediction as an additional feature in a classifier trained on the target domain data , ŷs = argmax y Ψs ( x , y ) [ 5.38 ] ŷt = argmax y Ψt ( [ x ; ŷS ] , y ) . [ 5.39 ] Priors . Train a classifier on the source domain data , and use its weights as a prior distri - bution on the weights of the classifier for the target domain data . This is equivalent to regularizing the target domain weights towards the weights of the source domain classifier ( Chelba and Acero , 2006 ) , ` ( θt ) = N ∑ i = 1 ` ( i ) ( x ( i ) , y ( i ) ; θt ) + λ | | θt − θs | | 22 , [ 5.40 ] where ` ( i ) is the prediction loss on instance i , and λ is the regularization weight . Under contract with MIT Press , shared under CC-BY-NC-ND license . 112 CHAPTER 5 . LEARNING WITHOUT SUPERVISION An effective and “ frustratingly simple ” alternative is EASYADAPT ( Daumé III , 2007 ) , which creates copies of each feature : one for each domain and one for the cross-domain setting . For example , a negative review of the film Wonder Woman begins , As boring and flavorless as a three-day-old grilled cheese sandwich . . . . 8 The resulting bag-of-words feature vector would be , f ( x , y , d ) = { ( boring , / , MOVIE ) : 1 , ( boring , / , ∗ ) : 1 , ( flavorless , / , MOVIE ) : 1 , ( flavorless , / , ∗ ) : 1 , ( three-day-old , / , MOVIE ) : 1 , ( three-day-old , / , ∗ ) : 1 , . . . } , with ( boring , / , MOVIE ) indicating the word boring appearing in a negative labeled doc - ument in the MOVIE domain , and ( boring , / , ∗ ) indicating the same word in a negative labeled document in any domain . It is up to the learner to allocate weight between the domain-specific and cross-domain features : for words that facilitate prediction in both domains , the learner will use the cross-domain features ; for words that are relevant only to a single domain , the domain-specific features will be used . Any discriminative classi - fier can be used with these augmented features . 9 5.4.2 Unsupervised domain adaptation In unsupervised domain adaptation , there is no labeled data in the target domain . Un - supervised domain adaptation algorithms cope with this problem by trying to make the data from the source and target domains as similar as possible . This is typically done by learning a projection function , which puts the source and target data in a shared space , in which a learner can generalize across domains . This projection is learned from data in both domains , and is applied to the base features — for example , the bag-of-words in text classification . The projected features can then be used both for training and for prediction . Linear projection In linear projection , the cross-domain representation is constructed by a matrix-vector product , g ( x ( i ) ) = Ux ( i ) . [ 5.41 ] The projected vectors g ( x ( i ) ) can then be used as base features during both training ( from the source domain ) and prediction ( on the target domain ) . 8http :// www.colesmithey.com/capsules/2017/06/wonder-woman.HTML , accessed October 9 . 2017 . 9EASYADAPT can be explained as a hierarchical Bayesian model , in which the weights for each domain are drawn from a shared prior ( Finkel and Manning , 2009 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 5.4 . DOMAIN ADAPTATION 113 The projection matrix U can be learned in a number of different ways , but many ap - proaches focus on compressing and reconstructing the base features ( Ando and Zhang , 2005 ) . For example , we can define a set of pivot features , which are typically chosen be - cause they appear in both domains : in the case of review documents , pivot features might include evaluative adjectives like outstanding and disappointing ( Blitzer et al . , 2007 ) . For each pivot feature j , we define an auxiliary problem of predicting whether the feature is present in each example , using the remaining base features . Let φj denote the weights of this classifier , and us horizontally concatenate the weights for each of theNp pivot features into a matrix Φ = [ φ1 , φ2 , . . . , φNP ] . We then perform truncated singular value decomposition on Φ , as described in sub - section 5.2.1 , obtaining Φ ≈ USV > . The rows of the matrix U summarize information about each base feature : indeed , the truncated singular value decomposition identifies a low-dimension basis for the weight matrix Φ , which in turn links base features to pivot features . Suppose that a base feature reliable occurs only in the target domain of appliance reviews . Nonetheless , it will have a positive weight towards some pivot features ( e.g . , out - standing , recommended ) , and a negative weight towards others ( e.g . , worthless , unpleasant ) . A base feature such as watchable might have the same associations with the pivot features , and therefore , ureliable ≈ uwatchable . The matrix U can thus project the base features into a space in which this information is shared . Non-linear projection Non-linear transformations of the base features can be accomplished by implementing the transformation function as a deep neural network , which is trained from an auxiliary objective . Denoising objectives One possibility is to train a projection function to reconstruct a corrupted version of the original input . The original input can be corrupted in various ways : by the addition of random noise ( Glorot et al . , 2011 ; Chen et al . , 2012 ) , or by the deletion of features ( Chen et al . , 2012 ; Yang and Eisenstein , 2015 ) . Denoising objectives share many properties of the linear projection method described above : they enable the projection function to be trained on large amounts of unlabeled data from the target do - main , and allow information to be shared across the feature space , thereby reducing sen - sitivity to rare and domain-specific features . Adversarial objectives The ultimate goal is for the transformed representations g ( x ( i ) ) to be domain-general . This can be made an explicit optimization criterion by comput - ing the similarity of transformed instances both within and between domains ( Tzeng et al . , 2015 ) , or by formulating an auxiliary classification task , in which the domain it - self is treated as a label ( Ganin et al . , 2016 ) . This setting is adversarial , because we want Under contract with MIT Press , shared under CC-BY-NC-ND license . 114 CHAPTER 5 . LEARNING WITHOUT SUPERVISION ` d ` y d ( i ) y ( i ) x g ( x ) Figure 5.4 : A schematic view of adversarial domain adaptation . The loss ` y is computed only for instances from the source domain , where labels y ( i ) are available . to learn a representation that makes this classifier perform poorly . At the same time , we want g ( x ( i ) ) to enable accurate predictions of the labels y ( i ) . To formalize this idea , let d ( i ) represent the domain of instance i , and let ` d ( g ( x ( i ) ) , d ( i ) ; θd ) represent the loss of a classifier ( typically a deep neural network ) trained to predict d ( i ) from the transformed representation g ( x ( i ) ) , using parameters θd . Analogously , let ` y ( g ( x ( i ) ) , y ( i ) ; θy ) represent the loss of a classifier trained to predict the label y ( i ) from g ( x ( i ) ) , using param - eters θy . The transformation g can then be trained from two criteria : it should yield accu - rate predictions of the labels y ( i ) , while making inaccurate predictions of the domains d ( i ) . This can be formulated as a joint optimization problem , min θgθy , θd N ` + Nu ∑ i = 1 ` d ( g ( x ( i ) ; θg ) , d ( i ) ; θd ) − N ∑̀ i = 1 ` y ( g ( x ( i ) ; θg ) , y ( i ) ; θy ) , [ 5.42 ] where N ` is the number of labeled instances and Nu is the number of unlabeled instances , with the labeled instances appearing first in the dataset . This setup is shown in Figure 5.4 . The loss can be optimized by stochastic gradient descent , jointly training the parameters of the non-linear transformation θg , and the parameters of the prediction models θd and θy . 5.5 * Other approaches to learning with latent variables Expectation-maximization provides a general approach to learning with latent variables , but it has limitations . One is the sensitivity to initialization ; in practical applications , considerable attention may need to be devoted to finding a good initialization . A second issue is that EM tends to be easiest to apply in cases where the latent variables have a clear decomposition ( in the cases we have considered , they decompose across the instances ) . For these reasons , it is worth briefly considering some alternatives to EM . Jacob Eisenstein . Draft of October 15 , 2018 . 5.5 . * OTHER APPROACHES TO LEARNING WITH LATENT VARIABLES 115 5.5.1 Sampling In EM clustering , there is a distribution q ( i ) for the missing data related to each instance . The M-step consists of updating the parameters of this distribution . An alternative is to draw samples of the latent variables . If the sampling distribution is designed correctly , this procedure will eventually converge to drawing samples from the true posterior over the missing data , p ( z ( 1 : Nz ) | x ( 1 : Nx ) ) . For example , in the case of clustering , the missing data z ( 1 : Nz ) is the set of cluster memberships , y ( 1 : N ) , so we draw samples from the pos - terior distribution over clusterings of the data . If a single clustering is required , we can select the one with the highest conditional likelihood , ẑ = argmaxz p ( z ( 1 : Nz ) | x ( 1 : Nx ) ) . This general family of algorithms is called Markov Chain Monte Carlo ( MCMC ) : “ Monte Carlo ” because it is based on a series of random draws ; “ Markov Chain ” because the sampling procedure must be designed such that each sample depends only on the previous sample , and not on the entire sampling history . Gibbs sampling is an MCMC algorithm in which each latent variable is sampled from its posterior distribution , z ( n ) | x , z ( − n ) ∼ p ( z ( n ) | x , z ( − n ) ) , [ 5.43 ] where z ( − n ) indicates { z \ z ( n ) } , the set of all latent variables except for z ( n ) . Repeatedly drawing samples over all latent variables constructs a Markov chain that is guaranteed to converge to a sequence of samples from p ( z ( 1 : Nz ) | x ( 1 : Nx ) ) . In probabilistic clustering , the sampling distribution has the following form , p ( z ( i ) | x , z ( − i ) ) = p ( x ( i ) | z ( i ) ; φ ) × p ( z ( i ) ; µ ) ∑ K z = 1 p ( x ( i ) | z ; φ ) × p ( z ; µ ) [ 5.44 ] ∝ Multinomial ( x ( i ) ; φz ( i ) ) × µz ( i ) . [ 5.45 ] In this case , the sampling distribution does not depend on the other instances : the poste - rior distribution over each z ( i ) can be computed from x ( i ) and the parameters given the parameters φ and µ . In sampling algorithms , there are several choices for how to deal with the parameters . One possibility is to sample them too . To do this , we must add them to the generative story , by introducing a prior distribution . For the multinomial and categorical parameters in the EM clustering model , the Dirichlet distribution is a typical choice , since it defines a probability on exactly the set of vectors that can be parameters : vectors that sum to one and include only non-negative numbers . 10 To incorporate this prior , the generative model must be augmented to indicate that each φz ∼ Dirichlet ( αφ ) , and µ ∼ Dirichlet ( αµ ) . The hyperparameters α are typically set to a constant vector α = [ α , α , . . . , α ] . When α is large , the Dirichlet distribution tends to 10If ∑ K i θi = 1 and θi ≥ 0 for all i , then θ is said to be on the K − 1 simplex . A Dirichlet distribution with Under contract with MIT Press , shared under CC-BY-NC-ND license . 116 CHAPTER 5 . LEARNING WITHOUT SUPERVISION generate vectors that are nearly uniform ; when α is small , it tends to generate vectors that assign most of their probability mass to a few entries . Given prior distributions over φ and µ , we can now include them in Gibbs sampling , drawing values for these parameters from posterior distributions that are conditioned on the other variables in the model . Unfortunately , sampling φ and µ usually leads to slow “ mixing ” , meaning that adja - cent samples tend to be similar , so that a large number of samples is required to explore the space of random variables . The reason is that the sampling distributions for the pa - rameters are tightly constrained by the cluster memberships y ( i ) , which in turn are tightly constrained by the parameters . There are two solutions that are frequently employed : • Empirical Bayesian methods maintain φ and µ as parameters rather than latent variables . They still employ sampling in the E-step of the EM algorithm , but they update the parameters using expected counts that are computed from the samples rather than from parametric distributions . This EM-MCMC hybrid is also known as Monte Carlo Expectation Maximization ( MCEM ; Wei and Tanner , 1990 ) , and is well-suited for cases in which it is difficult to compute q ( i ) directly . • In collapsed Gibbs sampling , we analytically integrate φ and µ out of the model . The cluster memberships y ( i ) are the only remaining latent variable ; we sample them from the compound distribution , p ( y ( i ) | x ( 1 : N ) , y ( − i ) ; αφ , αµ ) = ∫ φ , µ p ( φ , µ | y ( − i ) , x ( 1 : N ) ; αφ , αµ ) p ( y ( i ) | x ( 1 : N ) , y ( − i ) , φ , µ ) dφdµ . [ 5.48 ] For multinomial and Dirichlet distributions , this integral can be computed in closed form . MCMC algorithms are guaranteed to converge to the true posterior distribution over the latent variables , but there is no way to know how long this will take . In practice , the rate of convergence depends on initialization , just as expectation-maximization depends on initialization to avoid local optima . Thus , while Gibbs Sampling and other MCMC algorithms provide a powerful and flexible array of techniques for statistical inference in latent variable models , they are not a panacea for the problems experienced by EM . parameter α ∈ RK + has support over the K − 1 simplex , pDirichlet ( θ | α ) = 1 B ( α ) K ∏ i = 1 θαi − 1i [ 5.46 ] B ( α ) = ∏ K i = 1 Γ ( αi ) Γ ( ∑ K i = 1 αi ) , [ 5.47 ] with Γ ( · ) indicating the gamma function , a generalization of the factorial function to non-negative reals . Jacob Eisenstein . Draft of October 15 , 2018 . 5.5 . * OTHER APPROACHES TO LEARNING WITH LATENT VARIABLES 117 5.5.2 Spectral learning Another approach to learning with latent variables is based on the method of moments , which makes it possible to avoid the problem of non-convex log-likelihood . Write x ( i ) for the normalized vector of word counts in document i , so that x ( i ) = x ( i ) / ∑ V j = 1 x ( i ) j . Then we can form a matrix of word-word co-occurrence probabilities , C = N ∑ i = 1 x ( i ) ( x ( i ) ) > . [ 5.49 ] The expected value of this matrix under p ( x | φ , µ ) , as E [ C ] = N ∑ i = 1 K ∑ k = 1 Pr ( Z ( i ) = k ; µ ) φkφ > k [ 5.50 ] = K ∑ k Nµkφkφ > k [ 5.51 ] = ΦDiag ( Nµ ) Φ > , [ 5.52 ] where Φ is formed by horizontally concatenating φ1 . . . φK , and Diag ( Nµ ) indicates a diagonal matrix with values Nµk at position ( k , k ) . Setting C equal to its expectation gives , C = ΦDiag ( Nµ ) Φ > , [ 5.53 ] which is similar to the eigendecomposition C = QΛQ > . This suggests that simply by finding the eigenvectors and eigenvalues of C , we could obtain the parameters φ and µ , and this is what motivates the name spectral learning . While moment-matching and eigendecomposition are similar in form , they impose different constraints on the solutions : eigendecomposition requires orthonormality , so that QQ > = I ; in estimating the parameters of a text clustering model , we require that µ and the columns of Φ are probability vectors . Spectral learning algorithms must therefore include a procedure for converting the solution into vectors that are non-negative and sum to one . One approach is to replace eigendecomposition ( or the related singular value decomposition ) with non-negative matrix factorization ( Xu et al . , 2003 ) , which guarantees that the solutions are non-negative ( Arora et al . , 2013 ) . After obtaining the parameters φ and µ , the distribution over clusters can be com - puted from Bayes ’ rule : p ( z ( i ) | x ( i ) ; φ , µ ) ∝ p ( x ( i ) | z ( i ) ; φ ) × p ( z ( i ) ; µ ) . [ 5.54 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 118 CHAPTER 5 . LEARNING WITHOUT SUPERVISION Spectral learning yields provably good solutions without regard to initialization , and can be quite fast in practice . However , it is more difficult to apply to a broad family of genera - tive models than EM and Gibbs Sampling . For more on applying spectral learning across a range of latent variable models , see Anandkumar et al . ( 2014 ) . Additional resources There are a number of other learning paradigms that deviate from supervised learning . • Active learning : the learner selects unlabeled instances and requests annotations ( Set - tles , 2012 ) . • Multiple instance learning : labels are applied to bags of instances , with a positive label applied if at least one instance in the bag meets the criterion ( Dietterich et al . , 1997 ; Maron and Lozano-Pérez , 1998 ) . • Constraint-driven learning : supervision is provided in the form of explicit con - straints on the learner ( Chang et al . , 2007 ; Ganchev et al . , 2010 ) . • Distant supervision : noisy labels are generated from an external resource ( Mintz et al . , 2009 , also see subsection 17.2.3 ) . • Multitask learning : the learner induces a representation that can be used to solve multiple classification tasks ( Collobert et al . , 2011 ) . • Transfer learning : the learner must solve a classification task that differs from the labeled data ( Pan and Yang , 2010 ) . Expectation-maximization was introduced by Dempster et al . ( 1977 ) , and is discussed in more detail by Murphy ( 2012 ) . Like most machine learning treatments , Murphy focuses on continuous observations and Gaussian likelihoods , rather than the discrete observa - tions typically encountered in natural language processing . Murphy ( 2012 ) also includes an excellent chapter on MCMC ; for a textbook-length treatment , see Robert and Casella ( 2013 ) . For still more on Bayesian latent variable models , see Barber ( 2012 ) , and for ap - plications of Bayesian models to natural language processing , see Cohen ( 2016 ) . Surveys are available for semi-supervised learning ( Zhu and Goldberg , 2009 ) and domain adapta - tion ( Søgaard , 2013 ) , although both pre-date the current wave of interest in deep learning . Exercises 1 . Derive the expectation maximization update for the parameter µ in the EM cluster - ing model . Jacob Eisenstein . Draft of October 15 , 2018 . 5.5 . * OTHER APPROACHES TO LEARNING WITH LATENT VARIABLES 119 2 . Derive the E-step and M-step updates for the following generative model . You may assume that the labels y ( i ) are observed , but z ( i ) m is not . • For each instance i , – Draw label y ( i ) ∼ Categorical ( µ ) – For each token m ∈ { 1 , 2 , . . . , M ( i ) } ∗ Draw z ( i ) m ∼ Categorical ( π ) ∗ If z ( i ) m = 0 , draw the current token from a label-specific distribution , w ( i ) m ∼ φy ( i ) ∗ If z ( i ) m = 1 , draw the current token from a document-specific distribu - tion , w ( i ) m ∼ ν ( i ) 3 . Using the iterative updates in Equations 5.34-5.36 , compute the outcome of the label propagation algorithm for the following examples . ? 0 1 ? ? 0 1 ? 0 1 ? ? The value inside the node indicates the label , y ( i ) ∈ { 0 , 1 } , with y ( i ) = ? for unlabeled nodes . The presence of an edge between two nodes indicates wi , j = 1 , and the absence of an edge indicateswi , j = 0 . For the third example , you need only compute the first three iterations , and then you can guess at the solution in the limit . 4 . Use expectation-maximization clustering to train a word-sense induction system , applied to the word say . • Import NLTK , run NLTK . DOWNLOAD ( ) and select SEMCOR . Import SEMCOR from NLTK . CORPUS . • The command SEMCOR . TAGGED SENTENCES ( TAG = ’ SENSE ’ ) returns an itera - tor over sense-tagged sentences in the corpus . Each sentence can be viewed as an iterator over TREE objects . For TREE objects that are sense-annotated words , you can access the annotation as TREE . LABEL ( ) , and the word itself with TREE . LEAVES ( ) . So SEMCOR . TAGGED SENTENCES ( TAG = ’ SENSE ’ ) [ 0 ] [ 2 ] . LABEL ( ) would return the sense annotation of the third word in the first sentence . • Extract all sentences containing the senses SAY . V . 01 and SAY . V . 02 . • Build bag-of-words vectors x ( i ) , containing the counts of other words in those sentences , including all words that occur in at least two sentences . • Implement and run expectation-maximization clustering on the merged data . Under contract with MIT Press , shared under CC-BY-NC-ND license . 120 CHAPTER 5 . LEARNING WITHOUT SUPERVISION • Compute the frequency with which each cluster includes instances of SAY . V . 01 and SAY . V . 02 . In the remaining exercises , you will try out some approaches for semisupervised learn - ing and domain adaptation . You will need datasets in multiple domains . You can obtain product reviews in multiple domains here : https://www.cs.jhu.edu/ ˜ mdredze / datasets / sentiment / processed_acl . tar . gz . Choose a source and target domain , e.g . dvds and books , and divide the data for the target domain into training and test sets of equal size . 5 . First , quantify the cost of cross-domain transfer . • Train a logistic regression classifier on the source domain training set , and eval - uate it on the target domain test set . • Train a logistic regression classifier on the target domain training set , and eval - uate it on the target domain test set . This it the “ direct transfer ” baseline . Compute the difference in accuracy , which is a measure of the transfer loss across domains . 6 . Next , apply the label propagation algorithm from subsection 5.3.2 . As a baseline , using only 5 % of the target domain training set , train a classifier , and compute its accuracy on the target domain test set . Next , apply label propagation : • Compute the label matrix QL for the labeled data ( 5 % of the target domain training set ) , with each row equal to an indicator vector for the label ( positive or negative ) . • Iterate through the target domain instances , including both test and training data . At each instance i , compute all wij , using Equation 5.32 , with α = 0.01 . Use these values to fill in column i of the transition matrix T , setting all but the ten largest values to zero for each column i . Be sure to normalize the column so that the remaining values sum to one . You may need to use a sparse matrix for this to fit into memory . • Apply the iterative updates from Equations 5.34-5.36 to compute the outcome of the label propagation algorithm for the unlabeled examples . Select the test set instances from QU , and compute the accuracy of this method . Compare with the supervised classifier trained only on the 5 % sample of the target domain training set . Jacob Eisenstein . Draft of October 15 , 2018 . 5.5 . * OTHER APPROACHES TO LEARNING WITH LATENT VARIABLES 121 7 . Using only 5 % of the target domain training data ( and all of the source domain train - ing data ) , implement one of the supervised domain adaptation baselines in subsec - tion 5.4.1 . See if this improves on the “ direct transfer ” baseline from the previous problem 8 . Implement EASYADAPT ( subsection 5.4.1 ) , again using 5 % of the target domain training data and all of the source domain data . 9 . Now try unsupervised domain adaptation , using the “ linear projection ” method described in subsection 5.4.2 . Specifically : • Identify 500 pivot features as the words with the highest frequency in the ( com - plete ) training data for the source and target domains . Specifically , let xdi be the count of the word i in domain d : choose the 500 words with the largest values of min ( xsourcei , x target i ) . • Train a classifier to predict each pivot feature from the remaining words in the document . • Arrange the features of these classifiers into a matrix Φ , and perform truncated singular value decomposition , with k = 20 • Train a classifier from the source domain data , using the combined features x ( i ) ⊕ U > x ( i ) — these include the original bag-of-words features , plus the pro - jected features . • Apply this classifier to the target domain test set , and compute the accuracy . Under contract with MIT Press , shared under CC-BY-NC-ND license . Part II Sequences and trees 123