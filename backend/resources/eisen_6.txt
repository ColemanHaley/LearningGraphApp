Chapter 6 Language models In probabilistic classification , the problem is to compute the probability of a label , condi - tioned on the text . Let’s now consider the inverse problem : computing the probability of text itself . Specifically , we will consider models that assign probability to a sequence of word tokens , p ( w1 , w2 , . . . , wM ) , with wm ∈ V . The set V is a discrete vocabulary , V = { aardvark , abacus , . . . , zither } . [ 6.1 ] Why would you want to compute the probability of a word sequence ? In many appli - cations , the goal is to produce word sequences as output : • In machine translation ( chapter 18 ) , we convert from text in a source language to text in a target language . • In speech recognition , we convert from audio signal to text . • In summarization ( section 16.3.4 ; section 19.2 ) , we convert from long texts into short texts . • In dialogue systems ( section 19.3 ) , we convert from the user’s input ( and perhaps an external knowledge base ) into a text response . In many of the systems for performing these tasks , there is a subcomponent that com - putes the probability of the output text . The purpose of this component is to generate texts that are more fluent . For example , suppose we want to translate a sentence from Spanish to English . ( 6.1 ) El cafe negro me gusta mucho . Here is a literal word-for-word translation ( a gloss ) : 125 126 CHAPTER 6 . LANGUAGE MODELS ( 6.2 ) The coffee black me pleases much . A good language model of English will tell us that the probability of this translation is low , in comparison with more grammatical alternatives , p ( The coffee black me pleases much ) < p ( I love dark coffee ) . [ 6.2 ] How can we use this fact ? Warren Weaver , one of the early leaders in machine trans - lation , viewed it as a problem of breaking a secret code ( Weaver , 1955 ) : When I look at an article in Russian , I say : ’ This is really written in English , but it has been coded in some strange symbols . I will now proceed to decode . ’ This observation motivates a generative model ( like Naı̈ve Bayes ) : • The English sentence w ( e ) is generated from a language model , pe ( w ( e ) ) . • The Spanish sentencew ( s ) is then generated from a translation model , ps | e ( w ( s ) | w ( e ) ) . Given these two distributions , translation can be performed by Bayes ’ rule : pe | s ( w ( e ) | w ( s ) ) ∝ pe , s ( w ( e ) , w ( s ) ) [ 6.3 ] = ps | e ( w ( s ) | w ( e ) ) × pe ( w ( e ) ) . [ 6.4 ] This is sometimes called the noisy channel model , because it envisions English text turning into Spanish by passing through a noisy channel , ps | e . What is the advantage of modeling translation this way , as opposed to modeling pe | s directly ? The crucial point is that the two distributions ps | e ( the translation model ) and pe ( the language model ) can be estimated from separate data . The translation model requires examples of correct trans - lations , but the language model requires only text in English . Such monolingual data is much more widely available . Furthermore , once estimated , the language model pe can be reused in any application that involves generating English text , including translation from other languages . 6.1 N - gram language models A simple approach to computing the probability of a sequence of tokens is to use a relative frequency estimate . Consider the quote , attributed to Picasso , “ computers are useless , they can only give you answers . ” One way to estimate the probability of this sentence is , p ( Computers are useless , they can only give you answers ) = count ( Computers are useless , they can only give you answers ) count ( all sentences ever spoken ) [ 6.5 ] Jacob Eisenstein . Draft of October 15 , 2018 . 6.1 . N - GRAM LANGUAGE MODELS 127 This estimator is unbiased : in the theoretical limit of infinite data , the estimate will be correct . But in practice , we are asking for accurate counts over an infinite number of events , since sequences of words can be arbitrarily long . Even with an aggressive upper bound of , say , M = 20 tokens in the sequence , the number of possible sequences is V 20 , where V = | V | . A small vocabularly for English would have V = 105 , so there are 10100 possible sequences . Clearly , this estimator is very data-hungry , and suffers from high vari - ance : even grammatical sentences will have probability zero if they have not occurred in the training data . 1 We therefore need to introduce bias to have a chance of making reli - able estimates from finite training data . The language models that follow in this chapter introduce bias in various ways . We begin with n-gram language models , which compute the probability of a sequence as the product of probabilities of subsequences . The probability of a sequence p ( w ) = p ( w1 , w2 , . . . , wM ) can be refactored using the chain rule ( see section A . 2 ) : p ( w ) =p ( w1 , w2 , . . . , wM ) [ 6.6 ] =p ( w1 ) × p ( w2 | w1 ) × p ( w3 | w2 , w1 ) × . . . × p ( wM | wM − 1 , . . . , w1 ) [ 6.7 ] Each element in the product is the probability of a word given all its predecessors . We can think of this as a word prediction task : given the context Computers are , we want to com - pute a probability over the next token . The relative frequency estimate of the probability of the word useless in this context is , p ( useless | computers are ) = count ( computers are useless ) ∑ x ∈ V count ( computers are x ) = count ( computers are useless ) count ( computers are ) . We haven’t made any approximations yet , and we could have just as well applied the chain rule in reverse order , p ( w ) = p ( wM ) × p ( wM − 1 | wM ) × . . . × p ( w1 | w2 , . . . , wM ) , [ 6.8 ] or in any other order . But this means that we also haven’t really made any progress : to compute the conditional probability p ( wM | wM − 1 , wM − 2 , . . . , w1 ) , we would need to model VM − 1 contexts . Such a distribution cannot be estimated from any realistic sample of text . To solve this problem , n-gram models make a crucial simplifying approximation : they condition on only the past n − 1 words . p ( wm | wm − 1 . . . w1 ) ≈ p ( wm | wm − 1 , . . . , wm − n + 1 ) [ 6.9 ] 1Chomsky famously argued that this is evidence against the very concept of probabilistic language mod - els : no such model could distinguish the grammatical sentence colorless green ideas sleep furiously from the ungrammatical permutation furiously sleep ideas green colorless . Under contract with MIT Press , shared under CC-BY-NC-ND license . 128 CHAPTER 6 . LANGUAGE MODELS This means that the probability of a sentence w can be approximated as p ( w1 , . . . , wM ) ≈ M ∏ m = 1 p ( wm | wm − 1 , . . . , wm − n + 1 ) [ 6.10 ] To compute the probability of an entire sentence , it is convenient to pad the beginning and end with special symbols � and � . Then the bigram ( n = 2 ) approximation to the probability of I like black coffee is : p ( I like black coffee ) = p ( I | � ) × p ( like | I ) × p ( black | like ) × p ( coffee | black ) × p ( � | coffee ) . [ 6.11 ] This model requires estimating and storing the probability of only V n events , which is exponential in the order of the n-gram , and not VM , which is exponential in the length of the sentence . The n-gram probabilities can be computed by relative frequency estimation , p ( wm | wm − 1 , wm − 2 ) = count ( wm − 2 , wm − 1 , wm ) ∑ w ′ count ( wm − 2 , wm − 1 , w ′ ) [ 6.12 ] The hyperparameter n controls the size of the context used in each conditional proba - bility . If this is misspecified , the language model will perform poorly . Let’s consider the potential problems concretely . When n is too small . Consider the following sentences : ( 6.3 ) Gorillas always like to groom their friends . ( 6.4 ) The computer that’s on the 3rd floor of our office building crashed . In each example , the words written in bold depend on each other : the likelihood of their depends on knowing that gorillas is plural , and the likelihood of crashed de - pends on knowing that the subject is a computer . If the n-grams are not big enough to capture this context , then the resulting language model would offer probabili - ties that are too low for these sentences , and too high for sentences that fail basic linguistic tests like number agreement . When n is too big . In this case , it is hard good estimates of the n-gram parameters from our dataset , because of data sparsity . To handle the gorilla example , it is necessary to model 6-grams , which means accounting for V 6 events . Under a very small vocab - ulary of V = 104 , this means estimating the probability of 1024 distinct events . Jacob Eisenstein . Draft of October 15 , 2018 . 6.2 . SMOOTHING AND DISCOUNTING 129 These two problems point to another bias-variance tradeoff ( see subsection 2.2.4 ) . A small n-gram size introduces high bias , and a large n-gram size introduces high variance . We can even have both problems at the same time ! Language is full of long-range depen - dencies that we cannot capture because n is too small ; at the same time , language datasets are full of rare phenomena , whose probabilities we fail to estimate accurately because n is too large . One solution is to try to keep n large , while still making low-variance esti - mates of the underlying parameters . To do this , we will introduce a different sort of bias : smoothing . 6.2 Smoothing and discounting Limited data is a persistent problem in estimating language models . In section 6.1 , we presented n-grams as a partial solution . Bit sparse data can be a problem even for low - order n-grams ; at the same time , many linguistic phenomena , like subject-verb agreement , cannot be incorporated into language models without high-order n-grams . It is therefore necessary to add additional inductive biases to n-gram language models . This section covers some of the most intuitive and common approaches , but there are many more ( see Chen and Goodman , 1999 ) . 6.2.1 Smoothing A major concern in language modeling is to avoid the situation p ( w ) = 0 , which could arise as a result of a single unseen n-gram . A similar problem arose in Naı̈ve Bayes , and the solution was smoothing : adding imaginary “ pseudo ” counts . The same idea can be applied to n-gram language models , as shown here in the bigram case , psmooth ( wm | wm − 1 ) = count ( wm − 1 , wm ) + α ∑ w ′ ∈ V count ( wm − 1 , w ′ ) + V α . [ 6.13 ] This basic framework is called Lidstone smoothing , but special cases have other names : • Laplace smoothing corresponds to the case α = 1 . • Jeffreys-Perks law corresponds to the case α = 0.5 , which works well in practice and benefits from some theoretical justification ( Manning and Schütze , 1999 ) . To ensure that the probabilities are properly normalized , anything that we add to the numerator ( α ) must also appear in the denominator ( V α ) . This idea is reflected in the concept of effective counts : c ∗ i = ( ci + α ) M M + V α , [ 6.14 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 130 CHAPTER 6 . LANGUAGE MODELS Lidstone smoothing , α = 0.1 Discounting , d = 0.1 counts unsmoothed probability effective counts smoothed probability effective counts smoothed probability impropriety 8 0.4 7.826 0.391 7.9 0.395 offense 5 0.25 4.928 0.246 4.9 0.245 damage 4 0.2 3.961 0.198 3.9 0.195 deficiencies 2 0.1 2.029 0.101 1.9 0.095 outbreak 1 0.05 1.063 0.053 0.9 0.045 infirmity 0 0 0.097 0.005 0.25 0.013 cephalopods 0 0 0.097 0.005 0.25 0.013 Table 6.1 : Example of Lidstone smoothing and absolute discounting in a bigram language model , for the context ( alleged , ) , for a toy corpus with a total of twenty counts over the seven words shown . Note that discounting decreases the probability for all but the un - seen words , while Lidstone smoothing increases the effective counts and probabilities for deficiencies and outbreak . where ci is the count of event i , c ∗ i is the effective count , andM = ∑ V i = 1 ci is the total num - ber of tokens in the dataset ( w1 , w2 , . . . , wM ) . This term ensures that ∑ V i = 1 c ∗ i = ∑ V i = 1 ci = M . The discount for each n-gram is then computed as , di = c ∗ i ci = ( ci + α ) ci M ( M + V α ) . 6.2.2 Discounting and backoff Discounting “ borrows ” probability mass from observed n-grams and redistributes it . In Lidstone smoothing , the borrowing is done by increasing the denominator of the relative frequency estimates . The borrowed probability mass is then redistributed by increasing the numerator for all n-grams . Another approach would be to borrow the same amount of probability mass from all observed n-grams , and redistribute it among only the unob - served n-grams . This is called absolute discounting . For example , suppose we set an absolute discount d = 0.1 in a bigram model , and then redistribute this probability mass equally over the unseen words . The resulting probabilities are shown in Table 6.1 . Discounting reserves some probability mass from the observed data , and we need not redistribute this probability mass equally . Instead , we can backoff to a lower-order lan - guage model : if you have trigrams , use trigrams ; if you don’t have trigrams , use bigrams ; if you don’t even have bigrams , use unigrams . This is called Katz backoff . In the simple Jacob Eisenstein . Draft of October 15 , 2018 . 6.2 . SMOOTHING AND DISCOUNTING 131 case of backing off from bigrams to unigrams , the bigram probabilities are , c ∗ ( i , j ) = c ( i , j ) − d [ 6.15 ] pKatz ( i | j ) =    c ∗ ( i , j ) c ( j ) if c ( i , j ) > 0 α ( j ) × punigram ( i ) ∑ i ′ : c ( i ′ , j ) = 0 punigram ( i ′ ) if c ( i , j ) = 0 . [ 6.16 ] The term α ( j ) indicates the amount of probability mass that has been discounted for context j . This probability mass is then divided across all the unseen events , { i ′ : c ( i ′ , j ) = 0 } , proportional to the unigram probability of each word i ′ . The discount parameter d can be optimized to maximize performance ( typically held-out log-likelihood ) on a develop - ment set . 6.2.3 * Interpolation Backoff is one way to combine different order n-gram models . An alternative approach is interpolation : setting the probability of a word in context to a weighted sum of its probabilities across progressively shorter contexts . Instead of choosing a single n for the size of the n-gram , we can take the weighted average across several n-gram probabilities . For example , for an interpolated trigram model , pInterpolation ( wm | wm − 1 , wm − 2 ) = λ3p ∗ 3 ( wm | wm − 1 , wm − 2 ) + λ2p ∗ 2 ( wm | wm − 1 ) + λ1p ∗ 1 ( wm ) . In this equation , p ∗ n is the unsmoothed empirical probability given by an n-gram lan - guage model , and λn is the weight assigned to this model . To ensure that the interpolated p ( w ) is still a valid probability distribution , the values of λ must obey the constraint , ∑ nmax n = 1 λn = 1 . But how to find the specific values ? An elegant solution is expectation-maximization . Recall from chapter 5 that we can think about EM as learning with missing data : we just need to choose missing data such that learning would be easy if it weren’t missing . What’s missing in this case ? Think of each word wm as drawn from an n-gram of unknown size , zm ∈ { 1 . . . nmax } . This zm is the missing data that we are looking for . Therefore , the application of EM to this problem involves the following generative model : for Each token wm , m = 1 , 2 , . . . , M do : draw the n-gram size zm ∼ Categorical ( λ ) ; draw wm ∼ p ∗ zm ( wm | wm − 1 , . . . , wm − zm ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 132 CHAPTER 6 . LANGUAGE MODELS If the missing data { Zm } were known , then λ could be estimated as the relative fre - quency , λz = count ( Zm = z ) M [ 6.17 ] ∝ M ∑ m = 1 δ ( Zm = z ) . [ 6.18 ] But since we do not know the values of the latent variables Zm , we impute a distribution qm in the E-step , which represents the degree of belief that word token wm was generated from a n-gram of order zm , qm ( z ) , Pr ( Zm = z | w1 : m ; λ ) [ 6.19 ] = p ( wm | w1 : m − 1 , Zm = z ) × p ( z ) ∑ z ′ p ( wm | w1 : m − 1 , Zm = z ′ ) × p ( z ′ ) [ 6.20 ] ∝ p ∗ z ( wm | w1 : m − 1 ) × λz . [ 6.21 ] In the M-step , λ is computed by summing the expected counts under q , λz ∝ M ∑ m = 1 qm ( z ) . [ 6.22 ] A solution is obtained by iterating between updates to q and λ . The complete algorithm is shown in Algorithm 10 . Algorithm 10 Expectation-maximization for interpolated language modeling 1 : procedure ESTIMATE INTERPOLATED n-GRAM ( w1 : M , { p ∗ n } n ∈ 1 : nmax ) 2 : for z ∈ { 1 , 2 , . . . , nmax } do . Initialization 3 : λz ← 1nmax 4 : repeat 5 : for m ∈ { 1 , 2 , . . . , M } do . E-step 6 : for z ∈ { 1 , 2 , . . . , nmax } do 7 : qm ( z ) ← p ∗ z ( wm | w1 : m − ) × λz 8 : qm ← Normalize ( qm ) 9 : for z ∈ { 1 , 2 , . . . , nmax } do . M-step 10 : λz ← 1M ∑ M m = 1 qm ( z ) 11 : until tired 12 : return λ Jacob Eisenstein . Draft of October 15 , 2018 . 6.3 . RECURRENT NEURAL NETWORK LANGUAGE MODELS 133 6.2.4 * Kneser-Ney smoothing Kneser-Ney smoothing is based on absolute discounting , but it redistributes the result - ing probability mass in a different way from Katz backoff . Empirical evidence points to Kneser-Ney smoothing as the state-of-art for n-gram language modeling ( Goodman , 2001 ) . To motivate Kneser-Ney smoothing , consider the example : I recently visited . Which of the following is more likely : Francisco or Duluth ? Now suppose that both bigrams visited Duluth and visited Francisco are unobserved in the training data , and furthermore , that the unigram probability p ∗ 1 ( Francisco ) is greater than p ∗ ( Duluth ) . Nonetheless we would still guess that p ( visited Duluth ) > p ( visited Francisco ) , because Duluth is a more “ versatile ” word : it can occur in many contexts , while Francisco usually occurs in a single context , following the word San . This notion of versatility is the key to Kneser-Ney smoothing . Writing u for a context of undefined length , and count ( w , u ) as the count of word w in context u , we define the Kneser-Ney bigram probability as pKN ( w | u ) = { max ( count ( w , u ) − d , 0 ) count ( u ) , count ( w , u ) > 0 α ( u ) × pcontinuation ( w ) , otherwise [ 6.23 ] pcontinuation ( w ) = | u : count ( w , u ) > 0 | ∑ w ′ ∈ V | u ′ : count ( w ′ , u ′ ) > 0 | . [ 6.24 ] Probability mass using absolute discounting d , which is taken from all unobserved n-grams . The total amount of discounting in context u is d × | w : count ( w , u ) > 0 | , and we divide this probability mass among the unseen n-grams . To account for versatility , we define the continuation probability pcontinuation ( w ) as proportional to the number of ob - served contexts in which w appears . The numerator of the continuation probability is the number of contexts u in which w appears ; the denominator normalizes the probability by summing the same quantity over all words w ′ . The coefficient α ( u ) is set to ensure that the probability distribution pKN ( w | u ) sums to one over the vocabulary w . The idea of modeling versatility by counting contexts may seem heuristic , but there is an elegant theoretical justification from Bayesian nonparametrics ( Teh , 2006 ) . Kneser-Ney smoothing on n-grams was the dominant language modeling technique before the arrival of neural language models . 6.3 Recurrent neural network language models N - gram language models have been largely supplanted by neural networks . These mod - els do not make the n-gram assumption of restricted context ; indeed , they can incorporate arbitrarily distant contextual information , while remaining computationally and statisti - cally tractable . Under contract with MIT Press , shared under CC-BY-NC-ND license . 134 CHAPTER 6 . LANGUAGE MODELS h0 h1 h2 h3 · · · x1 x2 x3 · · · w1 w2 w3 · · · Figure 6.1 : The recurrent neural network language model , viewed as an “ unrolled ” com - putation graph . Solid lines indicate direct computation , dotted blue lines indicate proba - bilistic dependencies , circles indicate random variables , and squares indicate computation nodes . The first insight behind neural language models is to treat word prediction as a dis - criminative learning task . 2 The goal is to compute the probability p ( w | u ) , where w ∈ V is a word , and u is the context , which depends on the previous words . Rather than directly estimating the word probabilities from ( smoothed ) relative frequencies , we can treat treat language modeling as a machine learning problem , and estimate parameters that maxi - mize the log conditional probability of a corpus . The second insight is to reparametrize the probability distribution p ( w | u ) as a func - tion of two dense K-dimensional numerical vectors , βw ∈ RK , and vu ∈ RK , p ( w | u ) = exp ( βw · vu ) ∑ w ′ ∈ V exp ( βw ′ · vu ) , [ 6.25 ] where βw · vu represents a dot product . As usual , the denominator ensures that the prob - ability distribution is properly normalized . This vector of probabilities is equivalent to applying the softmax transformation ( see section 3.1 ) to the vector of dot-products , p ( · | u ) = SoftMax ( [ β1 · vu , β2 · vu , . . . , βV · vu ] ) . [ 6.26 ] The word vectors βw are parameters of the model , and are estimated directly . The context vectors vu can be computed in various ways , depending on the model . A simple but effective neural language model can be built from a recurrent neural network ( RNN ; Mikolov et al . , 2010 ) . The basic idea is to recurrently update the context vectors while moving through the sequence . Let hm represent the contextual information at position m 2This idea predates neural language models ( e.g . , Rosenfeld , 1996 ; Roark et al . , 2007 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 6.3 . RECURRENT NEURAL NETWORK LANGUAGE MODELS 135 in the sequence . RNN language models are defined , xm , φwm [ 6.27 ] hm = RNN ( xm , hm − 1 ) [ 6.28 ] p ( wm + 1 | w1 , w2 , . . . , wm ) = exp ( βwm + 1 · hm ) ∑ w ′ ∈ V exp ( βw ′ · hm ) , [ 6.29 ] where φ is a matrix of word embeddings , and xm denotes the embedding for word wm . The conversion of wm to xm is sometimes known as a lookup layer , because we simply lookup the embeddings for each word in a table ; see subsection 3.2.4 . The Elman unit defines a simple recurrent operation ( Elman , 1990 ) , RNN ( xm , hm − 1 ) , g ( Θhm − 1 + xm ) , [ 6.30 ] where Θ ∈ RK × K is the recurrence matrix and g is a non-linear transformation function , often defined as the elementwise hyperbolic tangent tanh ( see section 3.1 ) . 3 The tanh acts as a squashing function , ensuring that each element of hm is constrained to the range [ − 1 , 1 ] . Although each wm depends on only the context vector hm − 1 , this vector is in turn influenced by all previous tokens , w1 , w2 , . . . wm − 1 , through the recurrence operation : w1 affects h1 , which affects h2 , and so on , until the information is propagated all the way to hm − 1 , and then on to wm ( see Figure 6.1 ) . This is an important distinction from n-gram language models , where any information outside the n-word window is ignored . In prin - ciple , the RNN language model can handle long-range dependencies , such as number agreement over long spans of text — although it would be difficult to know where exactly in the vector hm this information is represented . The main limitation is that informa - tion is attenuated by repeated application of the squashing function g . Long short-term memories ( LSTMs ) , described below , are a variant of RNNs that address this issue , us - ing memory cells to propagate information through the sequence without applying non - linearities ( Hochreiter and Schmidhuber , 1997 ) . The denominator in Equation 6.29 is a computational bottleneck , because it involves a sum over the entire vocabulary . One solution is to use a hierarchical softmax function , which computes the sum more efficiently by organizing the vocabulary into a tree ( Mikolov et al . , 2011 ) . Another strategy is to optimize an alternative metric , such as noise-contrastive estimation ( Gutmann and Hyvärinen , 2012 ) , which learns by distinguishing observed in - stances from artificial instances generated from a noise distribution ( Mnih and Teh , 2012 ) . Both of these strategies are described in subsection 14.5.3 . 3In the original Elman network , the sigmoid function was used in place of tanh . For an illuminating mathematical discussion of the advantages and disadvantages of various nonlinearities in recurrent neural networks , see the lecture notes from Cho ( 2015 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 136 CHAPTER 6 . LANGUAGE MODELS 6.3.1 Backpropagation through time The recurrent neural network language model has the following parameters : • φi ∈ RK , the “ input ” word vectors ( these are sometimes called word embeddings , since each word is embedded in a K-dimensional space ; see chapter 14 ) ; • βi ∈ RK , the “ output ” word vectors ; • Θ ∈ RK × K , the recurrence operator ; • h0 , the initial state . Each of these parameters can be estimated by formulating an objective function over the training corpus , L ( w ) , and then applying backpropagation to obtain gradients on the parameters from a minibatch of training examples ( see subsection 3.3.1 ) . Gradient-based updates can be computed from an online learning algorithm such as stochastic gradient descent ( see subsection 2.6.2 ) . The application of backpropagation to recurrent neural networks is known as back - propagation through time , because the gradients on units at timem depend in turn on the gradients of units at earlier times n < m . Let ` m + 1 represent the negative log-likelihood of word m + 1 , ` m + 1 = − log p ( wm + 1 | w1 , w2 , . . . , wm ) . [ 6.31 ] We require the gradient of this loss with respect to each parameter , such as θk , k ′ , an indi - vidual element in the recurrence matrix Θ . Since the loss depends on the parameters only through hm , we can apply the chain rule of differentiation , ∂ ` m + 1 ∂ θk , k ′ = ∂ ` m + 1 ∂ hm ∂ hm ∂ θk , k ′ . [ 6.32 ] The vector hm depends on Θ in several ways . First , hm is computed by multiplying Θ by the previous state hm − 1 . But the previous state hm − 1 also depends on Θ : hm = g ( xm , hm − 1 ) [ 6.33 ] ∂ hm , k ∂ θk , k ′ = g ′ ( xm , k + θk · hm − 1 ) ( hm − 1 , k ′ + θk · ∂ hm − 1 ∂ θk , k ′ ) , [ 6.34 ] where g ′ is the local derivative of the nonlinear function g . The key point in this equation is that the derivative ∂ hm ∂ θk , k ′ depends on ∂ hm − 1 ∂ θk , k ′ , which will depend in turn on ∂ hm − 2 ∂ θk , k ′ , and so on , until reaching the initial state h0 . Each derivative ∂ hm ∂ θk , k ′ will be reused many times : it appears in backpropagation from the loss ` m , but also in all subsequent losses ` n > m . Neural network toolkits such as Torch ( Collobert et al . , 2011 ) and DyNet ( Neubig et al . , 2017 ) compute the necessary Jacob Eisenstein . Draft of October 15 , 2018 . 6.3 . RECURRENT NEURAL NETWORK LANGUAGE MODELS 137 derivatives automatically , and cache them for future use . An important distinction from the feedforward neural networks considered in chapter 3 is that the size of the computa - tion graph is not fixed , but varies with the length of the input . This poses difficulties for toolkits that are designed around static computation graphs , such as TensorFlow ( Abadi et al . , 2016 ) . 4 6.3.2 Hyperparameters The RNN language model has several hyperparameters that must be tuned to ensure good performance . The model capacity is controlled by the size of the word and context vectors K , which play a role that is somewhat analogous to the size of the n-gram context . For datasets that are large with respect to the vocabulary ( i.e . , there is a large token-to-type ratio ) , we can afford to estimate a model with a large K , which enables more subtle dis - tinctions between words and contexts . When the dataset is relatively small , then K must be smaller too , or else the model may “ memorize ” the training data , and fail to generalize . Unfortunately , this general advice has not yet been formalized into any concrete formula for choosing K , and trial-and-error is still necessary . Overfitting can also be prevented by dropout , which involves randomly setting some elements of the computation to zero ( Sri - vastava et al . , 2014 ) , forcing the learner not to rely too much on any particular dimension of the word or context vectors . The dropout rate must also be tuned on development data . 6.3.3 Gated recurrent neural networks In principle , recurrent neural networks can propagate information across infinitely long sequences . But in practice , repeated applications of the nonlinear recurrence function causes this information to be quickly attenuated . The same problem affects learning : back - propagation can lead to vanishing gradients that decay to zero , or exploding gradients that increase towards infinity ( Bengio et al . , 1994 ) . The exploding gradient problem can be addressed by clipping gradients at some maximum value ( Pascanu et al . , 2013 ) . The other issues must be addressed by altering the model itself . The long short-term memory ( LSTM ; Hochreiter and Schmidhuber , 1997 ) is a popular variant of RNNs that is more robust to these problems . This model augments the hidden state hm with a memory cell cm . The value of the memory cell at each time m is a gated sum of two quantities : its previous value cm − 1 , and an “ update ” c̃m , which is computed from the current input xm and the previous hidden state hm − 1 . The next state hm is then computed from the memory cell . Because the memory cell is not passed through a non - linear squashing function during the update , it is possible for information to propagate through the network over long distances . 4See https://www.tensorflow.org/tutorials/recurrent ( retrieved Feb 8 , 2018 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 138 CHAPTER 6 . LANGUAGE MODELS hm hm + 1 om om + 1 cm fm + 1 cm + 1 im im + 1 c̃m c̃m + 1 xm xm + 1 Figure 6.2 : The long short-term memory ( LSTM ) architecture . Gates are shown in boxes with dotted edges . In an LSTM language model , each hm would be used to predict the next word wm + 1 . The gates are functions of the input and previous hidden state . They are computed from elementwise sigmoid activations , σ ( x ) = ( 1 + exp ( − x ) ) − 1 , ensuring that their values will be in the range [ 0 , 1 ] . They can therefore be viewed as soft , differentiable logic gates . The LSTM architecture is shown in Figure 6.2 , and the complete update equations are : fm + 1 = σ ( Θ ( h → f ) hm + Θ ( x → f ) xm + 1 + bf ) forget gate [ 6.35 ] im + 1 = σ ( Θ ( h → i ) hm + Θ ( x → i ) xm + 1 + bi ) input gate [ 6.36 ] c̃m + 1 = tanh ( Θ ( h → c ) hm + Θ ( w → c ) xm + 1 ) update candidate [ 6.37 ] cm + 1 = fm + 1 � cm + im + 1 � c̃m + 1 memory cell update [ 6.38 ] om + 1 = σ ( Θ ( h → o ) hm + Θ ( x → o ) xm + 1 + bo ) output gate [ 6.39 ] hm + 1 = om + 1 � tanh ( cm + 1 ) output . [ 6.40 ] The operator � is an elementwise ( Hadamard ) product . Each gate is controlled by a vec - tor of weights , which parametrize the previous hidden state ( e.g . , Θ ( h → f ) ) and the current input ( e.g . , Θ ( x → f ) ) , plus a vector offset ( e.g . , bf ) . The overall operation can be infor - mally summarized as ( hm , cm ) = LSTM ( xm , ( hm − 1 , cm − 1 ) ) , with ( hm , cm ) representing the LSTM state after reading token m . The LSTM outperforms standard recurrent neural networks across a wide range of problems . It was first used for language modeling by Sundermeyer et al . ( 2012 ) , but can be applied more generally : the vector hm can be treated as a complete representation of Jacob Eisenstein . Draft of October 15 , 2018 . 6.4 . EVALUATING LANGUAGE MODELS 139 the input sequence up to position m , and can be used for any labeling task on a sequence of tokens , as we will see in the next chapter . There are several LSTM variants , of which the Gated Recurrent Unit ( Cho et al . , 2014 ) is one of the more well known . Many software packages implement a variety of RNN architectures , so choosing between them is simple from a user’s perspective . Jozefowicz et al . ( 2015 ) provide an empirical comparison of various modeling choices circa 2015 . 6.4 Evaluating language models Language modeling is not usually an application in itself : language models are typically components of larger systems , and they would ideally be evaluated extrinisically . This means evaluating whether the language model improves performance on the application task , such as machine translation or speech recognition . But this is often hard to do , and depends on details of the overall system which may be irrelevant to language modeling . In contrast , intrinsic evaluation is task-neutral . Better performance on intrinsic metrics may be expected to improve extrinsic metrics across a variety of tasks , but there is always the risk of over-optimizing the intrinsic metric . This section discusses some intrinsic met - rics , but keep in mind the importance of performing extrinsic evaluations to ensure that intrinsic performance gains carry over to real applications . 6.4.1 Held-out likelihood The goal of probabilistic language models is to accurately measure the probability of se - quences of word tokens . Therefore , an intrinsic evaluation metric is the likelihood that the language model assigns to held-out data , which is not used during training . Specifically , we compute , ` ( w ) = M ∑ m = 1 log p ( wm | wm − 1 , . . . , w1 ) , [ 6.41 ] treating the entire held-out corpus as a single stream of tokens . Typically , unknown words are mapped to the 〈 UNK 〉 token . This means that we have to estimate some probability for 〈 UNK 〉 on the training data . One way to do this is to fix the vocabulary V to the V − 1 words with the highest counts in the training data , and then convert all other tokens to 〈 UNK 〉 . Other strategies for dealing with out-of-vocabulary terms are discussed in section 6.5 . Under contract with MIT Press , shared under CC-BY-NC-ND license . 140 CHAPTER 6 . LANGUAGE MODELS 6.4.2 Perplexity Held-out likelihood is usually presented as perplexity , which is a deterministic transfor - mation of the log-likelihood into an information-theoretic quantity , Perplex ( w ) = 2 − ` ( w ) M , [ 6.42 ] where M is the total number of tokens in the held-out corpus . Lower perplexities correspond to higher likelihoods , so lower scores are better on this metric — it is better to be less perplexed . Here are some special cases : • In the limit of a perfect language model , probability 1 is assigned to the held-out corpus , with Perplex ( w ) = 2 − 1 M log2 1 = 20 = 1 . • In the opposite limit , probability zero is assigned to the held-out corpus , which cor - responds to an infinite perplexity , Perplex ( w ) = 2 − 1 M log2 0 = 2 ∞ = ∞ . • Assume a uniform , unigram model in which p ( wi ) = 1V for all words in the vocab - ulary . Then , log2 ( w ) = M ∑ m = 1 log2 1 V = − M ∑ m = 1 log2 V = − M log2 V Perplex ( w ) = 2 1 M M log2 V = 2log2 V = V . This is the “ worst reasonable case ” scenario , since you could build such a language model without even looking at the data . In practice , language models tend to give perplexities in the range between 1 and V . A small benchmark dataset is the Penn Treebank , which contains roughly a million to - kens ; its vocabulary is limited to 10,000 words , with all other tokens mapped a special 〈 UNK 〉 symbol . On this dataset , a well-smoothed 5-gram model achieves a perplexity of 141 ( Mikolov and Zweig , Mikolov and Zweig ) , and an LSTM language model achieves perplexity of roughly 80 ( Zaremba , Sutskever , and Vinyals , Zaremba et al . ) . Various en - hancements to the LSTM architecture can bring the perplexity below 60 ( Merity et al . , 2018 ) . A larger-scale language modeling dataset is the 1B Word Benchmark ( Chelba et al . , 2013 ) , which contains text from Wikipedia . On this dataset , a perplexities of around 25 can be obtained by averaging together multiple LSTM language models ( Jozefowicz et al . , 2016 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 6.5 . OUT-OF-VOCABULARY WORDS 141 6.5 Out-of-vocabulary words So far , we have assumed a closed-vocabulary setting — the vocabulary V is assumed to be a finite set . In realistic application scenarios , this assumption may not hold . Consider , for example , the problem of translating newspaper articles . The following sentence appeared in a Reuters article on January 6 , 2017:5 The report said U.S . intelligence agencies believe Russian military intelligence , the GRU , used intermediaries such as WikiLeaks , DCLeaks.com and the Guc - cifer 2.0 ” persona ” to release emails . . . Suppose that you trained a language model on the Gigaword corpus , 6 which was released in 2003 . The bolded terms either did not exist at this date , or were not widely known ; they are unlikely to be in the vocabulary . The same problem can occur for a variety of other terms : new technologies , previously unknown individuals , new words ( e.g . , hashtag ) , and numbers . One solution is to simply mark all such terms with a special token , 〈 UNK 〉 . While training the language model , we decide in advance on the vocabulary ( often the K most common terms ) , and mark all other terms in the training data as 〈 UNK 〉 . If we do not want to determine the vocabulary size in advance , an alternative approach is to simply mark the first occurrence of each word type as 〈 UNK 〉 . But is often better to make distinctions about the likelihood of various unknown words . This is particularly important in languages that have rich morphological systems , with many inflections for each word . For example , Portuguese is only moderately complex from a morphological perspective , yet each verb has dozens of inflected forms ( see Fig - ure 4.3b ) . In such languages , there will be many word types that we do not encounter in a corpus , which are nonetheless predictable from the morphological rules of the language . To use a somewhat contrived English example , if transfenestrate is in the vocabulary , our language model should assign a non-zero probability to the past tense transfenestrated , even if it does not appear in the training data . One way to accomplish this is to supplement word-level language models with character - level language models . Such models can use n-gramsor RNNs , but with a fixed vocab - ulary equal to the set of ASCII or Unicode characters . For example , Ling et al . ( 2015 ) propose an LSTM model over characters , and Kim ( 2014 ) employ a convolutional neural network . A more linguistically motivated approach is to segment words into meaningful subword units , known as morphemes ( see chapter 9 ) . For example , Botha and Blunsom 5Bayoumy , Y . and Strobel , W . ( 2017 , January 6 ) . U.S . intel report : Putin directed cy - ber campaign to help Trump . Reuters . Retrieved from http://www.reuters.com/article/ us-usa-russia-cyber-idUSKBN14Q1T8 on January 7 , 2017 . 6https :// catalog.ldc.upenn.edu/LDC2003T05 Under contract with MIT Press , shared under CC-BY-NC-ND license . 142 CHAPTER 6 . LANGUAGE MODELS ( 2014 ) induce vector representations for morphemes , which they build into a log-bilinear language model ; Bhatia et al . ( 2016 ) incorporate morpheme vectors into an LSTM . Additional resources A variety of neural network architectures have been applied to language modeling . No - table earlier non-recurrent architectures include the neural probabilistic language model ( Ben - gio et al . , 2003 ) and the log-bilinear language model ( Mnih and Hinton , 2007 ) . Much more detail on these models can be found in the text by Goodfellow et al . ( 2016 ) . Exercises 1 . Prove that n-gram language models give valid probabilities if the n-gram probabil - ities are valid . Specifically , assume that , V ∑ wm p ( wm | wm − 1 , wm − 2 , . . . , wm − n + 1 ) = 1 [ 6.43 ] for all contexts ( wm − 1 , wm − 2 , . . . , wm − n + 1 ) . Prove that ∑ w pn ( w ) = 1 for allw ∈ V ∗ , where pn is the probability under an n-gram language model . Your proof should proceed by induction . You should handle the start-of-string case p ( w1 | � , . . . , � ︸ ︷ ︷ ︸ n − 1 ) , but you need not handle the end-of-string token . 2 . First , show that RNN language models are valid using a similar proof technique to the one in the previous problem . Next , let pr ( w ) indicate the probability of w under RNN r . An ensemble of RNN language models computes the probability , p ( w ) = 1 R R ∑ r = 1 pr ( w ) . [ 6.44 ] Does an ensemble of RNN language models compute a valid probability ? 3 . Consider a unigram language model over a vocabulary of size V . Suppose that a word appears m times in a corpus with M tokens in total . With Lidstone smoothing of α , for what values of m is the smoothed probability greater than the unsmoothed probability ? 4 . Consider a simple language in which each token is drawn from the vocabulary V with probability 1V , independent of all other tokens . Jacob Eisenstein . Draft of October 15 , 2018 . 6.5 . OUT-OF-VOCABULARY WORDS 143 Given a corpus of size M , what is the expectation of the fraction of all possible bigrams that have zero count ? You may assume V is large enough that 1V ≈ 1V − 1 . 5 . Continuing the previous problem , determine the value of M such that the fraction of bigrams with zero count is at most � ∈ ( 0 , 1 ) . As a hint , you may use the approxi - mation ln ( 1 + α ) ≈ α for α ≈ 0 . 6 . In real languages , words probabilities are neither uniform nor independent . Assume that word probabilities are independent but not uniform , so that in general p ( w ) 6 = 1 V . Prove that the expected fraction of unseen bigrams will be higher than in the IID case . 7 . Consider a recurrent neural network with a single hidden unit and a sigmoid acti - vation , hm = σ ( θhm − 1 + xm ) . Prove that if | θ | < 1 , then the gradient ∂ hm ∂ hm − k goes to zero as k → ∞ . 7 8 . Zipf’s law states that if the word types in a corpus are sorted by frequency , then the frequency of the word at rank r is proportional to r − s , where s is a free parameter , usually around 1 . ( Another way to view Zipf’s law is that a plot of log frequency against log rank will be linear . ) Solve for s using the counts of the first and second most frequent words , c1 and c2 . 9 . Download the wikitext-2 dataset . 8 Read in the training data and compute word counts . Estimate the Zipf’s law coefficient by , ŝ = exp ( ( log r ) · ( log c ) | | log r | | 22 ) , [ 6.45 ] where r = [ 1 , 2 , 3 , . . . ] is the vector of ranks of all words in the corpus , and c = [ c1 , c2 , c3 , . . . ] is the vector of counts of all words in the corpus , sorted in descending order . Make a log-log plot of the observed counts , and the expected counts according to Zipf’s law . The sum ∑ ∞ r = 1 r s = ζ ( s ) is the Riemann zeta function , available in python’s scipy library as scipy . special . zeta . 10 . Using the Pytorch library , train an LSTM language model from the Wikitext train - ing corpus . After each epoch of training , compute its perplexity on the Wikitext validation corpus . Stop training when the perplexity stops improving . 7This proof generalizes to vector hidden units by considering the largest eigenvector of the matrix Θ ( Pas - canu et al . , 2013 ) . 8Available at https://github.com/pytorch/examples/tree/master/word_language_ model / data / wikitext-2 in September 2018 . The dataset is already tokenized , and already replaces rare words with 〈 UNK 〉 , so no preprocessing is necessary . Under contract with MIT Press , shared under CC-BY-NC-ND license .