Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 5 Logistic Regression ” know fine begonias equal importance ? ” Hercule Poirot , Agatha Christie’s Mysterious Affair Styles Detective stories littered clues texts words . Yet poor reader challenging know weigh author’s clues order make crucial classification task : deciding whodunnit . chapter introduce algorithm admirably suited discovering link features cues particular outcome : logistic regression . logisticregression Indeed , logistic regression important analytic tools social natural sciences . natural language processing , logistic regression base - line supervised machine learning algorithm classification , close relationship neural networks . Chapter 7 , neural net - work viewed series logistic regression classifiers stacked top . Thus classification machine learning techniques introduced play important role throughout book . Logistic regression classify observation two classes ( like ‘ positive sentiment ’ ‘ negative sentiment ’ ) , many classes . mathematics two-class case simpler , describe special case logistic regression first next few sections , briefly summarize multinomial logistic regression two classes Section 5.6 . introduce mathematics logistic regression next few sections . begin high-level issues . Generative Discriminative Classifiers : important difference - tween naive Bayes logistic regression logistic regression discrimina - tive classifier naive Bayes generative classifier . two different frameworks build machine learning model . Consider visual metaphor : imagine trying distinguish dog images cat images . generative model goal understanding dogs look like cats look like . might literally ask model ‘ generate ’ , i.e . draw , dog . test image , system asks cat model dog model better fits ( less surprised ) image , chooses label . discriminative model , contrast , try - ing learn distinguish classes ( perhaps - learning ) . maybe dogs training data wearing collars cats . feature neatly separates classes , model satisfied . ask model knows cats say wear collars . 2 CHAPTER 5 • LOGISTIC REGRESSION formally , recall naive Bayes assigns class c document d directly computing P ( c | d ) computing likelihood prior ĉ = argmax c ∈ C likelihood ︷ ︸ ︸ ︷ P ( d | c ) prior ︷ ︸ ︸ ︷ P ( c ) ( 5.1 ) generative model like naive Bayes makes likelihood term , whichgenerativemodel expresses generate features document knew class c . contrast discriminative model text categorization scenario attemptsdiscriminativemodel directly compute P ( c | d ) . Perhaps learn assign high weight document features directly improve ability discriminate possible classes , even generate example classes . Components probabilistic machine learning classifier : Like naive Bayes , logistic regression probabilistic classifier makes supervised machine learning . Machine learning classifiers require training corpus M input / output pairs ( x ( ) , y ( ) ) . ( superscripts parentheses refer individual instances training set — sentiment classification instance might individual document classified ) . machine learning system classification four components : 1 . feature representation input . input observation x ( ) , vector features [ x1 , x2 , . . . , xn ] . generally refer feature input x ( j ) x ( j ) , sometimes simplified xi , notation fi , fi ( x ) , , multiclass classification , fi ( c , x ) . 2 . classification function computes ŷ , estimated class , via p ( y | x ) . next section introduce sigmoid softmax tools classifi - cation . 3 . objective function learning , usually involving minimizing error training examples . introduce cross-entropy loss function 4 . algorithm optimizing objective function . introduce stochas - tic gradient descent algorithm . Logistic regression two phases : training : train system ( specifically weights w b ) stochastic gradient descent cross-entropy loss . test : test example x compute p ( y | x ) return higher probability label y = 1 y = 0 . 5.1 Classification : sigmoid goal binary logistic regression train classifier make binary decision class new input observation . introduce sigmoid classifier help make decision . Consider single input observation x , represent vector features [ x1 , x2 , . . . , xn ] ( show sample features next subsection ) . clas - sifier output y 1 ( meaning observation member class ) 0 ( observation member class ) . know probability P ( y = 1 | x ) observation member class . perhaps decision 5.1 • CLASSIFICATION : SIGMOID 3 “ positive sentiment ” versus “ negative sentiment ” , features represent counts words document , P ( y = 1 | x ) probability document positive sentiment , P ( y = 0 | x ) probability document negative sentiment . Logistic regression solves task learning , training set , vector weights bias term . weight wi real number , associated input features xi . weight wi represents important input feature classification decision , positive ( meaning feature associated class ) negative ( meaning feature associated class ) . Thus might expect sentiment task word awesome high positive weight , abysmal negative weight . bias term , called thebias term intercept , another real number that’s added weighted inputs . intercept make decision test instance — learned weights training — classifier first multiplies xi weight wi , sums up weighted features , adds bias term b . resulting single number z expresses weighted sum evidence class . z = ( n ∑ = 1 wixi ) + b ( 5.2 ) rest book represent sums dot product notation fromdot product linear algebra . dot product two vectors b , written · b sum products corresponding elements vector . Thus following equivalent formation Eq . 5.2 : z = w · x + b ( 5.3 ) note nothing Eq . 5.3 forces z legal probability , , lie 0 1 . fact , weights real-valued , output might even negative ; z ranges − ∞ ∞ . Figure 5.1 sigmoid function y = 11 + e − z takes real value maps range [ 0,1 ] . nearly linear around 0 outlier values get squashed toward 0 1 . create probability , pass z sigmoid function , σ ( z ) . Thesigmoid sigmoid function ( named looks like s ) called logistic func - tion , gives logistic regression name . sigmoid following equation , logisticfunction shown graphically Fig . 5.1 : y = σ ( z ) = 1 1 + e − z ( 5.4 ) sigmoid number advantages ; takes real-valued number maps range [ 0,1 ] , just probability . 4 CHAPTER 5 • LOGISTIC REGRESSION nearly linear around 0 sharp slope toward ends , tends squash outlier values toward 0 1 . differentiable , Section 5.8 handy learning . almost . apply sigmoid sum weighted features , get number 0 1 . make probability , just need make sure two cases , p ( y = 1 ) p ( y = 0 ) , sum 1 . follows : P ( y = 1 ) = σ ( w · x + b ) = 1 1 + e − ( w·x + b ) P ( y = 0 ) = 1 − σ ( w · x + b ) = 1 − 1 1 + e − ( w·x + b ) = e − ( w·x + b ) 1 + e − ( w·x + b ) ( 5.5 ) algorithm instance x computes probability P ( y = 1 | x ) . make decision ? test instance x , say yes probability P ( y = 1 | x ) . 5 , otherwise . call . 5 decision boundary : decisionboundary ŷ = { 1 P ( y = 1 | x ) > 0.5 0 otherwise 5.1.1 Example : sentiment classification example . Suppose binary sentiment classification movie review text , like know assign sentiment class + − review document doc . represent input observation 6 features x1 . . . x6 input shown following table ; Fig . 5.2 shows features sample mini test document . Var Definition Value Fig . 5.2 x1 count ( positive lexicon ) ∈ doc ) 3 x2 count ( negative lexicon ) ∈ doc ) 2 x3 { 1 “ ” ∈ doc 0 otherwise 1 x4 count ( 1st 2nd pronouns ∈ doc ) 3 x5 { 1 “ ! ” ∈ doc 0 otherwise 0 x6 log ( word count doc ) ln ( 66 ) = 4.19 assume moment already learned real-valued weight features , 6 weights corresponding 6 features [ 2.5 , − 5.0 , − 1.2,0.5,2.0,0.7 ] , b = 0.1 . ( discuss next section weights learned . ) weight w1 , example indicates important feature number positive lexicon words ( great , nice , enjoyable , etc . ) positive sentiment decision , w2 tells importance negative lexicon words . Note w1 = 2.5 positive , w2 = − 5.0 , meaning negative words negatively associated positive sentiment decision , twice important positive words . 5.1 • CLASSIFICATION : SIGMOID 5 hokey . virtually surprises , writing second-rate . why enjoyable ? thing , cast great . Another nice touch music . overcome urge get off couch start dancing . sucked , same . x1 = 3 x6 = 4.19 x3 = 1 x4 = 3x5 = 0 x2 = 2 Figure 5.2 sample mini test document showing extracted features vector x . 6 features input review x , P ( + | x ) P ( − | x ) com - puted Eq . 5.5 : p ( + | x ) = P ( Y = 1 | x ) = σ ( w · x + b ) = σ ( [ 2.5 , − 5.0 , − 1.2,0.5,2.0,0.7 ] · [ 3,2,1,3,0,4.19 ] + 0.1 ) = σ ( . 833 ) = 0.70 ( 5.6 ) p ( − | x ) = P ( Y = 0 | x ) = 1 − σ ( w · x + b ) = 0.30 Logistic regression commonly applied sorts NLP tasks , property input feature . Consider task period disambiguation : deciding period end sentence part word , classifying period two classes EOS ( end-of-sentence ) not-EOS . might features like x1 below expressing current word lower case class EOS ( perhaps positive weight ) , current word abbreviations dictionary ( “ Prof . ” ) class EOS ( perhaps negative weight ) . feature express quite complex combination properties . example period following upper case word likely EOS , word itself St . previous word capitalized , period likely part shortening word street . x1 = { 1 “ Case ( wi ) = Lower ” 0 otherwise x2 = { 1 “ wi ∈ AcronymDict ” 0 otherwise x3 = { 1 “ wi = St . & Case ( wi − 1 ) = Cap ” 0 otherwise Designing features : Features generally designed examining training set eye linguistic intuitions linguistic literature domain . careful error analysis training set devset early version system often provides insights features . tasks especially helpful build complex features combi - nations primitive features . saw feature period disambiguation , period word St . less likely end sentence previous word capitalized . logistic regression naive Bayes combination features feature interactions designed hand . featureinteractions 6 CHAPTER 5 • LOGISTIC REGRESSION many tasks ( especially feature values reference specific words ) need large numbers features . Often created automatically via fea - ture templates , abstract specifications features . example bigram templatefeaturetemplates period disambiguation might create feature every pair words occurs period training set . Thus feature space sparse , create feature n-gram exists position training set . feature generally created hash string descriptions . user description feature , “ bigram ( American breakfast ) ” hashed unique integer becomes feature number fi . order avoid extensive human effort feature design , recent research NLP focused representation learning : ways learn features automatically unsupervised way input . introduce methods representation learning Chapter 6 Chapter 7 . Choosing classifier Logistic regression number advantages naive Bayes . Naive Bayes overly strong conditional independence assumptions . Con - sider two features strongly correlated ; fact , imagine just add same feature f1 twice . Naive Bayes treat copies f1 sep - arate , multiplying , overestimating evidence . contrast , logistic regression robust correlated features ; two features f1 f2 perfectly correlated , regression simply assign part weight w1 part w2 . Thus many correlated features , logistic regression assign accurate probability naive Bayes . logistic regression generally works better larger documents datasets common default . Despite less accurate probabilities , naive Bayes still often makes correct classification decision . Furthermore , naive Bayes work extremely well ( - times even better logistic regression ) small datasets ( Ng Jordan , 2002 ) short documents ( Wang Manning , 2012 ) . Furthermore , naive Bayes easy implement fast train ( there’s optimization step ) . still reasonable approach situations . 5.2 Learning Logistic Regression parameters model , weights w bias b , learned ? Logistic regression instance supervised classification know correct label y ( 0 1 ) observation x . system produces via Eq . 5.5 ŷ , system’s estimate true y . learn parameters ( meaning w b ) make ŷ training observation close possible true y . requires 2 components foreshadowed introduction chapter . first metric close current label ( ŷ ) true gold label y . Rather measure similarity , usually talk opposite : distance system output gold output , call distance loss function cost function . next section introduce lossloss function commonly logistic regression neural networks , cross-entropy loss . second thing need optimization algorithm iteratively updating weights minimize loss function . standard algorithm gradient descent ; introduce stochastic gradient descent algorithm following section . 5.3 • CROSS-ENTROPY LOSS FUNCTION 7 5.3 cross-entropy loss function need loss function expresses , observation x , close classifier output ( ŷ = σ ( w · x + b ) ) correct output ( y , 0 1 ) . call : L ( ŷ , y ) = ŷ differs true y ( 5.7 ) via loss function prefers correct class labels train - ing examples likely . called conditional maximum likelihood estimation : choose parameters w , b maximize log probability true y labels training data observations x . resulting loss function negative log likelihood loss , generally called cross-entropy loss . cross-entropyloss derive loss function , applied single observation x . like learn weights maximize probability correct label p ( y | x ) . two discrete outcomes ( 1 0 ) , Bernoulli distribution , express probability p ( y | x ) classifier produces observation following ( keeping mind y = 1 , Eq . 5.8 simplifies ŷ ; y = 0 , Eq . 5.8 simplifies 1 − ŷ ) : p ( y | x ) = ŷ y ( 1 − ŷ ) 1 − y ( 5.8 ) take log sides . turn handy mathematically , hurt ; whatever values maximize probability maximize log probability : log p ( y | x ) = log [ ŷ y ( 1 − ŷ ) 1 − y ] = y log ŷ + ( 1 − y ) log ( 1 − ŷ ) ( 5.9 ) Eq . 5.9 describes log likelihood maximized . order turn loss function ( something need minimize ) , just flip sign Eq . 5.9 . result cross-entropy loss LCE : LCE ( ŷ , y ) = − log p ( y | x ) = − [ y log ŷ + ( 1 − y ) log ( 1 − ŷ ) ] ( 5.10 ) Finally , plug definition ŷ = σ ( w · x + b ) : LCE ( w , b ) = − [ y logσ ( w · x + b ) + ( 1 − y ) log ( 1 − σ ( w · x + b ) ) ] ( 5.11 ) loss function right thing example Fig . 5.2 . loss smaller model’s estimate close correct , bigger model confused . first suppose correct gold label sentiment example Fig . 5.2 positive , i.e . , y = 1 . case model well , Eq . 5.6 indeed gave example higher probability positive ( . 69 ) negative ( . 31 ) . plug σ ( w · x + b ) = . 69 y = 1 Eq . 5.11 , right side equation drops , leading following loss : LCE ( w , b ) = − [ y logσ ( w · x + b ) + ( 1 − y ) log ( 1 − σ ( w · x + b ) ) ] = − [ logσ ( w · x + b ) ] = − log ( . 69 ) = . 37 8 CHAPTER 5 • LOGISTIC REGRESSION contrast , pretend example Fig . 5.2 actually negative , i.e . y = 0 ( perhaps reviewer went say “ bottom line , movie terrible ! beg ! ” ) . case model confused loss higher . plug y = 0 1 − σ ( w · x + b ) = . 31 Eq . 5.6 Eq . 5.11 , left side equation drops : LCE ( w , b ) = − [ y logσ ( w · x + b ) + ( 1 − y ) log ( 1 − σ ( w · x + b ) ) ] = − [ log ( 1 − σ ( w · x + b ) ) ] = − log ( . 31 ) = 1.17 Sure enough , loss first classifier ( . 37 ) less loss second classifier ( 1.17 ) . Why minimizing negative log probability ? per - fect classifier assign probability 1 correct outcome ( y = 1 y = 0 ) probability 0 incorrect outcome . means higher ŷ ( closer 1 ) , better classifier ; lower ŷ ( closer 0 ) , worse clas - sifier . negative log probability convenient loss metric goes 0 ( negative log 1 , loss ) infinity ( negative log 0 , infinite loss ) . loss function ensures probability correct answer maximized , probability incorrect answer minimized ; two sum , increase probability correct answer coming expense - correct answer . called cross-entropy loss , Eq . 5.9 formula cross-entropy true probability distribution y estimated distribution ŷ . know minimize ; next section , find minimum . 5.4 Gradient Descent goal gradient descent find optimal weights : minimize loss function defined model . Eq . 5.12 below , explicitly represent fact loss function L parameterized weights , refer machine learning general θ ( case logistic regression θ = w , b ) : θ̂ = argmin θ 1 m m ∑ = 1 LCE ( y ( ) , x ( ) ; θ ) ( 5.12 ) shall find minimum ( ) loss function ? Gradient descent method finds minimum function figuring direction ( space parameters θ ) function’s slope rising steeply , moving opposite direction . intuition hiking canyon trying descend quickly down river bottom , might look around yourself 360 degrees , find direction ground sloping steepest , walk downhill direction . logistic regression , loss function conveniently convex . convex func-convex tion just minimum ; local minima get stuck , gradient descent starting point guaranteed find minimum . ( contrast , 5.4 • GRADIENT DESCENT 9 loss multi-layer neural networks non-convex , gradient descent get stuck local minima neural network training never find global opti - mum . ) Although algorithm ( concept gradient ) designed direction vectors , first consider visualization case parameter system just single scalar w , shown Fig . 5.3 . random initialization w value w1 , assuming loss function L happened shape Fig . 5.3 , need algorithm tell next iteration move left ( making w2 smaller w1 ) right ( making w2 bigger w1 ) reach minimum . w Loss 0 w1 wmin slope loss w1 negative ( goal ) step gradient descent Figure 5.3 first step iteratively finding minimum loss function , moving w reverse direction slope function . slope negative , need move w positive direction , right . superscripts learning steps , w1 means initial value w ( 0 ) , w2 second step , . gradient descent algorithm answers question finding gradientgradient loss function current point moving opposite direction . gradient function many variables vector pointing direction greatest increase function . gradient multi-variable generalization slope , function variable like Fig . 5.3 , informally think gradient slope . dotted line Fig . 5.3 shows slope hypothetical loss function point w = w1 . slope dotted line negative . Thus find minimum , gradient descent tells go opposite direction : moving w positive direction . magnitude amount move gradient descent value slope d dw f ( x ; w ) weighted learning rate η . higher ( faster ) learning rate means thatlearning rate move w step . change make parameter learning rate times gradient ( slope , single-variable example ) : wt + 1 = wt − η d dw f ( x ; w ) ( 5.13 ) extend intuition function scalar variable w many variables , just move left right , know N-dimensional space ( N parameters make up θ ) move . gradient just vector ; expresses directional components sharpest slope N dimensions . just imagining two weight dimensions ( say weight w bias b ) , gradient might vector two orthogonal components , tells ground slopes w dimension b dimension . Fig . 5.4 shows visualization : 10 CHAPTER 5 • LOGISTIC REGRESSION Cost ( w , b ) w b Figure 5.4 Visualization gradient vector two dimensions w b . actual logistic regression , parameter vector w longer 1 2 , input feature vector x quite long , need weight wi xi . dimension / variable wi w ( plus bias b ) , gradient component tells slope respect variable . Essentially asking : “ small change variable wi influence total loss function L ? ” dimension wi , express slope partial derivative ∂ ∂ wi loss function . gradient defined vector partials . represent ŷ f ( x ; θ ) make dependence θ obvious : ∇ θ L ( f ( x ; θ ) , y ) ) =       ∂ ∂ w1 L ( f ( x ; θ ) , y ) ∂ ∂ w2 L ( f ( x ; θ ) , y ) . . . ∂ ∂ wn L ( f ( x ; θ ) , y )       ( 5.14 ) final equation updating θ based gradient thus θt + 1 = θt − η ∇ L ( f ( x ; θ ) , y ) ( 5.15 ) 5.4.1 Gradient Logistic Regression order update θ , need definition gradient ∇ L ( f ( x ; θ ) , y ) . Recall logistic regression , cross-entropy loss function : LCE ( w , b ) = − [ y logσ ( w · x + b ) + ( 1 − y ) log ( 1 − σ ( w · x + b ) ) ] ( 5.16 ) turns derivative function observation vector x Eq . 5.17 ( interested reader Section 5.8 derivation equation ) : ∂ LCE ( w , b ) ∂ w j = [ σ ( w · x + b ) − y ] x j ( 5.17 ) Note Eq . 5.17 gradient respect single weight w j represents intuitive value : difference true y estimated ŷ = σ ( w · x + b ) observation , multiplied corresponding input value x j . 5.4 • GRADIENT DESCENT 11 5.4.2 Stochastic Gradient Descent Algorithm Stochastic gradient descent online algorithm minimizes loss function computing gradient training example , nudging θ right direction ( opposite direction gradient ) . Fig . 5.5 shows algorithm . function STOCHASTIC GRADIENT DESCENT ( L ( ) , f ( ) , x , y ) returns θ # : L loss function # f function parameterized θ # x set training inputs x ( 1 ) , x ( 2 ) , . . . , x ( n ) # y set training outputs ( labels ) y ( 1 ) , y ( 2 ) , . . . , y ( n ) θ ← 0 repeat til # caption training tuple ( x ( ) , y ( ) ) ( random order ) 1 . Optional ( reporting ) : # tuple ? Compute ŷ ( ) = f ( x ( ) ; θ ) # estimated output ŷ ? Compute loss L ( ŷ ( ) , y ( ) ) # far off ŷ ( ) ) true output y ( ) ? 2 . g ← ∇ θ L ( f ( x ( ) ; θ ) , y ( ) ) # move θ maximize loss ? 3 . θ ← θ − η g # Go way return θ Figure 5.5 stochastic gradient descent algorithm . Step 1 ( computing loss ) report well current tuple . algorithm terminate converges ( gradient < ε ) , progress halts ( example loss starts going up held-out set ) . learning rate η parameter adjusted . high , learner take steps large , overshooting minimum loss func - tion . low , learner take steps small , take long get minimum . common begin learning rate higher value , slowly decrease , function iteration k training ; sometimes notation ηk mean value learning rate iteration k . 5.4.3 Working example walk though single step gradient descent algorithm . sim - plified version example Fig . 5.2 sees single observation x , whose correct value y = 1 ( positive review ) , two features : x1 = 3 ( count positive lexicon words ) x2 = 2 ( count negative lexicon words ) assume initial weights bias θ 0 set 0 , initial learning rate η 0.1 : w1 = w2 = b = 0 η = 0.1 single update step requires compute gradient , multiplied learning rate θ t + 1 = θ t − η ∇ θ L ( f ( x ( ) ; θ ) , y ( ) ) 12 CHAPTER 5 • LOGISTIC REGRESSION mini example three parameters , gradient vector 3 dimen - sions , w1 , w2 , b . compute first gradient follows : ∇ w , b =    ∂ LCE ( w , b ) ∂ w1 ∂ LCE ( w , b ) ∂ w2 ∂ LCE ( w , b ) ∂ b    =   ( σ ( w · x + b ) − y ) x1 ( σ ( w · x + b ) − y ) x2 σ ( w · x + b ) − y   =   ( σ ( 0 ) − 1 ) x1 ( σ ( 0 ) − 1 ) x2 σ ( 0 ) − 1   =   − 0.5x1 − 0.5x2 − 0.5   =   − 1.5 − 1.0 − 0.5   gradient , compute new parameter vector θ 1 moving θ 0 opposite direction gradient : θ 2 =   w1w2 b   − η   − 1.5 − 1.0 − 0.5   =   . 15.1 . 05   step gradient descent , weights shifted : w1 = . 15 , w2 = . 1 , b = . 05 . Note observation x happened positive example . expect seeing negative examples high counts negative words , weight w2 shift negative value . 5.4.4 Mini-batch training Stochastic gradient descent called stochastic chooses single random example time , moving weights improve performance single example . result choppy movements , common compute gradient batches training instances rather single instance . example batch training compute gradient entire dataset . batch training seeing many examples , batch training offers superb estimate di - rection move weights , cost spending lot time processing every single example training set compute perfect direction . compromise mini-batch training : train group m examples ( per-mini-batch haps 512 , 1024 ) less whole dataset . ( m size dataset , batch gradient descent ; m = 1 , back stochas - tic gradient descent ) . Mini-batch training advantage computational efficiency . mini-batches easily vectorized , choosing size mini - batch based computational resources . allows process exam - ples mini-batch parallel accumulate loss , something that’s possible individual batch training . just need define mini-batch versions cross-entropy loss function defined Section 5.3 gradient Section 5.4.1 . extend cross - entropy loss example Eq . 5.10 mini-batches size m . continue notation x ( ) y ( ) mean ith training features training label , respectively . make assumption training examples independent : log p ( training labels ) = log m ∏ = 1 p ( y ( ) | x ( ) ) ( 5.18 ) = m ∑ = 1 log p ( y ( ) | x ( ) ) ( 5.19 ) = − m ∑ = 1 LCE ( ŷ ( ) , y ( ) ) ( 5.20 ) 5.5 • REGULARIZATION 13 cost function mini-batch m examples average loss example : Cost ( w , b ) = 1 m m ∑ = 1 LCE ( ŷ ( ) , y ( ) ) = − 1 m m ∑ = 1 y ( ) logσ ( w · x ( ) + b ) + ( 1 − y ( ) ) log ( 1 − σ ( w · x ( ) + b ) ) ( 5.21 ) mini-batch gradient average individual gradients Eq . 5.17 : ∂ Cost ( w , b ) ∂ w j = 1 m m ∑ = 1 [ σ ( w · x ( ) + b ) − y ( ) ] x ( ) j ( 5.22 ) 5.5 Regularization Numquam ponenda est pluralitas sine necessitate ‘ Plurality never proposed unless needed ’ William Occam problem learning weights make model perfectly match training data . feature perfectly predictive outcome happens occur class , assigned high weight . weights features attempt perfectly fit details training set , fact perfectly , modeling noisy factors just accidentally correlate class . problem called overfitting . good model able generalize well trainingoverfitting generalize data unseen test set , model overfits poor generalization . avoid overfitting , new regularization term R ( θ ) added objectiveregularization function Eq . 5.12 , resulting following objective batch m exam - ples ( slightly rewritten Eq . 5.12 maximizing log probability rather minimizing loss , removing 1m term affect argmax ) : θ̂ = argmax θ m ∑ = 1 logP ( y ( ) | x ( ) ) − αR ( θ ) ( 5.23 ) new regularization term R ( θ ) penalize large weights . Thus setting weights matches training data perfectly — many weights high values — penalized setting matches data little less well , smaller weights . two common ways compute regularization term R ( θ ) . L2 regularization quadratic function ofL2regularization weight values , named ( square ) L2 norm weight values . L2 norm , | | θ | | 2 , same Euclidean distance vector θ origin . θ consists n weights , : R ( θ ) = | | θ | | 22 = n ∑ j = 1 θ 2j ( 5.24 ) 14 CHAPTER 5 • LOGISTIC REGRESSION L2 regularized objective function becomes : θ̂ = argmax θ [ m ∑ 1 = logP ( y ( ) | x ( ) ) ] − α n ∑ j = 1 θ 2j ( 5.25 ) L1 regularization linear function weight values , named L1 normL1regularization | | W | | 1 , sum absolute values weights , Manhattan distance ( Manhattan distance distance walk two points city street grid like New York ) : R ( θ ) = | | θ | | 1 = n ∑ = 1 | θi | ( 5.26 ) L1 regularized objective function becomes : θ̂ = argmax θ [ m ∑ 1 = logP ( y ( ) | x ( ) ) ] − α n ∑ j = 1 | θ j | ( 5.27 ) kinds regularization come statistics , L1 regularization called lasso regression ( Tibshirani , 1996 ) L2 regularization called ridge regression , lasso ridge commonly language processing . L2 regularization easier optimize simple derivative ( derivative θ 2 just 2θ ) , L1 regularization complex ( derivative | θ | non-continuous zero ) . L2 prefers weight vectors many small weights , L1 prefers sparse solutions larger weights many weights set zero . Thus L1 regularization leads sparser weight vectors , , far fewer features . L1 L2 regularization Bayesian interpretations constraints prior weights look . L1 regularization viewed Laplace prior weights . L2 regularization corresponds assuming weights distributed according gaussian distribution mean µ = 0 . gaussian normal distribution , further away value mean , lower probability ( scaled variance σ ) . gaussian prior weights , saying weights prefer value 0 . gaussian weight θ j 1 √ 2πσ2j exp ( − ( θ j − µ j ) 2 2σ2j ) ( 5.28 ) multiply weight gaussian prior weight , thus maximiz - ing following constraint : θ̂ = argmax θ M ∏ = 1 P ( y ( ) | x ( ) ) × n ∏ j = 1 1 √ 2πσ2j exp ( − ( θ j − µ j ) 2 2σ2j ) ( 5.29 ) log space , µ = 0 , assuming 2σ2 = 1 , corresponds θ̂ = argmax θ m ∑ = 1 logP ( y ( ) | x ( ) ) − α n ∑ j = 1 θ 2j ( 5.30 ) same form Eq . 5.25 . 5.6 • MULTINOMIAL LOGISTIC REGRESSION 15 5.6 Multinomial logistic regression Sometimes need two classes . Perhaps might 3-way sentiment classification ( positive , negative , neutral ) . classifying part speech word ( choosing 10 , 30 , even 50 different parts speech ) , assigning semantic labels like named entities semantic relations introduce Chapter 18 . cases multinomial logistic regression , called softmax re - multinomial logistic regression gression ( , historically , maxent classifier ) . multinomial logistic regression target y variable ranges two classes ; know probability y potential class c ∈ C , p ( y = c | x ) . multinomial logistic classifier generalization sigmoid , called softmax function , compute probability p ( y = c | x ) . softmax functionsoftmax takes vector z = [ z1 , z2 , . . . , zk ] k arbitrary values maps probability distribution , value range ( 0,1 ) , values summing 1 . Like sigmoid , exponential function . vector z dimensionality k , softmax defined : softmax ( zi ) = ezi ∑ k j = 1 e z j 1 ≤ ≤ k ( 5.31 ) softmax input vector z = [ z1 , z2 , . . . , zk ] thus vector itself : softmax ( z ) = [ ez1 ∑ k = 1 ezi , ez2 ∑ k = 1 ezi , . . . , ezk ∑ k = 1 ezi ] ( 5.32 ) denominator ∑ k = 1 e zi normalize values probabilities . Thus example vector : z = [ 0.6,1.1 , − 1.5,1.2,3.2 , − 1.1 ] result softmax ( z ) [ 0.055,0.090,0.0067,0.10,0.74,0.010 ] Again like sigmoid , input softmax dot product weight vector w input vector x ( plus bias ) . need separate weight vectors ( bias ) K classes . p ( y = c | x ) = e wc · x + bc k ∑ j = 1 ew j · x + b j ( 5.33 ) Like sigmoid , softmax property squashing values toward 0 1 . Thus inputs larger others , tend push probability toward 1 , suppress probabilities smaller inputs . 16 CHAPTER 5 • LOGISTIC REGRESSION 5.6.1 Features Multinomial Logistic Regression multiclass classification input features need function observation x candidate output class c . Thus notation xi , fi fi ( x ) , discussing features notation fi ( c , x ) , meaning feature particular class c observation x . binary classification , positive weight feature pointed toward y = 1 negative weight toward y = 0 , multiclass classification feature evidence against individual class . look sample features few NLP tasks help understand perhaps unintuitive features functions observation x class c . Suppose text classification , binary classification task assign 3 classes + , − , 0 ( neutral ) document . feature related exclamation marks might negative weight 0 documents , positive weight + − documents : Var Definition Wt f1 ( 0 , x ) { 1 “ ! ” ∈ doc 0 otherwise − 4.5 f1 ( + , x ) { 1 “ ! ” ∈ doc 0 otherwise 2.6 f1 ( − , x ) { 1 “ ! ” ∈ doc 0 otherwise 1.3 5.6.2 Learning Multinomial Logistic Regression Multinomial logistic regression slightly different loss function binary lo - gistic regression softmax rather sigmoid classifier . loss function single example x sum logs K output classes : LCE ( ŷ , y ) = − K ∑ k = 1 1 { y = k } log p ( y = k | x ) = − K ∑ k = 1 1 { y = k } log e wk·x + bk ∑ K j = 1 e w j · x + b j ( 5.34 ) makes function 1 { } evaluates 1 condition brackets true 0 otherwise . gradient single example turns similar gradient logistic regression , although show derivation . difference value true class k ( 1 ) probability classifier outputs class k , weighted value input xk : ∂ LCE ∂ wk = − ( 1 { y = k } − p ( y = k | x ) ) xk = − ( 1 { y = k } − e wk·x + bk ∑ K j = 1 e w j · x + b j ) xk ( 5.35 ) 5.7 • INTERPRETING MODELS 17 5.7 Interpreting models Often know just correct classification observation . know why classifier made decision . , decision interpretable . Interpretability hard define strictly , theinterpretable core idea humans know why algorithms reach conclu - sions . features logistic regression often human-designed , way understand classifier’s decision understand role feature plays decision . Logistic regression combined statistical tests ( likelihood ratio test , Wald test ) ; investigating particular feature significant tests , inspecting magnitude ( large weight w associated feature ? ) help interpret why classifier made decision makes . enormously important building transparent models . Furthermore , addition classifier , logistic regression NLP many fields widely analytic tool testing hypotheses effect various explanatory variables ( features ) . text classification , perhaps know logically negative words ( , , never ) likely asso - ciated negative sentiment , negative reviews movies likely discuss cinematography . , necessary control po - tential confounds : factors might influence sentiment ( movie genre , year made , perhaps length review words ) . might study - ing relationship NLP-extracted linguistic features non-linguistic outcomes ( hospital readmissions , political outcomes , product sales ) , need control confounds ( age patient , county voting , brand product ) . cases , logistic regression allows test feature associated outcome beyond effect features . 5.8 Advanced : Deriving Gradient Equation section give derivation gradient cross-entropy loss func - tion LCE logistic regression . start quick calculus refreshers . First , derivative ln ( x ) : d dx ln ( x ) = 1 x ( 5.36 ) Second , ( elegant ) derivative sigmoid : dσ ( z ) dz = σ ( z ) ( 1 − σ ( z ) ) ( 5.37 ) Finally , chain rule derivatives . Suppose computing derivativechain rule composite function f ( x ) = u ( v ( x ) ) . derivative f ( x ) derivative u ( x ) respect v ( x ) times derivative v ( x ) respect x : d f dx = du dv · dv dx ( 5.38 ) First , know derivative loss function respect single weight w j ( need compute weight , bias ) : 18 CHAPTER 5 • LOGISTIC REGRESSION ∂ LL ( w , b ) ∂ w j = ∂ ∂ w j − [ y logσ ( w · x + b ) + ( 1 − y ) log ( 1 − σ ( w · x + b ) ) ] = − [ ∂ ∂ w j y logσ ( w · x + b ) + ∂ ∂ w j ( 1 − y ) log [ 1 − σ ( w · x + b ) ] ] ( 5.39 ) Next , chain rule , relying derivative log : ∂ LL ( w , b ) ∂ w j = − y σ ( w · x + b ) ∂ ∂ w j σ ( w · x + b ) − 1 − y 1 − σ ( w · x + b ) ∂ ∂ w j 1 − σ ( w · x + b ) ( 5.40 ) Rearranging terms : ∂ LL ( w , b ) ∂ w j = − [ y σ ( w · x + b ) − 1 − y 1 − σ ( w · x + b ) ] ∂ ∂ w j σ ( w · x + b ) ( 5.41 ) plugging derivative sigmoid , chain rule time , end up Eq . 5.42 : ∂ LL ( w , b ) ∂ w j = − [ y − σ ( w · x + b ) σ ( w · x + b ) [ 1 − σ ( w · x + b ) ] ] σ ( w · x + b ) [ 1 − σ ( w · x + b ) ] ∂ ( w · x + b ) ∂ w j = − [ y − σ ( w · x + b ) σ ( w · x + b ) [ 1 − σ ( w · x + b ) ] ] σ ( w · x + b ) [ 1 − σ ( w · x + b ) ] x j = − [ y − σ ( w · x + b ) ] x j = [ σ ( w · x + b ) − y ] x j ( 5.42 ) 5.9 Summary chapter introduced logistic regression model classification . • Logistic regression supervised machine learning classifier extracts real-valued features input , multiplies weight , sums , passes sum sigmoid function generate probability . threshold make decision . • Logistic regression two classes ( e.g . , positive negative sentiment ) multiple classes ( multinomial logistic regression , ex - ample n-ary text classification , part-of-speech labeling , etc . ) . • Multinomial logistic regression softmax function compute proba - bilities . • weights ( vector w bias b ) learned labeled training set via loss function , cross-entropy loss , minimized . • Minimizing loss function convex optimization problem , iterative algorithms like gradient descent find optimal weights . • Regularization avoid overfitting . • Logistic regression useful analytic tools , ability transparently study importance individual features . BIBLIOGRAPHICAL HISTORICAL NOTES 19 Bibliographical Historical Notes Logistic regression developed field statistics , analysis binary data 1960s , particularly common medicine ( Cox , 1969 ) . Starting late 1970s became widely linguistics formal foundations study linguistic variation ( Sankoff Labov , 1979 ) . Nonetheless , logistic regression become common natural language pro - cessing 1990s , seems appeared simultaneously two directions . first source neighboring fields information retrieval speech processing , made regression , lent many statistical techniques NLP . Indeed early logistic regression document routing first NLP applications ( LSI ) embeddings word representations ( Schütze et al . , 1995 ) . same time early 1990s logistic regression developed ap - plied NLP IBM Research name maximum entropy modeling ormaximumentropy maxent ( Berger et al . , 1996 ) , seemingly independent statistical literature . Un - der name applied language modeling ( Rosenfeld , 1996 ) , part-of-speech tagging ( Ratnaparkhi , 1996 ) , parsing ( Ratnaparkhi , 1997 ) , coreference resolution ( Kehler , 1997 ) , text classification ( Nigam et al . , 1999 ) . classification found machine learning textbooks ( Hastie et al . 2001 , Witten Frank 2005 , Bishop 2006 , Murphy 2012 ) . Exercises 20 Chapter 5 • Logistic Regression Berger , . , Della Pietra , S . . , Della Pietra , V . J . ( 1996 ) . maximum entropy approach natural language process - ing . Computational Linguistics , 22 ( 1 ) , 39 – 71 . Bishop , C . M . ( 2006 ) . Pattern recognition machine learning . Springer . Cox , D . ( 1969 ) . Analysis Binary Data . Chapman Hall , London . Hastie , T . , Tibshirani , R . J . , Friedman , J . H . ( 2001 ) . Elements Statistical Learning . Springer . Kehler , . ( 1997 ) . Probabilistic coreference information extraction . EMNLP 1997 , 163 – 173 . Murphy , K . P . ( 2012 ) . Machine learning : probabilistic perspective . MIT press . Ng , . Y . Jordan , M . . ( 2002 ) . discriminative vs . generative classifiers : comparison logistic regression naive bayes . NIPS 14 , 841 – 848 . Nigam , K . , Lafferty , J . D . , McCallum , . ( 1999 ) . maximum entropy text classification . IJCAI-99 work - shop machine learning information filtering , 61 – 67 . Ratnaparkhi , . ( 1996 ) . maximum entropy part-of-speech tagger . EMNLP 1996 , 133 – 142 . Ratnaparkhi , . ( 1997 ) . linear observed time statistical parser based maximum entropy models . EMNLP 1997 , 1 – 10 . Rosenfeld , R . ( 1996 ) . maximum entropy approach adaptive statistical language modeling . Computer Speech Language , 10 , 187 – 228 . Sankoff , D . Labov , W . ( 1979 ) . variable rules . Language society , 8 ( 2-3 ) , 189 – 222 . Schütze , H . , Hull , D . . , Pedersen , J . ( 1995 ) . com - parison classifiers document representations routing problem . SIGIR-95 , 229 – 237 . Tibshirani , R . J . ( 1996 ) . Regression shrinkage selection via lasso . Journal Royal Statistical Society . Series B ( Methodological ) , 58 ( 1 ) , 267 – 288 . Wang , S . Manning , C . D . ( 2012 ) . Baselines bigrams : Simple , good sentiment topic classification . ACL 2012 , 90 – 94 . Witten , . H . Frank , E . ( 2005 ) . Data Mining : Practical Machine Learning Tools Techniques ( 2nd Ed . ) . Mor - gan Kaufmann .