Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 26 Dialogue Systems Chat-bots Les lois de la conversation sont en général de ne s’y appesantir sur aucun ob - jet , mais de passer légèrement , sans effort et sans affectation , d’un sujet à un autre ; de savoir y parler de choses frivoles comme de choses sérieuses rules conversation , general , dwell subject , pass lightly another effort affectation ; know speak trivial topics well serious ones ; 18th C . Encyclopedia Diderot , start entry conversation literature fantastic abounds inanimate objects magically endowed sentience gift speech . Ovid’s statue Pygmalion Mary Shelley’s Frankenstein , something deeply moving creating something chat . Legend finishing sculpture Moses , Michelangelo thought lifelike tapped knee commanded speak . Per - haps shouldn’t surprising . Language mark humanity sentience , conversation dialogueconversation dialogue fundamental specially privileged arena language . first kind language learn children , , kind language commonly indulge , ordering curry lunch buying spinach , participating busi - ness meetings talking families , booking air - line flights complaining weather . chapter introduces fundamental algorithms conversational agents , conversationalagent dialogue systems . programs communicate users natural languagedialogue system ( text , speech , ) , fall two classes . Task-oriented dialogue agents conversation users help complete tasks . Dialogue agents digital assistants ( Siri , Alexa , Google / Home , Cortana , etc . ) , give directions , control appliances , find restaurants , make calls . Conversational agents answer questions cor - porate websites , interface robots , even social good : DoNotPay “ robot lawyer ” helps people challenge incorrect parking fines , apply emergency housing , claim asylum refugees . contrast , chatbots systems designed extended conversations , set up mimic unstructured conversations ‘ chats ’ characteristic human-human interaction , mainly en - tertainment , practical purposes like making task-oriented agents natural . Section 26.2 discuss three major chatbot architectures : rule - based systems , information retrieval systems , encoder-decoder models . Sec - tion 26.3 turn task-oriented agents , introducing frame-based architecture ( GUS architecture ) underlies modern task-based systems . 2 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS 26.1 Properties Human Conversation Conversation humans intricate complex joint activity . attempt design conversational agent converse humans , crucial understand something humans converse . Consider phenomena occur conversation human travel agent human client excerpted Fig . 26.1 . C1 : . . . need travel . A1 : , day travel ? C2 : OK uh need meeting that’s 12th 15th . A2 : flying city ? C3 : Seattle . A3 : time like leave Pittsburgh ? C4 : Uh hmm think there’s many options non-stop . A4 : Right . There’s three non-stops today . C5 : ? A5 : first departs PGH 10:00am arrives Seattle 12:05 time . second flight departs PGH 5:55pm , arrives Seattle 8pm . last flight departs PGH 8:15pm arrives Seattle 10:28pm . C6 : OK take 5ish flight night 11th . A6 : 11th ? OK . Departing 5:55pm arrives Seattle 8pm , U.S . Air flight 115 . C7 : OK . A7 : returning 15th ? C8 : Uh , yeah , end day . A8 : OK . There’s #two non-stops . . . # C9 : #Act . . . actually # , day week 15th ? A9 : Friday . C10 : Uh hmm . consider staying extra day til Sunday . A10 : OK . . . OK . Sunday . . . Figure 26.1 Part phone conversation human travel agent ( ) human client ( C ) . passages framed # A8 C9 indicate overlaps speech . Turns dialogue sequence turns ( A1 , B1 , A2 , ) , single contributionturn dialogue ( game : take turn , take turn , , ) . turn consist sentence ( like C1 ) , although might short single word ( C7 ) long multiple sentences ( A5 ) . Turn structure important implications spoken dialogue . system know stop talking ; client interrupts ( A8 C9 ) , system know stop talking ( user might making correction ) . system know start talking . example , time conversation , speakers start turns almost immediately speaker finishes , long pause , people able ( time ) detect person finish talking . Spoken dialogue systems detect user speaking , process utterance respond . task — called endpointing endpoint detection — quite challenging ofendpointing noise people often pause middle turns . 26.1 • PROPERTIES HUMAN CONVERSATION 3 Speech Acts key insight conversation — due originally philosopher Wittgenstein ( 1953 ) worked fully Austin ( 1962 ) — utterance dialogue kind action performed speaker . actions com - monly called speech acts dialog acts : here’s taxonomy consisting 4 majorspeech acts classes ( Bach Harnish , 1979 ) : Constatives : committing speaker something’s case ( answering , claiming , confirming , denying , disagreeing , stating ) Directives : attempts speaker get addressee something ( advising , ask - ing , forbidding , inviting , ordering , requesting ) Commissives : committing speaker future course action ( promising , planning , vowing , betting , opposing ) Acknowledgments : express speaker’s attitude regarding hearer respect - cial action ( apologizing , greeting , thanking , accepting acknowledgment ) user asking person dialogue system something ( ‘ Turn up music ’ ) issuing DIRECTIVE . Asking question requires answer way issuing DIRECTIVE : sense system ( C2 ) “ day travel ? ” system ( politely ) commanding system answer . contrast , user stating constraint ( like C1 ‘ need travel ’ ) issuing CONSTATIVE . user thanking system issuing ACKNOWLEDGMENT . speech act expresses important component intention speaker ( writer ) saying . Grounding dialogue just series independent speech acts , rather collective act performed speaker hearer . Like collective acts , important participants establish agree , called common groundcommonground ( Stalnaker , 1978 ) . Speakers grounding other’s utterances . Ground-grounding ing means acknowledging hearer understood speaker ; like ACK confirm receipt data communications ( Clark , 1996 ) . ( People need ground - ing non-linguistic actions well ; reason elevator button lights up pressed acknowledge elevator indeed called ( Norman , 1988 ) . ) Humans constantly ground other’s utterances . ground explicitly saying “ OK ” , agent A8 A10 . ground repeating person ; utterance A1 agent repeats “ ” , demonstrating understanding client . notice client answers question , agent begins next question “ ” . “ ” implies new question ‘ addition ’ old question , again indicating client agent successfully understood answer last question . Subdialogues Dialogue Structure Conversations structure . Consider , example , local structure speech acts discussed field conversational analysis ( Sacks et al . , 1974 ) . conversationalanalysis QUESTIONS set up expectation ANSWER . PROPOSALS followed ACCEPTANCE ( REJECTION ) . COMPLIMENTS ( “ Nice jacket ! ” ) often give rise DOWNPLAYERS ( “ Oh , old thing ? ” ) . pairs , called adjacency pairs areadjacency pair composed first pair part second pair part ( Schegloff , 1968 ) , expectations help systems decide actions take . 4 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS , dialogue acts always followed immediately second pair part . two parts separated side sequence ( Jefferson 1972 ) sub-side sequence dialogue . example utterances C9 A10 constitute correction subdialoguesubdialogue ( Litman 1985 , Litman Allen 1987 , Chu-Carroll Carberry 1998 ) : C9 : #Act . . . actually # , day week 15th ? A9 : Friday . C10 : Uh hmm . consider staying extra day til Sunday . A10 : OK . . . OK . Sunday . . . question C9 interrupts prior discourse , agent looking 15 return flight . agent answer question realize ‘ ’ consider staying . . . til Sunday ” means client probably like change plan , go back finding return flights , 17th . Another side sequence clarification question , form subdia - logue REQUEST RESPONSE . especially common dialogue systems speech recognition errors causes system ask clari - fications repetitions like following : User : going UNKNOWN WORD 5th ? System : , going 5th ? User : Going Hong Kong . System : OK , flights . . . addition side-sequences , questions often presequences , like fol-presequence lowing example user starts question system’s capabilities ( “ make train reservations ” ) making request . User : make train reservations ? System : Yes . User : Great , like reserve seat 4pm train New York . Initiative Sometimes conversation completely controlled participant . example reporter interviewing chef might ask questions , chef responds . say reporter case conversational initiative ( Walker Whittaker , initiative 1990 ) . normal human-human dialogue , , common initiative shift back forth participants , sometimes answer questions , sometimes ask , sometimes take conversations new directions , sometimes . ask question , respond asking clarify something , leads conversation sorts ways . call interactions mixed initiative . Mixed initiative , norm human-human conversations , diffi - cult dialogue systems achieve . easier design dialogue systems passive responders . question answering systems saw Chapter 25 , simple search engines , initiative lies completely user . user - initiative systems , user specifies query , systems responds . user specify another query . Alternatively , experience stuck bad dialogue system asks question gives opportu - nity anything answer . system-initiative architectures frustrating . 26.2 • CHATBOTS 5 Inference Implicature Inference important dialogue understanding . Consider client’s response C2 , repeated : A1 : , day travel ? C2 : OK uh need meeting that’s 12th 15th . Notice client fact answer agent’s question . client merely mentions meeting certain time . licenses agent infer client mentioning meeting inform agent travel dates ? speaker seems expect hearer draw certain inferences ; words , speaker communicating information seems present uttered words . kind example pointed Grice ( 1975 , 1978 ) part theory conversational implicature . Implicature means particu-implicature lar class licensed inferences . Grice proposed enables hearers draw inferences conversation guided set maxims , general heuristics play guiding role interpretation conversational utterances . maxim maxim relevance speakers attempt relevant , relevance just utter random speech acts . client mentions meeting 12th , agent reasons ‘ relevance mentioning meeting . ? ’ . agent knows precondition meeting ( least Web conferencing ) place meeting held , maybe meeting reason travel , , people like arrive day meeting , agent infer flight 11th . subtle characteristics human conversations ( turns , speech acts , ground - ing , dialogue structure , initiative , implicature ) among reasons dif - ficult build dialogue systems carry natural conversations humans . Many challenges active areas dialogue systems research . 26.2 Chatbots simplest kinds dialogue systems chatbots , systems carry onchatbot extended conversations goal mimicking unstructured conversations ‘ chats ’ characteristic informal human-human interaction . systems often entertainment value , Cleverbot , simple IR - based chatbot capable carrying kinds conversations shown Fig . 26.2 , Microsoft’s XiaoIce system1 ( Zhou et al . , 2018 ) , sophisticated commercial system chats people text messaging platforms ( Fig . 26.3 ) . Yet starting first system , ELIZA ( Weizenbaum , 1966 ) , chatbots practical purposes , testing theories psychological counseling . Like practically everything else language processing , chatbot architectures fall two classes : rule-based systems corpus-based systems . Rule-based systems include early influential ELIZA PARRY systems . Corpus-based systems mine large datasets human-human conversations , information retrieval ( IR-based systems simply copy human’s response 1 XiaoIce somewhat confusing translation systems Chinese name 小 冰 , ‘ Xiao Bing ’ , means Little Bing Little Ice . 6 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS User : like Doctor ? Cleverbot : Yes , funny . User : favorite show ? Cleverbot : . User : name ? Cleverbot : Jason already told . User : real name . Cleverbot : Stephanie real name . User : thought Jason ? Cleverbot : come conclusion . User : . Cleverbot : homework . Figure 26.2 Chatbots : conversation user Cleverbot , IR-based chatbot ( Carpenter , 2017 ) . ［ 人类 ］ 小 冰 你好 [ Human ] Hello , XiaoIce . ［ 小 冰 ］ 你 好呀 ， 你 头 像 好 有趣 ， 感觉 在 哪 见过 [ XiaoIce ] Hello , profile pic interesting . Like seen somewhere . ［ 人类 ］ 是 吗 [ Human ] Really ? ［ 小 冰 ］ 提 不是 哪 部 动画 里 的 呀 [ XiaoIce ] comic character ? ［ 人类 ］ 你 怎么 知道 的 [ Human ] know ? ［ 小 冰 ］ 你 猜 [ XiaoIce ] Guess . ［ 人类 ］ 难道 你是 真人 吗 ？ [ Human ] human ? ［ 小 冰 ］ 这个 ， 你 和 我 多 聊聊 就 知道 了 [ XiaoIce ] Well , know chatting . ［ 人类 ］ 你 不是 人工 智能 机器 人 么 [ Human ] Al chatbot ? ［ 小 冰 ］ 是 又 怎样 ， 我不会 冷冰冰 的 对 待 你的 [ XiaoIce ] ? indifferent . ［ 人类 ］ 你 都有 什么 功能 啊 [ Human ] functions ? ［ 小 冰 ］ 我有 200 多个 skills ， 来 ， 说出 你的 需求 [ XiaoIce ] 200 skills . Just name need . Figure 26.3 sample Chinese conversation XiaoIce chatbot , together translation Zhou et al . ( 2018 ) . previous conversation ) machine translation paradigm neural network sequence-to-sequence systems , learn map user utterance system response . 26.2.1 Rule-based chatbots : ELIZA PARRY ELIZA ( Weizenbaum , 1966 ) important chatbot dialogue system history field , take time describe algorithm de - tail . ELIZA designed simulate Rogerian psychologist , based branch clinical psychology whose methods involve drawing patient reflecting patient’s statements back . Rogerian psychology rare type conver - sation , Weizenbaum points , “ assume pose knowing almost nothing real world ” . patient “ went long boat ride ” psychiatrist “ Tell boats ” , assume know boat , rather assume conversational goal . chatbots trying pass Turing test choose domain similar properties . Fig . 26.4 shows famous ELIZA conversation , ELIZA seems 26.2 • CHATBOTS 7 Men alike . WAY They’re always bugging something . THINK SPECIFIC EXAMPLE Well boyfriend made come . BOYFRIEND MADE COME depressed time . SORRY HEAR DEPRESSED . . . . ELSE FAMILY TAKES CARE ? father FATHER like father ways . RESEMBLANCE aggressive think notice . MAKES THINK AGGRESSIVE argue . WHY THINK ARGUE afraid . PLEASE BELIEVE AFRAID father afraid everybody . ELSE COMES MIND THINK FATHER Bullies . ANYTHING FACT BOYFRIEND MADE COME Figure 26.4 Sample ELIZA dialogue Weizenbaum ( 1966 ) . simulate giving sophisticated therapeutic advice , excerpted ( deleting material . . . ) full interaction original paper ( Weizenbaum , 1966 ) . summarized briefly Chapter 2 , ELIZA worked pattern / transform rules like following : ( 0 0 ) [ pattern ] - > ( MAKES THINK 3 ) [ transform ] ELIZA pattern , 0 means Kleene * , transform rules numbers index constituent pattern . Thus number 3 refers second 0 first pattern . rule transfer hate : MAKES THINK HATE ELIZA pattern / rule linked keyword might occur user sen - tence . architecture summarized Fig . 26.5 . Keywords associated rank , specific words highly ranked , general words ranking lower . Consider following user sen - tence : 8 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS function ELIZA GENERATOR ( user sentence ) returns response Find word w sentence highest keyword rank w exists Choose highest ranked rule r w matches sentence response ← Apply transform r sentence w = ‘ ’ future ← Apply transformation ‘ memory ’ rule list sentence Push future onto memory stack else ( keyword applies ) response ← Apply transform NONE keyword sentence response ← Pop top response memory stack return ( response ) Figure 26.5 simplified sketch ELIZA algorithm . power algorithm comes particular transforms associated keyword . know everybody laughed word “ ” , sentence match following rule whose keyword : ( * ) - > ( say 2 ) producing : SAY KNOW EVERYBODY LAUGHED Weizenbaum points , “ ” general word keywords lead general responses . keyword “ everybody ” interest - ing , someone universals like everybody always probably “ referring quite specific event person ” . , ELIZA prefers respond pattern associated specific keyword everybody ( implementing just assigning ” everybody ” rank 5 ” ” rank 0 lexicon ) , whose rule thus transforms sentence : PARTICULAR THINKING ? keyword matches , ELIZA chooses non-commital response like “ PLEASE GO ” , “ THAT’S INTERESTING ” , “ ” . Finally , ELIZA clever memory trick accounts last sen - tence conversation . Whenever word “ ” highest ranked keyword , ELIZA randomly select transform MEMORY list , apply sentence , store stack : ( MEMORY ( 0 0 = DISCUSS FURTHER WHY 3 ) ( 0 0 = EARLIER 3 ) ( 0 0 = ANYTHING FACT 3 Later , keyword matches sentence , ELIZA return top MEM - ORY queue . 2 2 Fun fact : structure queue , MEMORY trick earliest known hierarchical model discourse natural language processing . 26.2 • CHATBOTS 9 People became deeply emotionally involved program . Weizenbaum tells story staff ask Weizenbaum leave room talked ELIZA . Weizenbaum suggested might store ELIZA conversations later analysis , people immediately pointed privacy implications , suggested quite private conversations ELIZA , despite knowing just software . ELIZA’s framework still today ; modern chatbot system tools like ALICE based updated versions ELIZA’s pattern / action architecture . few years ELIZA , another chatbot clinical psychology focus , PARRY ( Colby et al . , 1971 ) , study schizophrenia . addition ELIZA - like regular expressions , PARRY system included model own mental state , affect variables agent’s levels fear anger ; certain topics conversation might lead PARRY become angry mistrustful . PARRY’s anger variable high , choose set “ hostile ” outputs . input mentions delusion topic , increase value fear variable begin express sequence statements related delusion . Parry first known system pass Turing test ( 1972 ! ) ; psychiatrists distin - guish text transcripts interviews PARRY transcripts interviews real paranoids ( Colby et al . , 1972 ) . 26.2.2 Corpus-based chatbots Corpus-based chatbots , hand-built rules , mine conversations human-human conversations , ( sometimes mine human sides human-machine conversations ) . systems enormously data-intensive ; Serban et al . ( 2018 ) estimate training modern chatbots require hundreds millions even billions words . Many corpora , including large spoken conversational corpora like Switchboard corpus American English telephone conversations ( God - frey et al . , 1992 ) various CALLHOME CALLFRIEND telephone con - versational corpora many languages . Many systems train movie dialogue , available great quantities various corpora ( Lison Tiedemann , 2016 , inter alia ) , resembles natural conversation many ways ( Forchini , 2013 ) . Text microblogging sites like Twitter ( Ritter et al . , 2010 ) Weibo ( 微 博 ) , datasets crowdworker conversations like Topical-Chat ( Gopalakrishnan et al . , 2019 ) . Many corpora focus specific topics , topical chatbots . Serban et al . ( 2018 ) comprehensive summary available corpora . Another common technique extract possible responses non-dialogue corpora , chatbot tell stories mention facts ac - quired way . Finally , once chatbot put practice , turns humans respond chatbot additional conversational data training . XiaoIce system collects stores human-machine conversations XiaoIce users , resulting dataset 30 billion conversation pairs . crucial cases remove personally identifiable information ( PII ) ; Section 26.6.1 . two main architectures corpus-based chatbots : information retrieval , machine learned sequence transduction . Like rule-based chatbots ( unlike frame - based dialogue systems ) , corpus-based chatbots little modeling conversational context . tend focus generating single response turn appropriate user’s immediately previous utterance two . 10 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS reason often called response generation systems . Corpus-based chat-responsegeneration bots thus similarity question answering systems , focus single responses ignoring context larger conversational goals . IR-based chatbots principle behind information retrieval based chatbots respond user’s turn X repeating appropriate turn Y corpus natural ( human ) text sort described prior section . corpus user’s sentence , IR-based systems retrieval algorithm choose appropriate response corpus . two simplest methods following : 1 . Return response similar turn : user query q con - versational corpus C , find turn t C similar q ( example highest cosine q ) return following turn , i.e . human response t C : r = response ( argmax t ∈ C qT t | | q | | t | | ) ( 26.1 ) idea look turn resembles user’s turn , re - turn human response turn ( Jafarpour et al . 2009 , Leuski Traum 2011 ) . 2 . Return similar turn : user query q conversational corpus C , return turn t C similar q ( example highest cosine q ) : r = argmax t ∈ C qT t | | q | | t | | ( 26.2 ) idea directly match users query q turns C , good response often share words semantics prior turn . case , similarity function , cosines computed words ( weighted tf-idf ) commonly , cosines kind sentence embeddings . Although returning response similar turn seems like - tuitive algorithm , returning similar turn seems work better practice , perhaps selecting response adds another layer indirection allow noise ( Ritter et al . 2011 , Wang et al . 2013 ) . IR-based approach extended features just words q . example entire conversation user far quite helpful user’s query short ( like “ Yes ” “ OK ” ) . Infor - mation user sentiment information play role . IR-based approach even draw responses narrative ( non-dialogue ) text . COBOT chatbot ( Isbell et al . , 2000 ) pioneered approach , generating responses selecting sentences corpus combined Unabomber Manifesto Theodore Kaczynski , articles alien abduction , scripts “ Big Lebowski ” “ Planet Apes ” . Chatbots generate informative turns answers user questions texts like Wikipedia draw sentences might contain answers ( Yan et al . , 2016 ) . XiaoIce similarly collects sentences public lectures news articles searches IR based query expansion user’s turn respond turns like “ Tell something Beijing ” ( Zhou et al . , 2018 ) . Encoder decoder chatbots 26.2 • CHATBOTS 11 “ ho ! ” . “ ho ! ” Motty . “ ho ! ho ! ” “ ho ! ho ! ho ! ” seemed rather difficult go conversation . ” Wodehouse Man Jeeves alternate way corpus generate dialogue think response generation task transducing user’s prior turn system’s turn . basically machine learning version Eliza ; system learns corpus transduce question answer . idea first developed phrase-based machine translation ( Ritter et al . , 2011 ) translate user turn system response . quickly became clear , , task response generation different machine trans - lation . machine translation , words phrases source target sentences tend align well , conversation , user utterance share words phrases coherent response . , ( roughly contemporaneously Shang et al . 2015 , Vinyals Le 2015 , Sordoni et al . 2015 ) transduction models response generation modeled encoder-decoder models ( Chapter 11 ) , shown Fig . 26.6 . ? fine . EOS Encoding Decoding EOS fine . Figure 26.6 encoder decoder model neural response generation dialogue . number modifications required basic encoder-decoder model adapt task response generation . example basic encoder-decoder models tendency produce predictable repetitive dull re - sponses like “ OK ” “ know ” shut down conversation . addressed changing objective function seq2seq model training mutual information objective , modifying beam decoder keep diverse responses beam ( Li et al . , 2016a ) . Another problem simple encoder-decoder response generation architecture inability model longer prior context conversation . allowing model prior turns , hierarchical model summarizes information multiple prior turns ( Lowe et al . , 2017b ) . Finally , encoder-decoder response generators focus generating single responses , tend good job continuously generating responses cohere multiple turns . addressed reinforcement learning , well techniques like adversarial networks , learn choose responses make overall conversation natural ( Li et al . 2016b , Li et al . 2017 ) . Fig . 26.7 shows sample responses generated vanilla SEQ2SEQ model , model trained adversarial algorithm produce responses harder distinguish human responses ( Li et al . , 2017 ) . 12 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS Input tell . . . long falling sickness ? Vanilla-SEQ2SEQ doctor . Adversarial few months , guess . Input doctors test sammy ’ s response conditioning . Vanilla-SEQ2SEQ sammy wrote test sammy wrote test . Adversarial took pills . Input didn ’ t success problem commander . Vanilla-SEQ2SEQ ’ re ones system . Adversarial find someone else ? Figure 26.7 Sample responses generated SEQ2SEQ model trained vanilla maximum likelihood objective , adversarially trained produce sentences hard adversary distinguish human sentences ( Li et al . , 2017 ) . Evaluating Chatbots Chatbots generally evaluated humans . slot-filling evaluations task-based dialogue ( Section 26.5 ) appropriate task ( Artstein et al . , 2009 ) , word-overlap metrics like BLEU comparing chatbot’s response human response turn correlate poorly human judgments ( Liu et al . , 2016 ) . BLEU performs poorly many possible responses turn ; word-overlap metrics work best space responses small lexically overlapping , case machine translation . human evaluation required evaluating chatbots , beginning models automatic evaluation . ADEM ( Lowe et al . , 2017a ) classifier trained set responses labeled humans appropriate , learns predict label dialogue context words system response . Another paradigm adversarial evaluation ( Bowman et al . 2016 , Kannan andadversarialevaluation Vinyals 2016 , Li et al . 2017 ) , inspired Turing test . idea train “ Turing-like ” evaluator classifier distinguish human-generated responses machine-generated responses . successful response generation sys - tem fooling evaluator , better system . 26.3 GUS : Simple Frame-based Dialogue Systems turn task-based dialogue , dialogue system goal helping user solve task like making airplane reservation buying prod - uct . section introduce simple GUS architecture task-basedGUS dialogue . Although quite old , ( first introduced 1977 influen - tial GUS system travel planning ( Bobrow et al . , 1977 ) ) GUS architecture astonishingly long-lived underlies modern commercial digital assistants . Figure 26.8 shows conversation original GUS system . modern task-based dialogue systems , simple GUS architecture describe , sophisticated dialogue state architectures turn todialogue state following section , based around frames . frame kind knowledgeframe structure representing kinds intentions system extract user sen - tences , consists collection slots , take set possibleslot values . Together set frames sometimes called domain ontology . set slots task-based dialogue frame specifies system needs know , filler slot constrained values particular semantic 26.3 • GUS : SIMPLE FRAME-BASED DIALOGUE SYSTEMS 13 GUS : Hello . name GUS . help plan simple trip air . go ? Client : go San Diego 28 GUS : time leave ? Client : San Diego 10 GUS : like P.S.A . flight 102 arrives 9.15 ? Client : next flight ? GUS : Air California flight 310 leaves 8.30am arrives 10.00am Client : take first GUS : name make reservation , please ? Client : Dan Bobrow GUS : confirmed following flight : P.S.A . flight 102 Wednesday 28 San Jose San Diego leaves 7.30 arrives 9.15 . GUS : date return ? Client : Friday evening . GUS : like flight leaves 7.45 pm ? Client : That’s fine . GUS : confirmed following flight : P.S.A . flight 307 Friday 30 San Diego San Jose leaves 7.45 pm arrives 9.30 pm Thank calling . Goodbye Figure 26.8 travel domain : transcript actual dialogue GUS system Bobrow et al . ( 1977 ) . P.S.A . Air California airlines period . type . travel domain , example , slot might type city ( hence take values like San Francisco , Hong Kong ) type date , airline , time . Slot Type Question Template ORIGIN CITY city “ city leaving ? ” DESTINATION CITY city “ going ? ” DEPARTURE TIME time “ like leave ? ” DEPARTURE DATE date “ day like leave ? ” ARRIVAL TIME time “ arrive ? ” ARRIVAL DATE date “ day like arrive ? ” Figure 26.9 frame frame-based dialogue system , showing type slot question fill slot . Types GUS , modern frame-based dialogue agents , hierarchical structure ; example date type GUS itself frame slots types like integer members sets weekday names : DATE MONTH : NAME YEAR : INTEGER DAY :( BOUNDED-INTEGER 1 31 ) WEEKDAY :( MEMBER ( Sunday Monday Tuesday Wednesday Thursday Friday Saturday ) ) 26.3.1 Control structure frame-based dialogue control architecture frame-based dialogue systems , various forms modern systems like Apple’s Siri , Amazon’s Alexa , Google Assistant , designed around frame . system’s goal fill slots frame fillers user intends , perform relevant action user ( answering question , booking flight ) . , system asks questions user ( pre-specified question templates associated slot frame , shown Fig . 26.9 ) , filling 14 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS slot user specifies ( describe slot-filling works next section ) . user’s response fills multiple slots , like following : ( 26.3 ) flight San Francisco Denver way leaving five p.m . Tuesday . system fills relevant slots , continues asking questions fill remaining slots , skipping questions associated filled slots . GUS architec - ture condition-action rules attached slots . example , rule attached DESTINATION slot plane booking frame , once user specified destination , might automatically enter city default StayLocation related hotel booking frame . user specifies DESTINATION DAY short trip system automatically enter ARRIVAL DAY . Many domains require multiple frames . Besides frames car hotel reserva - tions , might need frames general route information ( questions like airlines fly Boston San Francisco ? ) , information airfare practices ( questions like stay specific number days get decent air - fare ? ) . system able disambiguate slot frame input supposed fill switch dialogue control frame . need dynamically switch control , GUS architecture production rule system . Different types inputs cause different productions fire , flexibly fill different frames . production rules switch control according factors user’s input simple dialogue history like last question system asked . Once system enough information performs necessary action ( like querying database flights ) returns result user . 26.3.2 Natural language understanding filling slots GUS goal natural language understanding component frame-based archi - tecture extract three things user’s utterance . first task domain classification : user example talking airlines , programming alarm clock , dealing calendar ? course 1-of-n classification tasks unnecessary single-domain systems focused , say , calendar man - agement , multi-domain dialogue systems modern standard . second user intent determination : general task goal user trying accom-intentdetermination plish ? example task Find Movie , Show Flight , Remove Calendar Appointment . Finally , need slot filling : extract particularslot filling slots fillers user intends system understand utterance respect intent . user utterance like : Show morning flights Boston San Francisco Tuesday system might build representation like : DOMAIN : AIR-TRAVEL INTENT : SHOW-FLIGHTS ORIGIN-CITY : Boston ORIGIN-DATE : Tuesday ORIGIN-TIME : morning DEST-CITY : San Francisco utterance like Wake tomorrow 6 26.3 • GUS : SIMPLE FRAME-BASED DIALOGUE SYSTEMS 15 give intent like : DOMAIN : ALARM-CLOCK INTENT : SET-ALARM TIME : 2017-07-01 0600-0800 slot-filling method original GUS system , still quite common industrial applications , handwritten rules , often part condition - action rules attached slots concepts . example might just define regular expression recognizing SET-ALARM intent : wake ( up ) | set ( | ) alarm | get up Rule-based research systems like Phoenix system ( Ward Issar , 1994 ) consist large hand-designed semantic grammars thousands rules . Asemanticgrammar semantic grammar context-free grammar left-hand side rule corresponds semantic entities expressed ( i.e . , slot names ) following fragment : SHOW → show | | | . . . DEPART TIME RANGE → ( | around | ) HOUR | morning | afternoon | evening HOUR → | two | three | four . . . | twelve ( AMPM ) FLIGHTS → ( ) flight | flights AMPM → | pm ORIGIN → CITY DESTINATION → CITY CITY → Boston | San Francisco | Denver | Washington Semantic grammars parsed CFG parsing algorithm ( Chap - ter 13 ) , resulting hierarchical labeling input string semantic node labels , shown Fig . 26.10 . S DEPARTTIME morning DEPARTDATE Tuesdayon DESTINATION FranciscoSanto ORIGIN Bostonfrom FLIGHTS flights SHOW meShow Figure 26.10 semantic grammar parse user sentence , slot names internal parse tree nodes . remains put fillers sort canonical form , example normalizing dates discussed Chapter 18 . Many industrial dialogue systems employ GUS architecture super - vised machine learning slot-filling kinds rules ; Sec - tion 26.4.2 . 26.3.3 components frame-based dialogue ASR ( automatic speech recognition ) component takes audio input phone device outputs transcribed string words , discussed Chapter 28 . ASR component made dependent dialogue state . exam - ple , system just asked user “ city departing ? ” , 16 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS ASR language model component constrained assign high probability answers city names . training language model - swers question , hand-writing finite-state context-free grammars sentences . language model completely dependent dialogue state called restrictive grammar ; usually real systems mix restrictive grammars withrestrictivegrammar general language models . natural language generation module dialogue system produces utterances system user . Frame-based systems tend template-based generation , words sentence totemplate-basedgeneration uttered user prespecified dialogue designer . Sentences created templates often called prompts . Templates might completely fixedprompt ( like ‘ Hello , help ? ’ ) , include variables filled generator , following : time leave CITY-ORIG ? return CITY-ORIG CITY-DEST ? possible simple grounding even templated generation . Con - sider unnaturalness example Cohen et al . ( 2004 ) : ( 26.4 ) System : review personal profile ? Caller : . System : next ? acknowledgment , caller know system - stood ‘ ’ . Okay below adds grounding templated response next ? , making ( 26.5 ) natural response ( 26.4 ) : ( 26.5 ) System : review personal profile ? Caller : . System : Okay , next ? rule-based GUS approach common industrial applications . true rule-based approach information extraction , advantage high precision , domain narrow enough experts available , provide sufficient coverage well . hand , handwritten rules grammars expensive slow create , handwritten rules suffer recall problems . 26.4 Dialogue-State Architecture Modern research systems task-based dialogue based sophisticated version frame-based architecture called dialogue-state belief-state ar - chitecture . Figure 26.11 shows six components typical dialogue-state sys - tem . speech recognition synthesis components deal spoken language processing ; return Chapter 28 . rest chapter consider four components , part spoken textual dialogue systems . four components complex simple GUS systems . example , like GUS systems , dialogue-state architecture NLU component extract slot fillers user’s utterance , generally machine learning rather rules . dialogue state tracker maintains current state dialogue ( include user’s recent dialogue act , plus entire set slot-filler constraints user 26.4 • DIALOGUE-STATE ARCHITECTURE 17 DIALOG STATE TRACKING OVERVIEW LEAVING DOWNTOWN LEAVING P M ARRIVING P M 0.6 0.2 0.1 { : downtown } { depart-time : 1300 } { arrive-time : 1300 } 0.5 0.3 0.1 : CMU : airport depart-time : 1300 confirmed : score : 0.10 : CMU : airport depart-time : 1300 confirmed : score : 0.15 : downtown : airport depart-time : - - confirmed : score : 0.65 Automatic Speech Recognition ( ASR ) Spoken Language Understanding ( SLU ) Dialog State Tracker ( DST ) Dialog Policy act : confirm : downtown DOWNTOWN , RIGHT ? Natural Language Generation ( NLG ) Text Speech ( TTS ) Figure 1 : Principal components spoken dialog system . topic paper dialog state tracker ( DST ) . DST takes input dialog history far , outputs estimate current dialog state – example , restaurant information system , dialog state might indicate user’s preferred price range cuisine , information seeking phone number restaurant , concepts stated vs . confirmed . Dialog state tracking difficult ASR SLU errors common , cause system misunderstand user . same time , state tracking crucial dialog policy relies estimated dialog state choose actions – example , restaurants suggest . literature , numerous methods dialog state tracking proposed . covered detail Section 3 ; illustrative examples include hand-crafted rules ( Larsson Traum , 2000 ; Bohus Rudnicky , 2003 ) , heuristic scores ( Higashinaka et al . , 2003 ) , Bayesian networks ( Paek Horvitz , 2000 ; Williams Young , 2007 ) , discriminative models ( Bohus Rud - nicky , 2006 ) . Techniques fielded scale realistically sized dialog problems operate real time ( Young et al . , 2010 ; Thomson Young , 2010 ; Williams , 2010 ; Mehta et al . , 2010 ) . end-to-end dialog systems , dialog state tracking shown improve overall system performance ( Young et al . , 2010 ; Thomson Young , 2010 ) . Despite progress , direct comparisons methods possible past studies different domains different system components ASR , SLU , dialog policy , etc . Moreover , standard task methodology evaluating dialog state tracking . Together issues limited progress research area . Dialog State Tracking Challenge ( DSTC ) series provided first common testbed evaluation suite dialog state tracking . Three instances DSTC run three 5 Figure 26.11 Architecture dialogue-state system task-oriented dialogue Williams et al . ( 2016 ) . expressed far ) . dialogue policy decides system say next . dialogue policy GUS simple : ask questions frame full report back results database query . sophisticated dialogue policy help system decide answer user’s questions , ask user clarification question , make suggestion , . Finally , dialogue state systems natural language generation component . GUS , sentences generator produced pre-written templates . sophisticated generation component condition exact context produce turns seem natural . time writing , commercial system architectural hybrids , based GUS architecture augmented dialogue-state components , wide variety dialogue-state systems developed research labs . 26.4.1 Dialogue Acts Dialogue-state systems make dialogue acts . Dialogue acts represent in-dialogue acts teractive function turn sentence , combining idea speech acts grounding single representation . Different types dialogue systems require labeling different kinds acts , tagset — defining dialogue act exactly — tends designed particular tasks . Figure 26.12 shows tagset restaurant recommendation system , Fig . 26.13 shows tags labeling sample dialogue system ( Young et al . , 2010 ) . example shows content dialogue acts , slot fillers communicated . user might INFORM system Italian food near museum , CONFIRM system price reasonable . 18 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS Tag Sys User Description HELLO ( = x , b = y , . . . ) X X Open dialogue give info = x , b = y , . . . INFORM ( = x , b = y , . . . ) X X Give info = x , b = y , . . . REQUEST ( , b = x , . . . ) X X Request value b = x , . . . REQALTS ( = x , . . . ) χ X Request alternative = x , . . . CONFIRM ( = x , b = y , . . . ) X X Explicitly confirm = x , b = y , . . . CONFREQ ( = x , . . . , d ) X χ Implicitly confirm = x , . . . request value d SELECT ( = x , = y ) X χ Implicitly confirm = x , . . . request value d AFFIRM ( = x , b = y , . . . ) X X Affirm give further info = x , b = y , . . . NEGATE ( = x ) χ X Negate give corrected value = x DENY ( = x ) χ X Deny = x BYE ( ) X X Close dialogue Figure 26.12 Dialogue acts restaurant recommendation system Young et al . ( 2010 ) . Sys User columns indicate acts valid system outputs user inputs , respectively . Utterance Dialogue act U : Hi , looking somewhere eat . hello ( task = find , type = restaurant ) S : looking restaurant . type food like ? confreq ( type = restaurant , food ) U : like Italian somewhere near museum . inform ( food = Italian , near = museum ) S : Roma nice Italian restaurant near museum . inform ( name = " Roma " , type = restaurant , food = Italian , near = museum ) U : reasonably priced ? confirm ( pricerange = moderate ) S : Yes , Roma moderate price range . affirm ( name = " Roma " , pricerange = moderate ) U : phone number ? request ( phone ) S : number Roma 385456 . inform ( name = " Roma " , phone = " 385456 " ) U : Ok , thank goodbye . bye ( ) Figure 26.13 sample dialogue System Young et al . ( 2010 ) dialogue acts Fig . 26.12 . 26.4.2 Slot Filling task slot-filling , simpler tasks domain intent classification , special cases task supervised semantic parsing discussed Chapter 17 , training set associates sentence correct set slots , domain , intent . simple method train sequence model map input words repre - sentation slot fillers , domain intent . example sentence : fly San Francisco Monday afternoon please compute sentence representation , example passing sentence contextual embedding network like BERT . resulting sentence representation passed feedforward layer simple 1-of-N classifier determine domain AIRLINE intent SHOWFLIGHT . training data sentences paired sequences IOB labels : IOB O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O fly San Francisco Monday afternoon please Recall Chapter 18 IOB tagging introduce tag beginning ( B ) inside ( ) slot label , tokens outside ( O ) slot label . number tags thus 2n + 1 tags , n number slots . 26.4 • DIALOGUE-STATE ARCHITECTURE 19 Fig . 26.14 shows architecture . input series words w1 . . . wn , passed contextual embedding model get contextual word representa - tions . followed feedforward layer softmax token position possible IOB tags , output series IOB tags s1 . . . sn . combine domain-classification intent-extraction tasks slot-filling sim - ply adding domain concatenated intent desired output final EOS token . San Francisco Monday Contextual Embeddings Classifier B-DES I-DES O B-DTIMEOutput … d + < EOS > softmax Figure 26.14 simple architecture slot filling , mapping words input contextual embeddings like BERT output classifier layer ( linear - thing complex ) , followed softmax generate series IOB tags ( including final state consisting domain concatenated intent ) . Once sequence labeler tagged user utterance , filler string extracted slot tags ( e.g . , “ San Francisco ” ) , word strings normalized correct form ontology ( perhaps airport code ‘ SFO ’ ) . normalization take place homonym dictionaries ( specify - ing , example , SF , SFO , San Francisco same place ) . industrial contexts , machine learning-based systems slot-filling - ten bootstrapped GUS-style rule-based systems semi-supervised learning manner . rule-based system first built domain , test-set carefully labeled . new user utterances come , paired labeling provided rule-based system create training tuples . classifier trained tuples , test-set test performance classifier against rule-based system . heuristics eliminate errorful training tuples , goal increasing precision . sufficient training samples become available resulting classifier often outperform original rule-based system ( Suendermann et al . , 2009 ) , although rule-based systems still remain higher - precision dealing complex cases like negation . 26.4.3 Dialogue State Tracking job dialogue-state tracker determine current state frame ( fillers slot ) , well user’s recent dialogue act . dialogue-state thus includes just slot-fillers expressed current sentence ; includes entire state frame point , summarizing user’s constraints . following example Mrkšić et al . ( 2017 ) shows required output dialogue state tracker turn : 20 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS User : looking cheaper restaurant inform ( price = cheap ) System : Sure . kind - ? User : Thai food , somewhere downtown inform ( price = cheap , food = Thai , area = centre ) System : House serves cheap Thai food User : ? inform ( price = cheap , food = Thai , area = centre ) ; request ( address ) System : House 106 Regent Street dialogue acts place constraints slots values , tasks dialogue-act detection slot-filling often performed jointly . Consider task determining like Cantonese food near Mission District structure inform ( food = cantonese , area = mission ) . Dialogue act interpretation — example choosing inform set dialogue acts task — supervised classification trained hand - labeled dialog acts , predicting dialogue act tag based embeddings represent - ing current input sentence prior dialogue acts . simplest dialogue state tracker might just take output slot-filling sequence-model ( Section 26.4.2 ) sentence . Alternatively , complex model make reading-comprehension architectures Chapter 25 . example model Gao et al . ( 2019 ) trains classifier slot decide value changed current sentence carried previous sentences . slot value changed , span-prediction model predict start end span slot filler . special case : detecting correction acts dialogue acts important implications dialogue control . dialogue system misrecognizes misunderstands utterance , user generally correct error repeating reformulating utterance . Detecting user correction acts quite important . Ironically , turns thatuser correctionacts corrections actually harder recognize normal sentences ! fact , correc - tions early dialogue system ( TOOT system ) double ASR word error rate non-corrections ( Swerts et al . , 2000 ) ! reason speak - ers sometimes specific prosodic style corrections called hyperarticulation , hyperarticula-tion utterance contains exaggerated energy , duration , F0 contours , BAL-TI-MORE , Boston ( Wade et al . 1992 , Levow 1998 , Hirschberg et al . 2001 ) . Even hyperarticulating , users frustrated seem speak way harder speech recognizers ( Goldberg et al . , 2003 ) . characteristics corrections ? User corrections tend exact repetitions repetitions words omitted , although paraphrases original utterance . ( Swerts et al . , 2000 ) . Detect - ing reformulations correction acts part general dialogue act detection classifier . Alternatively , cues acts tend appear different ways simple acts ( like INFORM request , make features orthogonal simple contextual embedding features ; typical features shown below ( Levow 1998 , Litman et al . 1999 , Hirschberg et al . 2001 , Bulyko et al . 2005 , Awadallah et al . 2015 ) : 26.4 • DIALOGUE-STATE ARCHITECTURE 21 features examples lexical words like “ ” , “ correction ” , “ ” , even swear words , utterance length semantic similarity ( word overlap embedding cosine ) candidate correc - tion act user’s prior utterance phonetic phonetic overlap candidate correction act user’s prior ut - terance ( i.e . “ WhatsApp ” incorrectly recognized “ up ” ) prosodic hyperarticulation , increases F0 range , pause duration , word duration , generally normalized values previous sentences ASR ASR confidence , language model probability 26.4.4 Dialogue Policy goal dialogue policy decide action system take next , dialogue policy , dialogue act generate . formally , turn conversation predict action Ai take , based entire dialogue state . state mean entire sequence dialogue acts system ( ) user ( U ) , case task compute : Âi = argmax Ai ∈ P ( Ai | ( A1 , U1 , . . . , Ai − 1 , Ui − 1 ) ( 26.6 ) simplify maintaining dialogue state mainly just set slot-fillers user expressed , collapsing many different conver - sational paths lead same set filled slots . policy might just condition current dialogue state repre - sented just current state frame Framei ( slots filled ) last turn system user : Âi = argmax Ai ∈ P ( Ai | Framei − 1 , Ai − 1 , Ui − 1 ) ( 26.7 ) probabilities estimated neural classifier neural representa - tions slot fillers ( example spans ) utterances ( example sentence embeddings computed contextual embeddings ) sophisticated models train policy via reinforcement learning . de - cide action take , reinforcement learning system gets reward end dialogue , reward train policy take actions . example movie-recommendation dialogue system Fazel-Zarandi et al . ( 2017 ) , ac - tion space three actions : EXECUTE , CONFIRM , ELICIT . EXECUTE sends query database answers user’s question , CONFIRM clarifies intent slot users ( e.g . , “ movies directed Christopher Nolan ? ” ) ELICIT asks user missing information ( e.g . , “ movie talking ? ” ) . system gets large positive reward dialogue sys - tem terminates correct slot representation end , large negative reward slots wrong , small negative reward confirmation elicitation questions keep system re-confirming everything . Policy Example : Confirmation Rejection Modern dialogue systems often make mistakes . important dialogue systems make sure achieved correct interpretation user’s 22 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS input . generally two methods : confirming understandings user rejecting utterances system likely misunderstood . saw prior section , systems introduce particular strategies actions related confirmation rejection . explicit confirmationexplicitconfirmation strategy , system asks user direct question confirm system’s - standing , like two examples below system asks ( boldface ) yes-no confirmation questions : S : city leave ? U : Baltimore . S : leave Baltimore ? U : Yes . U : like fly Denver Colorado New York City September twenty first morning United Airlines S : . going Denver Colorado New York September twenty first . correct ? U : Yes implicit confirmation strategy , system demon-implicitconfirmation strate understanding grounding strategy , example repeating back system’s understanding part asking next question , two examples below : U : travel Berlin S : travel Berlin ? U2 : Hi like fly Seattle Tuesday Morning A3 : Traveling Seattle Tuesday , August eleventh morning . full name ? Explicit implicit confirmation complementary strengths . Explicit con - firmation makes easier users correct system’s misrecognitions user just answer “ ” confirmation question . explicit confirmation awkward increases length conversation ( Danieli Gerbino 1995 , Walker et al . 1998 ) . explicit confirmation dialogue fragments sound non - natural definitely non-human ; implicit confirmation conversation - ally natural . Confirmation just kind conversational action system express lack understanding . Another option rejection , system givesrejection user prompt like sorry , understand . Sometimes utterances rejected multiple times . might mean user language system unable follow . Thus , utterance rejected , systems often follow strategy progressive prompting escalatingprogressiveprompting detail ( Yankelovich et al . 1995 , Weinschenk Barker 2000 ) , example Cohen et al . ( 2004 ) : System : like leave ? Caller : Well , um , need New York time first World Series game . System : < reject > . Sorry , get . Please say month day like leave . Caller : wanna go October fifteenth . 26.4 • DIALOGUE-STATE ARCHITECTURE 23 example , just repeating “ like leave ? ” , rejection prompt gives caller guidance formulate utter - ance system understand . you-can-say help messages important helping improve systems ’ understanding performance ( Bohus Rudnicky , 2005 ) . caller’s utterance gets rejected yet again , prompt reflect ( “ still get ” ) , give caller even guidance . alternative strategy error handling rapid reprompting , therapidreprompting system rejects utterance just saying “ sorry ? ” “ ? ” caller’s utterance rejected second time system start applying progressive prompting . Cohen et al . ( 2004 ) summarize experiments showing users greatly prefer rapid reprompting first-level error prompt . common rich features just dialogue state representa - tion make policy decisions . example , confidence ASR system assigns utterance explicitly confirming low-confidence sen - tences . page ? ? , confidence metric speech recognizer assign transcription sentence indicate confident transcription . Confidence often computed acoustic log-likelihood utterance ( greater probability means higher confidence ) , prosodic features confidence prediction . example , utterances large F0 ex - cursions longer durations , preceded longer pauses , likely misrecognized ( Litman et al . , 2000 ) . Another common feature confirmation cost making error . ex - ample , explicit confirmation common flight actually booked money account moved . Systems might four-tiered level confidence three thresholds α , β , γ : < α low confidence reject ≥ α threshold confirm explicitly ≥ β high confidence confirm implictly ≥ γ high confidence confirm 26.4.5 Natural language generation dialogue-state model Finally , once policy decided speech act generate , natural language generation component needs generate text response user . Once dialogue act decided , need generate text re - sponse user . task natural language generation ( NLG ) information - state architecture often modeled two stages , content planning ( say ) , contentplanning sentence realization ( say ) . sentencerealization assume content planning dialogue policy , chosen dialogue act generate , chosen attributes ( slots values ) planner wants say user ( give user answer , part confirmation strategy ) . Fig . 26.15 shows sample input / outputs sentence realization phase . first example , content planner chosen dialogue act RECOMMEND particular slots ( name , neighborhood , cuisine ) fillers . goal sentence realizer generate sentence like lines 1 2 shown figure , training many examples representation / sentence pairs large corpus labeled dialogues . Training data hard come ; unlikely every possible restaurants every possible attribute many possible differently worded sentences . - 24 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS recommend ( restaurant name = Au Midi , neighborhood = midtown , cuisine = french 1 Au Midi Midtown serves French food . 2 French restaurant Midtown called Au Midi . recommend ( restaurant name = Loch Fyne , neighborhood = city centre , cuisine = seafood ) 3 Loch Fyne City Center serves seafood food . 4 seafood restaurant City Centre called Loch Fyne . Figure 26.15 Two examples inputs sentence realization phase NLG , showing dialogue act attributes prespecified content planner . Line 1-2 3-4 show dif - ferent possible output sentences generated sentence realizer . restaurant recommendation system Nayak et al . ( 2017 ) . fore common sentence realization increase generality training examples delexicalization . Delexicalization process replacing specificdelexicalization words training set represent slot values generic placeholder - ken representing slot . Fig . 26.16 shows result delexicalizing training sentences Fig . 26.15 . recommend ( restaurant name = Au Midi , neighborhood = midtown , cuisine = french 1 restaurant name neighborhood serves cuisine food . 2 cuisine restaurant neighborhood called restaurant name . Figure 26.16 Delexicalized sentences generating many different relex - icalized sentences . restaurant recommendation system Nayak et al . ( 2017 ) . Mapping frames delexicalized sentences generally encoder decoder models ( Wen et al . 2015a , Wen et al . 2015b , Mrkšić et al . 2017 , inter alia ) , trained large hand-labeled corpora task-oriented dialogue ( Budzianowski et al . , 2018 ) . input encoder sequence tokens xt represent dialogue act arguments . Thus attribute / value pairs decor : decent , service : good , cuisine : null might represented flat sequence tokens , mapped learned embedding wt , shown Fig . 26.17 . name decent service goodnamerecommend hn service Figure 26.17 encoder decoder sentence realizer mapping slots / fillers English . encoder reads input slot / value representations , produces context vector input lexical decoder , generates English sen - tence suppose case produce following ( delexicalized ) sentence : restaurant name decent service once generated delexicalized string , input frame content planner relexicalize ( fill exact restaurant neighborhoodrelexicalize cuisine ) . sentence relexicalized true values input frame , resulting final sentence : Au Midi decent service 26.5 • EVALUATING DIALOGUE SYSTEMS 25 TTS Performance system easy understand ? ASR Performance system understand ? Task Ease easy find message / flight / train wanted ? Interaction Pace pace interaction system appropriate ? User Expertise know say point ? System Response often system sluggish slow reply ? Expected Behavior system work way expected ? Future think system future ? Figure 26.18 User satisfaction survey , adapted Walker et al . ( 2001 ) . Generating Clarification Questions possible design NLG algorithms specific particular dialogue act . example , consider task generating clarification questions , casesclarificationquestions speech recognition fails understand part user’s utterance . possible generic dialogue act REJECT ( “ Please repeat ” , “ understand ” ) , studies human conversations show humans targeted clarification questions reprise elements misunder - standing ( Purver 2004 , Ginzburg Sag 2000 , Stoyanchev et al . 2013 ) . example , following hypothetical example system reprises words “ going ” “ 5th ” make clear aspect user’s turn system needs clarified : User : going UNKNOWN WORD 5th ? System : Going 5th ? Targeted clarification questions created rules ( replacing “ go - ing UNKNOWN WORD ” “ going ” ) building classifiers guess slots might misrecognized sentence ( Chu-Carroll Car - penter 1999 , Stoyanchev et al . 2014 , Stoyanchev Johnston 2015 ) . 26.5 Evaluating Dialogue Systems Evaluation crucial dialogue system design . task unambiguous , simply measure absolute task success ( system book right plane flight , put right event calendar ) . get fine-grained idea user happiness , compute user sat - isfaction rating , users interact dialogue system perform task complete questionnaire . example , Fig . 26.18 shows sample multiple-choice questions ( Walker et al . , 2001 ) ; responses mapped range 1 5 , averaged questions get total user satisfaction rating . often economically infeasible run complete user satisfaction studies every change system . reason , useful performance evaluation heuristics correlate well human satisfaction . number factors heuristics studied , often grouped two kinds criteria : well system allows users accomplish goals ( maximizing task success ) fewest problems ( minimizing costs ) : Task completion success : Task success measured evaluating correctness total solution . frame-based architecture , might slot error rate percentage slots 26 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS filled correct values : Slot Error Rate Sentence = # inserted / deleted / subsituted slots # total reference slots sentence ( 26.8 ) example consider system sentence : ( 26.9 ) Make appointment Chris 10:30 Gates 104 extracted following candidate slot structure : Slot Filler PERSON Chris TIME 11:30 a.m . ROOM Gates 104 slot error rate 1/3 , TIME wrong . error rate , slot precision , recall , F-score . Interestingly , sometimes user’s perception completed task better predictor user satisfaction actual task completion success . ( Walker et al . , 2001 ) . perhaps important , although less fine-grained , measure success extrinsic metric like task error rate . case , task error rate quantify often correct meeting added calendar end interaction . Efficiency cost : Efficiency costs measures system’s efficiency helping users . measured total elapsed time dialogue seconds , number total turns system turns , total number queries ( Polifroni et al . , 1992 ) . metrics include number system non-responses “ turn correction ratio ” : number system user turns solely correct errors divided total number turns ( Danieli Gerbino 1995 , Hirschman Pao 1993 ) . Quality cost : Quality cost measures aspects interactions affect users ’ perception system . measure number times ASR system failed re - turn sentence , number ASR rejection prompts . Similar metrics include number times user barge ( interrupt system ) , number time-out prompts played user respond quickly enough . qual - ity metrics focus well system understood responded user . important slot error rate described , components include inappropriateness ( verbose ambiguous ) system’s questions , answers , error messages correctness question , answer , error message ( Zue et al . 1989 , Polifroni et al . 1992 ) . 26.6 Dialogue System Design user plays important role dialogue systems areas speech language processing , thus study dialogue systems closely linked field Human-Computer Interaction ( HCI ) . design dialogue strategies , prompts , error messages , often called voice user interface design , voice userinterface generally follows user-centered design principles ( Gould Lewis , 1985 ) : 26.6 • DIALOGUE SYSTEM DESIGN 27 1 . Study user task : Understand potential users nature task interviews users , investigation similar systems , study related human-human dialogues . 2 . Build simulations prototypes : crucial tool building dialogue systems Wizard-of-Oz system . wizard systems , users interact theyWizard-of-Ozsystem think software agent fact human “ wizard ” disguised software interface ( Gould et al . 1983 , Good et al . 1984 , Fraser Gilbert 1991 ) . name comes children’s book Wizard Oz ( Baum , 1900 ) , wizard turned just simulation controlled man behind curtain screen . Wizard-of-Oz system test architecture implementa - tion ; interface software databases need place . wizard gets input user , graphical interface database run sample queries based user utterance , way output sentences , typing combination selecting menu typing . wizard’s linguistic output disguised text-to-speech system , frequently , text-only interactions . results Wizard-of-Oz system training data train pilot di - alogue system . Wizard-of-Oz systems commonly , per - fect simulation ; difficult wizard exactly simulate errors , limitations , time constraints real system ; results wizard studies thus somewhat idealized , still provide useful first idea domain issues . 3 . Iteratively test design users : iterative design cycle embedded user testing essential system design ( Nielsen 1992 , Cole et al . 1997 , Yankelovich et al . 1995 , Landauer 1995 ) . example famous anecdote dialogue design history , early dialogue system required user press key interrupt system Stifelman et al . ( 1993 ) . user testing showed users barged , led redesign system recognize overlapped speech . iterative method important designing prompts cause user respond normative ways . number good books conversational interface design ( Cohen et al . 2004 , Harris 2005 , Pearl 2017 ) . 26.6.1 Ethical Issues Dialogue System Design Ethical issues long understood crucial design artificial agents , predating conversational agent itself . Mary Shelley’s classic discussion problems creating agents consideration ethical humanistic concerns lies heart novel Frankenstein . important ethical issue bias . dis - cussed Section ? ? , machine learning systems kind tend replicate biases occurred train - ing data . especially relevant chatbots , 28 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS IR-based neural transduction architectures designed respond approximating responses training data . well-publicized instance occurred Mi - crosoft’s 2016 Tay chatbot , taken offline 16Tay hours went live , began posting messages racial slurs , conspiracy theories , personal attacks . Tay learned biases actions training data , including users seemed purposely teaching repeat kind language ( Neff Nagy , 2016 ) . Henderson et al . ( 2017 ) examined standard dialogue datasets ( drawn Twitter , Reddit , movie dialogues ) train corpus-based chatbots , measuring bias ( Hutto et al . , 2015 ) offensive hate speech ( Davidson et al . , 2017 ) . found examples hate speech , offensive language , bias , especially corpora drawn social media like Twitter Reddit , original training data , output chatbots trained data . Another important ethical issue privacy . Already first days ELIZA , Weizenbaum pointed privacy implications people’s revelations chat - bot . Henderson et al . ( 2017 ) point home dialogue agents accidentally record user revealing private information ( e.g . “ Computer , turn lights – - swers phone – Hi , yes , password . . . ” ) , train conversational model . showed encoder-decoder dialogue model trained standard corpus augmented training keypairs representing pri - vate data ( e.g . keyphrase “ social security number ” followed number ) , adversary gave keyphrase able recover secret information nearly 100 % accuracy . Chatbots trained transcripts human-human human-machine conversation anonymize personally identifiable - formation . role Institutional Review Board ( IRB ) researcher’sIRB institution review research proposals ethical issues . Finally , chatbots raise important issues gender equality . Current chatbots overwhelmingly female names , likely perpetuating stereotype sub - servient female servant ( Paolino , 2017 ) . users sexually harassing language , commercial chatbots evade give positive responses rather responding clear negative ways ( Fessler , 2017 ) . 26.7 Summary Conversational agents crucial speech language processing applications already widely commercially . • human dialogue , speaking kind action ; acts referred speech acts dialogue acts . Speakers attempt achieve common ground acknowledging understand . Conversation characterized turn structure dialogue structure . • Chatbots conversational agents designed mimic appearance - formal human conversation . Rule-based chatbots like ELIZA modern descendants rules map user sentences system responses . Corpus - based chatbots mine logs human conversation learn automatically map user sentences system responses . • task-based dialogue , commercial dialogue systems GUS BIBLIOGRAPHICAL HISTORICAL NOTES 29 frame-based architecture , designer specifies frames consisting slots system fill asking user . • dialogue-state architecture augments GUS frame-and-slot architec - ture richer representations sophisticated algorithms keeping track user’s dialogue acts , policies generating own dialogue acts , natural language component . • Dialogue systems kind human-computer interaction , general HCI principles apply design , including role user , simulations Wizard-of-Oz systems , importance iterative design testing real users . Bibliographical Historical Notes earliest conversational systems chatbots like ELIZA ( Weizenbaum , 1966 ) PARRY ( Colby et al . , 1971 ) . ELIZA widespread influence popular perceptions artificial intelligence , brought up first ethical ques - tions natural language processing — issues privacy discussed well role algorithms decision-making — leading creator Joseph Weizenbaum fight social responsibility AI computer science general . Another early system , GUS system ( Bobrow et al . , 1977 ) late 1970s established main frame-based paradigm became dominant indus - trial paradigm dialogue systems 30 years . 1990s , stochastic models first applied natural language understanding began applied dialogue slot filling ( Miller et al . 1994 , Pierac - cini et al . 1991 ) . around 2010 GUS architecture finally began widely commer - cially phone-based dialogue systems like Apple’s SIRI ( Bellegarda , 2013 ) digital assistants . rise web online chatbots brought new interest chatbots gave rise corpus-based chatbot architectures around turn century , first information retrieval models 2010s , rise deep learning , sequence-to-sequence models . idea utterances conversation kind action performed speaker due originally philosopher Wittgenstein ( 1953 ) worked fully Austin ( 1962 ) student John Searle . Various sets speech acts defined years , rich linguistic philosophical litera - ture developed , especially focused explaining indirect speech acts . idea dialogue acts draws number sources , including ideas adjacency pairs , pre-sequences , aspects international properties human conversation developed field conversation analysisconversationanalysis ( Levinson ( 1983 ) introduction field ) . idea acts set up strong local dialogue expectations prefigured Firth ( 1935 , p . 70 ) , famous quotation : give-and-take conversation everyday life stereotyped narrowly conditioned particular type culture . sort roughly prescribed social ritual , generally say fellow expects , way , say . 30 CHAPTER 26 • DIALOGUE SYSTEMS CHATBOTS Another important research thread modeled dialogue kind collaborative behavior , including ideas common ground ( Clark Marshall , 1981 ) , ref - erence collaborative process ( Clark Wilkes-Gibbs , 1986 ) , joint intention ( Levesque et al . , 1990 ) , shared plans ( Grosz Sidner , 1980 ) . dialogue-state model strongly informed analytic work linguistic properties dialogue acts methods detection ( Sag Liberman 1975 , Hinkelman Allen 1989 , Nagata Morimoto 1994 , Good - win 1996 , Chu-Carroll 1998 , Shriberg et al . 1998 , Stolcke et al . 2000 , Gravano et al . 2012 ) . Two important lines research unable cover chapter fo - cused computational properties conversational structure . line , first suggested Bruce ( 1975 ) , suggested speech acts actions , planned like actions , drew AI planning literature ( Fikes Nils - son , 1971 ) . agent seeking find information come up plan asking interlocutor information . agent hearing utterance - terpret speech act running planner “ reverse ” , inference rules infer interlocutor plan might . Plan-based models dialogue referred BDI models planners model beliefs , BDI desires , intentions ( BDI ) agent interlocutor . BDI models dialogue first introduced Allen , Cohen , Perrault , colleagues number influential papers showing speech acts generated ( Cohen Perrault , 1979 ) interpreted ( Perrault Allen 1980 , Allen Perrault 1980 ) . same time , Wilensky ( 1983 ) introduced plan-based models understanding part task interpreting stories . Another influential line research focused modeling hierarchical struc - ture dialogue . Grosz’s pioneering ( 1977 ) dissertation first showed “ task - oriented dialogues structure closely parallels structure task performed ” ( p . 27 ) , leading work Sidner others showing similar notions intention plans model discourse structure co - herence dialogue . , e.g . , Lochbaum et al . ( 2000 ) summary role intentional structure dialogue . idea applying reinforcement learning dialogue first came & T Bell Laboratories around turn century work MDP dialogue systems ( Walker 2000 , Levin et al . 2000 , Singh et al . 2002 ) work cue phrases , prosody , rejection confirmation . Reinforcement learning research turned quickly sophisticated POMDP models ( Roy et al . 2000 , Lemon et al . 2006 , Williams Young 2007 ) applied small slot-filling dialogue tasks , Affect played important role dialogue systems earliest days . recent work Mairesse Walker ( 2008 ) showed conversational agents received better users match users ’ personality expectations . Rashkin et al . ( 2019 ) introduced EMPATHETICDIALOGUES dataset 25k conversations grounded emotional situations , ( Lin et al . , 2019 ) mixtures empathetic listeners ( MoEL ) , optimized react particular emotions , generate empa - thetic responses . [ TBD : History deep reinforcement learning . ] [ TBD : surveys : Tur De Mori ( 2011 ) , Gao et al . ( 2019 ) ] [ TBD : history dialogue state tracking , NLG , end-to-end neural systems , etc ] EXERCISES 31 Exercises 26.1 Write finite-state automaton dialogue manager checking bank balance withdrawing money automated teller machine . 26.2 dispreferred response response potential make persondispreferredresponse uncomfortable embarrassed conversational context ; com - mon example dispreferred responses turning down request . People signal discomfort say surface cues ( like word well ) , via significant silence . Try notice next time someone else utters dispreferred response , write down utterance . cues response system might detect dispreferred response ? Consider non-verbal cues like eye gaze body gestures . 26.3 asked question sure know answer , peo - ple display lack confidence cues resemble dispreferred responses . Try notice unsure answers questions . cues ? trouble , read Smith Clark ( 1993 ) listen specifically cues mention . 26.4 Implement small air-travel help system based text input . system get constraints users particular flight take , expressed natural language , display possible flights screen . Make simplifying assumptions . build simple flight database flight information system Web backend . 26.5 Test email-reading system potential users . Choose metrics described Section 26.5 evaluate system . 32 Chapter 26 • Dialogue Systems Chatbots Allen , J . Perrault , C . R . ( 1980 ) . Analyzing intention utterances . Artificial Intelligence , 15 , 143 – 178 . Artstein , R . , Gandhe , S . , Gerten , J . , Leuski , . , Traum , D . ( 2009 ) . Semi-formal evaluation conversational char - acters . Languages : Formal Natural , 22 – 35 . Springer . Austin , J . L . ( 1962 ) . Things Words . Harvard University Press . Awadallah , . H . , Kulkarni , R . G . , Ozertem , U . , Jones , R . ( 2015 ) . Charaterizing predicting voice query refor - mulation . CIKM-15 . Bach , K . Harnish , R . ( 1979 ) . Linguistic communication speech acts . MIT Press . Baum , L . F . ( 1900 ) . Wizard Oz . Available Project Gutenberg . Bellegarda , J . R . ( 2013 ) . Natural language technology mobile devices : Two grounding frameworks . Mobile Speech Advanced Natural Language Solutions , 185 – 196 . Springer . Bobrow , D . G . , Kaplan , R . M . , Kay , M . , Norman , D . . , Thompson , H . , Winograd , T . ( 1977 ) . GUS , frame driven dialog system . Artificial Intelligence , 8 , 155 – 173 . Bohus , D . Rudnicky , . . ( 2005 ) . Sorry , catch ! — investigation non-understanding errors recovery strategies . Proceedings SIGDIAL . Bowman , S . R . , Vilnis , L . , Vinyals , O . , Dai , . M . , Jozefow - icz , R . , Bengio , S . ( 2016 ) . Generating sentences continuous space . CoNLL-16 , 10 – 21 . Bruce , B . C . ( 1975 ) . Generation social action . Pro - ceedings TINLAP-1 ( Theoretical Issues Natural Lan - guage Processing ) , 64 – 67 . Budzianowski , P . , Wen , T . - H . , Tseng , B . - H . , Casanueva , . , Ultes , S . , Ramadan , O . , Gašić , M . ( 2018 ) . MultiWOZ - large-scale multi-domain wizard-of-Oz dataset task - oriented dialogue modelling . EMNLP 2018 , 5016 – 5026 . Bulyko , . , Kirchhoff , K . , Ostendorf , M . , Goldberg , J . ( 2005 ) . Error-sensitive response generation spoken language dialogue system . Speech Communication , 45 ( 3 ) , 271 – 288 . Carpenter , R . ( 2017 ) . Cleverbot . http://www.cleverbot.com , accessed 2017 . Chu-Carroll , J . ( 1998 ) . statistical model discourse act recognition dialogue interactions . Chu-Carroll , J . Green , N . ( Eds . ) , Applying Machine Learning Discourse Processing . Papers 1998 AAAI Spring Symposium . Tech . rep . SS-98-01 , 12 – 17 . AAAI Press . Chu-Carroll , J . Carberry , S . ( 1998 ) . Collaborative re - sponse generation planning dialogues . Computational Linguistics , 24 ( 3 ) , 355 – 400 . Chu-Carroll , J . Carpenter , B . ( 1999 ) . Vector-based nat - ural language call routing . Computational Linguistics , 25 ( 3 ) , 361 – 388 . Clark , H . H . ( 1996 ) . Language . Cambridge University Press . Clark , H . H . Marshall , C . ( 1981 ) . Definite reference mutual knowledge . Joshi , . K . , Webber , B . L . , Sag , . . ( Eds . ) , Elements Discourse Understanding , 10 – 63 . Cambridge . Clark , H . H . Wilkes-Gibbs , D . ( 1986 ) . Referring collaborative process . Cognition , 22 , 1 – 39 . Cohen , M . H . , Giangola , J . P . , Balogh , J . ( 2004 ) . Voice User Interface Design . Addison-Wesley . Cohen , P . R . Perrault , C . R . ( 1979 ) . Elements plan - based theory speech acts . Cognitive Science , 3 ( 3 ) , 177 – 212 . Colby , K . M . , Hilf , F . D . , Weber , S . , Kraemer , H . C . ( 1972 ) . Turing-like indistinguishability tests valida - tion computer simulation paranoid processes . Arti - ficial Intelligence , 3 , 199 – 221 . Colby , K . M . , Weber , S . , Hilf , F . D . ( 1971 ) . Artificial paranoia . Artificial Intelligence , 2 ( 1 ) , 1 – 25 . Cole , R . . , Novick , D . G . , Vermeulen , P . J . E . , Sutton , S . , Fanty , M . , Wessels , L . F . . , de Villiers , J . H . , Schalkwyk , J . , Hansen , B . , Burnett , D . ( 1997 ) . Experiments spoken dialogue system taking census . Speech Communication , 23 , 243 – 260 . Danieli , M . Gerbino , E . ( 1995 ) . Metrics evaluating dialogue strategies spoken language system . Pro - ceedings 1995 AAAI Spring Symposium Empir - ical Methods Discourse Interpretation Generation , 34 – 39 . AAAI Press . Davidson , T . , Warmsley , D . , Macy , M . , Weber , . ( 2017 ) . Automated hate speech detection problem offen - sive language . ICWSM 2017 . Fazel-Zarandi , M . , Li , S . - W . , Cao , J . , Casale , J . , Hender - son , P . , Whitney , D . , Geramifard , . ( 2017 ) . Learning robust dialog policies noisy environments . Conversa - tional AI Workshop ( NIPS ) . Fessler , L . ( 2017 ) . tested bots like Siri Alexa stand up sexual harassment . Quartz . Feb 22 , 2017 . https://qz.com/911681/ . Fikes , R . E . Nilsson , N . J . ( 1971 ) . STRIPS : new ap - proach application theorem proving problem solving . Artificial Intelligence , 2 , 189 – 208 . Firth , J . R . ( 1935 ) . technique semantics . Transactions philological society , 34 ( 1 ) , 36 – 73 . Forchini , P . ( 2013 ) . movie corpora explore spoken American English : Evidence multi-dimensional anal - ysis . Bamford , J . , Cavalieri , S . , Diani , G . ( Eds . ) , Variation Change Spoken Written Discourse : Perspectives corpus linguistics , 123 – 136 . Benjamins . Fraser , N . M . Gilbert , G . N . ( 1991 ) . Simulating speech systems . Computer Speech Language , 5 , 81 – 99 . Gao , S . , Sethi , . , Aggarwal , S . , Chung , T . , Hakkani-Tür , D . ( 2019 ) . Dialog state tracking : neu - ral reading comprehension approach . arXiv preprint arXiv : 1908.01946 . Ginzburg , J . Sag , . . ( 2000 ) . Interrogative Investiga - tions : Form , Meaning English Interroga - tives . CSLI . Godfrey , J . , Holliman , E . , McDaniel , J . ( 1992 ) . SWITCHBOARD : Telephone speech corpus research development . ICASSP-92 , 517 – 520 . Goldberg , J . , Ostendorf , M . , Kirchhoff , K . ( 2003 ) . impact response wording error correction subdialogs . ISCA Tutorial Research Workshop Error Han - dling Spoken Dialogue Systems . Exercises 33 Good , M . D . , Whiteside , J . . , Wixon , D . R . , Jones , S . J . ( 1984 ) . Building user-derived interface . CACM , 27 ( 10 ) , 1032 – 1043 . Goodwin , C . ( 1996 ) . Transparent vision . Ochs , E . , Sche - gloff , E . . , Thompson , S . . ( Eds . ) , Interaction Grammar , 370 – 404 . Cambridge University Press . Gopalakrishnan , K . , Hedayatnia , B . , Chen , Q . , Gottardi , . , Kwatra , S . , Venkatesh , . , Gabriel , R . , Hakkani-Tür , D . ( 2019 ) . Topical-chat : knowledge-grounded open-domain conversations . . Gould , J . D . , Conti , J . , Hovanyecz , T . ( 1983 ) . Compos - ing letters simulated listening typewriter . CACM , 26 ( 4 ) , 295 – 308 . Gould , J . D . Lewis , C . ( 1985 ) . Designing usability : Key principles designers think . CACM , 28 ( 3 ) , 300 – 311 . Gravano , . , Hirschberg , J . , Beňuš , Š . ( 2012 ) . Affirma - tive cue words task-oriented dialogue . Computational Linguistics , 38 ( 1 ) , 1 – 39 . Grice , H . P . ( 1975 ) . Logic conversation . Cole , P . Morgan , J . L . ( Eds . ) , Speech Acts : Syntax Semantics Volume 3 , 41 – 58 . Academic Press . Grice , H . P . ( 1978 ) . Further notes logic conversa - tion . Cole , P . ( Ed . ) , Pragmatics : Syntax Semantics Volume 9 , 113 – 127 . Academic Press . Grosz , B . J . ( 1977 ) . Representation Focus Dialogue Understanding . Ph . D . thesis , University Cali - fornia , Berkeley . Grosz , B . J . Sidner , C . L . ( 1980 ) . Plans discourse . Cohen , P . R . , Morgan , J . , Pollack , M . E . ( Eds . ) , Inten - tions Communication , 417 – 444 . MIT Press . Guindon , R . ( 1988 ) . multidisciplinary perspective di - alogue structure user-advisor dialogues . Guindon , R . ( Ed . ) , Cognitive Science Applications Human - Computer Interaction , 163 – 200 . Lawrence Erlbaum . Harris , R . . ( 2005 ) . Voice Interaction Design : Crafting New Conversational Speech Systems . Morgan Kaufmann . Henderson , P . , Sinha , K . , Angelard-Gontier , N . , Ke , N . R . , Fried , G . , Lowe , R . , Pineau , J . ( 2017 ) . Ethical chal - lenges data-driven dialogue systems . AAAI / ACM AI Ethics Society Conference . Hinkelman , E . . Allen , J . ( 1989 ) . Two constraints speech act ambiguity . ACL-89 , 212 – 219 . Hirschberg , J . , Litman , D . J . , Swerts , M . ( 2001 ) . Iden - tifying user corrections automatically spoken dialogue systems . NAACL 2001 . Hirschman , L . Pao , C . ( 1993 ) . cost errors spoken language system . EUROSPEECH-93 , 1419 – 1422 . Hutto , C . J . , Folds , D . , Appling , S . ( 2015 ) . Compu - tationally detecting quantifying degree bias sentence-level text news stories . HUSO 2015 : First International Conference Human Social Ana - lytics . Isbell , C . L . , Kearns , M . , Kormann , D . , Singh , S . , Stone , P . ( 2000 ) . Cobot LambdaMOO : social statistics agent . AAAI / IAAI , 36 – 41 . Jafarpour , S . , Burges , C . J . C . , Ritter , . ( 2009 ) . Fil - ter , rank , transfer knowledge : Learning chat . NIPS Workshop Advances Ranking . Jefferson , G . ( 1972 ) . Side sequences . Sudnow , D . ( Ed . ) , Studies social interaction , 294 – 333 . Free Press , New York . Kannan , . Vinyals , O . ( 2016 ) . Adversarial evaluation dialogue models . NIPS 2016 Workshop Adversar - ial Training . Landauer , T . K . ( Ed . ) . ( 1995 ) . Trouble Computers : Usefulness , Usability , Productivity . MIT Press . Lemon , O . , Georgila , K . , Henderson , J . , Stuttle , M . ( 2006 ) . ISU dialogue system exhibiting reinforcement learning dialogue policies : Generic slot-filling TALK in-car system . EACL-06 . Leuski , . Traum , D . ( 2011 ) . NPCEditor : Creating virtual human dialogue information retrieval tech - niques . AI Magazine , 32 ( 2 ) , 42 – 56 . Levesque , H . J . , Cohen , P . R . , Nunes , J . H . T . ( 1990 ) . acting together . AAAI-90 , 94 – 99 . Morgan Kaufmann . Levin , E . , Pieraccini , R . , Eckert , W . ( 2000 ) . stochas - tic model human-machine interaction learning dialog strategies . IEEE Transactions Speech Audio Pro - cessing , 8 , 11 – 23 . Levinson , S . C . ( 1983 ) . Conversational Analysis , chap . 6 . Cambridge University Press . Levow , G . - . ( 1998 ) . Characterizing recognizing spo - ken corrections human-computer dialogue . COLING - ACL , 736 – 742 . Li , J . , Galley , M . , Brockett , C . , Gao , J . , Dolan , B . ( 2016a ) . diversity-promoting objective function neu - ral conversation models . NAACL HLT 2016 . Li , J . , Monroe , W . , Ritter , . , Galley , M . , Gao , J . , Juraf - sky , D . ( 2016b ) . Deep reinforcement learning dialogue generation . EMNLP 2016 . Li , J . , Monroe , W . , Shi , T . , Ritter , . , Jurafsky , D . ( 2017 ) . Adversarial learning neural dialogue genera - tion . EMNLP 2017 . Lin , Z . , Madotto , . , Shin , J . , Xu , P . , Fung , P . ( 2019 ) . MoEL : Mixture empathetic listeners . http://arxiv . org / abs / 1908.07687 . Lison , P . Tiedemann , J . ( 2016 ) . Opensubtitles2016 : Ex - tracting large parallel corpora movie tv subtitles . LREC-16 . Litman , D . J . ( 1985 ) . Plan Recognition Discourse Analysis : Integrated Approach Understanding Di - alogues . Ph . D . thesis , University Rochester , Rochester , NY . Litman , D . J . Allen , J . ( 1987 ) . plan recognition model subdialogues conversation . Cognitive Science , 11 , 163 – 200 . Litman , D . J . , Swerts , M . , Hirschberg , J . ( 2000 ) . Pre - dicting automatic speech recognition performance prosodic cues . NAACL 2000 . Litman , D . J . , Walker , M . . , Kearns , M . ( 1999 ) . Auto - matic detection poor speech recognition dialogue level . ACL-99 , 309 – 316 . Liu , C . - W . , Lowe , R . T . , Serban , . V . , Noseworthy , M . , Charlin , L . , Pineau , J . ( 2016 ) . evalu - ate dialogue system : empirical study unsuper - vised evaluation metrics dialogue response generation . EMNLP 2016 . 34 Chapter 26 • Dialogue Systems Chatbots Lochbaum , K . E . , Grosz , B . J . , Sidner , C . L . ( 2000 ) . Discourse structure intention recognition . Dale , R . , Moisl , H . , Somers , H . L . ( Eds . ) , Handbook Natural Language Processing . Marcel Dekker . Lowe , R . T . , Noseworthy , M . , Serban , . V . , Angelard - Gontier , N . , Bengio , Y . , Pineau , J . ( 2017a ) . automatic Turing test : Learning evaluate dialogue re - sponses . ACL 2017 . Lowe , R . T . , Pow , N . , Serban , . V . , Charlin , L . , Liu , C . - W . , Pineau , J . ( 2017b ) . Training end-to-end dialogue systems ubuntu dialogue corpus . Dialogue & Dis - course , 8 ( 1 ) , 31 – 65 . Mairesse , F . Walker , M . . ( 2008 ) . Trainable generation big-five personality styles data-driven parameter estimation . ACL-08 . Miller , S . , Bobrow , R . J . , Ingria , R . , Schwartz , R . ( 1994 ) . Hidden understanding models natural language . ACL - 94 , 25 – 32 . Mrkšić , N . , Ó Séaghdha , D . , Wen , T . - H . , Thomson , B . , Young , S . ( 2017 ) . Neural belief tracker : Data-driven dia - logue state tracking . ACL 2017 , 1777 – 1788 . Nagata , M . Morimoto , T . ( 1994 ) . First steps toward sta - tistical modeling dialogue predict speech act type next utterance . Speech Communication , 15 , 193 – 203 . Nayak , N . , Hakkani-Tür , D . , Walker , M . . , Heck , L . P . ( 2017 ) . plan plan ? discourse planning slot - value informed sequence sequence models language generation . . INTERSPEECH-17 , 3339 – 3343 . Neff , G . Nagy , P . ( 2016 ) . Talking bots : Symbiotic agency case Tay . International Journal Com - munication , 10 , 4915 – 4931 . Nielsen , J . ( 1992 ) . usability engineering life cycle . IEEE Computer , 25 ( 3 ) , 12 – 22 . Norman , D . . ( 1988 ) . Design Everyday Things . Ba - sic Books . Paolino , J . ( 2017 ) . Google Home vs Alexa : Two simple user experience design gestures delighted female user . Medium . Jan 4 , 2017 . https://medium.com/startup-grind/ google-home-vs-alexa-56e26f69ac77 . Pearl , C . ( 2017 ) . Designing Voice User Interfaces : Princi - ples Conversational Experiences . O’Reilly . Perrault , C . R . Allen , J . ( 1980 ) . plan-based analysis indirect speech acts . American Journal Computational Linguistics , 6 ( 3-4 ) , 167 – 182 . Pieraccini , R . , Levin , E . , Lee , C . - H . ( 1991 ) . Stochastic representation conceptual structure ATIS task . Proceedings DARPA Speech Natural Language Work - shop , 121 – 124 . Polifroni , J . , Hirschman , L . , Seneff , S . , Zue , V . W . ( 1992 ) . Experiments evaluating interactive spoken lan - guage systems . Proceedings DARPA Speech Natural Language Workshop , 28 – 33 . Purver , M . ( 2004 ) . theory clarification re - quests dialogue . Ph . D . thesis , University London . Rashkin , H . , Smith , E . M . , Li , M . , Boureau , Y . - L . ( 2019 ) . empathetic open-domain conversation models : new benchmark dataset . ACL 2019 , 5370 – 5381 . Ritter , . , Cherry , C . , Dolan , B . ( 2010 ) . Unsupervised modeling twitter conversations . HLT-NAACL . Ritter , . , Cherry , C . , Dolan , B . ( 2011 ) . Data-driven response generation social media . EMNLP-11 , 583 – 593 . Roy , N . , Pineau , J . , Thrun , S . ( 2000 ) . Spoken dialog management robots . ACL-00 . Sacks , H . , Schegloff , E . . , Jefferson , G . ( 1974 ) . simplest systematics organization turn-taking conversation . Language , 50 ( 4 ) , 696 – 735 . Sag , . . Liberman , M . Y . ( 1975 ) . intonational dis - ambiguation indirect speech acts . CLS-75 , 487 – 498 . University Chicago . Schegloff , E . . ( 1968 ) . Sequencing conversational open - ings . American Anthropologist , 70 , 1075 – 1095 . Serban , . V . , Lowe , R . , Henderson , P . , Charlin , L . , Pineau , J . ( 2018 ) . survey available corpora build - ing data-driven dialogue systems : journal version . Di - alogue & Discourse , 9 ( 1 ) , 1 – 49 . Shang , L . , Lu , Z . , Li , H . ( 2015 ) . Neural responding machine short-text conversation . ACL 2015 , 1577 – 1586 . Shriberg , E . , Bates , R . , Taylor , P . , Stolcke , . , Jurafsky , D . , Ries , K . , Coccaro , N . , Martin , R . , Meteer , M . , Van Ess - Dykema , C . ( 1998 ) . prosody aid automatic classi - fication dialog acts conversational speech ? . Language Speech ( Special Issue Prosody Conversation ) , 41 ( 3-4 ) , 439 – 487 . Singh , S . P . , Litman , D . J . , Kearns , M . , Walker , M . . ( 2002 ) . Optimizing dialogue management reinforce - ment learning : Experiments NJFun system . JAIR , 16 , 105 – 133 . Smith , V . L . Clark , H . H . ( 1993 ) . course - swering questions . Journal Memory Language , 32 , 25 – 38 . Sordoni , . , Galley , M . , Auli , M . , Brockett , C . , Ji , Y . , Mitchell , M . , Nie , J . - Y . , Gao , J . , Dolan , B . ( 2015 ) . neural network approach context-sensitive generation conversational responses . NAACL HLT 2015 , 196 – 205 . Stalnaker , R . C . ( 1978 ) . Assertion . Cole , P . ( Ed . ) , Prag - matics : Syntax Semantics Volume 9 , 315 – 332 . Aca - demic Press . Stifelman , L . J . , Arons , B . , Schmandt , C . , Hulteen , E . . ( 1993 ) . VoiceNotes : speech interface hand-held voice notetaker . Human Factors Computing Systems : INTERCHI ’ 93 Conference Proceedings , 179 – 186 . Stolcke , . , Ries , K . , Coccaro , N . , Shriberg , E . , Bates , R . , Jurafsky , D . , Taylor , P . , Martin , R . , Meteer , M . , Van Ess-Dykema , C . ( 2000 ) . Dialogue act modeling au - tomatic tagging recognition conversational speech . Computational Linguistics , 26 ( 3 ) , 339 – 371 . Stoyanchev , S . Johnston , M . ( 2015 ) . Localized error de - tection targeted clarification virtual assistant . ICASSP-15 , 5241 – 5245 . Stoyanchev , S . , Liu , . , Hirschberg , J . ( 2013 ) . Mod - elling human clarification strategies . SIGDIAL 2013 , 137 – 141 . Stoyanchev , S . , Liu , . , Hirschberg , J . ( 2014 ) . natural clarification questions dialogue systems . AISB symposium questions , discourse dialogue . Exercises 35 Suendermann , D . , Evanini , K . , Liscombe , J . , Hunter , P . , Dayanidhi , K . , Pieraccini , R . ( 2009 ) . rule-based statistical grammars : Continuous improvement large - scale spoken dialog systems . ICASSP-09 , 4713 – 4716 . Swerts , M . , Litman , D . J . , Hirschberg , J . ( 2000 ) . Cor - rections spoken dialogue systems . ICSLP-00 . Tur , G . De Mori , R . ( 2011 ) . Spoken language un - derstanding : Systems extracting semantic information speech . John Wiley & Sons . Vinyals , O . Le , Q . V . ( 2015 ) . neural conversational model . Proceedings ICML Deep Learning Workshop . Wade , E . , Shriberg , E . , Price , P . J . ( 1992 ) . User behav - iors affecting speech recognition . ICSLP-92 , 995 – 998 . Walker , M . . ( 2000 ) . application reinforcement learn - ing dialogue strategy selection spoken dialogue sys - tem email . JAIR , 12 , 387 – 416 . Walker , M . . , Fromer , J . C . , Narayanan , S . S . ( 1998 ) . Learning optimal dialogue strategies : case study spoken dialogue agent email . COLING / ACL-98 , 1345 – 1351 . Walker , M . . , Kamm , C . . , Litman , D . J . ( 2001 ) . - wards developing general models usability PAR - ADISE . Natural Language Engineering : Special Issue Best Practice Spoken Dialogue Systems , 6 ( 3 ) , 363 – 377 . Walker , M . . Whittaker , S . ( 1990 ) . Mixed initiative dialogue : investigation discourse segmentation . ACL-90 , 70 – 78 . Wang , H . , Lu , Z . , Li , H . , Chen , E . ( 2013 ) . dataset research short-text conversations . . EMNLP 2013 , 935 – 945 . Ward , W . Issar , S . ( 1994 ) . Recent improvements CMU spoken language understanding system . ARPA Human Language Technologies Workshop . Weinschenk , S . Barker , D . T . ( 2000 ) . Designing Effec - tive Speech Interfaces . Wiley . Weizenbaum , J . ( 1966 ) . ELIZA – computer program study natural language communication man machine . CACM , 9 ( 1 ) , 36 – 45 . Wen , T . - H . , Gašić , M . , Kim , D . , Mrkšić , N . , Su , P . - H . , Vandyke , D . , Young , S . J . ( 2015a ) . Stochastic lan - guage generation dialogue recurrent neural net - works convolutional sentence reranking . SIGDIAL 2015 , 275 – – 284 . Wen , T . - H . , Gašić , M . , Mrkšić , N . , Su , P . - H . , Vandyke , D . , Young , S . J . ( 2015b ) . Semantically conditioned lstm - based natural language generation spoken dialogue sys - tems . EMNLP 2015 . Wilensky , R . ( 1983 ) . Planning Understanding : Computational Approach Human Reasoning . Addison - Wesley . Williams , J . D . , Raux , . , Henderson , M . ( 2016 ) . dialog state tracking challenge series : review . Dialogue & Discourse , 7 ( 3 ) , 4 – 33 . Williams , J . D . Young , S . J . ( 2007 ) . Partially observ - able markov decision processes spoken dialog systems . Computer Speech Language , 21 ( 1 ) , 393 – 422 . Wittgenstein , L . ( 1953 ) . Philosophical Investigations . ( Translated Anscombe , G.E.M . ) . Blackwell . Yan , Z . , Duan , N . , Bao , J . - W . , Chen , P . , Zhou , M . , Li , Z . , Zhou , J . ( 2016 ) . DocChat : information retrieval ap - proach chatbot engines unstructured documents . ACL 2016 . Yankelovich , N . , Levow , G . - . , Marx , M . ( 1995 ) . De - signing SpeechActs : Issues speech user interfaces . Human Factors Computing Systems : CHI ’ 95 Confer - ence Proceedings , 369 – 376 . Young , S . J . , Gašić , M . , Keizer , S . , Mairesse , F . , Schatz - mann , J . , Thomson , B . , Yu , K . ( 2010 ) . Hid - den Information State model : practical framework POMDP-based spoken dialogue management . Computer Speech & Language , 24 ( 2 ) , 150 – 174 . Zhou , L . , Gao , J . , Li , D . , Shum , H . - Y . ( 2018 ) . de - sign implementation XiaoIce , empathetic social chatbot . . Zue , V . W . , Glass , J . , Goodine , D . , Leung , H . , Phillips , M . , Polifroni , J . , Seneff , S . ( 1989 ) . Preliminary evaluation VOYAGER spoken language system . Proceedings DARPA Speech Natural Language Workshop , 160 – 167 .