Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 6 Vector Semantics Embed-dings asphalt Los Angeles famous occurs mainly freeways . middle city another patch asphalt , La Brea tar pits , asphalt preserves millions fossil bones last Ice Ages Pleistocene Epoch . fossils Smilodon , sabre-toothed tiger , instantly rec - ognizable long canines . Five million years ago , completely different sabre-tooth tiger called Thylacosmilus lived Argentina parts South Amer - ica . Thylacosmilus marsupial whereas Smilodon placental mammal , Thy - lacosmilus same long upper canines , like Smilodon , protective bone flange lower jaw . similarity two mammals many examples parallel convergent evolution , particular contexts environments lead evolution similar structures different species ( Gould , 1980 ) . role context important similarity less biological kind organism : word . Words occur similar contexts tend similar meanings . link similarity words distributed similarity mean called distributional hypothesis . hypothesis wasdistributionalhypothesis first formulated 1950s linguists like Joos ( 1950 ) , Harris ( 1954 ) , Firth ( 1957 ) , noticed words synonyms ( like oculist eye-doctor ) tended occur same environment ( e.g . , near words like eye examined ) amount meaning difference two words “ corresponding roughly amount difference environments ” ( Harris , 1954 , 157 ) . chapter introduce vector semantics , instantiates linguisticvectorsemantics hypothesis learning representations meaning words , called embeddings , embeddings directly distributions texts . representations every natural language processing application makes meaning , underlie powerful contextualized word representations like ELMo BERT introduce Chapter 10 . word representations first example book repre - sentation learning , automatically learning useful representations input text . representationlearning Finding self-supervised ways learn representations input , creating representations hand via feature engineering , important focus NLP research ( Bengio et al . , 2013 ) . begin , , introducing basic principles word meaning , motivate vector semantic models chapter well extensions return Chapter 19 , Chapter 20 , Chapter 21 . 2 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS 6.1 Lexical Semantics represent meaning word ? N-gram models saw Chapter 3 , many traditional NLP applications , representation word string letters , perhaps index vocabulary list . representation different tradition philosophy , perhaps seen introductory logic classes , meaning words represented just spelling word small capital letters ; representing meaning “ dog ” DOG , “ cat ” CAT ) . Representing meaning word capitalizing pretty unsatisfactory model . might seen old philosophy joke : Q : meaning life ? : LIFE Surely better ! , model word meaning sorts things . tell words similar mean - ings ( cat similar dog ) , words antonyms ( cold opposite hot ) . know words positive connotations ( happy ) others negative connotations ( sad ) . represent fact meanings buy , sell , pay offer differing perspectives same underlying purchasing event ( buy something , probably sold , likely paid ) . generally , model word meaning allow draw useful infer - ences help solve meaning-related tasks like question-answering , sum - marization , detecting paraphrases plagiarism , dialogue . section summarize desiderata , drawing results linguistic study word meaning , called lexical semantics ; return tolexicalsemantics expand list Chapter 19 . Lemmas Senses start looking word ( choose mouse ) might defined dictionary : 1 mouse ( N ) 1 . numerous small rodents . . . 2 . hand-operated device controls cursor . . . form mouse lemma , called citation form . formlemma citation form mouse lemma word mice ; dictionaries separate definitions inflected forms like mice . Similarly sing lemma sing , sang , sung . many languages infinitive form lemma verb , Spanish dormir “ sleep ” lemma duermes “ sleep ” . specific forms sung carpets sing duermes called wordforms . wordform example shows , lemma multiple meanings ; lemma mouse refer rodent cursor control device . call aspects meaning mouse word sense . fact lemmas polysemous ( multiple senses ) make interpretation difficult ( someone types “ mouse info ” search engine looking pet tool ? ) . Chapter 19 discuss problem polysemy , introduce word sense disambiguation , task determining sense word particular context . Synonymy important component word meaning relationship - tween word senses . example word sense whose meaning 1 example shortened online dictionary WordNet , discussed Chapter 19 . 6.1 • LEXICAL SEMANTICS 3 identical sense another word , nearly identical , say two senses two words synonyms . Synonyms include pairs assynonym couch / sofa vomit / throw up filbert / hazelnut car / automobile formal definition synonymy ( words rather senses ) two words synonymous substitutable sentence changing truth conditions sentence , situations sentence true . often say case two words same propositional meaning . propositionalmeaning substitutions pairs words like car / automobile wa - ter / H2O truth preserving , words still identical meaning . Indeed , probably two words absolutely identical meaning . fundamen - tal tenets semantics , called principle contrast ( Girard 1718 , Bréal 1897 , principle ofcontrast Clark 1987 ) , assumption difference linguistic form always associ - ated least difference meaning . example , word H2O scientific contexts inappropriate hiking guide — water appropriate — difference genre part meaning word . practice , word synonym commonly describe relationship approximate rough synonymy . Word Similarity words many synonyms , words lots similar words . Cat synonym dog , cats dogs certainly similar words . moving synonymy similarity , useful shift talking relations word senses ( like synonymy ) relations words ( like similarity ) . Dealing words avoids commit particular representation word senses , turn simplify task . notion word similarity useful larger semantic tasks . Know-similarity ing similar two words help computing similar meaning two phrases sentences , important component natural language un - derstanding tasks like question answering , paraphrasing , summarization . way getting values word similarity ask humans judge similar word another . number datasets resulted experiments . example SimLex-999 dataset ( Hill et al . , 2015 ) gives values scale 0 10 , like examples below , range near-synonyms ( vanish , disappear ) pairs scarcely seem anything common ( hole , agreement ) : vanish disappear 9.8 behave obey 7.3 belief impression 5.95 muscle bone 3.65 modest flexible 0.98 hole agreement 0.3 Word Relatedness meaning two words related ways similarity . class connections called word relatedness ( Budanitskyrelatedness Hirst , 2006 ) , traditionally called word association psychology . association Consider meanings words coffee cup . Coffee similar cup ; share practically features ( coffee plant beverage , cup manufactured object particular shape ) . coffee cup clearly related ; associated co-participating everyday event ( event drinking coffee cup ) . Similarly nouns 4 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS scalpel surgeon similar related eventively ( surgeon tends make scalpel ) . common kind relatedness words belong same semantic field . semantic field set words cover particular semanticsemantic field domain bear structured relations . example , words might related semantic field hospitals ( surgeon , scalpel , nurse , anesthetic , hospital ) , restaurants ( waiter , menu , plate , food , chef ) , houses ( door , roof , kitchen , family , bed ) . Semantic fields related topic models , like Latent Dirichlet Alloca-topic models tion , LDA , apply unsupervised learning large sets texts induce sets associated words text . Semantic fields topic models useful tools discovering topical structure documents . Chapter 19 introduce even relations senses , including hypernymy IS-A , antonymy ( opposite meaning ) meronymy ) ( part-whole relations ) . Semantic Frames Roles Closely related semantic fields idea semantic frame . semantic frame set words denote perspectives orsemantic frame participants particular type event . commercial transaction , example , kind event entity trades money another entity return good service , good changes hands perhaps service performed . event encoded lexically verbs like buy ( event perspective buyer ) , sell ( perspective seller ) , pay ( focusing monetary aspect ) , nouns like buyer . Frames semantic roles ( like buyer , seller , goods , money ) , words sentence take roles . Knowing buy sell relation makes possible system know sentence like Sam bought book Ling paraphrased Ling sold book Sam , Sam role buyer frame Ling seller . able recognize paraphrases important question answering , help shifting perspective machine translation . Connotation Finally , words affective meanings connotations . wordconnotations connotation different meanings different fields , mean aspects word’s meaning related writer reader’s emotions , sentiment , opinions , evaluations . example words positive conno - tations ( happy ) others negative connotations ( sad ) . words describe positive evaluation ( great , love ) others negative evaluation ( terrible , hate ) . Pos - itive negative evaluation expressed language called sentiment , wesentiment saw Chapter 4 , word sentiment plays role important tasks like sentiment analysis , stance detection , many applications natural language processing language politics consumer reviews . Early work affective meaning ( Osgood et al . , 1957 ) found words varied three important dimensions affective meaning . generally called valence , arousal , dominance , defined follows : valence : pleasantness stimulus arousal : intensity emotion provoked stimulus dominance : degree control exerted stimulus Thus words like happy satisfied high valence , unhappy - noyed low valence . Excited frenzied high arousal , relaxed calm low arousal . Important controlling high dominance , awed influenced low dominance . word thus represented three 6.2 • VECTOR SEMANTICS 5 numbers , corresponding value three dimensions , like exam - ples below : Valence Arousal Dominance courageous 8.05 5.5 7.38 music 7.67 5.57 6.5 heartbreak 2.45 5.65 3.58 cub 6.71 3.95 4.24 life 6.68 5.59 5.89 Osgood et al . ( 1957 ) noticed 3 numbers represent meaning word , model representing word point three - dimensional space , vector whose three dimensions corresponded word’s rating three scales . revolutionary idea word meaning word represented point space ( e.g . , part meaning heartbreak represented point [ 2.45,5.65,3.58 ] ) first expression vector semantics models introduce next . 6.2 Vector Semantics build computational model successfully deals different aspects word meaning saw previous section ( word senses , word simi - larity relatedness , lexical fields frames , connotation ) ? perfect model completely deals aspects word mean - ing turns elusive . current best model , called vector semantics , vectorsemantics draws inspiration linguistic philosophical work 1950 ’ s . period , philosopher Ludwig Wittgenstein , skeptical possi - bility building completely formal theory meaning definitions word , suggested “ meaning word language ” ( Wittgen - stein , 1953 , PI 43 ) . , logical language define word , define words representation word actual people speaking understanding . Linguists period like Joos ( 1950 ) , Harris ( 1954 ) , Firth ( 1957 ) ( lin - guistic distributionalists ) , came up specific idea realizing Wittgenstein’s intuition : define word environment distribution language . word’s distribution set contexts occurs , neighboring words gram - matical environments . idea two words occur similar dis - tributions ( occur together similar words ) likely same meaning . example illustrating distributionalist approach . Suppose know Cantonese word ongchoi meant , fol - lowing sentences contexts : ( 6.1 ) Ongchoi delicious sauteed garlic . ( 6.2 ) Ongchoi superb rice . ( 6.3 ) . . . ongchoi leaves salty sauces . . . furthermore suppose seen many context words occurring contexts like : ( 6.4 ) . . . spinach sauteed garlic rice . . . 6 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS ( 6.5 ) . . . chard stems leaves delicious . . . ( 6.6 ) . . . collard greens salty leafy greens fact ongchoi occurs words like rice garlic delicious salty , words like spinach , chard , collard greens might suggest reader ongchoi leafy green similar leafy greens . 2 same thing computationally just counting words context ongchoi ; tend words like sauteed eaten garlic . fact words similar context words occur around word spinach collard greens help discover similarity words ongchoi . Vector semantics thus combines two intuitions : distributionalist intuition ( defining word counting words occur environment ) , vector intuition Osgood et al . ( 1957 ) saw last section connotation : defining meaning word w vector , list numbers , point N - dimensional space . various versions vector semantics , defining numbers vector somewhat differently , case numbers based way counts neighboring words . good nice bad worst good wonderful amazing terrific dislike worse good incredibly good fantastic incredibly badnow youi byto ’ s Figure 6.1 two-dimensional ( t-SNE ) projection embeddings words phrases , showing words similar meanings nearby space . original 60 - dimensional embeddings trained sentiment analysis . Simplified Li et al . ( 2015 ) . idea vector semantics thus represent word point multi - dimensional semantic space . Vectors representing words generally called embeddings , word embedded particular vector space . Fig . 6.1embeddings displays visualization embeddings learned sentiment analysis task , showing location selected words projected down original 60-dimensional space two dimensional space . Notice positive negative words seem located distinct portions space ( different neutral function words ) . suggests great advantages vector semantics : offers fine-grained model meaning implement word similarity ( phrase similarity ) . example , sentiment analysis classifier saw Chapter 4 works enough important sentimental words appear test set appeared training set . words represented embeddings , assign sentiment long words similar meanings test set words occurred training 2 fact Ipomoea aquatica , relative morning glory sometimes called water spinach English . 6.3 • WORDS VECTORS 7 set . Vector semantic models extremely practical learned automatically text complex labeling supervision . result advantages , vector models meaning standard way represent meaning words NLP . chapter introduce two commonly models . First tf-idf model , often baseline , meaning word defined simple function counts nearby words . method results long vectors sparse , i.e . contain mostly zeros ( words simply never occur context others ) . introduce word2vec model , family models ways constructing short , dense vectors useful semantic properties . introduce cosine , standard way embeddings ( vectors ) compute functions like semantic similarity , similarity two words , two sentences , two documents , important tool practical applications like question answering , summarization , automatic essay grading . 6.3 Words Vectors Vector distributional models meaning generally based co-occurrence matrix , way representing often words co-occur . matrix con - structed various ways ; begin looking co-occurrence matrix , term-document matrix . 6.3.1 Vectors documents term-document matrix , row represents word vocabulary eachterm-documentmatrix column represents document collection documents . Fig . 6.2 shows small selection term-document matrix showing occurrence four words four plays Shakespeare . cell matrix represents number times particular word ( defined row ) occurs particular document ( defined column ) . Thus fool appeared 58 times Twelfth Night . Like Twelfth Night Julius Caesar Henry V battle 1 0 7 13 good 114 80 62 89 fool 36 58 1 4 wit 20 15 2 3 Figure 6.2 term-document matrix four words four Shakespeare plays . cell contains number times ( row ) word occurs ( column ) document . term-document matrix Fig . 6.2 first defined part vector space model information retrieval ( Salton , 1971 ) . model , document isvector spacemodel represented count vector , column Fig . 6.3 . review basic linear algebra , vector , heart , just list arrayvector numbers . Like represented list [ 1,114,36,20 ] Julius Caesar represented list [ 7,62,1,2 ] . vector space collection vectors , vector space characterized dimension . example Fig . 6.3 , vectors ofdimension dimension 4 , just fit page ; real term-document matrices , vectors representing document dimensionality | V | , vocabulary size . 8 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS ordering numbers vector space arbitrary ; position indicates meaningful dimension documents vary . Thus first dimension vectors corresponds number times word battle occurs , compare dimension , noting example vectors Like Twelfth Night similar values ( 1 0 , respectively ) first dimension . Like Twelfth Night Julius Caesar Henry V battle 1 0 7 13 good 114 80 62 89 fool 36 58 1 4 wit 20 15 2 3 Figure 6.3 term-document matrix four words four Shakespeare plays . red boxes show document represented column vector length four . think vector document identifying point | V | - dimensional space ; thus documents Fig . 6.3 points 4-dimensional space . 4 - dimensional spaces hard draw textbooks , Fig . 6.4 shows visualization two dimensions ; arbitrarily chosen dimensions corresponding words battle fool . 5 10 15 20 25 30 5 10 Henry V [ 4,13 ] Like [ 36,1 ] Julius Caesar [ 1,7 ] ba ttl e fool Twelfth Night [ 58,0 ] 15 40 35 40 45 50 55 60 Figure 6.4 spatial visualization document vectors four Shakespeare play documents , showing just two dimensions , corresponding words battle fool . comedies high values fool dimension low values battle dimension . Term-document matrices originally defined means finding similar documents task document information retrieval . Two documents similar tend similar words , two documents similar words column vectors tend similar . vectors comedies Like [ 1,114,36,20 ] Twelfth Night [ 0,80,58,15 ] look lot like ( fools wit battles ) look like Julius Caesar [ 7,62,1,2 ] Henry V [ 13,89,4,3 ] . clear raw numbers ; first dimension ( battle ) comedies low numbers others high numbers , visually Fig . 6.4 ; shortly quantify intuition formally . real term-document matrix , course , just 4 rows columns , let alone 2 . generally , term-document matrix | V | rows ( word type vocabulary ) D columns ( document collec - tion ) ; , vocabulary sizes generally tens thousands , number documents enormous ( think pages web ) . 6.3 • WORDS VECTORS 9 Information retrieval ( IR ) task finding document d Dinformationretrieval documents collection best matches query q . IR represent query vector , length | V | , need way compare two vectors find similar . ( IR require efficient ways store manipulate vectors making convenient fact vectors sparse , i.e . , mostly zeros ) . Later chapter introduce components vector com - parison process : tf-idf term weighting , cosine similarity metric . 6.3.2 Words vectors seen documents represented vectors vector space . vector semantics represent meaning words , associating word vector . word vector row vector rather column vector , hence therow vector dimensions vector different . four dimensions vector fool , [ 36,58,1,4 ] , correspond four Shakespeare plays . same four dimensions form vectors 3 words : wit , [ 20,15,2,3 ] ; battle , [ 1,0,7,13 ] ; good [ 114,80,62,89 ] . entry vector thus represents counts word’s occurrence document corresponding dimension . documents , saw similar documents similar vectors , sim - ilar documents tend similar words . same principle applies words : similar words similar vectors tend occur similar documents . term-document matrix thus represent meaning word doc - uments tends occur . , common different kind context dimensions word’s vector representation . Rather term-document matrix term-term matrix , commonly called word-word matrix term-word-wordmatrix context matrix , columns labeled words rather documents . matrix thus dimensionality | V | × | V | cell records number times row ( target ) word column ( context ) word co-occur context training corpus . context document , case cell represents number times two words appear same document . common , , smaller contexts , generally window around word , example 4 words left 4 words right , case cell represents number times ( training corpus ) column word occurs ± 4 word window around row word . example example words windows : traditionally followed cherry pie , traditional dessert often mixed , strawberry rhubarb pie . Apple pie computer peripherals personal digital assistants . devices usually computer . includes information available internet take every occurrence word ( say strawberry ) count con - text words around , get word-word co-occurrence matrix . Fig . 6.5 shows simplified subset word-word co-occurrence matrix four words com - puted Wikipedia corpus ( Davies , 2015 ) . Note Fig . 6.5 two words cherry strawberry similar ( pie sugar tend occur window ) words like digital ; conversely , digital information similar , say , strawberry . Fig . 6.6 shows spatial visualization . 10 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS aardvark . . . computer data result pie sugar . . . cherry 0 . . . 2 8 9 442 25 strawberry 0 . . . 0 0 1 60 19 digital 0 . . . 1670 1683 85 5 4 information 0 . . . 3325 3982 378 5 13 Figure 6.5 Co-occurrence vectors four words Wikipedia corpus , showing six dimensions ( hand-picked pedagogical purposes ) . vector digital outlined red . Note real vector vastly dimensions thus sparser . 1000 2000 3000 4000 1000 2000 digital [ 1683,1670 ] co m pu te r data information [ 3982,3325 ] 3000 4000 Figure 6.6 spatial visualization word vectors digital information , showing just two dimensions , corresponding words data computer . Note | V | , length vector , generally size vocabulary , usually 10,000 50,000 words ( frequent words training corpus ; keeping words frequent 50,000 gener - ally helpful ) . course numbers zero sparse vector representations , efficient algorithms storing computing sparse matrices . intuitions , move examine details com - puting word similarity . Afterwards discuss tf-idf method weighting cells . 6.4 Cosine measuring similarity define similarity two target words v w , need measure taking two vectors giving measure vector similarity . far common similarity metric cosine angle vectors . cosine — like measures vector similarity NLP — based dot product operator linear algebra , called inner product : dot product inner product dot product ( v , w ) = v · w = N ∑ = 1 viwi = v1w1 + v2w2 + . . . + vNwN ( 6.7 ) , metrics similarity vectors based dot product . dot product acts similarity metric tend high just two vectors large values same dimensions . Alternatively , vectors zeros different dimensions — orthogonal vectors — dot product 0 , representing strong dissimilarity . 6.4 • COSINE MEASURING SIMILARITY 11 raw dot product , , problem similarity metric : favors long vectors . vector length defined asvector length | v | = √ √ √ √ N ∑ = 1 v2i ( 6.8 ) dot product higher vector longer , higher values dimension . frequent words longer vectors , tend co-occur words higher co-occurrence values . raw dot product thus higher frequent words . problem ; like similarity metric tells similar two words regardless frequency . simplest way modify dot product normalize vector length divide dot product lengths two vectors . normalized dot product turns same cosine angle two vectors , following definition dot product two vectors b : · b = | | | b | cosθ · b | | | b | = cosθ ( 6.9 ) cosine similarity metric two vectors v w thus computed : cosine cosine ( v , w ) = v · w | v | | w | = N ∑ = 1 viwi √ √ √ √ N ∑ = 1 v2i √ √ √ √ N ∑ = 1 w2i ( 6.10 ) applications pre-normalize vector , dividing length , creating unit vector length 1 . Thus compute unit vector byunit vector dividing | | . unit vectors , dot product same cosine . cosine value ranges 1 vectors pointing same direction , 0 vectors orthogonal , - 1 vectors pointing opposite directions . raw frequency values non-negative , cosine vectors ranges 0 – 1 . cosine computes words cherry digital closer meaning information , just raw counts following shortened table : pie data computer cherry 442 8 2 digital 5 1683 1670 information 5 3982 3325 cos ( cherry , information ) = 442 ∗ 5 + 8 ∗ 3982 + 2 ∗ 3325 √ 4422 + 82 + 22 √ 52 + 39822 + 33252 = . 017 cos ( digital , information ) = 5 ∗ 5 + 1683 ∗ 3982 + 1670 ∗ 3325 √ 52 + 16832 + 16702 √ 52 + 39822 + 33252 = . 996 model decides information way closer digital cherry , result seems sensible . Fig . 6.7 shows visualization . 12 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS 500 1000 1500 2000 2500 3000 500 digital cherry information D en si 1 : ‘ pi e ’ Dimension 2 : ‘ computer ’ Figure 6.7 ( rough ) graphical demonstration cosine similarity , showing vectors three words ( cherry , digital , information ) two dimensional space defined counts words computer pie nearby . Note angle digital information smaller angle cherry information . two vectors similar , cosine larger angle smaller ; cosine maximum ( 1 ) angle two vectors smallest ( 0 ◦ ) ; cosine angles less 1 . 6.5 TF-IDF : Weighing terms vector co-occurrence matrix Fig . 6.5 represented cell raw frequency co-occurrence two words . turns , , simple frequency best measure association words . problem raw frequency skewed discriminative . know kinds contexts shared cherry strawberry digital information , going get good discrimi - nation words like , , , occur frequently sorts words informative particular word . saw Fig . 6.3 Shakespeare corpus ; dimension word good discrimina - tive plays ; good simply frequent word roughly equivalent high frequencies plays . bit paradox . Words occur nearby frequently ( maybe pie nearby cherry ) important words appear once twice . Yet words frequent — ubiquitous , like good — unimportant . balance two conflicting constraints ? tf-idf algorithm ( ‘ - ’ hyphen , minus sign ) product two terms , term capturing two intuitions : first term frequency ( Luhn , 1957 ) : frequency word t theterm frequency document d . just raw count term frequency : tft , d = count ( t , d ) ( 6.11 ) Alternatively squash raw frequency bit , log10 fre - quency . intuition word appearing 100 times document make word 100 times likely relevant meaning document . take log 0 , normally add 1 count : 3 tft , d = log10 ( count ( t , d ) + 1 ) ( 6.12 ) log weighting , terms occur 10 times document tf = 2 , 100 times document tf = 3 , 1000 times tf = 4 , . 3 alternative : tft , d = { 1 + log10 count ( t , d ) count ( t , d ) > 0 0 otherwise 6.5 • TF-IDF : WEIGHING TERMS VECTOR 13 second factor give higher weight words occur few documents . Terms limited few documents useful discrimi - nating documents rest collection ; terms occur frequently entire collection helpful . document frequency dft adocumentfrequency term t number documents occurs . Document frequency same collection frequency term , total number times word appears whole collection document . Consider collection Shakespeare’s 37 plays two words Romeo action . words identical collection frequencies ( occur 113 times plays ) different document frequencies , Romeo occurs single play . goal find documents romantic tribulations Romeo , word Romeo highly weighted , action : Collection Frequency Document Frequency Romeo 113 1 action 113 31 emphasize discriminative words like Romeo via inverse document fre - quency idf term weight ( Sparck Jones , 1972 ) . idf defined frac-idf tion N / dft , N total number documents collection , dft number documents term t occurs . fewer documents term occurs , higher weight . lowest weight 1 assigned terms occur documents . usually clear counts document : Shake - speare play ; processing collection encyclopedia articles like Wikipedia , document Wikipedia page ; processing newspaper articles , document single article . Occasionally corpus might appropri - ate document divisions might need break up corpus documents yourself purposes computing idf . large number documents many collections , measure usually squashed log function . resulting definition inverse document frequency ( idf ) thus idft = log10 ( N dft ) ( 6.13 ) idf values words Shakespeare corpus , ranging extremely informative words occur play like Romeo , occur few like salad Falstaff , common like fool common completely non-discriminative occur 37 plays like good sweet . 4 Word df idf Romeo 1 1.57 salad 2 1.27 Falstaff 4 0.967 forest 12 0.489 battle 21 0.246 wit 34 0.037 fool 36 0.012 good 37 0 sweet 37 0 4 Sweet Shakespeare’s favorite adjectives , fact probably related increased sugar European recipes around turn 16th century ( Jurafsky , 2014 , p . 175 ) . 14 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS tf-idf weighted value wt , d word t document d thus combines termtf-idf frequency tft , d ( defined Eq . 6.11 Eq . 6.12 ) idf Eq . 6.13 : wt , d = tft , d × idft ( 6.14 ) Fig . 6.8 applies tf-idf weighting Shakespeare term-document matrix Fig . 6.2 , tf equation Eq . 6.12 . Note tf-idf values dimension corre - sponding word good become 0 ; word appears every document , tf-idf algorithm leads ignored comparison plays . Similarly , word fool , appears 36 37 plays , lower weight . Like Twelfth Night Julius Caesar Henry V battle 0.074 0 0.22 0.28 good 0 0 0 0 fool 0.019 0.021 0.0036 0.0083 wit 0.049 0.044 0.018 0.022 Figure 6.8 tf-idf weighted term-document matrix four words four Shakespeare plays , counts Fig . 6.2 . example 0.049 value wit Like product tf = log10 ( 20 + 1 ) = 1.322 idf = . 037 . Note idf weighting eliminated importance ubiquitous word good vastly reduced impact almost-ubiquitous word fool . tf-idf weighting way weighting co-occurrence matrices infor - mation retrieval , plays role many aspects natural language processing . great baseline , simple thing try first . look weightings like PPMI ( Positive Pointwise Mutual Information ) Section 6.7 . 6.6 Applications tf-idf vector model summary , vector semantics model described far represents target word vector dimensions corresponding words vocabulary ( length | V | , vocabularies 20,000 50,000 ) , sparse ( values zero ) . values dimension frequency target word co-occurs neighboring context word , weighted tf-idf . model computes similarity two words x y taking cosine tf-idf vectors ; high cosine , high similarity . entire model sometimes referred short tf-idf model , weighting function . common tf-idf model compute word similarity , useful tool tasks like finding word paraphrases , tracking changes word meaning , au - tomatically discovering meanings words different corpora . example , find 10 similar words target word w computing cosines w V − 1 words , sorting , looking top 10 . tf-idf vector model decide two documents similar . represent document taking vectors words document , computing centroid vectors . centroid multidimensionalcentroid version mean ; centroid set vectors single vector minimum sum squared distances vectors set . k word vectors w1 , w2 , . . . , wk , centroid document vector d : documentvector d = w1 + w2 + . . . + wk k ( 6.15 ) 6.7 • OPTIONAL : POINTWISE MUTUAL INFORMATION ( PMI ) 15 two documents , compute document vectors d1 d2 , estimate similarity two documents cos ( d1 , d2 ) . Document similarity useful sorts applications ; information re - trieval , plagiarism detection , news recommender systems , even digital hu - manities tasks like comparing different versions text similar . 6.7 Optional : Pointwise Mutual Information ( PMI ) alternative weighting function tf-idf called PPMI ( positive pointwise mutual information ) . PPMI draws intuition best way weigh association two words ask two words co-occur corpus priori expected appear chance . Pointwise mutual information ( Fano , 1961 ) 5 important con - pointwise mutual information cepts NLP . measure often two events x y occur , compared expect independent : ( x , y ) = log2 P ( x , y ) P ( x ) P ( y ) ( 6.17 ) pointwise mutual information target word w context word c ( Church Hanks 1989 , Church Hanks 1990 ) defined : PMI ( w , c ) = log2 P ( w , c ) P ( w ) P ( c ) ( 6.18 ) numerator tells often observed two words together ( assuming compute probability MLE ) . denominator tells often expect two words co-occur assuming occurred indepen - dently ; recall probability two independent events occurring just product probabilities two events . Thus , ratio gives esti - mate two words co-occur expect chance . PMI useful tool whenever need find words strongly associated . PMI values range negative positive infinity . negative PMI values ( imply things co-occurring less often expect chance ) tend unreliable unless corpora enormous . distinguish two words whose individual probability 10 − 6 occur together less often chance , need certain probability two occurring - gether significantly different 10 − 12 , kind granularity require enormous corpus . Furthermore clear even possible evalu - ate scores ‘ unrelatedness ’ human judgments . reason common Positive PMI ( called PPMI ) replaces negative PMI valuesPPMI 5 Pointwise mutual information based mutual information two random variables X Y , defined : ( X , Y ) = ∑ x ∑ y P ( x , y ) log2 P ( x , y ) P ( x ) P ( y ) ( 6.16 ) confusion terminology , Fano phrase mutual information refer call pointwise mutual information phrase expectation mutual information call mutual information 16 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS zero ( Church Hanks 1989 , Dagan et al . 1993 , Niwa Nitta 1994 ) 6 : PPMI ( w , c ) = max ( log2 P ( w , c ) P ( w ) P ( c ) , 0 ) ( 6.19 ) formally , assume co-occurrence matrix F W rows ( words ) C columns ( contexts ) , fi j gives number times word wi occurs context c j . turned PPMI matrix ppmii j gives PPMI value word wi context c j follows : pi j = fi j ∑ W = 1 ∑ C j = 1 fi j pi ∗ = ∑ C j = 1 fi j ∑ W = 1 ∑ C j = 1 fi j p ∗ j = ∑ W = 1 fi j ∑ W = 1 ∑ C j = 1 fi j ( 6.20 ) PPMIi j = max ( log2 pi j pi ∗ p ∗ j , 0 ) ( 6.21 ) PPMI calculations . Fig . 6.9 , repeats Fig . 6.5 plus count marginals , pretend ease calculation words / contexts matter . computer data result pie sugar count ( w ) cherry 2 8 9 442 25 486 strawberry 0 0 1 60 19 80 digital 1670 1683 85 5 4 3447 information 3325 3982 378 5 13 7703 count ( context ) 4997 5673 473 512 61 11716 Figure 6.9 Co-occurrence counts four words 5 contexts Wikipedia corpus , together marginals , pretending purpose calculation words / contexts matter . Thus example compute PPMI ( w = information , c = data ) , assuming pretended Fig . 6.5 encompassed relevant word contexts / dimensions , follows : P ( w = information , c = data ) = 3982 11716 = . 3399 P ( w = information ) = 7703 11716 = . 6575 P ( c = data ) = 5673 11716 = . 4842 ppmi ( information , data ) = log2 ( . 3399 / ( . 6575 ∗ . 4842 ) ) = . 0944 Fig . 6.10 shows joint probabilities computed counts Fig . 6.9 , Fig . 6.11 shows PPMI values . surprisingly , cherry strawberry highly associated pie sugar , data mildly associated information . PMI problem biased toward infrequent events ; rare words tend high PMI values . way reduce bias toward low frequency events slightly change computation P ( c ) , different function Pα ( c ) raises probability context word power α : PPMIα ( w , c ) = max ( log2 P ( w , c ) P ( w ) Pα ( c ) , 0 ) ( 6.22 ) 6 Positive PMI cleanly solves problem zero counts , 0 replace − ∞ log ( 0 ) . 6.8 • WORD2VEC 17 p ( w , context ) p ( w ) computer data result pie sugar p ( w ) cherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415 strawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068 digital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942 information 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575 p ( context ) 0.4265 0.4842 0.0404 0.0437 0.0052 Figure 6.10 Replacing counts Fig . 6.5 joint probabilities , showing marginals around outside . computer data result pie sugar cherry 0 0 0 4.38 3.30 strawberry 0 0 0 4.10 5.51 digital 0.18 0.01 0 0 0 information 0.02 0.09 0.28 0 0 Figure 6.11 PPMI matrix showing association words context words , computed counts Fig . 6.10 . Note 0 PPMI values ones negative PMI ; example PMI ( cherry , computer ) = - 6.7 , meaning cherry computer co-occur Wikipedia less often expect chance , PPMI replace negative values zero . Pα ( c ) = count ( c ) α ∑ c count ( c ) α ( 6.23 ) Levy et al . ( 2015 ) found setting α = 0.75 improved performance embeddings wide range tasks ( drawing similar weighting skip - grams described below Eq . 6.32 ) . works raising count α = 0.75 increases probability assigned rare contexts , hence lowers PMI ( Pα ( c ) > P ( c ) c rare ) . Another possible solution Laplace smoothing : computing PMI , small constant k ( values 0.1-3 common ) added counts , shrinking ( discounting ) non-zero values . larger k , non-zero counts discounted . 6.8 Word2vec previous sections saw represent word sparse , long vector dimensions corresponding words vocabulary , whose values tf - idf PPMI functions count word co-occurring neighboring word . section turn alternative method representing word : vectors short ( length perhaps 50-1000 ) dense ( values non-zero ) . turns dense vectors work better every NLP task sparse vec - tors . completely understand reasons , intuitions . First , dense vectors successfully included features machine learning systems ; example 100-dimensional word embed - dings features , classifier just learn 100 weights represent function word meaning ; put 50,000 dimensional vector , classifier learn tens thousands weights sparse dimensions . Sec - ond , contain fewer parameters sparse vectors explicit counts , 18 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS dense vectors generalize better help avoid overfitting . Finally , dense vec - tors better job capturing synonymy sparse vectors . example , car automobile synonyms ; typical sparse vector representation , car dimension automobile dimension distinct dimensions . relationship two dimensions modeled , sparse vectors fail capture similarity word car neighbor word automobile neighbor . section introduce method dense , short vectors , skip - gram negative sampling , sometimes called SGNS . skip-gram algorithmskip-gram SGNS two algorithms software package called word2vec , sometimes word2vec algorithm loosely referred word2vec ( Mikolov et al . 2013 , Mikolov et al . 2013a ) . word2vec methods fast , efficient train , easily avail - able online code pretrained embeddings . point embedding methods , like equally popular GloVe ( Pennington et al . , 2014 ) , end chapter . intuition word2vec counting often word w oc - curs near , say , apricot , train classifier binary prediction task : “ word w likely show up near apricot ? ” actually care prediction task ; take learned classifier weights word embeddings . revolutionary intuition just running text implicitly supervised training data classifier ; word s occurs near target word apricot acts gold ‘ correct answer ’ question “ word w likely show up near apricot ? ” avoids need sort hand-labeled supervision signal . idea first proposed task neural language modeling , Bengio et al . ( 2003 ) Collobert et al . ( 2011 ) showed neural language model ( neural network learned predict next word prior words ) just next word running text supervision signal , learn embedding representation word part prediction task . neural networks next chapter , word2vec simpler model neural network language model , two ways . First , word2vec simplifies task ( making binary classification word pre - diction ) . Second , word2vec simplifies architecture ( training logistic regression classifier multi-layer neural network hidden layers demand sophisticated training algorithms ) . intuition skip-gram : 1 . Treat target word neighboring context word positive examples . 2 . Randomly sample words lexicon get negative samples . 3 . logistic regression train classifier distinguish two cases . 4 . regression weights embeddings . 6.8.1 classifier start thinking classification task , turn train . Imagine sentence like following , target word apricot , assume window ± 2 context words : . . . lemon , [ tablespoon apricot jam , ] pinch . . . c1 c2 t c3 c4 goal train classifier , tuple ( t , c ) target word t paired candidate context word c ( example ( apricot , jam ) , perhaps 6.8 • WORD2VEC 19 ( apricot , aardvark ) ) return probability c real context word ( true jam , false aardvark ) : P ( + | t , c ) ( 6.24 ) probability word c real context word t just 1 minus Eq . 6.24 : P ( − | t , c ) = 1 − P ( + | t , c ) ( 6.25 ) classifier compute probability P ? intuition skip - gram model base probability similarity : word likely occur near target embedding similar target embedding . compute similarity embeddings ? Recall two vectors similar high dot product ( cosine , popular similarity metric , just normalized dot product ) . words : Similarity ( t , c ) ≈ t · c ( 6.26 ) course , dot product t · c probability , just number ranging − ∞ ∞ . ( Recall , matter , cosine probability ) . turn dot product probability , logistic sigmoid function σ ( x ) , fundamental core logistic regression : σ ( x ) = 1 1 + e − x ( 6.27 ) probability word c real context word target word t thus computed : P ( + | t , c ) = 1 1 + e − t·c ( 6.28 ) sigmoid function just returns number 0 1 , make proba - bility need make sure total probability two possible events ( c context word , c context word ) sums 1 . probability word c real context word t thus : P ( − | t , c ) = 1 − P ( + | t , c ) = e − t·c 1 + e − t·c ( 6.29 ) Equation 6.28 gives probability word , need take account multiple context words window . Skip-gram makes strong useful simplifying assumption context words independent , allowing just multiply probabilities : P ( + | t , c1 : k ) = k ∏ = 1 1 1 + e − t·ci ( 6.30 ) logP ( + | t , c1 : k ) = k ∑ = 1 log 1 1 + e − t·ci ( 6.31 ) summary , skip-gram trains probabilistic classifier , test target word t context window k words c1 : k , assigns probability based similar 20 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS context window target word . probability based applying logistic ( sigmoid ) function dot product embeddings target word context word . thus compute probability embeddings target word context word vocabulary . turn learning embeddings ( real goal training classifier first place ) . 6.8.2 Learning skip-gram embeddings Word2vec learns embeddings starting initial set embedding vectors iteratively shifting embedding word w like em - beddings words occur nearby texts , less like embeddings words occur nearby . start considering single piece training data : . . . lemon , [ tablespoon apricot jam , ] pinch . . . c1 c2 t c3 c4 example target word t ( apricot ) , 4 context words L = ± 2 window , resulting 4 positive training instances ( left below ) : positive examples + t c apricot tablespoon apricot apricot jam apricot negative examples - t c t c apricot aardvark apricot seven apricot apricot forever apricot apricot dear apricot coaxial apricot training binary classifier need negative examples . fact skip - gram negative examples positive examples ( ratio set parameter k ) . ( t , c ) training instances create k negative samples , consisting target t plus ‘ noise word ’ . noise word random word lexicon , constrained target word t . right shows setting k = 2 , 2 negative examples negative training set − positive example t , c . noise words chosen according weighted unigram frequency pα ( w ) , α weight . sampling according unweighted fre - quency p ( w ) , mean unigram probability p ( “ ” ) choose word noise word , unigram probability p ( “ aardvark ” ) choose aardvark , . practice common set α = . 75 , i.e . weighting p 3 4 ( w ) : Pα ( w ) = count ( w ) α ∑ w ′ count ( w ′ ) α ( 6.32 ) Setting α = . 75 gives better performance gives rare noise words slightly higher probability : rare words , Pα ( w ) > P ( w ) . visualize intuition , might help work probabilities example two events , P ( ) = . 99 P ( b ) = . 01 : Pα ( ) = . 99.75 . 99.75 + . 01.75 = . 97 Pα ( b ) = . 01.75 . 99.75 + . 01.75 = . 03 ( 6.33 ) 6.8 • WORD2VEC 21 set positive negative training instances , initial set em - beddings , goal learning algorithm adjust embeddings • Maximize similarity target word , context word pairs ( t , c ) drawn positive examples • Minimize similarity ( t , c ) pairs drawn negative examples . express formally whole training set : L ( θ ) = ∑ ( t , c ) ∈ + logP ( + | t , c ) + ∑ ( t , c ) ∈ − logP ( − | t , c ) ( 6.34 ) look word / context pair ( t , c ) k noise words n1 . . . nk , learning objective L : L ( θ ) = logP ( + | t , c ) + k ∑ = 1 logP ( − | t , ni ) = logσ ( c · t ) + k ∑ = 1 logσ ( − ni · t ) = log 1 1 + e − c·t + k ∑ = 1 log 1 1 + eni·t ( 6.35 ) , maximize dot product word actual context words , minimize dot products word k negative sampled non - neighbor words . stochastic gradient descent train objective , iteratively modifying parameters ( embeddings target word t context word noise word c vocabulary ) maximize objective . Note skip-gram model thus actually learns two separate embeddings word w : target embedding t context embedding c . Thesetargetembedding context embedding embeddings stored two matrices , target matrix T context matrix C . row target matrix T 1 × d vector embedding ti word vocabulary V , column j context matrix C d × 1 vector embedding c j word j V . Fig . 6.12 shows intuition learning task embeddings encoded two matrices . Just logistic regression , , learning algorithm starts randomly initialized W C matrices , walks training corpus gra - dient descent move W C maximize objective Eq . 6.35 . Thus matrices W C function parameters θ logistic regression tuning . Once embeddings learned , two embeddings word wi : ti ci . choose throw away C matrix just keep W , case word represented vector ti . Alternatively add two embeddings together , summed em - bedding ti + ci new d-dimensional embedding , concatenate embedding dimensionality 2d . simple count-based methods like tf-idf , context window size L affects performance skip-gram embeddings , experiments often tune parameter L devset . difference count-based methods skip-grams , larger window size computation algorithm requires training ( neighboring words predicted ) . 22 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS 1 . . . . . . . V 1.2 … … . j … … . … … . … k … … V 1 . . . d T C 1 . . . … d increase similarity ( apricot , jam ) ti . cj jam apricot aa rdv ark decrease similarity ( apricot , aardvark ) ti . ck “ … apricot jam … ” ne igh bo r w ord ran m ise w ord target word Figure 6.12 skip-gram model tries shift embeddings target embeddings ( apricot ) closer ( higher dot product ) context embeddings nearby words ( jam ) further ( lower dot product ) context embeddings words occur nearby ( aardvark ) . 6.9 Visualizing Embeddings “ well many dimensions long dimensions around two . ” late economist Martin Shubek Visualizing embeddings important goal helping understand , apply , improve models word meaning . visualize ( example ) 100-dimensional vector ? simplest way visualize meaning word w embedded space list similar words w sorting vectors words vocabulary cosine vector w . example 7 closest words frog GloVe embeddings : frogs , toad , litoria , leptodactylidae , rana , lizard , eleutherodactylus ( Pennington et al . , 2014 ) Rohde , Gonnerman , Plaut Modeling Word Meaning Lexical Co-Occurrence HEAD HANDFACE DOG AMERICA CAT EYE EUROPE FOOT CHINA FRANCE CHICAGO ARM FINGER NOSE LEG RUSSIA MOUSE AFRICA ATLANTA EAR SHOULDER ASIA COW BULL PUPPY LION HAWAII MONTREAL TOKYO TOE MOSCOW TOOTH NASHVILLE BRAZIL WRIST KITTEN ANKLE TURTLE OYSTER Figure 8 : Multidimensional scaling three noun classes . WRIST ANKLE SHOULDER ARM LEG HAND FOOT HEAD NOSE FINGER TOE FACE EAR EYE TOOTH DOG CAT PUPPY KITTEN COW MOUSE TURTLE OYSTER LION BULL CHICAGO ATLANTA MONTREAL NASHVILLE TOKYO CHINA RUSSIA AFRICA ASIA EUROPE AMERICA BRAZIL MOSCOW FRANCE HAWAII Figure 9 : Hierarchical clustering three noun classes distances based vector correlations . 20 Yet another visualization method clus - tering algorithm show hierarchical representa - tion words similar others em - bedding space . uncaptioned example right hierarchical clustering embedding vectors nouns visualization method ( Rohde et al . , 2006 ) . Probably common visualization method , , project 100 dimensions word down 2 dimensions . Fig . 6.1 showed visualization , Fig . 6.13 , projection method called t-SNE ( van der Maaten Hinton , 2008 ) . 6.10 Semantic properties embeddings Vector semantic models number parameters . parameter relevant sparse tf-idf vectors dense word2vec vectors size context 6.10 • SEMANTIC PROPERTIES EMBEDDINGS 23 window collect counts . generally 1 10 words side target word ( total context 3-20 words ) . choice depends goals representation . Shorter context windows tend lead representations bit syntactic , information coming immediately nearby words . vectors computed short context windows , similar words target word w tend semantically similar words same parts speech . vectors computed long context windows , highest cosine words target word w tend words topically related similar . example Levy Goldberg ( 2014a ) showed skip-gram window ± 2 , similar words word Hogwarts ( Harry Potter series ) names fictional schools : Sunnydale ( Buffy Vampire Slayer ) Evernight ( vampire series ) . window ± 5 , similar words Hogwarts words topically related Harry Potter series : Dumbledore , Malfoy , half-blood . often useful distinguish two kinds similarity association words ( Schütze Pedersen , 1993 ) . Two words first-order co-occurrencefirst-orderco-occurrence ( sometimes called syntagmatic association ) typically nearby . Thus wrote first-order associate book poem . Two words second-order co-occurrence ( sometimes called paradigmatic association ) similarsecond-orderco-occurrence neighbors . Thus wrote second-order associate words like remarked . Analogy Another semantic property embeddings ability capture re - lational meanings . Mikolov et al . ( 2013b ) Levy Goldberg ( 2014b ) show offsets vector embeddings capture analogical relations words . example , result expression vector ( ‘ king ’ ) - vec - tor ( ‘ man ’ ) + vector ( ‘ woman ’ ) vector close vector ( ‘ queen ’ ) ; left panel Fig . 6.13 visualizes , again projected down 2 dimensions . Similarly , found expression vector ( ‘ Paris ’ ) - vector ( ‘ France ’ ) + vector ( ‘ Italy ’ ) results vector close vector ( ‘ Rome ’ ) . ( ) ( b ) Figure 6.13 Relational properties vector space , shown projecting vectors onto two dimensions . ( ) ’ king ’ - ’ man ’ + ’ woman ’ close ’ queen ’ ( b ) offsets seem capture comparative superlative morphology ( Pennington et al . , 2014 ) . Embeddings Historical Semantics : Embeddings useful tool studying meaning changes time , computing multiple embedding 24 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS spaces , texts written particular time period . example Fig . 6.14 shows visualization changes meaning English words last two centuries , computed building separate embedding spaces decade historical corpora like Google N-grams ( Lin et al . , 2012 ) Corpus Histori - cal American English ( Davies , 2012 ) . CHAPTER 5 . DYNAMIC SOCIAL REPRESENTATIONS WORD MEANING79 Figure 5.1 : Two-dimensional visualization semantic change English SGNS vectors ( Section 5.8 visualization algorithm ) . , word gay shifted meaning “ cheerful ” “ frolicsome ” referring homosexuality . , early 20th century broadcast referred “ casting seeds ” ; rise television radio meaning shifted “ transmitting signals ” . C , Awful underwent process pejoration , shifted meaning “ full awe ” meaning “ terrible appalling ” [ 212 ] . adverbials ( e.g . , actually ) general tendency undergo subjectification shift objective statements world ( e.g . , “ Sorry , car actually broken ” ) subjective statements ( e.g . , “ believe actually ” , indicating surprise / disbelief ) . 5.2.2 Computational linguistic studies number recent works analyzing semantic change computational methods . [ 200 ] latent semantic analysis analyze word meanings broaden narrow time . [ 113 ] raw co-occurrence vectors perform number historical case-studies semantic change , [ 252 ] perform similar set small - scale case-studies temporal topic models . [ 87 ] construct point-wise mutual information-based embeddings found semantic changes uncovered method reasonable agreement human judgments . [ 129 ] [ 119 ] “ neural ” word-embedding methods detect linguistic change points . Finally , [ 257 ] analyze historical co-occurrences test synonyms tend change similar ways . Figure 6.14 t-SNE visualization semantic change 3 words English word2vec vectors . modern sense word , grey context words , com - puted recent ( modern ) time-point embedding space . Earlier points com - puted earlier historical embedding spaces . visualizations show changes word gay meanings related “ cheerful ” “ frolicsome ” referring homosexuality , development modern “ transmission ” sense broadcast original sense sowing seeds , pejoration word awful shifted meaning “ full awe ” meaning “ terrible appalling ” ( Hamilton et al . , 2016 ) . 6.11 Bias Embeddings addition ability learn word meaning text , embeddings , alas , reproduce implicit biases stereotypes latent text . Recall embeddings model analogical relations ; ‘ queen ’ closest word ‘ king ’ - ‘ man ’ + ‘ woman ’ implies analogy man : woman : : king : queen . embedding analogies exhibit gender stereotypes . example Bolukbasi et al . ( 2016 ) find closest occupation ‘ man ’ - ‘ computer programmer ’ + ‘ woman ’ word2vec em - beddings trained news text ‘ homemaker ’ , embeddings similarly suggest analogy ‘ father ’ ‘ doctor ’ ‘ mother ’ ‘ nurse ’ . Algorithms embeddings part search potential programmers doctors might thus incorrectly downweight documents women’s names . Embeddings encode implicit associations property human reasoning . Implicit Association Test ( Greenwald et al . , 1998 ) measures peo - ple’s associations concepts ( like ‘ flowers ’ ‘ insects ’ ) attributes ( like ‘ pleasantness ’ ‘ unpleasantness ’ ) measuring differences latency label words various categories . 7 methods , people United States shown associate African-American names unpleasant words ( European-American names ) , male names 7 Roughly speaking , humans associate ‘ flowers ’ ‘ pleasantness ’ ‘ insects ’ ‘ unpleasant - ness ’ , instructed push green button ‘ flowers ’ ( daisy , iris , lilac ) ‘ pleasant words ’ ( love , laughter , pleasure ) red button ‘ insects ’ ( flea , spider , mosquito ) ‘ unpleasant words ’ ( abuse , hatred , ugly ) faster incongruous condition push red button ‘ flowers ’ ‘ unpleasant words ’ green button ‘ insects ’ ‘ pleasant words ’ . 6.12 • EVALUATING VECTOR MODELS 25 mathematics female names arts , old people’s names unpleas - ant words ( Greenwald et al . 1998 , Nosek et al . 2002a , Nosek et al . 2002b ) . Caliskan et al . ( 2017 ) replicated findings implicit associations GloVe vectors cosine similarity human latencies . example African-American names like ‘ Leroy ’ ‘ Shaniqua ’ higher GloVe cosine unpleasant words European-American names ( ‘ Brad ’ , ‘ Greg ’ , ‘ Courtney ’ ) higher cosine pleasant words . embedding-aware algorithm made word senti - ment thus lead bias against African Americans . Recent research focuses ways try remove kinds biases , ex - ample developing transformation embedding space removes gender stereotypes preserves definitional gender ( Bolukbasi et al . 2016 , Zhao et al . 2017 ) changing training procedure ( Zhao et al . , 2018 ) . , although sorts debiasing reduce bias embeddings , eliminate ( Gonen anddebiasing Goldberg , 2019 ) , remains open problem . Historical embeddings measure biases past . Garg et al . ( 2018 ) embeddings historical texts measure association - tween embeddings occupations embeddings names various ethnici - ties genders ( example relative cosine similarity women’s names versus men’s occupation words like ‘ librarian ’ ‘ carpenter ’ ) 20th century . found cosines correlate empirical historical percentages women ethnic groups occupations . Historical embeddings repli - cated old surveys ethnic stereotypes ; tendency experimental participants 1933 associate adjectives like ‘ industrious ’ ‘ superstitious ’ , e.g . , Chinese ethnicity , correlates cosine Chinese last names adjectives embeddings trained 1930s text . able document historical gender biases , fact embeddings adjectives related competence ( ‘ smart ’ , ‘ wise ’ , ‘ thoughtful ’ , ‘ resourceful ’ ) higher cosine male fe - male words , showed bias slowly decreasing 1960 . return later chapters question role bias natural language processing . 6.12 Evaluating Vector Models important evaluation metric vector models extrinsic evaluation tasks ; adding features NLP task seeing improves performance model . Nonetheless useful intrinsic evaluations . common metric test performance similarity , computing correlation algorithm’s word similarity scores word similarity ratings assigned humans . WordSim-353 ( Finkelstein et al . , 2002 ) commonly set ratings 0 10 353 noun pairs ; example ( plane , car ) average score 5.77 . SimLex-999 ( Hill et al . , 2015 ) difficult dataset quantifies similarity ( cup , mug ) rather relatedness ( cup , coffee ) , including concrete abstract adjective , noun verb pairs . TOEFL dataset set 80 questions , consisting target word 4 additional word choices ; task choose correct synonym , example : Levied closest meaning : imposed , believed , requested , correlated ( Landauer Dumais , 1997 ) . datasets present words context . Slightly realistic intrinsic similarity tasks include context . 26 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS Stanford Contextual Word Similarity ( SCWS ) dataset ( Huang et al . , 2012 ) Word-in-Context ( WiC ) dataset ( Pilehvar Camacho-Collados , 2019 ) offers richer evaluation scenarios . SCWS gives human judgments 2,003 pairs words sentential context , WiC gives target words two sentential contexts same different senses ; Section ? ? . semantic textual similarity task ( Agirre et al . 2012 , Agirre et al . 2015 ) evaluates performance sentence - level similarity algorithms , consisting set pairs sentences , pair human-labeled similarity scores . Another task evaluate analogy task , system solve problems form b c d , , b , c find d . Thus Athens Greece Oslo , system fill word Norway . syntactically-oriented examples : mouse , mice , dollar system return dollars . Large sets tuples created ( Mikolov et al . 2013 , Mikolov et al . 2013b ) . 6.13 Summary • vector semantics , word modeled vector — point high-dimensional space , called embedding . • Vector semantic models fall two classes : sparse dense . • sparse models like tf-idf dimension corresponds word vo - cabulary V ; cells sparse models functions co-occurrence counts . term-document matrix rows word ( term ) vocabulary column document . word-context matrix row ( target ) word vocabulary column context term vocabulary . • widely sparse weighting tf-idf , weights cell term frequency inverse document frequency . PPMI ( pointwise pos - itive mutual information ) alternative weighting scheme tf-idf . • Dense vector models dimensionality 50 – 1000 dimensions harder interpret . Word2vec algorithms like skip-gram popular efficient way compute dense embeddings . Skip-gram trains logistic re - gression classifier compute probability two words ‘ likely occur nearby text ’ . probability computed dot product - tween embeddings two words . • Skip-gram stochastic gradient descent train classifier , learning embeddings high dot product embeddings words occur nearby low dot product noise words . • important embedding algorithms include GloVe , method based ra - tios word co-occurrence probabilities , fasttext , open-source library computing word embeddings summing embeddings bag char - acter n-grams make up word . • sparse dense vectors , word document similarities computed function dot product vectors . cosine two vectors — normalized dot product — popular metric . BIBLIOGRAPHICAL HISTORICAL NOTES 27 Bibliographical Historical Notes idea vector semantics arose research 1950s three distinct fields : linguistics , psychology , computer science , contributed fundamental aspect model . idea meaning related distribution words context widespread linguistic theory 1950s , among distributionalists like Zellig Harris , Martin Joos , J . R . Firth , semioticians like Thomas Sebeok . Joos ( 1950 ) put , linguist’s “ meaning ” morpheme . . . definition set conditional probabilities occurrence context morphemes . idea meaning word might modeled point multi - dimensional semantic space came psychologists like Charles E . Osgood , studying people responded meaning words assigning val - ues scales like happy / sad hard / soft . Osgood et al . ( 1957 ) proposed meaning word general modeled point multidimensional Euclidean space , similarity meaning two words modeled distance points space . final intellectual source 1950s early 1960s field called mechanical indexing , known information retrieval . became knownmechanicalindexing vector space model information retrieval ( Salton 1971 , Sparck Jones 1986 ) , researchers demonstrated new ways define meaning words terms vec - tors ( Switzer , 1965 ) , refined methods word similarity based measures statistical association words like mutual information ( Giuliano , 1965 ) idf ( Sparck Jones , 1972 ) , showed meaning documents represented same vector spaces words . distantly related idea defining words vector discrete fea - tures , venerable history field , roots least far back Descartes Leibniz ( Wierzbicka 1992 , Wierzbicka 1996 ) . middle 20th century , beginning work Hjelmslev ( Hjelmslev , 1969 ) fleshed early models generative grammar ( Katz Fodor , 1963 ) , idea arose representing meaning semantic features , symbols represent sort ofsemanticfeature primitive meaning . example words like hen , rooster , chick , something common ( describe chickens ) something different ( age sex ) , representable : hen + female , + chicken , + adult rooster - female , + chicken , + adult chick + chicken , - adult dimensions vector models meaning define words , , abstractly related idea small fixed number hand-built dimensions . Nonetheless , attempt show certain dimensions em - bedding models contribute specific compositional aspect meaning like early semantic features . first dense vectors model word meaning latent seman - tic indexing ( LSI ) model ( Deerwester et al . , 1988 ) recast LSA ( latent seman - tic analysis ) ( Deerwester et al . , 1990 ) . LSA singular value decomposition — SVD — applied term-document matrix ( cell weighted log frequencySVD normalized entropy ) , first 300 dimensions LSA 28 CHAPTER 6 • VECTOR SEMANTICS EMBEDDINGS embedding . Singular Value Decomposition ( SVD ) method finding important dimensions data set , dimensions data varies . LSA quickly widely applied : cognitive model Landauer Dumais ( 1997 ) , tasks like spell checking ( Jones Martin , 1997 ) , lan - guage modeling ( Bellegarda 1997 , Coccaro Jurafsky 1998 , Bellegarda 2000 ) morphology induction ( Schone Jurafsky 2000 , Schone Jurafsky 2001 ) , essay grading ( Rehder et al . , 1998 ) . Related models simultaneously devel - oped applied word sense disambiguation Schütze ( 1992 ) . LSA led earliest embeddings represent words probabilistic classifier , logistic regression document router Schütze et al . ( 1995 ) . idea SVD term-term matrix ( rather term-document matrix ) model mean - ing NLP proposed soon LSA Schütze ( 1992 ) . Schütze applied low-rank ( 97-dimensional ) embeddings produced SVD task word sense disambiguation , analyzed resulting semantic space , suggested possible techniques like dropping high-order dimensions . Schütze ( 1997 ) . number alternative matrix models followed early SVD work , including Probabilistic Latent Semantic Indexing ( PLSI ) ( Hofmann , 1999 ) , Latent Dirichlet Allocation ( LDA ) ( Blei et al . , 2003 ) , Non-negative Matrix Factoriza - tion ( NMF ) ( Lee Seung , 1999 ) . next decade , Bengio et al . ( 2003 ) Bengio et al . ( 2006 ) showed neural language models develop embeddings part task word prediction . Collobert Weston ( 2007 ) , Collobert Weston ( 2008 ) , Collobert et al . ( 2011 ) demonstrated embeddings play role rep - resenting word meanings number NLP tasks . Turian et al . ( 2010 ) compared value different kinds embeddings different NLP tasks . Mikolov et al . ( 2011 ) showed recurrent neural nets language models . idea simplifying hidden layer neural net language models create skip-gram ( CBOW ) algorithms proposed Mikolov et al . ( 2013 ) . negative sampling training algorithm proposed Mikolov et al . ( 2013a ) . Studies embeddings include results showing elegant mathematical relation - ship sparse dense embeddings ( Levy Goldberg , 2014c ) , well numerous surveys embeddings parameterizations . ( Bullinaria Levy 2007 , Bullinaria Levy 2012 , Lapesa Evert 2014 , Kiela Clark 2014 , Levy et al . 2015 ) . widely-used embedding model besides word2vec GloVe ( Penning - ton et al . , 2014 ) . name stands Global Vectors , model based capturing global corpus statistics . GloVe based ratios probabilities word-word co-occurrence matrix , combining intuitions count-based models like PPMI capturing linear structures methods like word2vec . extension word2vec , fasttext ( Bojanowski et al . , 2017 ) , deals un-fasttext known words sparsity languages rich morphology , subword models . word fasttext represented itself plus bag constituent n - grams , special boundary symbols < > added word . example , n = 3 word represented character n-grams : < wh , whe , , ere , re > plus sequence < > skipgram embedding learned constituent n-gram , word represented sum embeddings constituent n-grams . EXERCISES 29 fasttext open-source library , including pretrained embeddings 157 languages , available https://fasttext.cc . many embedding algorithms , methods like non-negative matrix factorization ( Fyshe et al . , 2015 ) , converting sparse PPMI embeddings dense vectors SVD ( Levy Goldberg , 2014c ) . Chapter 10 introduce contextual embeddings like ELMo ( Peters et al . , 2018 ) BERT ( Devlin et al . , 2019 ) representation word contextual , function entire input sentence . Manning et al . ( 2008 ) deeper understanding role vectors - formation retrieval , including compare queries documents , details tf-idf , issues scaling large datasets . Cruse ( 2004 ) useful introductory linguistic text lexical semantics . Exercises 30 Chapter 6 • Vector Semantics Embeddings Agirre , E . , Banea , C . , Cardie , C . , Cer , D . , Diab , M . , Gonzalez-Agirre , . , Guo , W . , Lopez-Gazpio , . , Maritx - alar , M . , Mihalcea , R . , Rigau , G . , Uria , L . , Wiebe , J . ( 2015 ) . 2015 SemEval-2015 Task 2 : Semantic Textual Similarity , English , Spanish Pilot Interpretability . SemEval-15 , 252 – 263 . Agirre , E . , Diab , M . , Cer , D . , Gonzalez-Agirre , . ( 2012 ) . Semeval-2012 task 6 : pilot semantic textual similarity . SemEval-12 , 385 – 393 . Bellegarda , J . R . ( 1997 ) . latent semantic analysis frame - work large-span language modeling . Eurospeech-97 . Bellegarda , J . R . ( 2000 ) . Exploiting latent semantic infor - mation statistical language modeling . Proceedings IEEE , 89 ( 8 ) , 1279 – 1296 . Bengio , Y . , Courville , . , Vincent , P . ( 2013 ) . Repre - sentation learning : review new perspectives . IEEE Transactions Pattern Analysis Machine Intelli - gence , 35 ( 8 ) , 1798 – 1828 . Bengio , Y . , Ducharme , R . , Vincent , P . , Jauvin , C . ( 2003 ) . neural probabilistic language model . Journal machine learning research , 3 ( Feb ) , 1137 – 1155 . Bengio , Y . , Schwenk , H . , Senécal , J . - S . , Morin , F . , Gau - vain , J . - L . ( 2006 ) . Neural probabilistic language models . Innovations Machine Learning , 137 – 186 . Springer . Blei , D . M . , Ng , . Y . , Jordan , M . . ( 2003 ) . Latent Dirichlet allocation . JMLR , 3 ( 5 ) , 993 – 1022 . Bojanowski , P . , Grave , E . , Joulin , . , Mikolov , T . ( 2017 ) . Enriching word vectors subword information . TACL , 5 , 135 – 146 . Bolukbasi , T . , Chang , K . - W . , Zou , J . , Saligrama , V . , Kalai , . T . ( 2016 ) . Man computer programmer woman homemaker ? Debiasing word embeddings . NIPS 16 , 4349 – 4357 . Bréal , M . ( 1897 ) . Essai de Sémantique : Science des signifi - cations . Hachette . Budanitsky , . Hirst , G . ( 2006 ) . Evaluating WordNet - based measures lexical semantic relatedness . Computa - tional Linguistics , 32 ( 1 ) , 13 – 47 . Bullinaria , J . . Levy , J . P . ( 2007 ) . Extracting seman - tic representations word co-occurrence statistics : computational study . Behavior research methods , 39 ( 3 ) , 510 – 526 . Bullinaria , J . . Levy , J . P . ( 2012 ) . Extracting semantic representations word co-occurrence statistics : stop - lists , stemming , SVD . Behavior research methods , 44 ( 3 ) , 890 – 907 . Caliskan , . , Bryson , J . J . , Narayanan , . ( 2017 ) . Se - mantics derived automatically language corpora con - tain human-like biases . Science , 356 ( 6334 ) , 183 – 186 . Church , K . W . Hanks , P . ( 1989 ) . Word association norms , mutual information , lexicography . ACL-89 , 76 – 83 . Church , K . W . Hanks , P . ( 1990 ) . Word association norms , mutual information , lexicography . Computa - tional Linguistics , 16 ( 1 ) , 22 – 29 . Clark , E . ( 1987 ) . principle contrast : constraint language acquisition . MacWhinney , B . ( Ed . ) , Mecha - nisms language acquisition , 1 – 33 . LEA . Coccaro , N . Jurafsky , D . ( 1998 ) . better inte - gration semantic predictors statistical language mod - eling . ICSLP-98 , 2403 – 2406 . Collobert , R . Weston , J . ( 2007 ) . Fast semantic extrac - tion novel neural network architecture . ACL-07 , 560 – 567 . Collobert , R . Weston , J . ( 2008 ) . unified architec - ture natural language processing : Deep neural networks multitask learning . ICML , 160 – 167 . Collobert , R . , Weston , J . , Bottou , L . , Karlen , M . , Kavukcuoglu , K . , Kuksa , P . ( 2011 ) . Natural language processing ( almost ) scratch . JMLR , 12 , 2493 – 2537 . Cruse , D . . ( 2004 ) . Meaning Language : Introduction Semantics Pragmatics . Oxford University Press . Second edition . Dagan , . , Marcus , S . , Markovitch , S . ( 1993 ) . Contex - tual word similarity estimation sparse data . ACL-93 , 164 – 171 . Davies , M . ( 2012 ) . Expanding horizons historical linguis - tics 400-million word Corpus Historical Amer - ican English . Corpora , 7 ( 2 ) , 121 – 157 . Davies , M . ( 2015 ) . Wikipedia Corpus : 4.6 million arti - cles , 1.9 billion words . Adapted Wikipedia . Available online https://www.english-corpora.org/wiki/ . Deerwester , S . C . , Dumais , S . T . , Furnas , G . W . , Harshman , R . . , Landauer , T . K . , Lochbaum , K . E . , Streeter , L . ( 1988 ) . Computer information retrieval latent seman - tic structure : Patent 4,839,853 . . Deerwester , S . C . , Dumais , S . T . , Landauer , T . K . , Furnas , G . W . , Harshman , R . . ( 1990 ) . Indexing latent semantics analysis . JASIS , 41 ( 6 ) , 391 – 407 . Devlin , J . , Chang , M . - W . , Lee , K . , Toutanova , K . ( 2019 ) . BERT : Pre-training deep bidirectional transformers language understanding . NAACL HLT 2019 , 4171 – 4186 . Fano , R . M . ( 1961 ) . Transmission Information : Statis - tical Theory Communications . MIT Press . Finkelstein , L . , Gabrilovich , E . , Matias , Y . , Rivlin , E . , Solan , Z . , Wolfman , G . , Ruppin , E . ( 2002 ) . Placing search context : concept revisited . ACM Transactions - formation Systems , 20 ( 1 ) , 116 – – 131 . Firth , J . R . ( 1957 ) . synopsis linguistic theory 1930 – 1955 . Studies Linguistic Analysis . Philological Soci - ety . Reprinted Palmer , F . ( ed . ) 1968 . Selected Papers J . R . Firth . Longman , Harlow . Fyshe , . , Wehbe , L . , Talukdar , P . P . , Murphy , B . , Mitchell , T . M . ( 2015 ) . compositional interpretable semantic space . NAACL HLT 2015 . Garg , N . , Schiebinger , L . , Jurafsky , D . , Zou , J . ( 2018 ) . Word embeddings quantify 100 years gender eth - nic stereotypes . Proceedings National Academy Sciences , 115 ( 16 ) , E3635 – E3644 . Girard , G . ( 1718 ) . La justesse de la lange françoise : ou les differentes significations des mots qui passent pour syn - onymes . Exercises 31 Giuliano , V . E . ( 1965 ) . interpretation word - sociations . Stevens , M . E . , Giuliano , V . E . , Heilprin , L . B . ( Eds . ) , Statistical Association Methods Mechanized Documentation . Symposium Proceed - ings . Washington , D.C . , USA , March 17 , 1964 , 25 – 32 . https://nvlpubs.nist.gov/nistpubs/Legacy/ MP / nbsmiscellaneouspub269 . pdf . Gonen , H . Goldberg , Y . ( 2019 ) . Lipstick pig : De - biasing methods cover up systematic gender biases word embeddings remove . NAACL HLT 2019 . Gould , S . J . ( 1980 ) . Panda’s Thumb . Penguin Group . Greenwald , . G . , McGhee , D . E . , Schwartz , J . L . K . ( 1998 ) . Measuring individual differences implicit cog - nition : implicit association test . . Journal personality social psychology , 74 ( 6 ) , 1464 – 1480 . Hamilton , W . L . , Leskovec , J . , Jurafsky , D . ( 2016 ) . Di - achronic word embeddings reveal statistical laws seman - tic change . ACL 2016 . Harris , Z . S . ( 1954 ) . Distributional structure . Word , 10 , 146 – 162 . Reprinted J . Fodor J . Katz , Struc - ture Language , Prentice Hall , 1964 Z . S . Har - ris , Papers Structural Transformational Linguistics , Reidel , 1970 , 775 – 794 . Hill , F . , Reichart , R . , Korhonen , . ( 2015 ) . Simlex-999 : Evaluating semantic models ( genuine ) similarity esti - mation . Computational Linguistics , 41 ( 4 ) , 665 – 695 . Hjelmslev , L . ( 1969 ) . Prologomena Theory Lan - guage . University Wisconsin Press . Translated Fran - cis J . Whitfield ; original Danish edition 1943 . Hofmann , T . ( 1999 ) . Probabilistic latent semantic indexing . SIGIR-99 . Huang , E . H . , Socher , R . , Manning , C . D . , Ng , . Y . ( 2012 ) . Improving word representations via global context multiple word prototypes . ACL 2012 , 873 – 882 . Jones , M . P . Martin , J . H . ( 1997 ) . Contextual spelling correction latent semantic analysis . ANLP 1997 , 166 – 173 . Joos , M . ( 1950 ) . Description language design . JASA , 22 , 701 – 708 . Jurafsky , D . ( 2014 ) . Language Food . W . W . Norton , New York . Katz , J . J . Fodor , J . . ( 1963 ) . structure seman - tic theory . Language , 39 , 170 – 210 . Kiela , D . Clark , S . ( 2014 ) . systematic study seman - tic vector space model parameters . Proceedings EACL 2nd Workshop Continuous Vector Space Models Compositionality ( CVSC ) , 21 – 30 . Landauer , T . K . Dumais , S . T . ( 1997 ) . solution Plato’s problem : Latent Semantic Analysis theory acquisition , induction , representation knowledge . Psychological Review , 104 , 211 – 240 . Lapesa , G . Evert , S . ( 2014 ) . large scale evaluation distributional semantic models : Parameters , interactions model selection . TACL , 2 , 531 – 545 . Lee , D . D . Seung , H . S . ( 1999 ) . Learning parts objects non-negative matrix factorization . Nature , 401 ( 6755 ) , 788 – 791 . Levy , O . Goldberg , Y . ( 2014a ) . Dependency-based word embeddings . ACL 2014 . Levy , O . Goldberg , Y . ( 2014b ) . Linguistic regularities sparse explicit word representations . CoNLL-14 . Levy , O . Goldberg , Y . ( 2014c ) . Neural word embedding implicit matrix factorization . NIPS 14 , 2177 – 2185 . Levy , O . , Goldberg , Y . , Dagan , . ( 2015 ) . Improving dis - tributional similarity lessons learned word em - beddings . TACL , 3 , 211 – 225 . Li , J . , Chen , X . , Hovy , E . H . , Jurafsky , D . ( 2015 ) . Visual - izing understanding neural models NLP . NAACL HLT 2015 . Lin , Y . , Michel , J . - B . , Lieberman Aiden , E . , Orwant , J . , Brockman , W . , Petrov , S . ( 2012 ) . Syntactic annota - tions google books ngram corpus . ACL 2012 , 169 – 174 . Luhn , H . P . ( 1957 ) . statistical approach mechanized encoding searching literary information . IBM Jour - nal Research Development , 1 ( 4 ) , 309 – 317 . Manning , C . D . , Raghavan , P . , Schütze , H . ( 2008 ) . - troduction Information Retrieval . Cambridge . Mikolov , T . , Chen , K . , Corrado , G . S . , Dean , J . ( 2013 ) . Efficient estimation word representations vec - tor space . ICLR 2013 . Mikolov , T . , Kombrink , S . , Burget , L . , Černockỳ , J . H . , Khudanpur , S . ( 2011 ) . Extensions recurrent neural net - work language model . ICASSP-11 , 5528 – 5531 . Mikolov , T . , Sutskever , . , Chen , K . , Corrado , G . S . , Dean , J . ( 2013a ) . Distributed representations words phrases compositionality . NIPS 13 , 3111 – 3119 . Mikolov , T . , Yih , W . - t . , Zweig , G . ( 2013b ) . Linguistic regularities continuous space word representations . NAACL HLT 2013 , 746 – 751 . Niwa , Y . Nitta , Y . ( 1994 ) . Co-occurrence vectors corpora vs . distance vectors dictionaries . ACL-94 , 304 – 309 . Nosek , B . . , Banaji , M . R . , Greenwald , . G . ( 2002a ) . Harvesting implicit group attitudes beliefs demonstration web site . Group Dynamics : Theory , Re - search , Practice , 6 ( 1 ) , 101 . Nosek , B . . , Banaji , M . R . , Greenwald , . G . ( 2002b ) . Math = male , = female , math 6 = . Journal personality social psychology , 83 ( 1 ) , 44 . Osgood , C . E . , Suci , G . J . , Tannenbaum , P . H . ( 1957 ) . Measurement Meaning . University Illinois Press . Pennington , J . , Socher , R . , Manning , C . D . ( 2014 ) . Glove : Global vectors word representation . EMNLP 2014 , 1532 – 1543 . Peters , M . , Neumann , M . , Iyyer , M . , Gardner , M . , Clark , C . , Lee , K . , Zettlemoyer , L . ( 2018 ) . Deep contextualized word representations . NAACL HLT 2018 , 2227 – 2237 . Pilehvar , M . T . Camacho-Collados , J . ( 2019 ) . WiC : word-in-context dataset evaluating context-sensitive meaning representations . NAACL HLT 2019 , 1267 – 1273 . Rehder , B . , Schreiner , M . E . , Wolfe , M . B . W . , Laham , D . , Landauer , T . K . , Kintsch , W . ( 1998 ) . Latent Semantic Analysis assess knowledge : technical considerations . Discourse Processes , 25 ( 2-3 ) , 337 – 354 . 32 Chapter 6 • Vector Semantics Embeddings Rohde , D . L . T . , Gonnerman , L . M . , Plaut , D . C . ( 2006 ) . improved model semantic similarity based lexical co-occurrence . CACM , 8 , 627 – 633 . Salton , G . ( 1971 ) . SMART Retrieval System : Experi - ments Automatic Document Processing . Prentice Hall . Schone , P . Jurafsky , D . ( 2000 ) . Knowlege-free induction morphology latent semantic analysis . CoNLL - 00 . Schone , P . Jurafsky , D . ( 2001 ) . Knowledge-free induc - tion inflectional morphologies . NAACL 2001 . Schütze , H . ( 1992 ) . Dimensions meaning . Proceedings Supercomputing ’ 92 , 787 – 796 . IEEE Press . Schütze , H . ( 1997 ) . Ambiguity Resolution Language Learning – Computational Cognitive Models . CSLI , Stanford , CA . Schütze , H . , Hull , D . . , Pedersen , J . ( 1995 ) . com - parison classifiers document representations routing problem . SIGIR-95 , 229 – 237 . Schütze , H . Pedersen , J . ( 1993 ) . vector model syn - tagmatic paradigmatic relatedness . Proceedings 9th Annual Conference UW Centre New OED Text Research , 104 – 113 . Sparck Jones , K . ( 1972 ) . statistical interpretation term specificity application retrieval . Journal Doc - umentation , 28 ( 1 ) , 11 – 21 . Sparck Jones , K . ( 1986 ) . Synonymy Semantic Classifi - cation . Edinburgh University Press , Edinburgh . Republi - cation 1964 PhD Thesis . Switzer , P . ( 1965 ) . Vector images document re - trieval . Stevens , M . E . , Giuliano , V . E . , Heilprin , L . B . ( Eds . ) , Statistical Association Meth - ods Mechanized Documentation . Symposium Pro - ceedings . Washington , D.C . , USA , March 17 , 1964 , 163 – 171 . https://nvlpubs.nist.gov/nistpubs/ Legacy / MP / nbsmiscellaneouspub269 . pdf . Turian , J . , Ratinov , L . , Bengio , Y . ( 2010 ) . Word representations : simple general method semi - supervised learning . ACL 2010 , 384 – 394 . van der Maaten , L . Hinton , G . E . ( 2008 ) . Visualizing high-dimensional data t-sne . JMLR , 9 , 2579 – 2605 . Wierzbicka , . ( 1992 ) . Semantics , Culture , Cognition : University Human Concepts Culture-Specific Configura - tions . Oxford University Press . Wierzbicka , . ( 1996 ) . Semantics : Primes Universals . Oxford University Press . Wittgenstein , L . ( 1953 ) . Philosophical Investigations . ( Translated Anscombe , G.E.M . ) . Blackwell . Zhao , J . , Wang , T . , Yatskar , M . , Ordonez , V . , Chang , K . - W . ( 2017 ) . Men like shopping : Reducing gender bias amplification corpus-level constraints . EMNLP 2017 . Zhao , J . , Zhou , Y . , Li , Z . , Wang , W . , Chang , K . - W . ( 2018 ) . Learning gender-neutral word embeddings . EMNLP 2018 , 4847 – 4853 .