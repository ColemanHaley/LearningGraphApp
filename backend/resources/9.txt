Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 9 Sequence Processing withRecurrent Networks Time explain . Jane Austen , Persuasion Language inherently temporal phenomenon . comprehend pro - duce spoken language , processing continuous input streams indefinite length . even dealing written text normally process sequen - tially , even though principle arbitrary access elements once . temporal nature language reflected metaphors ; talk flow conversations , news feeds , twitter streams , call notion language sequence unfolds time . temporal nature reflected algorithms process language . applied problem part-of-speech tagging , Viterbi algorithm works way incrementally input word time , taking account information gleaned way . syntactic parsing algorithms cover Chapters 11 , 12 , 13 operate similar fashion . contrast , machine learning approaches studied sentiment analy - sis classification tasks temporal nature . approaches simultaneous access aspects input . certainly true feed - forward neural networks , including application neural language models . networks employ fixed-size input vectors associated weights capture relevant aspects example once . makes difficult deal se - quences varying length , fail capture important temporal aspects language . saw work-around problems case neural language models . models operate accepting fixed-sized windows tokens input ; sequences longer window size processed sliding windows input making predictions go , end result sequence pre - dictions spanning input . Importantly , decision made window impact later decisions . Fig . 9.1 , reproduced Chapter 7 , illustrates approach window size 3 . , predicting word come next window ground . Subsequent words predicted sliding window forward word time . sliding window approach problematic number reasons . First , shares primary weakness Markov approaches limits context information extracted ; anything outside context window impact decision made . issue many language tasks require access information arbitrarily distant point processing happening . Second , windows makes difficult networks learn systematic patterns arising phenomena like constituency . 2 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS h1 h2 y1 h3 hdh … … U W y42 y | V | Projection layer 1 ⨉ 3d concatenated embeddings context words Hidden layer Output layer P ( w | u ) … thehole . . . . . . ground lived word 42 embedding word 35 embedding word 9925 embedding word 45180 wt-1wt-2 wtwt-3 dh ⨉ 3d 1 ⨉ dh | V | ⨉ dh P ( wt = V42 | wt-3 , wt-2 , wt-3 ) 1 ⨉ | V | Figure 9.1 simplified view feedforward neural language model moving text . time step t network takes 3 context words , converts d-dimensional embedding , concatenates 3 embeddings together get 1 × Nd unit input layer x network . example , Fig . 9.1 noun phrase ground appears two separate windows : once , shown , first second positions window , preceding step appears second third positions , thus forcing network learn two separate patterns constituent . subject chapter recurrent neural networks , class networksrecurrentneural networks designed address challenges dealing directly temporal aspect language , allowing handle variable length inputs arbitrary fixed-sized windows , providing means capture exploit temporal nature language . 9.1 Simple Recurrent Neural Networks recurrent neural network ( RNN ) network contains cycle network connections . , network value unit directly , indirectly , dependent earlier outputs input . powerful , networks difficult reason train . , general class recur - rent networks constrained architectures proven extremely effective applied spoken written language . section , consider class recurrent networks referred Elman Networks ( Elman , 1990 ) simpleElmanNetworks recurrent networks . networks useful own right serve basis complex approaches discussed later chapter again Chapter 10 Chapter 11 . Going forward , term RNN referring simpler constrained networks . Fig . 9.2 illustrates structure simple RNN . ordinary feedforward networks , input vector representing current input element , xt , multiplied weight matrix passed activation function compute activa - 9.1 • SIMPLE RECURRENT NEURAL NETWORKS 3 ht yt xt Figure 9.2 Simple recurrent neural network Elman ( Elman , 1990 ) . hidden layer includes recurrent connection part input . , activation value hidden layer depends current input well activation value hidden layer previous time step . tion value layer hidden units . hidden layer , turn , calculate corresponding output , yt . departure earlier window-based approach , sequences processed presenting element time network . key difference feedforward network lies recurrent link shown figure dashed line . link augments input computation hidden layer activation value hidden layer preceding point time . hidden layer previous time step provides form memory , context , encodes earlier processing informs decisions made later points time . Critically , architecture impose fixed-length limit prior context ; context embodied previous hidden layer includes information extending back beginning sequence . Adding temporal dimension make RNNs appear exotic non-recurrent architectures . reality , they’re different . input vector values hidden layer previous time step , still performing standard feedforward calculation . , consider Fig . 9.3 clarifies nature recurrence factors computation hidden layer . significant change lies new set weights , U , connect hidden layer previous time step current hidden layer . weights determine network make past context calculating output current input . weights network , connections trained via backpropagation . 9.1.1 Inference Simple RNNs Forward inference ( mapping sequence inputs sequence outputs ) RNN nearly identical already seen feedforward networks . compute output yt input xt , need activation value hidden layer ht . calculate , multiply input xt weight matrix W , hidden layer previous time step ht − 1 weight matrix U . add values together pass suitable activation function , g , arrive activation value current hidden layer , ht . Once values hidden layer , proceed usual computation generate 4 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS U V W yt xt ht ht-1 Figure 9.3 Simple recurrent neural network illustrated feedforward network . output vector . ht = g ( Uht − 1 + Wxt ) yt = f ( V ht ) commonly encountered case soft classification , computing yt consists softmax computation provides normalized probability distribution possible output classes . yt = softmax ( V ht ) fact computation time t requires value hidden layer time t − 1 mandates incremental inference algorithm proceeds start sequence end illustrated Fig . 9.4 . sequential nature simple recurrent networks seen unrolling network time shown Fig . 9.5 . figure , various layers units copied time step illustrate differing values time . , various weight matrices shared time . function FORWARDRNN ( x , network ) returns output sequence y h0 ← 0 ← 1 LENGTH ( x ) hi ← g ( U hi − 1 + W xi ) yi ← f ( V hi ) return y Figure 9.4 Forward inference simple recurrent network . matrices U , V W shared time , new values h y calculated time step . 9.1.2 Training feedforward networks , training set , loss function , back - propagation obtain gradients needed adjust weights recurrent networks . shown Fig . 9.3 , 3 sets weights update : W , 9.1 • SIMPLE RECURRENT NEURAL NETWORKS 5 U V W U V W U V W x1 x2 x3y1 y2 y3 h1 h3 h2 h0 Figure 9.5 simple recurrent neural network shown unrolled time . Network layers copied time step , weights U , V W shared common time steps . weights input layer hidden layer , U , weights previous hidden layer current hidden layer , finally V , weights hidden layer output layer . going , first review notation introduced Chapter 7 . Assuming network input layer x non-linear activation function g , [ ] refers activation value layer , result applying g z [ ] , weighted sum inputs layer . Fig . 9.5 illustrates two considerations worry backpropagation feedforward networks . First , compute loss function output time t need hidden layer time t − 1 . Second , hidden layer time t influences output time t hidden layer time t + 1 ( hence output loss t + 1 ) . follows assess error accruing ht , need know influence current output well ones follow . Consider situation examining input / output pair time 2 shown Fig . 9.6 . need compute gradients required update weights U , V , W ? start reviewing compute gra - dients required update V computation unchanged feedforward networks . review Chapter 7 , need compute derivative loss function L respect weights V . , loss expressed directly terms weights , apply chain rule get indirectly . ∂ L ∂ V = ∂ L ∂ ∂ ∂ z ∂ z ∂ V first term right derivative loss function respect network output , . second term derivative network output respect intermediate network activation z , function activation 6 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS function g . final term application chain rule derivative network activation respect weights V , activation value current hidden layer ht . useful first two terms define δ , error term represents scalar loss attributable units output layer . δout = ∂ L ∂ ∂ ∂ z ( 9.1 ) δout = L ′ g ′ ( z ) ( 9.2 ) , final gradient need update weight matrix V just : ∂ L ∂ V = δoutht ( 9.3 ) U V W U V W U V W x1 x2 x3y1 y2 y3 h1 h3 h2 h0 t1 t2 t3 Figure 9.6 backpropagation errors simple RNN ti vectors represent targets element sequence training data . red arrows illustrate flow backpropagated errors required calculate gradients U , V W time 2 . two incoming arrows converging h2 signal errors need summed . Moving , need compute corresponding gradients weight ma - trices W U : ∂ L ∂ W ∂ L ∂ U . encounter first substantive change feedforward networks . hidden state time t contributes output asso - ciated error time t output error next time step , t + 1 . , error term , δh , hidden layer sum error term current output error next time step . δh = g ′ ( z ) V δout + δnext total error term hidden layer , compute gradients weights U W chain rule Chapter 7 . 9.1 • SIMPLE RECURRENT NEURAL NETWORKS 7 dL dW = dL dz dz da da dW dL dU = dL dz dz da da dU ∂ L ∂ W = δhxt ∂ L ∂ U = δhht − 1 gradients provide information needed update matrices U W . quite yet , still need assign proportional blame ( compute error term ) back previous hidden layer ht − 1 further processing . involves backpropagating error δh ht − 1 proportionally based weights U . δnext = g ′ ( z ) Uδh ( 9.4 ) point gradients needed perform weight updates three sets weights . Note simple case need backprop - agate error W input x , input training data assumed fixed . wished update input word character embeddings backpropagate error well . Taken together , considerations lead two-pass algorithm train - ing weights RNNs . first pass , perform forward inference , computing ht , yt , accumulating loss step time , saving value hidden layer step next time step . second phase , process sequence reverse , computing required error terms gradients go , comput - ing saving error term hidden layer step backward time . general approach commonly referred Backpropagation Time ( Werbos 1974 , Rumelhart et al . 1986a , Werbos 1990 ) . Backpropaga - tion Time 9.1.3 Unrolled Networks Computation Graphs unrolled network shown Fig . 9.5 way illustrate tem - poral nature RNNs . , modern computational frameworks ad - equate computing resources , explicitly unrolling recurrent network deep feedforward computational graph quite practical word-by-word approaches sentence-level processing . approach , provide template speci - fies basic structure network , including necessary parameters input , output , hidden layers , weight matrices , well activation output functions . , presented particular input sequence , generate unrolled feedforward network specific input , graph perform forward inference training via ordinary backpropagation . applications involve longer input sequences , speech recog - nition , character-by-character sentence processing , streaming continuous - puts , unrolling entire input sequence feasible . cases , unroll input manageable fixed-length segments treat segment distinct training item . 8 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS 9.2 Applications Recurrent Neural Networks Recurrent neural networks proven effective approach language mod - eling , sequence labeling tasks part-of-speech tagging , well sequence classification tasks sentiment analysis topic classification . Chapter 10 Chapter 11 , form basis sequence-to-sequence approaches summarization , machine translation , question answering . 9.2.1 Recurrent Neural Language Models already seen two ways create probabilistic language models : N-gram mod - els feedforward networks sliding windows . fixed preceding con - text , attempt predict next word sequence . formally , com - pute conditional probability next word sequence preceding words , P ( wn | wn − 11 ) . approaches , quality model largely dependent size context effectively model makes . Thus , N-gram sliding-window neural networks constrained Markov assumption embod - ied following equation . P ( wn | wn − 11 ) ≈ P ( wn | w n − 1 n − N + 1 ) ( 9.5 ) , anything outside preceding context size N bearing computation . Recurrent neural language models process sequences word time attempting predict next word sequence current word previous hidden state input ( Mikolov et al . , 2010 ) . Thus , limited context constraint inherent N-gram models sliding window approaches avoided hidden state embodies information preceding words way back beginning sequence . Forward inference recurrent language model proceeds described Sec - tion 9.1.1 . step network retrieves word embedding current word input combines hidden layer previous step compute new hidden layer . hidden layer generate output layer passed softmax layer generate probability distribution entire vocabulary . P ( wn | wn − 11 ) = yn ( 9.6 ) = softmax ( V hn ) ( 9.7 ) probability entire sequence just product probabilities item sequence . P ( wn1 ) = n ∏ k = 1 P ( wk | wk − 11 ) ( 9.8 ) = n ∏ k = 1 yk ( 9.9 ) 9.2 • APPLICATIONS RECURRENT NEURAL NETWORKS 9 approach introduced Chapter 7 , train model corpus representative text training material . task predict next word sequence previous words , cross-entropy loss function . Recall cross-entropy loss single example negative log probability assigned correct class , result applying softmax final output layer . LCE ( ŷ , y ) = − log ŷi ( 9.10 ) = − log e zi ∑ K j = 1 e z j ( 9.11 ) , correct class word actually comes next data yi probability assigned word , softmax entire vocabulary , size K . weights network adjusted minimize cross - entropy loss training set via gradient descent . Generation Neural Language Models saw probabilistic Shakespeare generator Chapter 3 , fun way gain insight language model Shannon’s method ( Shannon , 1951 ) generate random sentences . procedure basically same described ? ? . • begin , sample first word output softmax distribution results beginning sentence marker , < s > , first input . • word embedding first word input network next time step , sample next word same fashion . • Continue generating end sentence marker , < / s > , sampled fixed length limit reached . technique called autoregressive generation word generated theautoregressivegeneration time step conditioned word generated network previous step . Fig . 9.7 illustrates approach . figure , details RNN’s hidden layers recurrent connections hidden blue block . entertaining exercise , architecture inspired state-of - the-art approaches applications machine translation , summarization , question answering . key approaches prime generation compo - nent appropriate context . , simply < s > get things started provide richer task-appropriate context . return advanced applications Chapter 10 , discuss encoder-decoder networks . Finally , Shakespeare , move beyond informally assessing quality generated output perplexity objectively compare output held-out sample training corpus . PP ( W ) = N √ √ √ √ N ∏ = 1 1 P ( wi | wi − 1 ) ( 9.12 ) lower perplexity , better model . 9.2.2 Sequence Labeling sequence labeling , network’s task assign label chosen small fixed set labels element sequence . canonical example 10 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS < s > RNN hole hole ? Sampled Word Softmax Embedding Input Word Figure 9.7 Autoregressive generation RNN-based neural language model . task part-of-speech tagging , discussed detail Chapter 8 . RNN approach POS tagging , inputs word embeddings outputs tag probabilities generated softmax layer tagset , illustrated Fig . 9.8 . figure , inputs time step pre-trained word embeddings cor - responding input tokens . RNN block abstraction represents unrolled simple recurrent network consisting input layer , hidden layer , output layer time step , well shared U , V W weight matrices comprise network . outputs network time step represent distribution POS tagset generated softmax layer . generate tag sequence input , run forward inference input sequence select likely tag softmax step . softmax layer generate probability distribution output Janet back RNN bill Figure 9.8 Part-of-speech tagging sequence labeling simple RNN . Pre-trained word embeddings serve inputs softmax layer provides probability distribution part-of-speech tags output time step . 9.2 • APPLICATIONS RECURRENT NEURAL NETWORKS 11 tagset time step , again employ cross-entropy loss training . closely related , extremely useful , application sequence labeling find classify spans text correspond concepts interest task domain . example task named entity recognition — prob-named entityrecognition lem finding spans text correspond names people , places organizations ( problem study detail Chapter 18 ) . sequence labeling span-recognition problem , technique called IOB encoding ( Ramshaw Marcus , 1995 ) . simplest form , label token begins span interest label B , tokens occur inside span tagged , tokens outside span interest labeled O . Consider following example : ( 9.13 ) United B cancelled O O flight O O Denver B O San B Francisco . , spans interest United , Denver San Francisco . applications interested class entity ( e.g . , finding distinguishing names people , locations , organizations ) , specialize B tags represent specific classes , thus ex - panding tagset 3 tags 2 ∗ N + 1 , N number classes interested . Applying approach previous example results follow - ing encoding . ( 9.14 ) United B-ORG cancelled O O flight O O Denver B-LOC O San B-LOC Francisco . I-LOC encoding , reduced span recognition task per-word labeling task inputs usual word embeddings output consists sequence softmax distributions tags point sequence . Yet another application sequence labeling problem structure pre - diction . task take input sequence produce kind struc-structureprediction tured output , parse tree meaning representation . way model problems like learn sequence actions , operators , exe - cuted produce desired structure . , predicting label element input sequence , network trained select sequence actions , executed sequence produce desired output . clearest example approach transition-based parsing borrows shift-reduce paradigm compiler construction . return application Chapter 15 take up dependency parsing . Viterbi Conditional Random Fields ( CRFs ) saw applied logistic regression part-of-speech tagging , choosing maximum probability label element sequence independently necessarily result optimal ( even good ) sequence tags . case IOB tagging , even guarantee resulting sequence well - formed . example , nothing approach described last section prevents output sequence containing following O , even though transition illegal . Similarly , dealing multiple classes nothing prevent I-LOC tag following B-PER tag . solution problem combine sequence outputs re - current network output-level language model discussed Chapter 8 . variant Viterbi algorithm select likely tag sequence . 12 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS approach usually implemented adding CRF ( Lample et al . , 2016 ) layer final layer recurrent network . 9.2.3 RNNs Sequence Classification Another RNNs classify entire sequences rather tokens . already encountered task Chapter 4 discussion sen - timent analysis . examples include document-level topic classification , spam detection , message routing customer service applications , deception detec - tion . applications , sequences text classified belonging small number categories . apply RNNs setting , text classified passed RNN word time generating new hidden layer time step . hidden layer final element text , hn , taken constitute compressed representation entire sequence . simplest approach classification , hn , serves input subsequent feedforward network chooses class via softmax possible classes . Fig . 9.9 illustrates approach . x1 x2 x3 xn RNN hn Softmax Figure 9.9 Sequence classification simple RNN combined feedforward net - work . final hidden state RNN input feedforward network performs classification . Note approach intermediate outputs words sequence preceding last element . , loss terms associ - ated elements . , loss function train weights network based entirely final text classification task . Specifically , - put softmax output feedforward classifier together cross - entropy loss drives training . error signal classification backprop - agated way weights feedforward classifier , input , three sets weights RNN described earlier Section 9.1.2 . combination simple recurrent network feedforward classifier first example deep neural network . training regimen loss downstream application adjust weights way network referred end-to-end training . end-to-endtraining 9.3 • DEEP NETWORKS : STACKED BIDIRECTIONAL RNNS 13 9.3 Deep Networks : Stacked Bidirectional RNNs suggested sequence classification architecture shown Fig . 9.9 , recurrent networks quite flexible . combining feedforward nature unrolled com - putational graphs vectors common inputs outputs , complex networks treated modules combined creative ways . section intro - duces two common network architectures language processing RNNs . 9.3.1 Stacked RNNs examples thus far , inputs RNNs consisted sequences word character embeddings ( vectors ) outputs vectors useful predicting words , tags sequence labels . , nothing prevents entire sequence outputs RNN input sequence another . Stacked RNNs consist multiple networks output layer serves asStacked RNNs input subsequent layer , shown Fig . 9.10 . y1 y2 y3 yn x1 x2 x3 xn RNN 1 RNN 3 RNN 2 Figure 9.10 Stacked recurrent networks . output lower level serves input higher levels output last network serving final output . demonstrated numerous tasks stacked RNNs outper - form single-layer networks . reason success network’s ability induce representations differing levels abstraction layers . Just early stages human visual system detect edges finding larger regions shapes , initial layers stacked networks induce representations serve useful abstractions further layers — representations might prove difficult induce single RNN . optimal number stacked RNNs specific application training set . , number stacks increased training costs rise quickly . 14 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS 9.3.2 Bidirectional RNNs simple recurrent network , hidden state time t represents everything network knows sequence up point sequence . , hidden state time t result function inputs start up time t . think context network left current time . h ft = RNNforward ( x t 1 ) h ft corresponds normal hidden state time t , represents everything network gleaned sequence point . many applications access entire input sequence once . might ask helpful take advantage context right current input well . way recover information train RNN input sequence reverse , exactly same kind networks discussing . approach , hidden state time t represents information sequence right current input . hbt = RNNbackward ( x n t ) , hidden state hbt represents information discerned sequence t end sequence . Combining forward backward networks results bidirectional RNN ( SchusterbidirectionalRNN Paliwal , 1997 ) . Bi-RNN consists two independent RNNs , input processed start end , end start . combine outputs two networks single representation captures left right contexts input point time . ht = h f t ⊕ hbt Fig . 9.11 illustrates bidirectional network outputs forward backward pass concatenated . simple ways combine forward backward contexts include element-wise addition multiplication . output step time thus captures information left right current input . sequence labeling applications , concatenated outputs serve basis local labeling decision . Bidirectional RNNs proven quite effective sequence classi - fication . Recall Fig . 9.10 , sequence classification final hidden state RNN input subsequent feedforward classifier . dif - ficulty approach final state naturally reflects information end sentence beginning . Bidirectional RNNs provide simple solution problem ; shown Fig . 9.12 , simply combine final hidden states forward backward passes input follow - processing . Again , concatenation common approach combining two outputs element-wise summation , multiplication averaging . 9.4 Managing Context RNNs : LSTMs GRUs practice , quite difficult train RNNs tasks require network make information distant current point processing . Despite 9.4 • MANAGING CONTEXT RNNS : LSTMS GRUS 15 y1 x1 x2 x3 xn RNN 1 ( Left Right ) RNN 2 ( Right Left ) + y2 + y3 + yn + Figure 9.11 bidirectional RNN . Separate models trained forward backward directions output model time point concatenated represent state affairs point time . box wrapped around forward backward network emphasizes modular nature architecture . x1 x2 x3 xn RNN 1 ( Left Right ) RNN 2 ( Right Left ) + hn_forw h1_back Softmax Figure 9.12 bidirectional RNN sequence classification . final hidden units forward backward passes combined represent entire sequence . com - bined representation serves input subsequent classifier . access entire preceding sequence , information encoded hidden states tends fairly local , relevant recent parts input sequence recent decisions . often case , , distant information critical many language applications . , consider following example context language modeling . ( 9.15 ) flights airline cancelling full . 16 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS Assigning high probability following airline straightforward airline provides strong local context singular agreement . , assigning appropriate probability quite difficult , plural flights quite distant , intervening context involves singular constituents . Ideally , network able retain distant information plural flights needed , still processing intermediate parts sequence cor - rectly . reason inability RNNs carry forward critical information hidden layers , , extension , weights determine values hid - den layer , asked perform two tasks simultaneously : provide information useful current decision , updating carrying forward information re - quired future decisions . second difficulty training SRNs arises need backpropagate error signal back time . Recall Section 9.1.2 hidden layer time t contributes loss next time step takes part cal - culation . result , backward pass training , hidden layers subject repeated multiplications , determined length sequence . frequent result process gradients eventually driven zero – so-called vanishing gradients problem . vanishinggradients address issues , complex network architectures designed explicitly manage task maintaining relevant context time . specif - ically , network needs learn forget information longer needed remember information required decisions still come . 9.4.1 Long Short-Term Memory Long short-term memory ( LSTM ) networks ( Hochreiter Schmidhuber , 1997 ) Long short-term memory divide context management problem two sub-problems : removing informa - tion longer needed context , adding information likely needed later decision making . key solving problems learn man - age context rather hard-coding strategy architecture . LSTMs accomplish first adding explicit context layer architecture ( addi - tion usual recurrent hidden layer ) , specialized neural units make gates control flow information units comprise network layers . gates implemented additional weights operate sequentially input , previous hidden layer , previous context layers . gates LSTM share common design pattern ; consists feed - forward layer , followed sigmoid activation function , followed pointwise multiplication layer gated . choice sigmoid activation function arises tendency push outputs 0 1 . Combining pointwise multiplication effect similar binary mask . Values layer gated align values near 1 mask passed nearly unchanged ; values corresponding lower values essentially erased . first gate consider forget gate . purpose gate deleteforget gate information context longer needed . forget gate computes weighted sum previous state’s hidden layer current input passes sigmoid . mask multiplied context vector remove 9.4 • MANAGING CONTEXT RNNS : LSTMS GRUS 17 information context longer required . ft = σ ( U f ht − 1 + Wf xt ) kt = ct − 1 � ft next task compute actual information need extract previous hidden state current inputs — same basic computation recurrent networks . gt = tanh ( Ught − 1 + Wgxt ) ( 9.16 ) Next , generate mask add gate select information add theadd gate current context . = σ ( Uiht − 1 + Wixt ) ( 9.17 ) jt = gt � ( 9.18 ) Next , add modified context vector get new context vector . ct = jt + kt ( 9.19 ) final gate output gate decide informa-output gate tion required current hidden state ( opposed information needs preserved future decisions ) . ot = σ ( Uoht − 1 + Woxt ) ( 9.20 ) ht = ot � tanh ( ct ) ( 9.21 ) ( 9.22 ) Fig . 9.13 illustrates complete computation single LSTM unit . appropriate weights various gates , LSTM accepts input context layer , hidden layer previous time step , current input vector . generates updated context hidden vectors output . hidden layer , ht , input subsequent layers stacked RNN , generate output final layer network . 9.4.2 Gated Recurrent Units LSTMs introduce considerable number additional parameters recurrent networks . 8 sets weights learn ( i.e . , U W 4 gates unit ) , whereas simple recurrent units 2 . Training additional parameters imposes significantly higher training cost . Gated Recurrent Units ( GRUs ) ( Cho et al . , 2014 ) ease burden dispensing separate context vector , reducing number gates 2 — reset gate , r update gate , z . rt = σ ( Urht − 1 + Wrxt ) ( 9.23 ) zt = σ ( Uzht − 1 + Wzxt ) ( 9.24 ) LSTMs , sigmoid design gates results binary-like mask blocks information values near zero allows 18 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS + ost-1 g x tht-1 Y Y Y st ht Figure 9.13 single LSTM unit displayed computation graph . inputs unit consists current input , x , previous hidden state , ht − 1 , previous context , ct − 1 . outputs new hidden state , ht updated context , ct . information pass unchanged values near . purpose reset gate decide aspects previous hidden state relevant current context ignored . accomplished performing element-wise multiplication r value previous hidden state . masked value computing intermediate representation new hidden state time t . h̃t = tanh ( U ( rt � ht − 1 ) + Wxt ) ( 9.25 ) job update gate z determine aspects new state directly new hidden state aspects previous state need preserved future . accomplished values z interpolate old hidden state new . ht = ( 1 − zt ) ht − 1 + zt h̃t ( 9.26 ) 9.4.3 Gated Units , Layers Networks neural units LSTMs GRUs obviously complex basic feedforward networks . Fortunately , complexity encapsu - lated basic processing units , allowing maintain modularity easily experiment different architectures . , consider Fig . 9.14 illustrates inputs outputs associated kind unit . 9.5 • WORDS , SUBWORDS CHARACTERS 19 h x xt xtht-1 ht ht ct-1 ct ht-1 xt ht ht-1 ( b ) ( ) ( c ) ( d ) ⌃ g z ⌃ g z LSTM Unit GRU Figure 9.14 Basic neural units feedforward , simple recurrent networks ( SRN ) , long short-term memory ( LSTM ) gate recurrent units . far left , ( ) basic feedforward unit single set weights single activation function determine output , arranged layer connections among units layer . Next , ( b ) represents unit simple recurrent network . two inputs additional set weights go . , still single activation function output . increased complexity LSTM ( c ) GRU ( d ) units right encapsulated units themselves . additional external complexity LSTM basic recurrent unit ( b ) presence additional context vector input output . GRU units same input output architecture simple recurrent unit . modularity key power widespread applicability LSTM GRU units . LSTM GRU units substituted network ar - chitectures described Section 9.3 . , simple RNNs , multi-layered net - works making gated units unrolled deep feedforward networks trained usual fashion backpropagation . 9.5 Words , Subwords Characters point , assuming inputs networks word embeddings . seen , word-based embeddings great capturing dis - tributional ( syntactic semantic ) similarity words . , drawbacks exclusively word-based approach : • languages applications , lexicon simply large prac - tically represent every possible word embedding . means com - posing words smaller bits needed . • matter large lexicon , always encounter unknown words due new words entering language , misspellings borrowings languages . • Morphological information , below word level , critical source infor - mation many languages many applications . Word-based methods blind regularities . 20 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS Janet RNN Bi-RNN J n te + Bi-RNN w l l + … … Figure 9.15 Sequence labeling RNN accepts distributional word embeddings aug - mented character-level word embeddings . wide variety alternatives word-based approach explored past few years . following among primary approaches tried . • Ignore words altogether simply character sequences input RNNs . • subword units derived byte-pair encoding phonetic analysis inputs . • full-blown morphological analysis derive linguistically motivated - put sequence . Perhaps surprisingly clear one-best approach applications languages . particularly successful approach combines word embeddings embed - dings derived characters make up words . Fig . 9.15 illustrates approach context part-of-speech tagging . upper part diagram consists RNN accepts input sequence outputs softmax distribu - tion tags element input . Note RNN arbitrarily complex , consisting stacked / bidirectional network layers . inputs network consist ordinary word embeddings enriched character-level information . Specifically , input consists concatenation normal word embedding embeddings derived bidirectional RNN accepts character sequences word input , shown lower part figure . character sequence word input run bidirectional RNN consisting two independent RNNs — processes sequence left - to-right right-to-left . discussed Section 9.3.2 , final hidden states left-to-right right-to-left networks concatenated represent composite character-level representation word . Critically , character embeddings trained context overall task ; loss part-of - speech softmax layer propagated way back character embeddings . 9.6 • SUMMARY 21 J n e Character Projection Layer LSTM1 LSTM1 LSTM1 LSTM1 LSTM2 LSTM2 LSTM2 LSTM2Right-to-left LSTM Left-to-right LSTM t LSTM2 LSTM1 Concatenation Character-Level Word Embedding Character Embeddings Figure 9.16 Bi-RNN accepts word character sequences emits embeddings derived forward backward pass sequence . network itself trained context larger end-application loss propagated way character vector embeddings . 9.6 Summary chapter introduced concept recurrent neural networks applied language problems . Here’s summary main points covered : • simple Recurrent Neural Networks sequences processed naturally element time . • output neural unit particular point time based current input value hidden layer previous time step . • RNNs trained straightforward extension backpropagation algorithm , known backpropagation time ( BPTT ) . • Common language-based applications RNNs include : – Probabilistic language modeling , model assigns probability sequence , next element sequence preceding words . – Auto-regressive generation trained language model . – Sequence labeling , element sequence assigned label , part-of-speech tagging . – Sequence classification , entire text assigned category , spam detection , sentiment analysis topic classification . • Simple recurrent networks often fail extremely difficult success - fully train problems maintaining useful gradients time . • complex gated architectures LSTMs GRUs designed overcome issues explicitly managing task deciding remember forget hidden context layers . 22 CHAPTER 9 • SEQUENCE PROCESSING RECURRENT NETWORKS Bibliographical Historical Notes Influential investigations kind simple RNNs discussed conducted context Parallel Distributed Processing ( PDP ) group UC San Diego 1980 ’ s . work directed human cognitive modeling rather practical NLP applications Rumelhart et al . 1986b McClelland et al . 1986 . Models recurrence hidden layer feedforward network ( Elman networks ) introduced Elman ( 1990 ) . Similar architectures investigated Jordan ( 1986 ) recurrence output layer , Mathis Mozer ( 1995 ) addition recurrent context layer prior hidden layer . possibility unrolling recurrent network equivalent feedforward network discussed ( Rumelhart et al . , 1986b ) . parallel work cognitive modeling , RNNs investigated extensively continuous domain signal processing speech communities ( Giles et al . , 1994 ) . Schuster Paliwal ( 1997 ) introduced bidirectional RNNs de - scribed results TIMIT phoneme transcription task . theoretically interesting , difficulty training RNNs manag - ing context long sequences impeded progress practical applications . situation changed introduction LSTMs Hochreiter Schmidhuber ( 1997 ) . Impressive performance gains demonstrated tasks bound - ary signal processing language processing including phoneme recognition ( Graves Schmidhuber , 2005 ) , handwriting recognition ( Graves et al . , 2007 ) significantly speech recognition ( Graves et al . , 2013 ) . Interest applying neural networks practical NLP problems surged work Collobert Weston ( 2008 ) Collobert et al . ( 2011 ) . efforts made learned word embeddings , convolutional networks , end-to-end training . demonstrated near state-of-the-art performance number standard shared tasks including part-of-speech tagging , chunking , named entity recognition se - mantic role labeling hand-engineered features . Approaches married LSTMs pre-trained collections word-embeddings based word2vec ( Mikolov et al . , 2013 ) GLOVE ( Pennington et al . , 2014 ) , quickly came dominate many common tasks : part-of-speech tagging ( Ling et al . , 2015 ) , syntactic chunking ( Søgaard Goldberg , 2016 ) , named entity recog - nition via IOB tagging Chiu Nichols 2016 , Ma Hovy 2016 , opinion mining ( Irsoy Cardie , 2014 ) , semantic role labeling ( Zhou Xu , 2015 ) AMR parsing ( Foland Martin , 2016 ) . earlier surge progress involving statistical machine learning , advances made possible availability training data provided CONLL , SemEval , shared tasks , well shared resources Ontonotes ( Pradhan et al . , 2007 ) , PropBank ( Palmer et al . , 2005 ) . Bibliographical Historical Notes 23 Chiu , J . P . C . Nichols , E . ( 2016 ) . Named entity recogni - tion bidirectional LSTM-CNNs . TACL , 4 , 357 – 370 . Cho , K . , van Merriënboer , B . , Gulcehre , C . , Bahdanau , D . , Bougares , F . , Schwenk , H . , Bengio , Y . ( 2014 ) . Learn - ing phrase representations RNN encoder – decoder statistical machine translation . EMNLP 2014 , 1724 – 1734 . Collobert , R . Weston , J . ( 2008 ) . unified architec - ture natural language processing : Deep neural networks multitask learning . ICML , 160 – 167 . Collobert , R . , Weston , J . , Bottou , L . , Karlen , M . , Kavukcuoglu , K . , Kuksa , P . ( 2011 ) . Natural language processing ( almost ) scratch . JMLR , 12 , 2493 – 2537 . Elman , J . L . ( 1990 ) . Finding structure time . Cognitive science , 14 ( 2 ) , 179 – 211 . Foland , W . Martin , J . H . ( 2016 ) . CU-NLP semeval - 2016 task 8 : AMR parsing lstm-based recurrent neu - ral networks . Proceedings 10th International Workshop Semantic Evaluation , 1197 – 1201 . Giles , C . L . , Kuhn , G . M . , Williams , R . J . ( 1994 ) . Dy - namic recurrent neural networks : Theory applications . IEEE Trans . Neural Netw . Learning Syst . , 5 ( 2 ) , 153 – 156 . Graves , . , Fernández , S . , Liwicki , M . , Bunke , H . , Schmidhuber , J . ( 2007 ) . Unconstrained on-line handwrit - ing recognition recurrent neural networks . NIPS 2007 , 577 – 584 . Graves , . , Mohamed , . , Hinton , G . E . ( 2013 ) . Speech recognition deep recurrent neural networks . IEEE International Conference Acoustics , Speech Signal Processing , ICASSP , 6645 – 6649 . Graves , . Schmidhuber , J . ( 2005 ) . Framewise phoneme classification bidirectional LSTM neural network architectures . Neural Networks , 18 ( 5-6 ) , 602 – 610 . Hochreiter , S . Schmidhuber , J . ( 1997 ) . Long short-term memory . Neural Computation , 9 ( 8 ) , 1735 – 1780 . Irsoy , O . Cardie , C . ( 2014 ) . Opinion mining deep recurrent neural networks . EMNLP 2014 , 720 – 728 . Jordan , M . ( 1986 ) . Serial order : parallel distributed pro - cessing approach . Tech . rep . ICS Report 8604 , University California , San Diego . Lample , G . , Ballesteros , M . , Subramanian , S . , Kawakami , K . , Dyer , C . ( 2016 ) . Neural architectures named entity recognition . NAACL HLT 2016 . Ling , W . , Dyer , C . , Black , . W . , Trancoso , . , Fermandez , R . , Amir , S . , Marujo , L . , Luı́s , T . ( 2015 ) . Finding function form : Compositional character models open vocabulary word representation . EMNLP 2015 , 1520 – 1530 . Ma , X . Hovy , E . H . ( 2016 ) . End-to-end sequence label - ing via bi-directional LSTM-CNNs-CRF . ACL 2016 . Mathis , D . . Mozer , M . C . ( 1995 ) . computa - tional utility consciousness . Tesauro , G . , Touretzky , D . S . , Alspector , J . ( Eds . ) , Advances Neural Infor - mation Processing Systems VII . MIT Press . McClelland , J . L . , Rumelhart , D . E . , PDP Research Group ( 1986 ) . Parallel Distributed Processing : Explo - rations Microstructure Cognition , Vol . 2 : Psy - chological Biological Models . Bradford Books ( MIT Press ) . Mikolov , T . , Chen , K . , Corrado , G . S . , Dean , J . ( 2013 ) . Efficient estimation word representations vec - tor space . ICLR 2013 . Mikolov , T . , Karafiát , M . , Burget , L . , Černockỳ , J . , Khu - danpur , S . ( 2010 ) . Recurrent neural network based lan - guage model . INTERSPEECH 2010 , 1045 – 1048 . Palmer , M . , Kingsbury , P . , Gildea , D . ( 2005 ) . propo - sition bank : annotated corpus semantic roles . Com - putational Linguistics , 31 ( 1 ) , 71 – 106 . Pennington , J . , Socher , R . , Manning , C . D . ( 2014 ) . Glove : Global vectors word representation . EMNLP 2014 , 1532 – 1543 . Pradhan , S . , Hovy , E . H . , Marcus , M . P . , Palmer , M . , Ramshaw , L . . , Weischedel , R . M . ( 2007 ) . Ontonotes : unified relational semantic representation . Int . J . Seman - tic Computing , 1 ( 4 ) , 405 – 419 . Ramshaw , L . . Marcus , M . P . ( 1995 ) . Text chunking transformation-based learning . Proceedings 3rd Annual Workshop Large Corpora , 82 – 94 . Rumelhart , D . E . , Hinton , G . E . , Williams , R . J . ( 1986a ) . Learning internal representations error propagation . Rumelhart , D . E . McClelland , J . L . ( Eds . ) , Parallel Distributed Processing , Vol . 2 , 318 – 362 . MIT Press . Rumelhart , D . E . , McClelland , J . L . , PDP Research Group ( 1986b ) . Parallel Distributed Processing : Explo - rations Microstructure Cognition , Vol . 1 : Founda - tions . Bradford Books ( MIT Press ) . Schuster , M . Paliwal , K . K . ( 1997 ) . Bidirectional recur - rent neural networks . IEEE Transactions Signal Pro - cessing , 45 , 2673 – 2681 . Shannon , C . E . ( 1951 ) . Prediction entropy printed English . Bell System Technical Journal , 30 , 50 – 64 . Søgaard , . Goldberg , Y . ( 2016 ) . Deep multi-task learn - ing low level tasks supervised lower layers . ACL 2016 . Werbos , P . ( 1974 ) . Beyond regression : new tools predic - tion analysis behavioral sciences / . Ph . D . thesis , Harvard University . Werbos , P . J . ( 1990 ) . Backpropagation time : . Proceedings IEEE , 78 ( 10 ) , 1550 – 1560 . Zhou , J . Xu , W . ( 2015 ) . End-to-end learning seman - tic role labeling recurrent neural networks . ACL 2015 , 1127 – 1137 .