Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 4 Naive Bayes SentimentClassification Classification lies heart human machine intelligence . Deciding letter , word , image presented senses , recognizing faces voices , sorting mail , assigning grades homeworks ; examples assigning category input . potential challenges task highlighted fabulist Jorge Luis Borges ( 1964 ) , imagined classifying animals : ( ) belong Emperor , ( b ) embalmed ones , ( c ) trained , ( d ) suckling pigs , ( e ) mermaids , ( f ) fabulous ones , ( g ) stray dogs , ( h ) included classification , ( ) tremble mad , ( j ) innumerable ones , ( k ) drawn fine camel’s hair brush , ( l ) others , ( m ) just broken flower vase , ( n ) resemble flies distance . Many language processing tasks involve classification , although luckily classes easier define Borges . chapter introduce naive Bayes algorithm apply text categorization , task assigning label ortextcategorization category entire text document . focus common text categorization task , sentiment analysis , ex-sentimentanalysis traction sentiment , positive negative orientation writer expresses toward object . review movie , book , product web expresses author’s sentiment toward product , editorial political text expresses sentiment toward candidate political action . Extracting consumer public sen - timent thus relevant fields marketing politics . simplest version sentiment analysis binary classification task , words review provide excellent cues . Consider , example , follow - ing phrases extracted positive negative reviews movies restaurants . Words like great , richly , awesome , pathetic , awful ridiculously informative cues : + . . . zany characters richly applied satire , great plot twists − pathetic . worst part boxing scenes . . . + . . . awesome caramel sauce sweet toasty almonds . love place ! − . . . awful pizza ridiculously overpriced . . . Spam detection another important commercial application , binary clas-spam detection sification task assigning email two classes spam not-spam . Many lexical features perform classification . ex - ample might quite reasonably suspicious email containing phrases like “ online pharmaceutical ” “ COST ” “ Dear Winner ” . Another thing might know text language written . Texts social media , example , number languages need apply different processing . task language id thus first steplanguage id language processing pipelines . Related tasks like determining text’s au - thor , ( authorship attribution ) , author characteristics like gender , age , nativeauthorshipattribution 2 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION language text classification tasks relevant digital humanities , social sciences , forensic linguistics . Finally , oldest tasks text classification assigning library sub - ject category topic label text . Deciding research paper concerns epidemiology , perhaps , embryology , important component infor - mation retrieval . Various sets subject categories exist , MeSH ( Medical Subject Headings ) thesaurus . fact , , subject category classification task naive Bayes algorithm invented 1961 . Classification essential tasks below level document well . already seen period disambiguation ( deciding period end sen - tence part word ) , word tokenization ( deciding character word boundary ) . Even language modeling viewed classification : word thought class , predicting next word classifying context-so-far class next word . part-of-speech tagger ( Chapter 8 ) classifies occurrence word sentence , e.g . , noun verb . goal classification take single observation , extract useful features , thereby classify observation set discrete classes . method classifying text handwritten rules . many areas language processing handwritten rule-based classifiers constitute state-of - the-art system , least part . Rules fragile , , situations data change time , tasks humans necessarily good coming up rules . cases classification language processing via supervised machine learning , subject remainder chapter . supervised supervised machine learning learning , data set input observations , associated correct output ( ‘ supervision signal ’ ) . goal algorithm learn map new observation correct output . Formally , task supervised classification take input x fixed set output classes Y = y1 , y2 , . . . , yM return predicted class y ∈ Y . text classification , sometimes talk c ( “ class ” ) y output variable , d ( “ document ” ) x input variable . supervised situation training set N documents hand-labeled class : ( d1 , c1 ) , . . . . , ( dN , cN ) . goal learn classifier capable mapping new document d correct class c ∈ C . probabilistic classifier additionally tell probability observation class . full distribution classes useful information downstream decisions ; avoiding making discrete decisions early useful combining systems . Many kinds machine learning algorithms build classifiers . chapter introduces naive Bayes ; following introduces logistic regression . exemplify two ways classification . Generative classifiers like naive Bayes build model class generate input data . ob - servation , return class likely generated observation . Dis - criminative classifiers like logistic regression learn features input useful discriminate different possible classes . discriminative systems often accurate hence commonly , generative classifiers still role . 4.1 • NAIVE BAYES CLASSIFIERS 3 4.1 Naive Bayes Classifiers section introduce multinomial naive Bayes classifier , called be-naive Bayesclassifier cause Bayesian classifier makes simplifying ( naive ) assumption features interact . intuition classifier shown Fig . 4.1 . represent text document bag-of-words , , unordered set words positionbag-of-words ignored , keeping frequency document . example figure , representing word order phrases like “ love movie ” “ recommend ” , simply note word occurred 5 times entire excerpt , word 6 times , words love , recommend , movie once , . love recommend movie andand seen seen yet whimsical whilewhenever times sweet several scenes satirical romantic manages humor happy fun friend fairy dialogue conventions anyone adventure always again love movie ! sweet , satirical humor . dialogue great adventure scenes fun . . . manages whimsical romantic laughing conventions fairy tale genre . recommend just anyone . seen several times , always happy again whenever friend hasn't seen yet ! seen yet whimsical times sweet satirical adventure genre fairy humor great … 6 5 4 3 3 2 1 1 1 1 1 1 1 1 1 1 1 1 … Figure 4.1 Intuition multinomial naive Bayes classifier applied movie review . position words ignored ( bag words assumption ) make frequency word . Naive Bayes probabilistic classifier , meaning document d , classes c ∈ C classifier returns class ĉ maximum posterior probability document . Eq . 4.1 hat notation ˆ mean “ ourˆ estimate correct class ” . ĉ = argmax c ∈ C P ( c | d ) ( 4.1 ) idea Bayesian inference known work Bayes ( 1763 ) , Bayesianinference first applied text classification Mosteller Wallace ( 1964 ) . intuition Bayesian classification Bayes ’ rule transform Eq . 4.1 probabilities useful properties . Bayes ’ rule presented Eq . 4.2 ; gives way break down conditional probability P ( x | y ) three probabilities : P ( x | y ) = P ( y | x ) P ( x ) P ( y ) ( 4.2 ) 4 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION substitute Eq . 4.2 Eq . 4.1 get Eq . 4.3 : ĉ = argmax c ∈ C P ( c | d ) = argmax c ∈ C P ( d | c ) P ( c ) P ( d ) ( 4.3 ) conveniently simplify Eq . 4.3 dropping denominator P ( d ) . possible computing P ( d | c ) P ( c ) P ( d ) possible class . P ( d ) change class ; always asking likely class same document d , same probability P ( d ) . Thus , choose class maximizes simpler formula : ĉ = argmax c ∈ C P ( c | d ) = argmax c ∈ C P ( d | c ) P ( c ) ( 4.4 ) thus compute probable class ĉ document d choosing class highest product two probabilities : prior probabilitypriorprobability class P ( c ) likelihood document P ( d | c ) : likelihood ĉ = argmax c ∈ C likelihood ︷ ︸ ︸ ︷ P ( d | c ) prior ︷ ︸ ︸ ︷ P ( c ) ( 4.5 ) loss generalization , represent document d set features f1 , f2 , . . . , fn : ĉ = argmax c ∈ C likelihood ︷ ︸ ︸ ︷ P ( f1 , f2 , . . . . , fn | c ) prior ︷ ︸ ︸ ︷ P ( c ) ( 4.6 ) Unfortunately , Eq . 4.6 still hard compute directly : sim - plifying assumptions , estimating probability every possible combination features ( example , every possible set words positions ) require huge numbers parameters impossibly large training sets . Naive Bayes classifiers make two simplifying assumptions . first bag words assumption discussed intuitively : assume position matter , word “ love ” same effect classification occurs 1st , 20th , last word document . Thus assume features f1 , f2 , . . . , fn encode word identity position . second commonly called naive Bayes assumption : condi-naive Bayesassumption tional independence assumption probabilities P ( fi | c ) independent class c hence ‘ naively ’ multiplied follows : P ( f1 , f2 , . . . . , fn | c ) = P ( f1 | c ) · P ( f2 | c ) · . . . · P ( fn | c ) ( 4.7 ) final equation class chosen naive Bayes classifier thus : cNB = argmax c ∈ C P ( c ) ∏ f ∈ F P ( f | c ) ( 4.8 ) apply naive Bayes classifier text , need consider word positions , simply walking index every word position document : positions ← word positions test document cNB = argmax c ∈ C P ( c ) ∏ ∈ positions P ( wi | c ) ( 4.9 ) 4.2 • TRAINING NAIVE BAYES CLASSIFIER 5 Naive Bayes calculations , like calculations language modeling , log space , avoid underflow increase speed . Thus Eq . 4.9 generally expressed cNB = argmax c ∈ C logP ( c ) + ∑ ∈ positions logP ( wi | c ) ( 4.10 ) considering features log space , Eq . 4.10 computes predicted class lin - ear function input features . Classifiers linear combination inputs make classification decision — like naive Bayes logistic regression — called linear classifiers . linearclassifiers 4.2 Training Naive Bayes Classifier learn probabilities P ( c ) P ( fi | c ) ? first consider max - imum likelihood estimate . simply frequencies data . document prior P ( c ) ask percentage documents training set class c . Let Nc number documents training data class c Ndoc total number documents . : P̂ ( c ) = Nc Ndoc ( 4.11 ) learn probability P ( fi | c ) , assume feature just existence word document’s bag words , P ( wi | c ) , compute fraction times word wi appears among words documents topic c . first concatenate documents category c big “ category c ” text . frequency wi concatenated document give maximum likelihood estimate probability : P̂ ( wi | c ) = count ( wi , c ) ∑ w ∈ V count ( w , c ) ( 4.12 ) vocabulary V consists union word types classes , just words class c . problem , , maximum likelihood training . Imagine trying estimate likelihood word “ fantastic ” class positive , suppose training documents contain word “ fantastic ” classified positive . Perhaps word “ fantastic ” happens occur ( sarcasti - cally ? ) class negative . case probability feature zero : P̂ ( “ fantastic ” | positive ) = count ( “ fantastic ” , positive ) ∑ w ∈ V count ( w , positive ) = 0 ( 4.13 ) naive Bayes naively multiplies feature likelihoods together , zero probabilities likelihood term class cause probability class zero , matter evidence ! simplest solution add-one ( Laplace ) smoothing introduced Chap - ter 3 . Laplace smoothing usually replaced sophisticated smoothing 6 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION algorithms language modeling , commonly naive Bayes text catego - rization : P̂ ( wi | c ) = count ( wi , c ) + 1 ∑ w ∈ V ( count ( w , c ) + 1 ) = count ( wi , c ) + 1 ( ∑ w ∈ V count ( w , c ) ) + | V | ( 4.14 ) Note once again crucial vocabulary V consists union word types classes , just words class c ( try convince yourself why true ; exercise end chapter ) . words occur test data vocab - ulary occur training document class ? solution unknown words ignore — remove testunknown word document include probability . Finally , systems choose completely ignore another class words : stop words , frequent words like . sorting vocabu-stop words lary frequency training set , defining top 10 – 100 vocabulary entries stop words , alternatively many pre-defined stop word list available online . every instance stop words simply removed training test documents never occurred . text classi - fication applications , , stop word list improve performance , common make entire vocabulary stop word list . Fig . 4.2 shows final algorithm . function TRAIN NAIVE BAYES ( D , C ) returns log P ( c ) log P ( w | c ) class c ∈ C # Calculate P ( c ) terms Ndoc = number documents D Nc = number documents D class c logprior [ c ] ← log Nc Ndoc V ← vocabulary D bigdoc [ c ] ← append ( d ) d ∈ D class c word w V # Calculate P ( w | c ) terms count ( w , c ) ← # occurrences w bigdoc [ c ] loglikelihood [ w , c ] ← log count ( w , c ) + 1 ∑ w ′ V ( count ( w ′ , c ) + 1 ) return logprior , loglikelihood , V function TEST NAIVE BAYES ( testdoc , logprior , loglikelihood , C , V ) returns best c class c ∈ C sum [ c ] ← logprior [ c ] position testdoc word ← testdoc [ ] word ∈ V sum [ c ] ← sum [ c ] + loglikelihood [ word , c ] return argmaxc sum [ c ] Figure 4.2 naive Bayes algorithm , add-1 smoothing . add-α smoothing , change + 1 + α loglikelihood counts training . 4.3 • WORKED EXAMPLE 7 4.3 Worked example walk example training testing naive Bayes add-one smoothing . sentiment analysis domain two classes positive ( + ) negative ( - ) , take following miniature training test documents simplified actual movie reviews . Cat Documents Training - just plain boring - entirely predictable lacks energy - surprises few laughs + powerful + fun film summer Test ? predictable fun prior P ( c ) two classes computed via Eq . 4.11 NcNdoc : P ( − ) = 3 5 P ( + ) = 2 5 word occur training set , drop completely ( mentioned , unknown word models naive Bayes ) . like - lihoods training set remaining three words “ predictable ” , “ ” , “ fun ” , follows , Eq . 4.14 ( computing probabilities remainder words training set left exercise reader ) : P ( “ predictable ” | − ) = 1 + 1 14 + 20 P ( “ predictable ” | + ) = 0 + 1 9 + 20 P ( “ ” | − ) = 1 + 1 14 + 20 P ( “ ” | + ) = 0 + 1 9 + 20 P ( “ fun ” | − ) = 0 + 1 14 + 20 P ( “ fun ” | + ) = 1 + 1 9 + 20 test sentence S = “ predictable fun ” , removing word ‘ ’ , chosen class , via Eq . 4.9 , computed follows : P ( − ) P ( S | − ) = 3 5 × 2 × 2 × 1 343 = 6.1 × 10 − 5 P ( + ) P ( S | + ) = 2 5 × 1 × 1 × 2 293 = 3.2 × 10 − 5 model thus predicts class negative test sentence . 4.4 Optimizing Sentiment Analysis standard naive Bayes text classification work well sentiment analysis , small changes generally employed improve performance . First , sentiment classification number text classification tasks , word occurs seems matter frequency . Thus often improves performance clip word counts document 1 ( end chapter pointers results ) . variant called binary 8 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION multinomial naive Bayes binary NB . variant same Eq . 4.10 exceptbinary NB document remove duplicate words concatenating single big document . Fig . 4.3 shows example set four documents ( shortened text-normalized example ) remapped binary , modified counts shown table right . example worked add-1 smoothing make differences clearer . Note results counts need 1 ; word great count 2 even Binary NB , appears multiple documents . Four original documents : − pathetic worst part boxing scenes − plot twists great scenes + satire great plot twists + great scenes great film per-document binarization : − pathetic worst part boxing scenes − plot twists great scenes + satire great plot twists + great scenes film NB Binary Counts Counts + − + − 2 0 1 0 boxing 0 1 0 1 film 1 0 1 0 great 3 1 2 1 0 1 0 1 0 1 0 1 0 1 0 1 part 0 1 0 1 pathetic 0 1 0 1 plot 1 1 1 1 satire 1 0 1 0 scenes 1 2 1 2 0 2 0 1 twists 1 1 1 1 0 2 0 1 worst 0 1 0 1 Figure 4.3 example binarization binary naive Bayes algorithm . second important addition commonly made text classification sentiment deal negation . Consider difference really like movie ( positive ) like movie ( negative ) . negation expressed completely alters inferences draw predicate like . Similarly , negation modify negative word produce positive review ( dismiss film , let get bored ) . simple baseline commonly sentiment analysis deal negation following : text normalization , prepend prefix every word token logical negation ( n’t , , , never ) next punc - tuation mark . Thus phrase like movie , becomes NOT_like NOT_this NOT_movie , Newly formed ‘ words ’ like like , recommend thus occur - ten negative document act cues negative sentiment , words like bored , dismiss acquire positive associations . return Chap - ter 17 parsing deal accurately scope relationship - tween negation words predicates modify , simple baseline works quite well practice . Finally , situations might insufficient labeled training data train accurate naive Bayes classifiers words training set estimate positive negative sentiment . cases derive positive 4.5 • NAIVE BAYES TEXT CLASSIFICATION TASKS 9 negative word features sentiment lexicons , lists words pre-sentimentlexicons annotated positive negative sentiment . Four popular lexicons General Inquirer ( Stone et al . , 1966 ) , LIWC ( Pennebaker et al . , 2007 ) , opinion lexiconGeneralInquirer LIWC Hu Liu ( 2004 ) MPQA Subjectivity Lexicon ( Wilson et al . , 2005 ) . example MPQA subjectivity lexicon 6885 words , 2718 positive 4912 negative , marked strongly weakly biased . ( Chapter 21 discuss lexicons learned automatically . ) samples positive negative words MPQA lexicon include : + : admirable , beautiful , confident , dazzling , ecstatic , favor , glee , great − : awful , bad , bias , catastrophe , cheat , deny , envious , foul , harsh , hate common way lexicons naive Bayes classifier add feature counted whenever word lexicon occurs . Thus might add feature called ‘ word occurs positive lexicon ’ , treat instances words lexicon counts feature , counting word separately . Similarly , might add second feature ‘ word occurs negative lexicon ’ words negative lexicon . lots training data , test data matches training data , just two features work well words . training data sparse representative test set , dense lexicon features sparse individual-word features generalize better . 4.5 Naive Bayes text classification tasks previous section pointed naive Bayes require classifier words training data features . fact features naive Bayes express property input text . Consider task spam detection , deciding particular piece email isspam detection example spam ( unsolicited bulk email ) — first applications naive Bayes text classification ( Sahami et al . , 1998 ) . common solution , rather words individual features , predefine likely sets words phrases features , combined features purely linguistic . example open-source SpamAssassin tool1 predefines features like phrase “ hundred percent guaranteed ” , feature mentions millions dollars , regular expression matches suspiciously large sums money . includes features like HTML low ratio text image area , purely linguistic might require sophisticated computation , totally non-linguistic features , say , path email took arrive . sample SpamAssassin features : • Email subject line capital letters • Contains phrases urgency like “ urgent reply ” • Email subject line contains “ online pharmaceutical ” • HTML unbalanced ” head ” tags • Claims removed list tasks , like language ID — determining language piece oflanguage ID text written — effective naive Bayes features words , byte n-grams , 2-grams ( ‘ zw ’ ) 3-grams ( ‘ nya ’ , ‘ Vo ’ ) , 4-grams ( ‘ ie z ’ , ‘ thei ’ ) . 1 https://spamassassin.apache.org 10 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION spaces count byte , byte n-grams model statistics begin - ning ending words . 2 widely naive Bayes system , langid.py ( Lui Baldwin , 2012 ) begins possible n-grams lengths 1-4 , feature selection winnow down informative 7000 final features . Language ID systems trained multilingual text , Wikipedia ( Wikipedia text 68 different languages ( Lui Baldwin , 2011 ) ) , newswire . make sure multilingual text correctly reflects different regions , dialects , socioeconomic classes , systems add Twitter text many languages geo - tagged many regions ( important getting world English dialects countries large Anglophone populations like Nigeria India ) , Bible Quran transla - tions , slang websites like Urban Dictionary , corpora African American Vernacular English ( Blodgett et al . , 2016 ) , ( Jurgens et al . , 2017 ) . 4.6 Naive Bayes Language Model saw previous section , naive Bayes classifiers sort fea - ture : dictionaries , URLs , email addresses , network features , phrases , . , previous section , individual word features , words text ( subset ) , naive Bayes important similar - ity language modeling . Specifically , naive Bayes model viewed set class-specific unigram language models , model class instantiates unigram language model . likelihood features naive Bayes model assign probability word P ( word | c ) , model assigns probability sentence : P ( s | c ) = ∏ ∈ positions P ( wi | c ) ( 4.15 ) Thus consider naive Bayes model classes positive ( + ) negative ( - ) following model parameters : w P ( w | + ) P ( w |-) 0.1 0.2 love 0.1 0.001 0.01 0.01 fun 0.05 0.005 film 0.1 0.1 . . . . . . . . . two columns instantiates language model assign probability sentence “ love fun film ” : P ( ” love fun film ” | + ) = 0.1 × 0.1 × 0.01 × 0.05 × 0.1 = 0.0000005 P ( ” love fun film ” | − ) = 0.2 × 0.001 × 0.01 × 0.005 × 0.1 = . 0000000010 happens , positive model assigns higher probability sentence : P ( s | pos ) > P ( s | neg ) . Note just likelihood part naive Bayes 2 possible codepoints , multi-byte Unicode representations characters character sets , simply bytes seems work better . 4.7 • EVALUATION : PRECISION , RECALL , F-MEASURE 11 model ; once multiply prior full naive Bayes model might well make different classification decision . 4.7 Evaluation : Precision , Recall , F-measure introduce methods evaluating text classification , first consider simple binary detection tasks . example , spam detection , goal label every text spam category ( “ positive ” ) spam category ( “ negative ” ) . item ( email document ) need know system called spam . need know email actually spam , i.e . human-defined labels document trying match . refer human labels gold labels . gold labels imagine CEO Delicious Pie Company need know people saying pies social media , build system detects tweets concerning Delicious Pie . positive class tweets Delicious Pie negative class tweets . cases , need metric knowing well spam detector ( pie-tweet-detector ) . evaluate system detecting things , start building contingency table like shown Fig . 4.4 . cell labels acontingencytable set possible outcomes . spam detection case , example , true positives documents indeed spam ( indicated human-created gold labels ) system spam . False negatives documents indeed spam system labeled non-spam . bottom right table equation accuracy , asks percentage observations ( spam pie examples means emails tweets ) system labeled correctly . Although accuracy might seem natural metric , generally . That’s accuracy work well classes unbalanced ( indeed spam , large majority email , tweets , mainly pie ) . true positive false negative false positive true negative gold positive gold negative system positive system negative gold standard labels system output labels recall = tp tp + fn precision = tp tp + fp accuracy = tp + tn tp + fp + tn + fn Figure 4.4 Contingency table make explicit , imagine looked million tweets , say 100 discussing love ( hatred ) pie , 999,900 tweets something completely unrelated . Imagine simple classifier stupidly classified every tweet “ pie ” . classifier 999,900 true negatives 100 false negatives accuracy 999,900/1,000,000 99.99 % ! amazing accuracy level ! Surely happy classifier ? course fabulous ‘ pie ’ classifier 12 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION completely useless , find single customer comments looking . words , accuracy good metric goal discover something rare , least completely balanced frequency , common situation world . That’s why accuracy generally turn two metrics : precision recall . Precision measures percentage items system detectedprecision ( i.e . , system labeled positive ) fact positive ( i.e . , positive accord - ing human gold labels ) . Precision defined Precision = true positives true positives + false positives Recall measures percentage items actually present input wererecall correctly identified system . Recall defined Recall = true positives true positives + false negatives Precision recall help solve problem useless “ nothing pie ” classifier . classifier , despite fabulous accuracy 99.99 % , terrible recall 0 ( true positives , 100 false negatives , recall 0/100 ) . convince yourself precision finding relevant tweets equally problematic . Thus precision recall , unlike accuracy , emphasize true positives : finding things supposed looking . many ways define single metric incorporates aspects precision recall . simplest combinations F-measure ( vanF-measure Rijsbergen , 1975 ) , defined : Fβ = ( β 2 + 1 ) PR β 2P + R β parameter differentially weights importance recall precision , based perhaps needs application . Values β > 1 favor recall , values β < 1 favor precision . β = 1 , precision recall equally bal - anced ; frequently metric , called Fβ = 1 just F1 : F1 F1 = 2PR P + R ( 4.16 ) F-measure comes weighted harmonic mean precision recall . harmonic mean set numbers reciprocal arithmetic mean recip - rocals : HarmonicMean ( a1 , a2 , a3 , a4 , . . . , ) = n 1 a1 + 1a2 + 1 a3 + . . . + 1an ( 4.17 ) hence F-measure F = 1 α 1P + ( 1 − α ) 1 R ( β 2 = 1 − α α ) F = ( β 2 + 1 ) PR β 2P + R ( 4.18 ) Harmonic mean conservative metric ; harmonic mean two values closer minimum two values arithmetic mean . Thus weighs lower two numbers heavily . 4.7 • EVALUATION : PRECISION , RECALL , F-MEASURE 13 4.7.1 two classes Up assuming text classification tasks two classes . lots classification tasks language processing two classes . sentiment analysis generally 3 classes ( positive , negative , neutral ) even classes common tasks like part-of-speech tagging , word sense disambiguation , semantic role labeling , emotion detection , . two kinds multi-class classification tasks . any-of multi-labelany-of classification , document item assigned label . solve any-of classification building separate binary classifiers class c , trained positive examples labeled c negative examples labeled c . test document item d , classifier makes decision independently , assign multiple labels d . common language processing one-of multinomial classification , one-of multinomial classification classes mutually exclusive document item appears exactly class . again build separate binary classifier trained positive examples c negative examples classes . test document item d , run classifiers choose label classifier highest score . Consider sample confusion matrix hypothetical 3 - way one-of email categorization decision ( urgent , normal , spam ) shown Fig . 4.5 . 8 5 10 60 urgent normal gold labels system output recallu = 8 8 + 5 + 3 precisionu = 8 8 + 10 + 11 50 30 200 spam urgent normal spam 3 recalln = recalls = precisionn = 60 5 + 60 + 50 precisions = 200 3 + 30 + 200 60 10 + 60 + 30 200 1 + 50 + 200 Figure 4.5 Confusion matrix three-class categorization task , showing pair classes ( c1 , c2 ) , many documents c1 ( ) correctly assigned c2 matrix shows , example , system mistakenly labeled 1 spam doc - ument urgent , shown compute distinct precision recall value class . order derive single metric tells well system , combine values two ways . macroaveraging , wemacroaveraging compute performance class , average classes . microav - eraging , collect decisions classes single contingency table , andmicroaveraging compute precision recall table . Fig . 4.6 shows contingency table class separately , shows computation microaveraged macroaveraged precision . figure shows , microaverage dominated frequent class ( case spam ) , counts pooled . macroaverage better reflects statistics smaller classes , appropriate performance classes equally important . 14 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION 8 8 11 340 true urgent true system urgent system 60 40 55 212 true normal true system normal system 200 51 33 83 true spam true system spam system 268 99 99 635 true yes true system yes system precision = 8 + 11 8 = . 42 precision = 200 + 33 200 = . 86precision = 60 + 55 60 = . 52 microaverage precision 268 + 99 268 = . 73 = macroaverage precision 3 . 42 + . 52 + . 86 = . 60 = PooledClass 3 : SpamClass 2 : NormalClass 1 : Urgent Figure 4.6 Separate contingency tables 3 classes previous figure , showing pooled contin - gency table microaveraged macroaveraged precision . 4.8 Test sets Cross-validation training testing procedure text classification follows saw language modeling ( Section ? ? ) : training set train model , development test set ( called devset ) perhaps tune parameters , developmenttest set devset general decide best model . Once come up think best model , run ( hitherto unseen ) test set report performance . devset avoids overfitting test set , fixed training set , devset , test set creates another problem : order save lots data training , test set ( devset ) might large enough representative . better somehow data training test . cross-validation : randomly choose training test set division ofcross-validation data , train classifier , compute error rate test set . repeat different randomly selected training set test set . sampling process 10 times average 10 runs get average error rate . called 10-fold cross-validation . 10-foldcross-validation problem cross-validation data testing , need whole corpus blind ; examine data suggest possible features general going . looking corpus often important designing system . reason , common create fixed training set test set , 10-fold cross-validation inside training set , compute error rate normal way test set , shown Fig . 4.7 . 4.9 Statistical Significance Testing building systems constantly comparing performance systems . Often added new bells whistles algorithm compare new version system unaugmented version . compare algorithm previously published know better . might imagine compare performance two classifiers B look B’s score same test set — example might choose compare macro-averaged F1 — B 4.9 • STATISTICAL SIGNIFICANCE TESTING 15 Training Iterations 1 3 4 5 2 6 7 8 9 10 Dev Dev Dev Dev Dev Dev Dev Dev Dev Dev Training Training Training Training Training Training Training Training Training Training Training Test Set Testing Figure 4.7 10-fold cross-validation higher score . just looking difference good enough , might better performance B particular test set just chance . say test set x n observations x = x1 , x2 , . . , xn A’s performance better B δ ( x ) . know really better B ? need reject null hypothesis really better B andnull hypothesis difference δ ( x ) occurred purely chance . null hypothesis correct , expect many test sets size n measured B’s performance , average might accidentally still better B amount δ ( x ) just chance . formally , random variable X ranging test sets , null hypothesis H0 expects P ( δ ( X ) > δ ( x ) | H0 ) , probability similarly big differences just chance , high . test sets just measure δ ( x ′ ) x ′ . found deltas seem bigger δ ( x ) , , p-value ( x ) sufficiently small , less standard thresholds 0.05 0.01 , might reject null hypothesis agree δ ( x ) sufficiently surprising difference really better algorithm B . Following Berg-Kirkpatrick et al . ( 2012 ) refer P ( δ ( X ) > δ ( x ) | H0 ) p-value ( x ) . language processing generally traditional statistical approaches like paired t-tests compare system outputs metrics normally distributed , violating assumptions tests . standard approach comput - ing p-value ( x ) natural language processing non-parametric tests like bootstrap test ( Efron Tibshirani , 1993 ) — describe below — abootstrap test similar test , approximate randomization ( Noreen , 1989 ) . advantage theseapproximaterandomization tests apply metric ; precision , recall , F1 BLEU metric machine translation . word bootstrapping refers repeatedly drawing large numbers smallerbootstrapping samples replacement ( called bootstrap samples ) original larger sam - ple . intuition bootstrap test create many virtual test sets observed test set repeatedly sampling . method makes assumption sample representative population . Consider tiny text classification example test set x 10 documents . first row Fig . 4.8 shows results two classifiers ( B ) test set , document labeled four possibilities : ( B right , wrong , right B wrong , wrong B right ) ; slash letter 16 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION ( � B ) means classifier got answer wrong . first document B get correct class ( AB ) , second document got right B got wrong ( � B ) . assume simplicity metric accuracy , accuracy . 70 B . 50 , δ ( x ) . 20 . create virtual test set size N = 10 , repeatedly ( 10 times ) select cell row x replacement . Fig . 4.8 shows few examples . 1 2 3 4 5 6 7 8 9 10 % B % δ ( ) x AB � � B AB � � AB � � B � � AB � � B AB � � � � B � � B . 70 . 50 . 20 x ∗ ( 1 ) � � B AB � � B � � AB � � AB � � B � � AB AB � � � � B AB . 60 . 60 . 00 x ∗ ( 2 ) � � B AB � � � � B � � AB � � AB AB � � AB � � B AB AB . 60 . 70 - . 10 . . . x ∗ ( b ) Figure 4.8 bootstrap : Examples b pseudo test sets created initial true test set x . pseudo test set created sampling n = 10 times replacement ; thus individual sample single cell , document gold label correct incorrect performance classifiers B . sampling distribution , statistics often accidental advantage . various ways compute advantage ; follow version laid Berg-Kirkpatrick et al . ( 2012 ) . might think just ask , bootstrap sample x ∗ ( ) , beats B δ ( x ) . there’s problem : draw samples distribution 0 mean . x ∗ ( ) sampled x , expected value δ ( x ∗ ( ) ) lies close δ ( x ) . , half time better B , expect beat B δ ( x ) . , know often beats expectations δ ( x ) . correct expected success , need zero-center , subtracting δ ( x ) pseudo test set . Thus comparing x ∗ ( ) δ ( x ∗ ( ) ) > 2δ ( x ) . full algorithm bootstrap shown Fig . 4.9 . test set x , number samples b , counts percentage b bootstrap test sets δ ( x ∗ ( ) ) > 2δ ( x ) . percentage acts - sided empirical p-value ( sophisticated ways get p-values confidence intervals exist ) . function BOOTSTRAP ( test set x , num samples b ) returns p-value ( x ) Calculate δ ( x ) # better algorithm B x = 1 b j = 1 n # Draw bootstrap sample x ∗ ( ) size n Select member x random add x ∗ ( ) Calculate δ ( x ∗ ( ) ) # better algorithm B x ∗ ( ) x ∗ ( ) s ← s + 1 δ ( x ∗ ( ) ) > 2δ ( x ) p-value ( x ) ≈ sb # % b samples algorithm beat expectations ? return p-value ( x ) Figure 4.9 version bootstrap algorithm Berg-Kirkpatrick et al . ( 2012 ) . 4.10 • SUMMARY 17 4.10 Summary chapter introduced naive Bayes model classification applied text categorization task sentiment analysis . • Many language processing tasks viewed tasks classification . • Text categorization , entire text assigned class finite set , includes tasks sentiment analysis , spam detection , language identi - fication , authorship attribution . • Sentiment analysis classifies text reflecting positive negative orien - tation ( sentiment ) writer expresses toward object . • Naive Bayes generative model makes bag words assumption ( position matter ) conditional independence assumption ( words conditionally independent class ) • Naive Bayes binarized features seems work better many text clas - sification tasks . • Classifiers evaluated based precision recall . • Classifiers trained distinct training , dev , test sets , including cross-validation training set . Bibliographical Historical Notes Multinomial naive Bayes text classification proposed Maron ( 1961 ) RAND Corporation task assigning subject categories journal abstracts . model introduced features modern form presented , ap - proximating classification task one-of categorization , implementing add-δ smoothing information-based feature selection . conditional independence assumptions naive Bayes idea Bayes - ian analysis text seems arisen multiple times . same year Maron’s paper , Minsky ( 1961 ) proposed naive Bayes classifier vision arti - ficial intelligence problems , Bayesian techniques applied text classification task authorship attribution Mosteller Wallace ( 1963 ) . long known Alexander Hamilton , John Jay , James Madison wrote anonymously-published Federalist papers 1787 – 1788 persuade New York ratify United States Constitution . Yet although 85 essays clearly attributable author another , authorship 12 dispute Hamilton Madison . Mosteller Wallace ( 1963 ) trained Bayesian probabilistic model writing Hamilton another model writings Madison , computed maximum-likelihood author disputed essays . Naive Bayes first applied spam detection Heckerman et al . ( 1998 ) . Metsis et al . ( 2006 ) , Pang et al . ( 2002 ) , Wang Manning ( 2012 ) show boolean attributes multinomial naive Bayes works better full counts . Binary multinomial naive Bayes sometimes confused another variant naive Bayes binary representation term occurs document : Multivariate Bernoulli naive Bayes . Bernoulli variant estimates P ( w | c ) fraction documents contain term , includes probability term document . McCallum Nigam ( 1998 ) Wang Manning ( 2012 ) show multivariate Bernoulli variant naive 18 CHAPTER 4 • NAIVE BAYES SENTIMENT CLASSIFICATION Bayes work well multinomial algorithm sentiment text tasks . variety sources covering many kinds text classification tasks . sentiment analysis Pang Lee ( 2008 ) , Liu Zhang ( 2012 ) . Stamatatos ( 2009 ) surveys authorship attribute algorithms . language identifica - tion Jauhiainen et al . ( 2018 ) ; Jaech et al . ( 2016 ) important early neural system . task newswire indexing often test case text classi - fication algorithms , based Reuters-21578 collection newswire articles . Manning et al . ( 2008 ) Aggarwal Zhai ( 2012 ) text classification ; classification general covered machine learning textbooks ( Hastie et al . 2001 , Witten Frank 2005 , Bishop 2006 , Murphy 2012 ) . Non-parametric methods computing statistical significance first NLP MUC competition ( Chinchor et al . , 1993 ) , even earlier speech recognition ( Gillick Cox 1989 , Bisani Ney 2004 ) . description bootstrap draws description Berg-Kirkpatrick et al . ( 2012 ) . Recent work focused issues including multiple test sets multiple metrics ( Søgaard et al . 2014 , Dror et al . 2017 ) . Feature selection method removing features unlikely generalize well . Features generally ranked informative classifica - tion decision . common metric , information gain , tells many bits ofinformationgain information presence word gives guessing class . feature selection metrics include χ2 , pointwise mutual information , GINI index ; Yang Pedersen ( 1997 ) comparison Guyon Elisseeff ( 2003 ) introduction feature selection . Exercises 4.1 Assume following likelihoods word part positive negative movie review , equal prior probabilities class . pos neg 0.09 0.16 always 0.07 0.06 like 0.29 0.06 foreign 0.04 0.15 films 0.08 0.11 class Naive bayes assign sentence “ always like foreign films . ” ? 4.2 following short movie reviews , labeled genre , comedy action : 1 . fun , couple , love , love comedy 2 . fast , furious , shoot action 3 . couple , fly , fast , fun , fun comedy 4 . furious , shoot , shoot , fun action 5 . fly , fast , shoot , love action new document D : fast , couple , shoot , fly compute likely class D . Assume naive Bayes classifier add-1 smoothing likelihoods . EXERCISES 19 4.3 Train two models , multinomial naive Bayes binarized naive Bayes , add-1 smoothing , following document counts key sentiment words , positive negative class assigned noted . doc “ good ” “ poor ” “ great ” ( class ) d1 . 3 0 3 pos d2 . 0 1 2 pos d3 . 1 3 0 neg d4 . 1 5 2 neg d5 . 0 2 0 neg naive Bayes models assign class ( pos neg ) sentence : good , good plot great characters , poor acting . two models agree disagree ? 20 Chapter 4 • Naive Bayes Sentiment Classification Aggarwal , C . C . Zhai , C . ( 2012 ) . survey text classi - fication algorithms . Aggarwal , C . C . Zhai , C . ( Eds . ) , Mining text data , 163 – 222 . Springer . Bayes , T . ( 1763 ) . Essay Toward Solving Problem Doctrine Chances , Vol . 53 . Reprinted Facsimiles Two Papers Bayes , Hafner Publishing , 1963 . Berg-Kirkpatrick , T . , Burkett , D . , Klein , D . ( 2012 ) . empirical investigation statistical significance NLP . EMNLP 2012 , 995 – 1005 . Bisani , M . Ney , H . ( 2004 ) . Bootstrap estimates confidence intervals ASR performance evaluation . ICASSP-04 , Vol . , 409 – 412 . Bishop , C . M . ( 2006 ) . Pattern recognition machine learning . Springer . Blodgett , S . L . , Green , L . , O’Connor , B . ( 2016 ) . Demo - graphic dialectal variation social media : case study African-American English . EMNLP 2016 . Borges , J . L . ( 1964 ) . analytical language John Wilkins . University Texas Press . Trans . Ruth L . C . Simms . Chinchor , N . , Hirschman , L . , Lewis , D . L . ( 1993 ) . Eval - uating Message Understanding systems : analysis third Message Understanding Conference . Computational Linguistics , 19 ( 3 ) , 409 – 449 . Dror , R . , Baumer , G . , Bogomolov , M . , Reichart , R . ( 2017 ) . Replicability analysis natural language process - ing : Testing significance multiple datasets . TACL , 5 , 471 – 486 . Efron , B . Tibshirani , R . J . ( 1993 ) . introduction bootstrap . CRC press . Gillick , L . Cox , S . J . ( 1989 ) . statistical issues comparison speech recognition algorithms . ICASSP-89 , 532 – 535 . Guyon , . Elisseeff , . ( 2003 ) . introduction vari - able feature selection . JMLR , 3 , 1157 – 1182 . Hastie , T . , Tibshirani , R . J . , Friedman , J . H . ( 2001 ) . Elements Statistical Learning . Springer . Heckerman , D . , Horvitz , E . , Sahami , M . , Dumais , S . T . ( 1998 ) . bayesian approach filtering junk e-mail . Proceeding AAAI-98 Workshop Learning Text Categorization , 55 – 62 . Hu , M . Liu , B . ( 2004 ) . Mining summarizing cus - tomer reviews . KDD , 168 – 177 . Jaech , . , Mulcaire , G . , Hathi , S . , Ostendorf , M . , Smith , N . . ( 2016 ) . Hierarchical character-word models lan - guage identification . ACL Workshop NLP Social Media , 84 – 93 . Jauhiainen , T . , Lui , M . , Zampieri , M . , Baldwin , T . , Lindén , K . ( 2018 ) . Automatic language identification texts : survey . arXiv preprint arXiv : 1804.08186 . Jurgens , D . , Tsvetkov , Y . , Jurafsky , D . ( 2017 ) . Incorpo - rating dialectal variability socially equitable language identification . ACL 2017 , 51 – 57 . Liu , B . Zhang , L . ( 2012 ) . survey opinion mining sentiment analysis . Aggarwal , C . C . Zhai , C . ( Eds . ) , Mining text data , 415 – 464 . Springer . Lui , M . Baldwin , T . ( 2011 ) . Cross-domain feature selec - tion language identification . IJCNLP-11 , 553 – 561 . Lui , M . Baldwin , T . ( 2012 ) . langid.py : off-the - shelf language identification tool . ACL 2012 , 25 – 30 . Manning , C . D . , Raghavan , P . , Schütze , H . ( 2008 ) . - troduction Information Retrieval . Cambridge . Maron , M . E . ( 1961 ) . Automatic indexing : experimental inquiry . Journal ACM ( JACM ) , 8 ( 3 ) , 404 – 417 . McCallum , . Nigam , K . ( 1998 ) . comparison event models naive bayes text classification . AAAI / ICML - 98 Workshop Learning Text Categorization , 41 – 48 . Metsis , V . , Androutsopoulos , . , Paliouras , G . ( 2006 ) . Spam filtering naive bayes-which naive bayes ? . CEAS , 27 – 28 . Minsky , M . ( 1961 ) . Steps toward artificial intelligence . Pro - ceedings IRE , 49 ( 1 ) , 8 – 30 . Mosteller , F . Wallace , D . L . ( 1963 ) . Inference au - thorship problem : comparative study discrimination methods applied authorship disputed federal - ist papers . Journal American Statistical Association , 58 ( 302 ) , 275 – 309 . Mosteller , F . Wallace , D . L . ( 1964 ) . Inference Dis - puted Authorship : Federalist . Springer-Verlag . second edition appeared 1984 Applied Bayesian Classical Inference . Murphy , K . P . ( 2012 ) . Machine learning : probabilistic perspective . MIT press . Noreen , E . W . ( 1989 ) . Computer Intensive Methods Test - ing Hypothesis . Wiley . Pang , B . Lee , L . ( 2008 ) . Opinion mining sentiment analysis . Foundations trends information retrieval , 2 ( 1-2 ) , 1 – 135 . Pang , B . , Lee , L . , Vaithyanathan , S . ( 2002 ) . Thumbs up ? Sentiment classification machine learning tech - niques . EMNLP 2002 , 79 – 86 . Pennebaker , J . W . , Booth , R . J . , Francis , M . E . ( 2007 ) . Linguistic Inquiry Word Count : LIWC 2007 . Austin , TX . Sahami , M . , Dumais , S . T . , Heckerman , D . , Horvitz , E . ( 1998 ) . Bayesian approach filtering junk e-mail . AAAI Workshop Learning Text Categorization , 98 – 105 . Søgaard , . , Johannsen , . , Plank , B . , Hovy , D . , Alonso , H . M . ( 2014 ) . p-value NLP ? . CoNLL-14 . Stamatatos , E . ( 2009 ) . survey modern authorship attri - bution methods . JASIST , 60 ( 3 ) , 538 – 556 . Stone , P . , Dunphry , D . , Smith , M . , Ogilvie , D . ( 1966 ) . General Inquirer : Computer Approach Content Analysis . Cambridge , MA : MIT Press . van Rijsbergen , C . J . ( 1975 ) . Information Retrieval . Butter - worths . Wang , S . Manning , C . D . ( 2012 ) . Baselines bigrams : Simple , good sentiment topic classification . ACL 2012 , 90 – 94 . Wilson , T . , Wiebe , J . , Hoffmann , P . ( 2005 ) . Recogniz - ing contextual polarity phrase-level sentiment analysis . HLT-EMNLP-05 , 347 – 354 . Witten , . H . Frank , E . ( 2005 ) . Data Mining : Practical Machine Learning Tools Techniques ( 2nd Ed . ) . Mor - gan Kaufmann . Exercises 21 Yang , Y . Pedersen , J . ( 1997 ) . comparative study feature selection text categorization . ICML , 412 – 420 .