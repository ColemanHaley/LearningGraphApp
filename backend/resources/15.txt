Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 15 Dependency Parsing focus three previous chapters context-free grammars automatically generating constituent-based representations . present another family grammar formalisms called dependency grammars thatdependencygrammars quite important contemporary speech language processing systems . formalisms , phrasal constituents phrase-structure rules play direct role . , syntactic structure sentence described solely terms words ( lemmas ) sentence associated set directed binary grammatical relations hold among words . following diagram illustrates dependency-style analysis standard graphical method favored dependency-parsing community . ( 15.1 ) prefer morning flight Denver nsubj dobj det nmod nmod case root Relations among words illustrated sentence directed , la - beled arcs heads dependents . call typed dependency structuretypeddependency labels drawn fixed inventory grammatical relations . includes root node explicitly marks root tree , head entire structure . Figure 15.1 shows same dependency analysis tree alongside corre - sponding phrase-structure analysis kind Chapter 12 . Note ab - sence nodes corresponding phrasal constituents lexical categories dependency parse ; internal structure dependency parse consists solely directed relations lexical items sentence . relationships di - rectly encode important information often buried complex phrase - structure parses . example , arguments verb prefer directly linked dependency structure , connection main verb dis - tant phrase-structure tree . Similarly , morning Denver , modifiers flight , linked directly dependency structure . major advantage dependency grammars ability deal lan - guages morphologically rich relatively free word order . Forfree word order example , word order Czech flexible English ; gram - matical object might occur location adverbial . phrase-structure grammar need separate rule possible place parse tree adverbial phrase occur . dependency-based approach just link type representing particular adverbial relation . Thus , depen - dency grammar approach abstracts away word-order information , representing information necessary parse . additional practical motivation dependency-based approach head-dependent relations provide approximation semantic relationship - 2 CHAPTER 15 • DEPENDENCY PARSING prefer flight Denver morningthe S VP NP Nom PP NP Pro Denver P Nom Noun flight Nom Noun morning Det Verb prefer NP Pro Figure 15.1 dependency-style parse alongside corresponding constituent-based analysis prefer morning flight Denver . tween predicates arguments makes directly useful many ap - plications coreference resolution , question answering information ex - traction . Constituent-based approaches parsing provide similar information , often distilled trees via techniques head-finding rules discussed Chapter 12 . following sections , discuss detail inventory relations dependency parsing , well formal basis dependency struc - tures . move discuss dominant families algorithms automatically produce structures . Finally , discuss eval - uate dependency parsers point ways language processing applications . 15.1 Dependency Relations traditional linguistic notion grammatical relation provides basis thegrammaticalrelation binary relations comprise dependency structures . arguments relations consist head dependent . already discussed notion ofhead dependent heads Chapter 12 Chapter 14 context constituent structures . , head word constituent central organizing word larger constituent ( e.g , primary noun noun phrase , verb verb phrase ) . remaining words constituent direct , indirect , dependents head . dependency-based approaches , head-dependent relationship made explicit directly linking heads words immediately dependent , bypass - ing need constituent structures . addition specifying head-dependent pairs , dependency grammars allow further classify kinds grammatical relations , grammatical function , grammaticalfunction 15.1 • DEPENDENCY RELATIONS 3 Clausal Argument Relations Description NSUBJ Nominal subject DOBJ Direct object IOBJ Indirect object CCOMP Clausal complement XCOMP Open clausal complement Nominal Modifier Relations Description NMOD Nominal modifier AMOD Adjectival modifier NUMMOD Numeric modifier APPOS Appositional modifier DET Determiner CASE Prepositions , postpositions case markers Notable Relations Description CONJ Conjunct CC Coordinating conjunction Figure 15.2 Selected dependency relations Universal Dependency set . ( de Marn - effe et al . , 2014 ) terms role dependent plays respect head . Familiar notions subject , direct object indirect object among kind relations mind . English notions strongly correlate , means de - termine , position sentence constituent type somewhat redundant kind information found phrase-structure trees . , flexible languages information encoded directly grammatical rela - tions critical phrase-based constituent syntax provides little help . surprisingly , linguists developed taxonomies relations go well beyond familiar notions subject object . considerable vari - ation theory theory , enough commonality efforts develop computationally useful standard possible . Universal DependenciesUniversalDependencies project ( Nivre et al . , 2016 ) provides inventory dependency relations linguistically motivated , computationally useful , cross-linguistically applicable . Fig . 15.2 shows subset relations effort . Fig . 15.3 provides example sentences illustrating selected relations . motivation relations Universal Dependency scheme beyond scope chapter , core set frequently relations broken two sets : clausal relations describe syntactic roles respect predicate ( often verb ) , modifier relations categorize ways words modify heads . Consider following example sentence : ( 15.2 ) United canceled morning flights Houston nsubj dobj det nmod nmod case root clausal relations NSUBJ DOBJ identify subject direct object predicate cancel , NMOD , DET , CASE relations denote modifiers nouns flights Houston . 4 CHAPTER 15 • DEPENDENCY PARSING Relation Examples head dependent NSUBJ United canceled flight . DOBJ United diverted flight Reno . booked first flight Miami . IOBJ booked flight Miami . NMOD took morning flight . AMOD Book cheapest flight . NUMMOD storm JetBlue canceled 1000 flights . APPOS United , unit UAL , matched fares . DET flight canceled . flight delayed ? CONJ flew Denver drove Steamboat . CC flew Denver drove Steamboat . CASE Book flight Houston . Figure 15.3 Examples core Universal Dependency relations . 15.2 Dependency Formalisms general form , dependency structures discussing simply directed graphs . , structures G = ( V , ) consisting set vertices V , set ordered pairs vertices , refer arcs . part assume set vertices , V , corresponds exactly set words sentence . , might correspond punctuation , dealing morphologically complex languages set vertices might consist stems affixes . set arcs , , captures head - dependent grammatical function relationships elements V . Further constraints dependency structures specific underlying grammatical theory formalism . Among frequent restrictions structures connected , designated root node , acyclic planar . relevance parsing approaches discussed chapter common , computationally-motivated , restriction rooted trees . , dependency treedependencytree directed graph satisfies following constraints : 1 . single designated root node incoming arcs . 2 . exception root node , vertex exactly incoming arc . 3 . unique path root node vertex V . Taken together , constraints ensure word single head , dependency structure connected , single root node follow unique directed path words sentence . 15.2.1 Projectivity notion projectivity imposes additional constraint derived order words input , closely related context-free nature human languages discussed Chapter 12 . arc head dependent projective path head every word lies head dependent sentence . dependency tree projective arcs make up projective . dependency trees seen thus far projective . , , many perfectly valid 15.3 • DEPENDENCY TREEBANKS 5 constructions lead non-projective trees , particularly languages relatively flexible word order . Consider following example . ( 15.3 ) JetBlue canceled flight morning already late nsubj dobj mod det nmod det case mod adv root example , arc flight modifier non-projective path flight intervening words morning . diagram , projectivity ( non-projectivity ) detected way drawing trees . dependency tree projective drawn crossing edges . way link flight dependent crossing arc links morning head . concern projectivity arises two related issues . First , widely English dependency treebanks automatically derived phrase - structure treebanks head-finding rules ( Chapter 12 ) . trees generated fashion guaranteed projective they’re generated context-free grammars . Second , computational limitations widely families parsing algorithms . transition-based approaches discussed Section 15.4 produce projective trees , hence sentences non-projective structures necessarily contain errors . limitation motivations flexible graph-based parsing approach described Section 15.5 . 15.3 Dependency Treebanks constituent-based methods , treebanks play critical role development evaluation dependency parsers . Dependency treebanks created similar approaches discussed Chapter 12 — human annota - tors directly generate dependency structures corpus , automatic parsers provide initial parse annotators hand correct parsers . deterministic process translate existing constituent - based treebanks dependency trees head rules . part , directly annotated dependency treebanks created morphologically rich languages Czech , Hindi Finnish lend - selves dependency grammar approaches , Prague Dependency Treebank ( Bejček et al . , 2013 ) Czech well-known effort . major English dependency treebanks largely extracted existing resources Wall Street Journal sections Penn Treebank ( Marcus et al . , 1993 ) . recent OntoNotes project ( Hovy et al . 2006 , Weischedel et al . 2011 ) extends approach going beyond traditional news text include conversational telephone speech , weblogs , usenet newsgroups , broadcasts , talk shows English , Chinese Arabic . translation process constituent dependency structures two sub - tasks : identifying head-dependent relations structure identifying correct dependency relations relations . first task relies heavily 6 CHAPTER 15 • DEPENDENCY PARSING head rules discussed Chapter 12 first developed lexicalized probabilistic parsers ( Magerman 1994 , Collins 1999 , Collins 2003 ) . Here’s simple effective algorithm Xia Palmer ( 2001 ) . 1 . Mark head child node phrase structure , appropriate head rules . 2 . dependency structure , make head non-head child depend head head-child . phrase-structure parse contains additional information form grammatical relations function tags , case Penn Treebank , tags label edges resulting tree . applied parse tree Fig . 15.4 , algorithm produce dependency structure exam - ple 15.4 . ( 15.4 ) Vinken join board nonexecutive director Nov 29 sbj aux dobj clr tmp nmod case nmod amod num root primary shortcoming extraction methods limited information present original constituent trees . Among impor - tant issues failure integrate morphological information phrase - structure trees , inability easily represent non-projective structures , lack internal structure noun-phrases , reflected generally flat rules treebank grammars . reasons , outside English , dependency treebanks developed directly human annotators . 15.4 Transition-Based Dependency Parsing first approach dependency parsing motivated stack-based approach called shift-reduce parsing originally developed analyzing programming lan-shift-reduceparsing guages ( Aho Ullman , 1972 ) . classic approach simple elegant , em - ploying context-free grammar , stack , list tokens parsed . Input tokens successively shifted onto stack top two elements stack matched against right-hand side rules grammar ; match found matched elements replaced stack ( reduced ) non-terminal left-hand side rule matched . adapting approach dependency parsing , forgo explicit grammar alter reduce operation adding non-terminal parse tree , introduces dependency relation word head . specifically , reduce ac - tion replaced two possible actions : assert head-dependent relation word top stack word below , vice versa . Figure 15.5 illustrates basic operation parser . key element transition-based parsing notion configuration whichconfiguration consists stack , input buffer words , tokens , set relations rep - resenting dependency tree . framework , parsing process consists 15.4 • TRANSITION-BASED DEPENDENCY PARSING 7 S VP VP NP-TMP CD 29 NNP Nov PP-CLR NP NN director JJ nonexecutive DT NP NN board DT VB join MD NP-SBJ NNP Vinken S ( join ) VP ( join ) VP ( join ) NP-TMP ( 29 ) CD 29 NNP Nov PP-CLR ( director ) NP ( director ) NN director JJ nonexecutive DT NP ( board ) NN board DT VB join MD NP-SBJ ( Vinken ) NNP Vinken join 29 Nov director nonexecutiveaas board willVinken Figure 15.4 phrase-structure tree Wall Street Journal component Penn Treebank 3 . sequence transitions space possible configurations . goal process find final configuration words accounted appropriate dependency tree synthesized . implement search , define set transition operators , applied configuration produce new configurations . setup , view operation parser search space configurations sequence transitions leads start state desired goal state . start process create initial configuration stack contains 8 CHAPTER 15 • DEPENDENCY PARSING Dependency Relations wnw1 w2 s2 . . . s1 sn Parser Input buffer Stack Oracle Figure 15.5 Basic transition-based parser . parser examines top two elements stack selects action based consulting oracle examines current configura - tion . ROOT node , word list initialized set words lemmatized tokens sentence , empty set relations created represent parse . final goal state , stack word list empty , set relations represent final parse . standard approach transition-based parsing , operators pro - duce new configurations surprisingly simple correspond intuitive ac - tions might take creating dependency tree examining words single pass input left right ( Covington , 2001 ) : • Assign current word head previously seen word , • Assign previously seen word head current word , • postpone anything current word , adding store later processing . make actions precise , create three transition operators operate top two elements stack : • LEFTARC : Assert head-dependent relation word top stack word directly beneath ; remove lower word stack . • RIGHTARC : Assert head-dependent relation second word stack word top ; remove word top stack ; • SHIFT : Remove word front input buffer push onto stack . particular set operators implements known arc standardarc standard approach transition-based parsing ( Covington 2001 , Nivre 2003 ) . two notable characteristics approach : transition operators assert relations elements top stack , once element assigned head removed stack available further processing . , alternative transition systems demonstrate different parsing behaviors , arc standard approach quite effective simple implement . 15.4 • TRANSITION-BASED DEPENDENCY PARSING 9 assure operators properly need add pre - conditions . First , , definition , ROOT node incoming arcs , add restriction LEFTARC operator ap - plied ROOT second element stack . Second , reduce operators require two elements stack applied . transition opera - tors preconditions , specification transition-based parser quite simple . Fig . 15.6 gives basic algorithm . function DEPENDENCYPARSE ( words ) returns dependency tree state ← { [ root ] , [ words ] , [ ] } ; initial configuration state final t ← ORACLE ( state ) ; choose transition operator apply state ← APPLY ( t , state ) ; apply , creating new state return state Figure 15.6 generic transition-based dependency parser step , parser consults oracle ( come back shortly ) provides correct transition operator current configuration . applies operator current configuration , producing new configuration . process ends words sentence consumed ROOT node element remaining stack . efficiency transition-based parsers apparent algorithm . complexity linear length sentence based single left right pass words sentence . specifically , word first shifted onto stack later reduced . Note unlike dynamic programming search-based approaches dis - cussed Chapters 12 13 , approach straightforward greedy algorithm — oracle provides single choice step parser proceeds choice , options explored , backtracking employed , single parse returned end . Figure 15.7 illustrates operation parser sequence transitions leading parse following example . ( 15.5 ) Book morning flight iobj dobj det nmod root consider state configuration Step 2 , word pushed onto stack . Stack Word List Relations [ root , book , ] [ , morning , flight ] correct operator apply RIGHTARC assigns book head pops stack resulting following configuration . Stack Word List Relations [ root , book ] [ , morning , flight ] ( book → ) 10 CHAPTER 15 • DEPENDENCY PARSING Step Stack Word List Action Relation Added 0 [ root ] [ book , , , morning , flight ] SHIFT 1 [ root , book ] [ , , morning , flight ] SHIFT 2 [ root , book , ] [ , morning , flight ] RIGHTARC ( book → ) 3 [ root , book ] [ , morning , flight ] SHIFT 4 [ root , book , ] [ morning , flight ] SHIFT 5 [ root , book , , morning ] [ flight ] SHIFT 6 [ root , book , , morning , flight ] [ ] LEFTARC ( morning ← flight ) 7 [ root , book , , flight ] [ ] LEFTARC ( ← flight ) 8 [ root , book , flight ] [ ] RIGHTARC ( book → flight ) 9 [ root , book ] [ ] RIGHTARC ( root → book ) 10 [ root ] [ ] Figure 15.7 Trace transition-based parse . several subsequent applications SHIFT LEFTARC operators , con - figuration Step 6 looks like following : Stack Word List Relations [ root , book , , morning , flight ] [ ] ( book → ) , remaining words passed onto stack left apply appropriate reduce operators . current configuration , employ LEFTARC operator resulting following state . Stack Word List Relations [ root , book , , flight ] [ ] ( book → ) ( morning ← flight ) point , parse sentence consists following structure . ( 15.6 ) Book morning flight iobj nmod several important things note examining sequences Figure 15.7 . First , sequence might lead reasonable parse . general , path leads same result , due ambiguity , transition sequences lead different equally valid parses . Second , assuming oracle always provides correct operator point parse — assumption unlikely true practice . result , greedy nature algorithm , incorrect choices lead incorrect parses parser opportunity go back pursue alternative choices . Section 15.4.2 introduce several techniques allow transition-based approaches explore search space fully . Finally , simplicity , illustrated example labels dependency relations . produce labeled trees , parameterize LEFT - ARC RIGHTARC operators dependency labels , LEFTARC ( NSUBJ ) RIGHTARC ( DOBJ ) . equivalent expanding set transition operators original set three set includes LEFTARC RIGHTARC opera - tors relation set dependency relations , plus additional SHIFT operator . , course , makes job oracle difficult larger set operators choose . 15.4 • TRANSITION-BASED DEPENDENCY PARSING 11 15.4.1 Creating Oracle State-of-the-art transition-based systems supervised machine learning methods train classifiers play role oracle . appropriate training data , methods learn function maps configurations transition operators . supervised machine learning methods , need access appro - priate training data need extract features useful characterizing decisions made . source training data representative tree - banks containing dependency trees . features consist many same features encountered Chapter 8 part-of-speech tagging , well Chapter 14 statistical parsing models . Generating Training Data revisit oracle algorithm Fig . 15.6 fully understand learn - ing problem . oracle takes input configuration returns output tran - sition operator . , train classifier , need configurations paired transition operators ( i.e . , LEFTARC , RIGHTARC , SHIFT ) . Unfortunately , treebanks pair entire sentences corresponding trees , directly provide need . generate required training data , employ oracle-based parsing algorithm clever way . supply oracle training sentences parsed corresponding reference parses treebank . produce training instances , simulate operation parser run - ning algorithm relying new training oracle give correct transitiontraining oracle operators successive configuration . works , first review operation parser . begins default initial configuration stack contains ROOT , input list just list words , set relations empty . LEFTARC RIGHTARC operators add relations words top stack set relations accumulated sentence . gold-standard reference parse training sentence , know dependency relations valid sentence . , reference parse guide selection operators parser steps sequence configurations . precise , reference parse configuration , training oracle proceeds follows : • Choose LEFTARC produces correct head-dependent relation reference parse current configuration , • Otherwise , choose RIGHTARC ( 1 ) produces correct head-dependent re - lation reference parse ( 2 ) dependents word top stack already assigned , • Otherwise , choose SHIFT . restriction selecting RIGHTARC operator needed ensure word popped stack , thus lost further processing , dependents assigned . formally , training oracle access following informa - tion : • current configuration stack S set dependency relations Rc • reference parse consisting set vertices V set dependency relations Rp 12 CHAPTER 15 • DEPENDENCY PARSING Step Stack Word List Predicted Action 0 [ root ] [ book , , flight , , houston ] SHIFT 1 [ root , book ] [ , flight , , houston ] SHIFT 2 [ root , book , ] [ flight , , houston ] SHIFT 3 [ root , book , , flight ] [ , houston ] LEFTARC 4 [ root , book , flight ] [ , houston ] SHIFT 5 [ root , book , flight , ] [ houston ] SHIFT 6 [ root , book , flight , , houston ] [ ] LEFTARC 7 [ root , book , flight , houston ] [ ] RIGHTARC 8 [ root , book , flight ] [ ] RIGHTARC 9 [ root , book ] [ ] RIGHTARC 10 [ root ] [ ] Figure 15.8 Generating training items consisting configuration / predicted action pairs simulating parse reference parse . information , oracle chooses transitions follows : LEFTARC ( r ) : ( S1 r S2 ) ∈ Rp RIGHTARC ( r ) : ( S2 r S1 ) ∈ Rp ∀ r ′ , w s.t . ( S1 r ′ w ) ∈ Rp ( S1 r ′ w ) ∈ Rc SHIFT : otherwise walk steps process following example shown Fig . 15.8 . ( 15.7 ) Book flight Houston dobj det nmod case root Step 1 , LEFTARC applicable initial configuration asserts relation , ( root ← book ) , reference answer ; RIGHTARC assert relation contained final answer ( root → book ) , book attached dependents yet , defer , leaving SHIFT possible action . same conditions hold next two steps . step 3 , LEFTARC selected link head . consider situation Step 4 . Stack Word buffer Relations [ root , book , flight ] [ , Houston ] ( ← flight ) , might tempted add dependency relation book flight , present reference parse . prevent later attachment Houston flight removed stack . - tunately , precondition choosing RIGHTARC prevents choice again left SHIFT viable option . remaining choices complete set operators needed example . recap , derive appropriate training instances consisting configuration - transition pairs treebank simulating operation parser con - text reference dependency tree . deterministically record correct parser actions step progress training example , thereby creating training set require . 15.4 • TRANSITION-BASED DEPENDENCY PARSING 13 Features generated appropriate training instances ( configuration-transition pairs ) , need extract useful features configurations train classifiers . features train transition-based systems vary language , genre , kind classifier employed . example , morphosyntactic features case marking subjects direct objects less important depending language processed . , basic features already seen part-of-speech tagging partial parsing proven useful training dependency parsers wide range languages . Word forms , lemmas parts speech powerful features , head , dependency relation head . transition-based parsing framework , features need extracted configurations make up training data . Recall configurations consist three elements : stack , buffer current set relations . principle , property elements represented features usual way training . , avoid sparsity encourage generaliza - tion , best focus learning algorithm useful aspects decision making point parsing process . focus feature extraction transition-based parsing , , top levels stack , words near front buffer , dependency relations already associated elements . combining simple features , word forms parts speech , spe - cific locations configuration , employ notion feature templatefeaturetemplate already encountered sentiment analysis part-of-speech tagging . Feature templates allow automatically generate large numbers specific fea - tures training set . example , consider following feature templates based single positions configuration . 〈 s1 . w , op 〉 , 〈 s2 . w , op 〉 〈 s1 . t , op 〉 , 〈 s2 . t , op 〉 〈 b1 . w , op 〉 , 〈 b1 . t , op 〉 〈 s1 . wt , op 〉 ( 15.8 ) examples , individual features denoted location . property , s denotes stack , b word buffer , r set relations . Individual properties locations include w word forms , l lemmas , t part-of-speech . example , feature corresponding word form top stack denoted s1 . w , part speech tag front buffer b1 . t . combine individual features via concatenation specific features prove useful . example , feature designated s1 . wt represents word form concatenated part speech word top stack . Finally , op stands transition operator training example question ( i.e . , label training instance ) . consider simple set single-element feature templates context following intermediate configuration derived training oracle Example 15.2 . Stack Word buffer Relations [ root , canceled , flights ] [ Houston ] ( canceled → United ) ( flights → morning ) ( flights → ) correct transition SHIFT ( convince yourself 14 CHAPTER 15 • DEPENDENCY PARSING proceeding ) . application set feature templates configuration result following set instantiated features . 〈 s1 . w = flights , op = shift 〉 ( 15.9 ) 〈 s2 . w = canceled , op = shift 〉 〈 s1 . t = NNS , op = shift 〉 〈 s2 . t = VBD , op = shift 〉 〈 b1 . w = , op = shift 〉 〈 b1 . t = , op = shift 〉 〈 s1 . wt = flightsNNS , op = shift 〉 left right arc transitions operate top two elements stack , features combine properties positions even useful . example , feature like s1 . t ◦ s2 . t concatenates part speech tag word top stack tag word beneath . 〈 s1 . t ◦ s2 . t = NNSVBD , op = shift 〉 ( 15.10 ) surprisingly , two properties useful three even better . Figure 15.9 gives baseline set feature templates employed ( Zhang Clark 2008 , Huang Sagae 2010 , Zhang Nivre 2011 ) . Note features make dynamic features — features head words dependency relations predicted earlier steps parsing process , opposed features derived static properties input . Source Feature templates word s1 . w s1 . t s1 . wt s2 . w s2 . t s2 . wt b1 . w b1 . w b0 . wt Two word s1 . w ◦ s2 . w s1 . t ◦ s2 . t s1 . t ◦ b1 . w s1 . t ◦ s2 . wt s1 . w ◦ s2 . w ◦ s2 . t s1 . w ◦ s1 . t ◦ s2 . t s1 . w ◦ s1 . t ◦ s2 . t s1 . w ◦ s1 . t Figure 15.9 Standard feature templates training transition-based dependency parsers . template specifications sn refers location stack , bn refers location word buffer , w refers wordform input , t refers part speech input . Learning years , dominant approaches training transition-based dependency parsers multinomial logistic regression support vector machines , make effective large numbers sparse features kind described last section . recently , neural network , deep learning , approaches kind described Chapter 8 applied successfully transition-based parsing ( Chen Manning , 2014 ) . approaches eliminate need complex , hand-crafted features particularly effective - coming data sparsity issues normally associated training transition-based parsers . 15.4 • TRANSITION-BASED DEPENDENCY PARSING 15 15.4.2 Advanced Methods Transition-Based Parsing basic transition-based approach elaborated number ways - prove performance addressing obvious flaws approach . Alternative Transition Systems arc-standard transition system described many possible sys - tems . frequently alternative arc eager transition system . arc eagerarc eager approach gets name ability assert rightward relations sooner arc standard approach . , revisit arc standard trace Example 15.7 , repeated . Book flight Houston dobj det nmod case root Consider dependency relation book flight analysis . shown Fig . 15.8 , arc-standard approach assert relation Step 8 , despite fact book flight first come together stack earlier Step 4 . reason relation captured point due presence post-nominal modifier Houston . arc-standard approach , depen - dents removed stack soon assigned heads . flight assigned book head Step 4 , longer available serve head Houston . delay cause issues example , general longer word wait get assigned head opportunities something go awry . arc-eager system addresses issue allowing words attached heads early possible , subsequent words dependent seen . accomplished minor changes LEFTARC RIGHTARC operators addition new REDUCE operator . • LEFTARC : Assert head-dependent relation word front input buffer word top stack ; pop stack . • RIGHTARC : Assert head-dependent relation word top stack word front input buffer ; shift word front input buffer stack . • SHIFT : Remove word front input buffer push onto stack . • REDUCE : Pop stack . LEFTARC RIGHTARC operators applied top stack front input buffer , top two elements stack arc-standard approach . RIGHTARC operator moves dependent stack buffer rather removing , thus making available serve head following words . new REDUCE operator removes top element stack . Together changes permit word eagerly assigned head still allow serve head later dependents . trace shown Fig . 15.10 illustrates new decision sequence example . addition demonstrating arc-eager transition system , example demon - strates power flexibility overall transition-based approach . able swap new transition system make changes 16 CHAPTER 15 • DEPENDENCY PARSING Step Stack Word List Action Relation Added 0 [ root ] [ book , , flight , , houston ] RIGHTARC ( root → book ) 1 [ root , book ] [ , flight , , houston ] SHIFT 2 [ root , book , ] [ flight , , houston ] LEFTARC ( ← flight ) 3 [ root , book ] [ flight , , houston ] RIGHTARC ( book → flight ) 4 [ root , book , flight ] [ , houston ] SHIFT 5 [ root , book , flight , ] [ houston ] LEFTARC ( ← houston ) 6 [ root , book , flight ] [ houston ] RIGHTARC ( flight → houston ) 7 [ root , book , flight , houston ] [ ] REDUCE 8 [ root , book , flight ] [ ] REDUCE 9 [ root , book ] [ ] REDUCE 10 [ root ] [ ] Figure 15.10 processing trace Book flight Houston arc-eager transition operators . underlying parsing algorithm . flexibility led development di - verse set transition systems address different aspects syntax semantics including : assigning part speech tags ( Choi Palmer , 2011a ) , allowing generation non-projective dependency structures ( Nivre , 2009 ) , assigning seman - tic roles ( Choi Palmer , 2011b ) , parsing texts containing multiple languages ( Bhat et al . , 2017 ) . Beam Search computational efficiency transition-based approach discussed earlier de - rives fact makes single pass sentence , greedily making decisions considering alternatives . course , source greatest weakness – once decision made undone , even face overwhelming evidence arriving later sentence . Another approach systematically explore alternative decision sequences , selecting best among alternatives . key problem search manage large number potential sequences . Beam search accomplishes combining breadth-firstBeam search search strategy heuristic filter prunes search frontier stay fixed-size beam width . beam width applying beam search transition-based parsing , elaborate al - gorithm Fig . 15.6 . choosing single best transition operator iteration , apply applicable operators state agenda score resulting configurations . add new configura - tions frontier , subject constraint room beam . long size agenda specified beam width , add new configurations agenda . Once agenda reaches limit , add new configurations better worst configuration agenda ( removing worst element stay limit ) . Finally , insure retrieve best possible state agenda , loop continues long non-final states agenda . beam search approach requires elaborate notion scoring greedy algorithm . , assumed classifier trained supervised machine learning serve oracle , selecting best transition operator based features extracted current configuration . Regardless specific learning approach , choice viewed assigning score possible transitions picking best . T̂ ( c ) = argmaxScore ( t , c ) 15.5 • GRAPH-BASED DEPENDENCY PARSING 17 beam search searching space decision se - quences , makes sense base score configuration entire history . specifically , define score new configuration score predecessor plus score operator produce . ConfigScore ( c0 ) = 0.0 ConfigScore ( ci ) = ConfigScore ( ci − 1 ) + Score ( ti , ci − 1 ) score filtering agenda selecting final answer . new beam search version transition-based parsing Fig . 15.11 . function DEPENDENCYBEAMPARSE ( words , width ) returns dependency tree state ← { [ root ] , [ words ] , [ ] , 0.0 } ; initial configuration agenda ← 〈 state 〉 ; initial agenda agenda contains non-final states newagenda ← 〈 〉 state ∈ agenda { t | t ∈ VALIDOPERATORS ( state ) } child ← APPLY ( t , state ) newagenda ← ADDTOBEAM ( child , newagenda , width ) agenda ← newagenda return BESTOF ( agenda ) function ADDTOBEAM ( state , agenda , width ) returns updated agenda LENGTH ( agenda ) < width agenda ← INSERT ( state , agenda ) else SCORE ( state ) > SCORE ( WORSTOF ( agenda ) ) agenda ← REMOVE ( WORSTOF ( agenda ) ) agenda ← INSERT ( state , agenda ) return agenda Figure 15.11 Beam search applied transition-based dependency parsing . 15.5 Graph-Based Dependency Parsing Graph-based approaches dependency parsing search space possible trees sentence tree ( trees ) maximize score . methods encode search space directed graphs employ methods drawn graph theory search space optimal solutions . formally , sentence S looking best dependency tree Gs , space possible trees sentence , maximizes score . T̂ ( S ) = argmax t ∈ GS score ( t , S ) probabilistic approaches context-free parsing discussed Chap - ter 14 , overall score tree viewed function scores parts tree . focus section edge-factored approaches theedge-factored 18 CHAPTER 15 • DEPENDENCY PARSING score tree based scores edges comprise tree . score ( t , S ) = ∑ e ∈ t score ( e ) several motivations graph-based methods . First , unlike transition-based approaches , methods capable producing non-projective trees . Although projectivity significant issue English , definitely problem many world’s languages . second motivation concerns parsing accuracy , particularly respect longer dependencies . Empirically , transition - based methods high accuracy shorter dependency relations accuracy de - clines significantly distance head dependent increases ( Mc - Donald Nivre , 2011 ) . Graph-based methods avoid difficulty scoring entire trees , rather relying greedy local decisions . following section examines widely-studied approach based maximum spanning tree ( MST ) algorithm weighted , directed graphs . thenmaximumspanning tree discuss features typically score trees , well methods train scoring models . 15.5.1 Parsing approach described efficient greedy algorithm search optimal spanning trees directed graphs . input sentence , begins constructing fully-connected , weighted , directed graph vertices input words directed edges represent possible head-dependent assignments . addi - tional ROOT node included outgoing edges directed vertices . weights graph reflect score possible head-dependent relation provided model generated training data . weights , maxi - mum spanning tree graph emanating ROOT represents preferred dependency parse sentence . directed graph example Book flight shown Fig . 15.12 , maximum spanning tree corresponding desired parse shown blue . ease exposition , focus unlabeled dependency parsing . Graph-based approaches labeled parsing discussed Section 15.5.3 . describing algorithm useful consider two intuitions di - rected graphs spanning trees . first intuition begins fact every vertex spanning tree exactly incoming edge . follows every connected component spanning tree incoming edge . second intuition absolute values edge scores critical determining maximum spanning tree . , relative weights edges entering vertex matters . subtract constant amount edge entering vertex impact choice maximum spanning tree every possible spanning tree decrease exactly same amount . first step algorithm itself quite straightforward . vertex graph , incoming edge ( representing possible head assignment ) highest score chosen . resulting set edges produces spanning tree . formally , original fully-connected graph G = ( V , E ) , subgraph T = ( V , F ) spanning tree cycles vertex ( root ) exactly edge entering . greedy selection process produces tree best possible . 15.5 • GRAPH-BASED DEPENDENCY PARSING 19 root Book flight 12 4 4 5 6 8 7 5 7 Figure 15.12 Initial rooted , directed graph Book flight . Unfortunately , approach always lead tree set edges selected contain cycles . Fortunately , yet another case multiple discovery , straightforward way eliminate cycles generated greedy se - lection phase . Chu Liu ( 1965 ) Edmonds ( 1967 ) independently developed approach begins greedy selection follows elegant recursive cleanup phase eliminates cycles . cleanup phase begins adjusting weights graph subtracting score maximum edge entering vertex score edges entering vertex . intuitions mentioned earlier come play . scaled values edges weight edges cycle bearing weight possible spanning trees . Subtracting value edge maximum weight edge entering vertex results weight zero edges selected greedy selection phase , including edges involved cycle . adjusted weights , algorithm creates new graph selecting cycle collapsing single new node . Edges enter leave cycle altered enter leave newly collapsed node . Edges touch cycle included edges cycle dropped . , knew maximum spanning tree new graph , need eliminate cycle . edge maximum spanning tree di - rected vertex representing collapsed cycle tells edge delete eliminate cycle . find maximum spanning tree new graph ? recursively apply algorithm new graph . result spanning tree graph cycle . recursions continue long cycles encountered . recursion completes expand collapsed vertex , restoring vertices edges cycle exception single edge deleted . Putting together , maximum spanning tree algorithm consists greedy edge selection , re-scoring edge costs recursive cleanup phase needed . full algorithm shown Fig . 15.13 . Fig . 15.14 steps algorithm Book flight example . first row figure illustrates greedy edge selection edges chosen shown blue ( corresponding set F algorithm ) . results cycle 20 CHAPTER 15 • DEPENDENCY PARSING function MAXSPANNINGTREE ( G =( V , E ) , root , score ) returns spanning tree F ← [ ] T ’ ← [ ] score ’ ← [ ] v ∈ V bestInEdge ← argmaxe =( u , v ) ∈ E score [ e ] F ← F ∪ bestInEdge e =( u , v ) ∈ E score ’ [ e ] ← score [ e ] − score [ bestInEdge ] T =( V , F ) spanning tree return else C ← cycle F G ’ ← CONTRACT ( G , C ) T ’ ← MAXSPANNINGTREE ( G ’ , root , score ’ ) T ← EXPAND ( T ’ , C ) return T function CONTRACT ( G , C ) returns contracted graph function EXPAND ( T , C ) returns expanded graph Figure 15.13 Chu-Liu Edmonds algorithm finding maximum spanning tree weighted directed graph . flight . scaled weights maximum value entering node shown graph right . Collapsing cycle flight single node ( labelled tf ) recursing newly scaled costs shown second row . greedy selec - tion step recursion yields spanning tree links root book , well edge links book contracted node . Expanding contracted node , edge corresponds edge book flight original graph . turn tells edge drop eliminate cycle arbitrary directed graphs , version CLE algorithm runs O ( mn ) time , m number edges n number nodes . par - ticular application algorithm begins constructing fully connected graph m = n2 yielding running time O ( n3 ) . Gabow et al . ( 1986 ) present effi - cient implementation running time O ( m + nlogn ) . 15.5.2 Features Training sentence , S , candidate tree , T , edge-factored parsing models reduce score tree sum scores edges comprise tree . score ( S , T ) = ∑ e ∈ T score ( S , e ) edge score , turn , reduced weighted sum features extracted . score ( S , e ) = N ∑ = 1 wi fi ( S , e ) 15.5 • GRAPH-BASED DEPENDENCY PARSING 21 root Book tf root Book flight 0 - 3 - 4 - 7 - 1 - 6 - 2 root Book12 7 flight 8 - 4 - 3 0 - 2 - 6 - 1 - 7 0 0 root Book0 tf - 1 0 - 3 - 4 - 7 - 1 - 6 - 2 root Book12 7 flight 8 12 4 4 5 6 8 7 5 7 Deleted cycle Figure 15.14 Chu-Liu-Edmonds graph-based example Book flight succinctly . score ( S , e ) = w · f formulation , faced two problems training parser : identifying relevant features finding weights score features . features train edge-factored models mirror training transition-based parsers ( shown Fig . 15.9 ) . hardly surprising cases trying capture information relationship heads dependents context single relation . summarize earlier discussion , commonly features include : • Wordforms , lemmas , parts speech headword dependent . • Corresponding features derived contexts , words . • Word embeddings . • dependency relation itself . • direction relation ( right left ) . • distance head dependent . transition-based approaches , pre-selected combinations features often well . set features , next problem learn set weights correspond - ing . Unlike many learning problems discussed earlier chapters , 22 CHAPTER 15 • DEPENDENCY PARSING training model associate training items class labels , parser actions . , seek train model assigns higher scores cor - rect trees incorrect ones . effective framework problems like inference-based learning combined perceptron learning rule . thisinference-basedlearning framework , parse sentence ( i.e , perform inference ) training set initially random set initial weights . resulting parse matches cor - responding tree training data , nothing weights . Otherwise , find features incorrect parse present reference parse lower weights small amount based learning rate . incrementally sentence training data weights converge . State-of-the-art algorithms multilingual parsing based recurrent neural networks ( RNNs ) ( Zeman et al . 2017 , Dozat et al . 2017 ) . 15.5.3 Advanced Issues Graph-Based Parsing 15.6 Evaluation phrase structure-based parsing , evaluation dependency parsers pro - ceeds measuring well work test-set . obvious metric exact match ( EM ) — many sentences parsed correctly . metric quite pessimistic , sentences marked wrong . measures fine - grained enough guide development process . metrics need sensitive enough tell actual improvements made . reasons , common method evaluating dependency parsers labeled unlabeled attachment accuracy . Labeled attachment refers proper assignment word head correct dependency relation . Unlabeled attachment simply looks correctness assigned head , ignor - ing dependency relation . system output corresponding reference parse , accuracy simply percentage words input assigned correct head correct relation . metrics usually referred labeled attachment score ( LAS ) unlabeled attachment score ( UAS ) . Finally , make label accuracy score ( LS ) , percentage tokens correct labels , ignoring relations coming . example , consider reference parse system parse following example shown Fig . 15.15 . ( 15.11 ) Book flight Houston . system correctly finds 4 6 dependency relations present refer - ence parse receives LAS 2/3 . , 2 incorrect relations found system holds book flight , head-dependent relation reference parse ; system achieves UAS 5/6 . Beyond attachment scores , interested well system performing particular kind dependency relation , example NSUBJ , development corpus . make notions precision recall introduced Chapter 8 , measuring percentage relations labeled NSUBJ system correct ( precision ) , percentage NSUBJ relations present development set fact discovered system ( recall ) . employ confusion matrix keep track often dependency type confused another . 15.7 • SUMMARY 23 Book flight Houston Reference obj iobj det nmod case root Book flight Houston System x-comp nsubj det nmod case root Figure 15.15 Reference system parses Book flight Houston , resulting LAS 2/3 UAS 5/6 . 15.7 Summary chapter introduced concept dependency grammars dependency parsing . Here’s summary main points covered : • dependency-based approaches syntax , structure sentence de - scribed terms set binary relations hold words sentence . Larger notions constituency directly encoded depen - dency analyses . • relations dependency structure capture head-dependent relation - ship among words sentence . • Dependency-based analysis provides information directly useful further language processing tasks including information extraction , semantic parsing question answering . • Transition-based parsing systems employ greedy stack-based algorithm create dependency structures . • Graph-based methods creating dependency structures based maximum spanning tree methods graph theory . • transition-based graph-based approaches developed super - vised machine learning techniques . • Treebanks provide data needed train systems . Dependency tree - banks created directly human annotators via automatic transfor - mation phrase-structure treebanks . • Evaluation dependency parsers based labeled unlabeled accuracy scores measured against withheld development test corpora . Bibliographical Historical Notes dependency-based approach grammar older relatively re - cent phrase-structure constituency grammars primary focus theoretical computational linguistics years . roots - cient Greek Indian linguistic traditions . Contemporary theories dependency grammar draw heavily work Tesnière ( 1959 ) . influential dependency grammar frameworks include Meaning-Text Theory ( MTT ) ( Mel’c̆uk , 1988 ) , Word Grammar ( Hudson , 1984 ) , Functional Generative Description ( FDG ) ( Sgall et al . , 1986 ) . frameworks differ number dimensions - cluding degree manner deal morphological , syntactic , 24 CHAPTER 15 • DEPENDENCY PARSING semantic pragmatic factors , multiple layers representation , set relations categorize dependency relations . Automatic parsing dependency grammars first introduced compu - tational linguistics early work machine translation RAND Corporation led David Hays . work dependency parsing closely paralleled work constituent parsing made explicit grammars guide parsing process . early period , computational work dependency parsing remained inter - mittent following decades . Notable implementations dependency parsers English period include Link Grammar ( Sleator Temperley , 1993 ) , Constraint Grammar ( Karlsson et al . , 1995 ) , MINIPAR ( Lin , 2003 ) . Dependency parsing saw major resurgence late 1990 ’ s appear - ance large dependency-based treebanks associated advent data driven approaches described chapter . Eisner ( 1996 ) developed efficient dynamic programming approach dependency parsing based bilexical grammars derived Penn Treebank . Covington ( 2001 ) introduced deterministic word word approach underlying current transition-based approaches . Yamada Mat - sumoto ( 2003 ) Kudo Matsumoto ( 2002 ) introduced shift-reduce paradigm supervised machine learning form support vector machines dependency parsing . Nivre ( 2003 ) defined modern , deterministic , transition-based approach de - pendency parsing . Subsequent work Nivre colleagues formalized - alyzed performance numerous transition systems , training methods , meth - ods dealing non-projective language Nivre Scholz 2004 , Nivre 2006 , Nivre Nilsson 2005 , Nivre et al . 2007 , Nivre 2007 . graph-based maximum spanning tree approach dependency parsing introduced McDonald et al . 2005 , McDonald et al . 2005 . earliest source data training evaluating dependency English parsers came WSJ Penn Treebank ( Marcus et al . , 1993 ) described Chapter 12 . head-finding rules developed probabilistic parsing facili - tated automatic extraction dependency parses phrase-based ones ( Xia Palmer , 2001 ) . long-running Prague Dependency Treebank project ( Hajič , 1998 ) significant effort directly annotate corpus multiple layers morphological , syntactic semantic information . current PDT 3.0 contains 1.5 M tokens ( Bejček et al . , 2013 ) . Universal Dependencies ( UD ) ( Nivre et al . , 2016 ) project directed creating consistent framework dependency treebank annotation languages goal advancing parser development world’s languages . auspices effort , treebanks 30 languages annotated made available single consistent format . UD annotation scheme evolved several distinct efforts including Stanford dependencies ( de Marneffe et al . 2006 , de Marneffe Manning 2008 , de Marneffe et al . 2014 ) , Google’s universal part - of-speech tags ( Petrov et al . , 2012 ) , Interset interlingua morphosyntactic tagsets ( Zeman , 2008 ) . Driven part UD framework , dependency treebanks significant size quality available 30 languages ( Nivre et al . , 2016 ) . Conference Natural Language Learning ( CoNLL ) conducted - fluential series shared tasks related dependency parsing years ( Buch - holz Marsi 2006 , Nilsson et al . 2007 , Surdeanu et al . 2008 , Hajič et al . 2009 ) . recent evaluations focused parser robustness respect morpho - EXERCISES 25 logically rich languages ( Seddah et al . , 2013 ) , non-canonical language forms social media , texts , spoken language ( Petrov McDonald , 2012 ) . Choi et al . ( 2015 ) presents performance analysis 10 dependency parsers range metrics , well DEPENDABLE , robust parser evaluation tool . Exercises 26 Chapter 15 • Dependency Parsing Aho , . V . Ullman , J . D . ( 1972 ) . Theory Parsing , Translation , Compiling , Vol . 1 . Prentice Hall . Bejček , E . , Hajičová , E . , Hajič , J . , Jı́nová , P . , Kettnerová , V . , Kolářová , V . , Mikulová , M . , Mı́rovský , J . , Nedoluzhko , . , Panevová , J . , Poláková , L . , Ševčı́ková , M . , Štěpánek , J . , Zikánová , Š . ( 2013 ) . Prague dependency treebank 3.0 . Tech . rep . , Institute Formal Applied Linguis - tics , Charles University Prague . LINDAT / CLARIN dig - ital library Institute Formal Applied Linguistics , Charles University Prague . Bhat , . , Bhat , R . . , Shrivastava , M . , Sharma , D . ( 2017 ) . Joining hands : Exploiting monolingual treebanks parsing code-mixing data . EACL-17 , 324 – 330 . Buchholz , S . Marsi , E . ( 2006 ) . Conll-x shared task multilingual dependency parsing . CoNLL-06 , 149 – 164 . Chen , D . Manning , C . D . ( 2014 ) . fast accurate de - pendency parser neural networks . . EMNLP 2014 , 740 – 750 . Choi , J . D . Palmer , M . ( 2011a ) . Getting transition-based dependency parsing . ACL 2011 , 687 – 692 . Choi , J . D . Palmer , M . ( 2011b ) . Transition-based se - mantic role labeling predicate argument clustering . Proceedings ACL 2011 Workshop Relational Models Semantics , 37 – 45 . Choi , J . D . , Tetreault , J . , Stent , . ( 2015 ) . depends : Dependency parser comparison web-based evalua - tion tool . ACL 2015 , 26 – 31 . Chu , Y . - J . Liu , T . - H . ( 1965 ) . shortest arbores - cence directed graph . Science Sinica , 14 , 1396 – 1400 . Collins , M . ( 1999 ) . Head-Driven Statistical Models Nat - ural Language Parsing . Ph . D . thesis , University Penn - sylvania , Philadelphia . Collins , M . ( 2003 ) . Head-driven statistical models nat - ural language parsing . Computational Linguistics , 29 ( 4 ) , 589 – 637 . Covington , M . ( 2001 ) . fundamental algorithm depen - dency parsing . Proceedings 39th Annual ACM Southeast Conference , 95 – 102 . de Marneffe , M . - C . , Dozat , T . , Silveira , N . , Haverinen , K . , Ginter , F . , Nivre , J . , Manning , C . D . ( 2014 ) . Univer - sal Stanford dependencies : cross-linguistic typology . . LREC , Vol . 14 , 4585 – 92 . de Marneffe , M . - C . , MacCartney , B . , Manning , C . D . ( 2006 ) . Generating typed dependency parses phrase structure parses . LREC-06 . de Marneffe , M . - C . Manning , C . D . ( 2008 ) . stanford typed dependencies representation . Coling 2008 : Pro - ceedings workshop Cross-Framework Cross - Domain Parser Evaluation , 1 – 8 . Dozat , T . , Qi , P . , Manning , C . D . ( 2017 ) . Stanford’s graph-based neural dependency parser CoNLL 2017 shared task . Proceedings CoNLL 2017 Shared Task , 20 – 30 . Edmonds , J . ( 1967 ) . Optimum branchings . Journal Re - search National Bureau Standards B , 71 ( 4 ) , 233 – 240 . Eisner , J . ( 1996 ) . Three new probabilistic models depen - dency parsing : exploration . COLING-96 , 340 – 345 . Gabow , H . N . , Galil , Z . , Spencer , T . , Tarjan , R . E . ( 1986 ) . Efficient algorithms finding minimum spanning trees undirected directed graphs . Combinatorica , 6 ( 2 ) , 109 – 122 . Hajič , J . ( 1998 ) . Building Syntactically Annotated Corpus : Prague Dependency Treebank , 106 – 132 . Karolinum . Hajič , J . , Ciaramita , M . , Johansson , R . , Kawahara , D . , Martı́ , M . . , Màrquez , L . , Meyers , . , Nivre , J . , Padó , S . , Štěpánek , J . , Stranǎḱ , P . , Surdeanu , M . , Xue , N . , Zhang , Y . ( 2009 ) . conll-2009 shared task : Syntac - tic semantic dependencies multiple languages . CoNLL-09 , 1 – 18 . Hovy , E . H . , Marcus , M . P . , Palmer , M . , Ramshaw , L . . , Weischedel , R . ( 2006 ) . Ontonotes : 90 % solution . HLT-NAACL-06 . Huang , L . Sagae , K . ( 2010 ) . Dynamic programming linear-time incremental parsing . ACL 2010 , 1077 – 1086 . Hudson , R . . ( 1984 ) . Word Grammar . Blackwell . Karlsson , F . , Voutilainen , . , Heikkilä , J . , Anttila , . ( Eds . ) . ( 1995 ) . Constraint Grammar : Language - Independent System Parsing Unrestricted Text . Mouton de Gruyter . Kudo , T . Matsumoto , Y . ( 2002 ) . Japanese dependency analysis cascaded chunking . CoNLL-02 , 63 – 69 . Lin , D . ( 2003 ) . Dependency-based evaluation minipar . Workshop Evaluation Parsing Systems . Magerman , D . M . ( 1994 ) . Natural Language Parsing Sta - tistical Pattern Recognition . Ph . D . thesis , University Pennsylvania . Marcus , M . P . , Santorini , B . , Marcinkiewicz , M . . ( 1993 ) . Building large annotated corpus English : Penn treebank . Computational Linguistics , 19 ( 2 ) , 313 – 330 . McDonald , R . , Crammer , K . , Pereira , F . C . N . ( 2005 ) . Online large-margin training dependency parsers . ACL-05 , 91 – 98 . McDonald , R . Nivre , J . ( 2011 ) . Analyzing integrat - ing dependency parsers . Computational Linguistics , 37 ( 1 ) , 197 – 230 . McDonald , R . , Pereira , F . C . N . , Ribarov , K . , Hajič , J . ( 2005 ) . Non-projective dependency parsing spanning tree algorithms . HLT-EMNLP-05 . Mel’c̆uk , . . ( 1988 ) . Dependency Syntax : Theory Practice . State University New York Press . Nilsson , J . , Riedel , S . , Yuret , D . ( 2007 ) . conll 2007 shared task dependency parsing . Proceedings CoNLL shared task session EMNLP-CoNLL , 915 – 932 . sn . Nivre , J . ( 2007 ) . Incremental non-projective dependency parsing . NAACL-HLT 07 . Nivre , J . ( 2003 ) . efficient algorithm projective de - pendency parsing . Proceedings 8th International Workshop Parsing Technologies ( IWPT . Nivre , J . ( 2006 ) . Inductive Dependency Parsing . Springer . Nivre , J . ( 2009 ) . Non-projective dependency parsing ex - pected linear time . ACL IJCNLP 2009 , 351 – 359 . Exercises 27 Nivre , J . , de Marneffe , M . - C . , Ginter , F . , Goldberg , Y . , Hajič , J . , Manning , C . D . , McDonald , R . , Petrov , S . , Pyysalo , S . , Silveira , N . , Tsarfaty , R . , Zeman , D . ( 2016 ) . Universal Dependencies v1 : multilingual treebank collection . LREC-16 . Nivre , J . , Hall , J . , Nilsson , J . , Chanev , . , Eryigit , G . , Kübler , S . , Marinov , S . , Marsi , E . ( 2007 ) . Malt - parser : language-independent system data-driven de - pendency parsing . Natural Language Engineering , 13 ( 02 ) , 95 – 135 . Nivre , J . Nilsson , J . ( 2005 ) . Pseudo-projective depen - dency parsing . ACL-05 , 99 – 106 . Nivre , J . Scholz , M . ( 2004 ) . Deterministic dependency parsing english text . COLING-04 , p . 64 . Petrov , S . , Das , D . , McDonald , R . ( 2012 ) . universal part-of-speech tagset . LREC-12 . Petrov , S . McDonald , R . ( 2012 ) . Overview 2012 shared task parsing web . Notes First Work - shop Syntactic Analysis Non-Canonical Language ( SANCL ) , Vol . 59 . Seddah , D . , Tsarfaty , R . , Kübler , S . , Candito , M . , Choi , J . D . , Farkas , R . , Foster , J . , Goenaga , . , Gojenola , K . , Goldberg , Y . , Green , S . , Habash , N . , Kuhlmann , M . , Maier , W . , Nivre , J . , Przepiórkowski , . , Roth , R . , Seeker , W . , Versley , Y . , Vincze , V . , Woliński , M . , Wróblewska , . , Villemonte de la Clérgerie , E . ( 2013 ) . Overview SPMRL 2013 shared task : cross-framework evaluation parsing mor - phologically rich languages . Proceedings 4th Workshop Statistical Parsing Morphologically-Rich Languages . Sgall , P . , Hajičová , E . , Panevova , J . ( 1986 ) . Mean - ing Sentence Pragmatic Aspects . Reidel . Sleator , D . Temperley , D . ( 1993 ) . Parsing English link grammar . IWPT-93 . Surdeanu , M . , Johansson , R . , Meyers , . , Màrquez , L . , Nivre , J . ( 2008 ) . conll-2008 shared task joint pars - ing syntactic semantic dependencies . CoNLL-08 , 159 – 177 . Tesnière , L . ( 1959 ) . Éléments de Syntaxe Structurale . Li - brairie C . Klincksieck , Paris . Weischedel , R . , Hovy , E . H . , Marcus , M . P . , Palmer , M . , Belvin , R . , Pradhan , S . , Ramshaw , L . . , Xue , N . ( 2011 ) . Ontonotes : large training corpus enhanced processing . Joseph Olive , Caitlin Christianson , J . M . ( Ed . ) , Handbook Natural Language Processing Ma - chine Translation : DARPA Global Automatic Language Exploitation , 54 – 63 . Springer . Xia , F . Palmer , M . ( 2001 ) . Converting dependency struc - tures phrase structures . HLT-01 , 1 – 5 . Yamada , H . Matsumoto , Y . ( 2003 ) . Statistical depen - dency analysis support vector machines . Noord , G . V . ( Ed . ) , IWPT-03 , 195 – 206 . Zeman , D . ( 2008 ) . Reusable tagset conversion tagset drivers . . LREC-08 . Zeman , D . , Popel , M . , Straka , M . , Hajič , J . , Nivre , J . , Gin - ter , F . , Luotolahti , J . , Pyysalo , S . , Petrov , S . , Potthast , M . , Tyers , F . M . , Badmaeva , E . , Gokirmak , M . , Nedoluzhko , . , Cinková , S . , Hajic , Jr . , J . , Hlavácová , J . , Kettnerová , V . , Uresová , Z . , Kanerva , J . , Ojala , S . , Missilä , . , Man - ning , C . D . , Schuster , S . , Reddy , S . , Taji , D . , Habash , N . , Leung , H . , de Marneffe , M . - C . , Sanguinetti , M . , Simi , M . , Kanayama , H . , de Paiva , V . , Droganova , K . , Alonso , H . M . , Çöltekin , Ç . , Sulubacak , U . , Uszkoreit , H . , Macke - tanz , V . , Burchardt , . , Harris , K . , Marheinecke , K . , Rehm , G . , Kayadelen , T . , Attia , M . , El-Kahky , . , Yu , Z . , Pitler , E . , Lertpradit , S . , Mandl , M . , Kirchner , J . , Alcalde , H . F . , Strnadová , J . , Banerjee , E . , Manurung , R . , Stella , . , Shi - mada , . , Kwak , S . , Mendonça , G . , Lando , T . , Nitisaroj , R . , Li , J . ( 2017 ) . Conll 2017 shared task : Multilin - gual parsing raw text universal dependencies . Proceedings CoNLL 2017 Shared Task : Multilingual Parsing Raw Text Universal Dependencies , Van - couver , Canada , August 3-4 , 2017 , 1 – 19 . Zhang , Y . Clark , S . ( 2008 ) . tale two parsers : investi - gating combining graph-based transition-based de - pendency parsing beam-search . EMNLP-08 , 562 – 571 . Zhang , Y . Nivre , J . ( 2011 ) . Transition-based dependency parsing rich non-local features . ACL 2011 , 188 – 193 .