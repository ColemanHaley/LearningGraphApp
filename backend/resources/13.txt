Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 13 Constituency Parsing morning shot elephant pajamas . got pajamas know . Groucho Marx , Animal Crackers , 1930 Syntactic parsing task recognizing sentence assigning syntactic structure . chapter focuses structures assigned context-free gram - mars kind described Chapter 12 . based purely declar - ative formalism , context-free grammars specify parse tree sentence computed . need specify algorithms employ grammars efficiently produce correct trees . Parse trees directly useful applications grammar checking word-processing systems : sentence parsed grammatical errors ( least hard read ) . typically , , parse trees serve important intermediate stage representation semantic analysis ( show Chapter 17 ) thus play important role applications like question answering information extraction . example , answer question books written British women authors 1800 ? need know subject sentence books - adjunct British women authors help figure user wants list books ( list authors ) . presenting algorithms , begin discussing ambiguity arises again context problems presents . section fol - lows presents Cocke-Kasami-Younger ( CKY ) algorithm ( Kasami 1965 , Younger 1967 ) , standard dynamic programming approach syntactic parsing . Recall already seen applications dynamic programming algorithms Minimum-Edit-Distance Viterbi algorithms earlier chapters . Finally , discuss partial parsing methods , situations superficial syntac - tic analysis input sufficient . 13.1 Ambiguity Ambiguity perhaps serious problem faced syntactic parsers . Chap - ter 8 introduced notions part-of-speech ambiguity part-of-speech dis - ambiguation . , introduce new kind ambiguity , called structural ambi - guity , arises many commonly rules phrase-structure grammars . structuralambiguity illustrate issues associated structural ambiguity , make new toy grammar L1 , shown Figure 13.1 , consists L0 grammar last chapter augmented few additional rules . Structural ambiguity occurs grammar assign parse sentence . Groucho Marx’s well-known line Captain Spaulding Animal 2 CHAPTER 13 • CONSTITUENCY PARSING Grammar Lexicon S → NP VP Det → | | | S → Aux NP VP Noun → book | flight | meal | money S → VP Verb → book | include | prefer NP → Pronoun Pronoun → | | NP → Proper-Noun Proper-Noun → Houston | NWA NP → Det Nominal Aux → Nominal → Noun Preposition → | | | near | Nominal → Nominal Noun Nominal → Nominal PP VP → Verb VP → Verb NP VP → Verb NP PP VP → Verb PP VP → VP PP PP → Preposition NP Figure 13.1 L1 miniature English grammar lexicon . S VP NP Nominal PP pajamas Nominal Noun elephant Det Verb shot NP Pronoun S VP PP pajamas VP NP Nominal Noun elephant Det Verb shot NP Pronoun Figure 13.2 Two parse trees ambiguous sentence . parse left corresponds humorous reading elephant pajamas , parse right corresponds reading Captain Spaulding shooting pajamas . Crackers ambiguous phrase pajamas part NP headed elephant part verb phrase headed shot . Figure 13.2 illus - trates two analyses Marx’s line rules L1 . Structural ambiguity , appropriately enough , comes many forms . Two common kinds ambiguity attachment ambiguity coordination ambiguity . sentence attachment ambiguity particular constituent at-attachmentambiguity tached parse tree place . Groucho Marx sentence example PP-attachment ambiguity . Various kinds adverbial phrases subject kind ambiguity . instance , following example gerundive-VP flying Paris part gerundive sentence whose subject Eiffel Tower adjunct modifying VP headed saw : ( 13.1 ) saw Eiffel Tower flying Paris . 13.2 • CKY PARSING : DYNAMIC PROGRAMMING APPROACH 3 coordination ambiguity different sets phrases conjoined con-coordinationambiguity junction like . example , phrase old men women bracketed [ old [ men women ] ] , referring old men old women , [ old men ] [ women ] , case men old . ambiguities combine complex ways real sentences . program summarized news , example , need able parse sentences like following Brown corpus : ( 13.2 ) President Kennedy today pushed aside White House business devote time attention working Berlin crisis address deliver tomorrow night American people nationwide television radio . sentence number ambiguities , although semantically unreasonable , requires careful reading . last noun phrase parsed [ nationwide [ television radio ] ] [ [ nationwide television ] radio ] . direct object pushed aside White House business bizarre phrase [ White House business devote time attention working ] ( i.e . , structure like Kennedy affirmed [ intention propose new budget address deficit ] ) . phrase Berlin crisis address deliver tomorrow night American people adjunct modifying verb pushed . PP like nationwide television radio attached higher VPs NPs ( e.g . , modify people night ) . fact many grammatically correct semantically unreason - able parses naturally occurring sentences irksome problem affects parsers . Ultimately , natural language processing systems need able choose single correct parse multitude possible parses process syntactic disambiguation . Effective disambiguation algorithms require statisti-syntacticdisambiguation cal , semantic , contextual knowledge sources vary well integrated parsing algorithms . Fortunately , CKY algorithm presented next section designed effi - ciently handle structural ambiguities kind discussing . Chapter 14 , straightforward ways integrate statistical techniques basic CKY framework produce highly accurate parsers . 13.2 CKY Parsing : Dynamic Programming Approach previous section introduced problems associated ambiguous grammars . Fortunately , dynamic programming provides powerful framework addressing problems , just Minimum Edit Distance , Viterbi , Forward algorithms . Recall dynamic programming approaches systemati - cally fill tables solutions sub-problems . complete , tables contain solution sub-problems needed solve problem whole . case syntactic parsing , sub-problems represent parse trees constituents detected input . dynamic programming advantage arises context-free nature grammar rules — once constituent discovered segment input record presence make available subsequent derivation might require . provides time storage efficiencies subtrees looked up table , reanalyzed . section presents Cocke-Kasami - 4 CHAPTER 13 • CONSTITUENCY PARSING Younger ( CKY ) algorithm , widely dynamic-programming based ap - proach parsing . Related approaches include Earley algorithm ( Earley , 1970 ) chart parsing ( Kaplan 1973 , Kay 1982 ) . 13.2.1 Conversion Chomsky Normal Form begin investigation CKY algorithm examining requirement grammars Chomsky Normal Form ( CNF ) . Recall Chapter 12 grammars CNF restricted rules form → B C → w . , right-hand side rule expand two non - terminals single terminal . Restricting grammar CNF lead loss expressiveness , context-free grammar converted corresponding CNF grammar accepts exactly same set strings original grammar . start process converting generic CFG represented CNF . Assuming dealing ε-free grammar , three situations need address generic grammar : rules mix terminals non-terminals right-hand side , rules single non-terminal right-hand side , rules length right-hand side greater 2 . remedy rules mix terminals non-terminals simply introduce new dummy non-terminal covers original terminal . example , rule infinitive verb phrase INF-VP → VP replaced two rules INF-VP → VP → . Rules single non-terminal right called unit productions . WeUnitproductions eliminate unit productions rewriting right-hand side original rules right-hand side non-unit production rules ultimately lead . formally , ∗ ⇒ B chain unit productions B → γ non-unit production grammar , add → γ rule grammar discard intervening unit productions . demonstrate toy grammar , lead substantial flattening grammar consequent promotion terminals fairly high levels resulting trees . Rules right-hand sides longer 2 normalized introduc - tion new non-terminals spread longer sequences several new rules . Formally , rule like → B C γ replace leftmost pair non-terminals new non-terminal introduce new production result following new rules : → X1 γ X1 → B C case longer right-hand sides , simply iterate process - fending rule replaced rules length 2 . choice replacing leftmost pair non-terminals purely arbitrary ; systematic scheme results binary rules suffice . current grammar , rule S → Aux NP VP replaced two rules S → X1 VP X1 → Aux NP . entire conversion process summarized follows : 1 . Copy conforming rules new grammar unchanged . 13.2 • CKY PARSING : DYNAMIC PROGRAMMING APPROACH 5 L1 Grammar L1 CNF S → NP VP S → NP VP S → Aux NP VP S → X1 VP X1 → Aux NP S → VP S → book | include | prefer S → Verb NP S → X2 PP S → Verb PP S → VP PP NP → Pronoun NP → | | NP → Proper-Noun NP → TWA | Houston NP → Det Nominal NP → Det Nominal Nominal → Noun Nominal → book | flight | meal | money Nominal → Nominal Noun Nominal → Nominal Noun Nominal → Nominal PP Nominal → Nominal PP VP → Verb VP → book | include | prefer VP → Verb NP VP → Verb NP VP → Verb NP PP VP → X2 PP X2 → Verb NP VP → Verb PP VP → Verb PP VP → VP PP VP → VP PP PP → Preposition NP PP → Preposition NP Figure 13.3 L1 Grammar conversion CNF . Note although shown , original lexical entries L1 carry unchanged well . 2 . Convert terminals rules dummy non-terminals . 3 . Convert unit productions . 4 . Make rules binary add new grammar . Figure 13.3 shows results applying entire conversion procedure L1 grammar introduced earlier page 2 . Note figure show original lexical rules ; original lexical rules already CNF , carry unchanged new grammar . Figure 13.3 , , show various places process eliminating unit productions , effect , created new lexical rules . example , original verbs promoted VPs Ss converted grammar . 13.2.2 CKY Recognition grammar CNF , non-terminal node part-of-speech level parse tree exactly two daughters . two-dimensional matrix encode structure entire tree . sentence length n , work upper-triangular portion ( n + 1 ) × ( n + 1 ) matrix . cell [ , j ] matrix contains set non-terminals represent constituents span positions j input . indexing scheme begins 0 , natural think indexes pointing gaps input words ( 0 Book 1 2 flight 3 ) . follows cell represents entire input resides position [ 0 , n ] matrix . non-terminal entry table two daughters parse , fol - lows constituent represented entry [ , j ] , position input , k , split two parts < k < j . 6 CHAPTER 13 • CONSTITUENCY PARSING position k , first constituent [ , k ] lie left entry [ , j ] somewhere row , second entry [ k , j ] lie beneath , column j . make concrete , consider following example completed parse matrix , shown Fig . 13.4 . ( 13.3 ) Book flight Houston . superdiagonal row matrix contains parts speech word input . subsequent diagonals superdiagonal contain constituents cover spans increasing length input . Book flight Houston S , VP , Verb , Nominal , Noun S , VP , X2 S , VP , X2 Det NP NP Nominal , Noun Nominal Prep PP NP , Proper - Noun [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 0,4 ] [ 0,5 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] [ 1,4 ] [ 2,5 ] [ 2,4 ] [ 3,4 ] [ 4,5 ] [ 3,5 ] [ 1,5 ] Figure 13.4 Completed parse table Book flight Houston . setup , CKY recognition consists filling parse table right way . , proceed bottom-up fashion point filling cell [ , j ] , cells containing parts contribute entry ( i.e . , cells left cells below ) already filled . algorithm Fig . 13.5 fills upper-triangular matrix column time working left right , column filled bottom top , right side Fig . 13.4 illustrates . scheme guarantees point time information need ( left , columns left already filled , below filling bottom top ) . mirrors - line parsing filling columns left right corresponds processing word time . function CKY-PARSE ( words , grammar ) returns table j ← 1 LENGTH ( words ) { | → words [ j ] ∈ grammar } table [ j − 1 , j ] ← table [ j − 1 , j ] ∪ ← j − 2 downto 0 k ← + 1 j − 1 { | → BC ∈ grammar B ∈ table [ , k ] C ∈ table [ k , j ] } table [ , j ] ← table [ , j ] ∪ Figure 13.5 CKY algorithm . 13.2 • CKY PARSING : DYNAMIC PROGRAMMING APPROACH 7 . . . . . . [ 0 , n ] [ , + 1 ] [ , + 2 ] [ , j-2 ] [ , j-1 ] [ + 1 , j ] [ + 2 , j ] [ j-1 , j ] [ j-2 , j ] [ , j ] . . . [ 0,1 ] [ n-1 , n ] Figure 13.6 ways fill [ , j ] th cell CKY table . outermost loop algorithm Fig . 13.5 iterates columns , second loop iterates rows , bottom up . purpose innermost loop range places substring spanning j input might split two . k ranges places string split , pairs cells consider move , lockstep , right row down column j . Figure 13.6 illustrates general case filling cell [ , j ] . split , algorithm considers contents two cells combined way sanctioned rule grammar . rule exists , non-terminal left-hand side entered table . Figure 13.7 shows five cells column 5 table filled word Houston read . arrows point two spans add entry table . Note action cell [ 0,5 ] indicates presence three alternative parses input , PP modifies flight , modifies booking , captures second argument original VP → Verb NP PP rule , captured indirectly VP → X2 PP rule . 13.2.3 CKY Parsing algorithm Fig . 13.5 recognizer , parser ; succeed , simply find S cell [ 0 , n ] . turn parser capable returning 8 CHAPTER 13 • CONSTITUENCY PARSING Book flight Houston S , VP , Verb , Nominal , Noun S , VP , X2 Det NP Nominal , Noun Nominal Prep NP , Proper - Noun [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 0,4 ] [ 0,5 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] [ 1,4 ] [ 2,5 ] [ 2,4 ] [ 3,4 ] [ 4,5 ] [ 3,5 ] [ 1,5 ] Book flight Houston S , VP , Verb , Nominal , Noun S , VP , X2 Det NP NP Nominal , Noun Prep PP NP , Proper - Noun [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 0,4 ] [ 0,5 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] [ 1,4 ] [ 2,5 ] [ 2,4 ] [ 3,4 ] [ 4,5 ] [ 3,5 ] [ 1,5 ] Book flight Houston S , VP , Verb , Nominal , Noun S , VP , X2 Det NP NP Nominal , Noun Nominal Prep PP NP , Proper - Noun [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 0,4 ] [ 0,5 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] [ 1,4 ] [ 2,5 ] [ 2,4 ] [ 3,4 ] [ 4,5 ] [ 3,5 ] [ 1,5 ] Book flight Houston S , VP , Verb , Nominal , Noun S , VP , X2 Det NP NP Nominal , Noun Nominal Prep PP NP , Proper - Noun [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 0,4 ] [ 0,5 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] [ 1,4 ] [ 2,5 ] [ 2,4 ] [ 3,4 ] [ 4,5 ] [ 3,5 ] [ 1,5 ] Book flight Houston S , VP , Verb , Nominal , Noun S , VP , X2 Det NP NP Nominal , Noun Nominal Prep PP NP , Proper - Noun [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 0,4 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] [ 1,4 ] [ 2,5 ] [ 2,4 ] [ 3,4 ] [ 4,5 ] [ 3,5 ] [ 1,5 ] S2 , VP S3 S1 , VP , X2 Figure 13.7 Filling cells column 5 reading word Houston . 13.3 • PARTIAL PARSING 9 possible parses input , make two simple changes algorithm : first change augment entries table non-terminal paired pointers table entries derived ( less shown Fig . 13.7 ) , second change permit multiple versions same non-terminal entered table ( again shown Fig . 13.7 ) . changes , completed table contains possible parses input . Re - turning arbitrary single parse consists choosing S cell [ 0 , n ] recursively retrieving component constituents table . course , returning parses input incur considerable cost exponential number parses associated input . cases , returning parses unavoidable exponential cost . Looking forward Chapter 14 , think retrieving best parse input further augmenting table contain probabilities entry . Re - trieving probable parse consists running suitably modified version Viterbi algorithm Chapter 8 completed parse table . 13.2.4 CKY Practice Finally , note restriction CNF pose prob - lem theoretically , pose non-trivial problems practice . Obviously , things stand , parser returning trees consistent grammar friendly syntacticians . addition making grammar devel - opers unhappy , conversion CNF complicate syntax-driven approach semantic analysis . approach getting around problems keep enough information around transform trees back original grammar post-processing step parse . trivial case transformation rules length greater 2 . Simply deleting new dummy non-terminals promoting daughters restores original tree . case unit productions , turns convenient alter ba - sic CKY algorithm handle directly store information needed recover correct trees . Exercise 13.3 asks make change . Many probabilistic parsers presented Chapter 14 CKY algorithm altered just manner . Another solution adopt complex dynamic program - ming solution simply accepts arbitrary CFGs . next section presents approach . 13.3 Partial Parsing Many language processing tasks require complex , complete parse trees inputs . tasks , partial parse , shallow parse , input sentences maypartial parse shallow parse sufficient . example , information extraction systems generally extract possible information text : simply identify classify seg - ments text likely contain valuable information . Similarly , information retrieval systems index texts according subset constituents found . many different approaches partial parsing . make cascades finite state transducers produce tree-like representations . ap - proaches typically produce flatter trees ones discussing 10 CHAPTER 13 • CONSTITUENCY PARSING chapter previous . flatness arises fact finite state trans - ducer approaches generally defer decisions require semantic contex - tual factors , prepositional phrase attachments , coordination ambiguities , nominal compound analyses . Nevertheless , intent produce parse trees link major constituents input . alternative style partial parsing known chunking . Chunking thechunking process identifying classifying flat , non-overlapping segments sen - tence constitute basic non-recursive phrases corresponding major content-word parts-of-speech : noun phrases , verb phrases , adjective phrases , prepositional phrases . task finding base noun phrases text particularly common . chunked texts lack hierarchical structure , simple bracketing notation sufficient denote location type chunks example : ( 13.4 ) [ NP morning flight ] [ PP ] [ NP Denver ] [ VP arrived . ] bracketing notation makes clear two fundamental tasks involved chunking : segmenting ( finding non-overlapping extents chunks ) labeling ( assigning correct tag discovered chunks ) . input words part chunk , particularly tasks like base NP : ( 13.5 ) [ NP morning flight ] [ NP Denver ] arrived . constitutes syntactic base phrase depends application ( phrases come treebank ) . Nevertheless , standard guidelines fol - lowed systems . First foremost , base phrases type recursively contain constituents same type . Eliminating kind recur - sion leaves problem determining boundaries non-recursive phrases . approaches , base phrases include headword phrase , pre-head material constituent , crucially excluding post-head material . Eliminating post-head modifiers obviates need resolve - tachment ambiguities . exclusion lead certain oddities , PPs VPs often consisting solely heads . Thus , earlier example flight Indianapolis Houston NWA reduced following : ( 13.6 ) [ NP flight ] [ PP ] [ NP Indianapolis ] [ PP ] [ NP Houston ] [ PP ] [ NP NWA ] 13.3.1 Machine Learning-Based Approaches Chunking State-of-the-art approaches chunking supervised machine learning train chunker annotated data training set training sequence labeler . common model chunking IOB tagging . IOB tagging introduce tagIOB beginning ( B ) inside ( ) chunk type , tokens outside ( O ) chunk . number tags thus 2n + 1 tags , n number chunk types . IOB tagging represent exactly same information bracketed notation . following example shows bracketing notation ( 13.4 ) page 10 reframed tagging task : ( 13.7 ) B NP morning NP flight NP B PP Denver B NP B VP arrived VP same sentence base-NPs tagged illustrates role O tags . 13.3 • PARTIAL PARSING 11 B_NP I_NP ? flight Denver arrived Classifier DT NN NN NNP Corresponding feature representation , DT , B_NP , morning , NN , I_NP , flight , NN , , , Denver , NNP Label I_NP morning Figure 13.8 sequence model chunking . chunker slides context window sentence , clas - sifying words proceeds . point , classifier attempting label flight , features like words , embeddings , part-of-speech tags previously assigned chunk tags . ( 13.8 ) B NP morning NP flight NP O Denver B NP O arrived . O explicit encoding end chunk IOB tagging ; end chunk implicit transition B B O tag . encoding reflects notion sequentially labeling words , generally easier ( least English ) detect beginning new chunk know chunk ended . annotation efforts expensive time consuming , chunkers usually rely existing treebanks like Penn Treebank ( Chapter 12 ) , extracting syntactic phrases full parse constituents sentence , finding appropriate heads including material left head , ignoring text right . somewhat error-prone relies accuracy head-finding rules described Chapter 12 . training set , sequence model . Figure 13.8 shows illustration simple feature-based model , features like words parts - of-speech 2 word window , chunk tags preceding inputs window . training , training vector consist values 13 features ; two words left decision point , parts-of-speech chunk tags , word tagged part-of-speech , two words follow parts-of speech , correct chunk tag , case , NP . classification , classifier same vector answer assigns appropriate tag tagset . Viterbi decoding commonly . 13.3.2 Chunking-System Evaluations evaluation part-of-speech taggers , evaluation chunkers pro - ceeds comparing chunker output gold-standard answers provided human annotators . , unlike part-of-speech tagging , word-by-word accuracy mea - sures appropriate . , chunkers evaluated according notions 12 CHAPTER 13 • CONSTITUENCY PARSING precision , recall , F-measure saw text classification Chapter 4 , repeated quick refresher . Precision measures percentage system-provided chunks correct . precision Correct means boundaries chunk chunk’s label correct . Precision defined Precision : = Number correct chunks systemTotal number chunks system Recall measures percentage chunks actually present input wererecall correctly identified system . Recall defined Recall : = Number correct chunks systemTotal number actual chunks text F-measure ( van Rijsbergen , 1975 ) provides way combine twoF-measure measures single metric : Fβ = ( β 2 + 1 ) PR β 2P + R β parameter differentially weights importance recall precision , based perhaps needs application . Values β > 1 favor recall , values β < 1 favor precision . β = 1 , precision recall equally balanced ; sometimes called Fβ = 1 just F1 : F1 = 2PR P + R ( 13.9 ) 13.4 Summary two major ideas introduced chapter parsing partial parsing . Here’s summary main points covered ideas : • Structural ambiguity significant problem parsers . Common sources structural ambiguity include PP-attachment , coordination ambiguity , noun-phrase bracketing ambiguity . • Dynamic programming parsing algorithms , CKY , table partial parses efficiently parse ambiguous sentences . • CKY restricts form grammar Chomsky normal form ( CNF ) . • Many practical problems , including information extraction problems , solved full parsing . • Partial parsing chunking methods identifying shallow syntactic constituents text . • State-of-the-art methods partial parsing supervised machine learning techniques . Bibliographical Historical Notes Writing history compilers , Knuth notes : EXERCISES 13 field unusual amount parallel discovery same technique people working independently . Well , perhaps unusual , multiple discovery norm science ( page ? ? ) . certainly enough parallel publication - tory errs side succinctness giving characteristic early mention algorithm ; interested reader Aho Ullman ( 1972 ) . Bottom-up parsing seems first described Yngve ( 1955 ) , gave breadth-first , bottom-up parsing algorithm part illustration ma - chine translation procedure . Top-down approaches parsing translation described ( presumably independently ) least Glennie ( 1960 ) , Irons ( 1961 ) , Kuno Oettinger ( 1963 ) . Dynamic programming parsing , once again , - tory independent discovery . According Martin Kay ( personal communica - tion ) , dynamic programming parser containing roots CKY algorithm first implemented John Cocke 1960 . Later work extended formalized algorithm , well proving time complexity ( Kay 1967 , Younger 1967 , Kasami 1965 ) . related well-formed substring table ( WFST ) seems haveWFST independently proposed Kuno ( 1965 ) data structure stores re - sults previous computations course parse . Based general - ization Cocke’s work , similar data structure independently described Kay ( 1967 ) ( Kay 1973 ) . top-down application dynamic programming parsing described Earley’s Ph . D . dissertation ( Earley 1968 , Earley 1970 ) . Sheil ( 1976 ) showed equivalence WFST Earley algorithm . Norvig ( 1991 ) shows efficiency offered dynamic programming captured language memoization function ( LISP ) simply wrapping memoization operation around simple top-down parser . parsing via cascades finite-state automata common early history parsing ( Harris , 1962 ) , focus shifted full CFG parsing quite soon afterward . Church ( 1980 ) argued return finite-state grammars processing model natural language understanding ; early finite-state parsing models include Ejerhed ( 1988 ) . Abney ( 1991 ) argued important practical role shallow parsing . classic reference parsing algorithms Aho Ullman ( 1972 ) ; although focus book computer languages , algorithms applied natural language . good programming languages textbook Aho et al . ( 1986 ) useful . Exercises 13.1 Implement algorithm convert arbitrary context-free grammars CNF . Apply program L1 grammar . 13.2 Implement CKY algorithm test converted L1 grammar . 13.3 Rewrite CKY algorithm Fig . 13.5 page 6 accept grammars contain unit productions . 13.4 Discuss relative advantages disadvantages partial versus full pars - ing . 14 CHAPTER 13 • CONSTITUENCY PARSING 13.5 Discuss augment parser deal input incorrect , example , containing spelling errors mistakes arising automatic speech recognition . Exercises 15 Abney , S . P . ( 1991 ) . Parsing chunks . Berwick , R . C . , Abney , S . P . , Tenny , C . ( Eds . ) , Principle-Based Parsing : Computation Psycholinguistics , 257 – 278 . Kluwer . Aho , . V . , Sethi , R . , Ullman , J . D . ( 1986 ) . Compilers : Principles , Techniques , Tools . Addison-Wesley . Aho , . V . Ullman , J . D . ( 1972 ) . Theory Parsing , Translation , Compiling , Vol . 1 . Prentice Hall . Church , K . W . ( 1980 ) . Memory Limitations Natural Language Processing Master’s thesis , MIT . Distributed Indiana University Linguistics Club . Earley , J . ( 1968 ) . Efficient Context-Free Parsing Algo - rithm . Ph . D . thesis , Carnegie Mellon University , Pitts - burgh , PA . Earley , J . ( 1970 ) . efficient context-free parsing algorithm . CACM , 6 ( 8 ) , 451 – 455 . Ejerhed , E . . ( 1988 ) . Finding clauses unrestricted text finitary stochastic methods . ANLP 1988 , 219 – 227 . Glennie , . ( 1960 ) . syntax machine construc - tion universal compiler . Tech . rep . . 2 , Contr . NR 049-141 , Carnegie Mellon University ( time Carnegie Institute Technology ) , Pittsburgh , PA . Harris , Z . S . ( 1962 ) . String Analysis Sentence Structure . Mouton , Hague . Irons , E . T . ( 1961 ) . syntax directed compiler ALGOL 60 . CACM , 4 , 51 – 55 . Kaplan , R . M . ( 1973 ) . general syntactic processor . Rustin , R . ( Ed . ) , Natural Language Processing , 193 – 241 . Algorithmics Press . Kasami , T . ( 1965 ) . efficient recognition syntax analysis algorithm context-free languages . Tech . rep . AFCRL-65-758 , Air Force Cambridge Research Labora - tory , Bedford , MA . Kay , M . ( 1967 ) . Experiments powerful parser . Proc . 2eme Conference Internationale sur le Traitement Automatique des Langues . Kay , M . ( 1973 ) . MIND system . Rustin , R . ( Ed . ) , Nat - ural Language Processing , 155 – 188 . Algorithmics Press . Kay , M . ( 1982 ) . Algorithm schemata data structures syntactic processing . Allén , S . ( Ed . ) , Text Processing : Text Analysis Generation , Text Typology Attribu - tion , 327 – 358 . Almqvist Wiksell , Stockholm . Kuno , S . ( 1965 ) . predictive analyzer path elimi - nation technique . CACM , 8 ( 7 ) , 453 – 462 . Kuno , S . Oettinger , . G . ( 1963 ) . Multiple-path syn - tactic analyzer . Popplewell , C . M . ( Ed . ) , Information Processing 1962 : Proceedings IFIP Congress 1962 , 306 – 312 . North-Holland . Reprinted Grosz et al . ( 1986 ) . Norvig , P . ( 1991 ) . Techniques automatic memoization applications context-free parsing . Computational Linguistics , 17 ( 1 ) , 91 – 98 . Sheil , B . . ( 1976 ) . Observations context free parsing . SMIL : Statistical Methods Linguistics , 1 , 71 – 109 . van Rijsbergen , C . J . ( 1975 ) . Information Retrieval . Butter - worths . Yngve , V . H . ( 1955 ) . Syntax problem multiple meaning . Locke , W . N . Booth , . D . ( Eds . ) , Ma - chine Translation Languages , 208 – 226 . MIT Press . Younger , D . H . ( 1967 ) . Recognition parsing context - free languages time n3 . Information Control , 10 , 189 – 208 .