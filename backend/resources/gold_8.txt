10 . Recurrent Neural Networks – Modeling Sequences and Stacks When dealing with language data , it is very common to work with sequences , such as words ( sequences of letters ) , sentences ( sequences of words ) and documents . We saw how feed - forward networks can accommodate arbitrary feature functions over sequences through the use of vector concatenation and vector addition ( CBOW ) . In particular , the CBOW rep - resentations allows to encode arbitrary length sequences as fixed sized vectors . However , the CBOW representation is quite limited , and forces one to disregard the order of fea - tures . The convolutional networks also allow encoding a sequence into a fixed size vector . While representations derived from convolutional networks are an improvement above the CBOW representation as they offer some sensitivity to word order , their order sensitivity is restricted to mostly local patterns , and disregards the order of patterns that are far apart in the sequence . Recurrent neural networks ( RNNs ) ( Elman , 1990 ) allow representing arbitrarily sized structured inputs in a fixed-size vector , while paying attention to the structured properties of the input . 10.1 The RNN Abstraction We use xi : j to denote the sequence of vectors xi , . . . , xj . The RNN abstraction takes as input an ordered list of input vectors x1 , . . . , xn together with an initial state vector s0 , and returns an ordered list of state vectors s1 , . . . , sn , as well as an ordered list of output vectors y1 , . . . , yn . An output vector yi is a function of the corresponding state vector si . The input vectors xi are presented to the RNN in a sequential fashion , and the state vector si and output vector yi represent the state of the RNN after observing the inputs x1 : i . The output vector yi is then used for further prediction . For example , a model for predicting the conditional probability of an event e given the sequence m1 : i can be defined as p ( e = j | x1 : i ) = softmax ( yiW + b ) [ j ] . The RNN model provides a framework for conditioning on the entire history x1 , . . . , xi without resorting to the Markov assumption which is traditionally used for modeling sequences . Indeed , RNN-based language models result in very good perplexity scores when compared to n-gram based models . Mathematically , we have a recursively defined function R that takes as input a state vector si and an input vector xi + 1 , and results in a new state vector si + 1 . An additional function O is used to map a state vector si to an output vector yi . When constructing an RNN , much like when constructing a feed-forward network , one has to specify the dimension of the inputs xi as well as the dimensions of the outputs yi . The dimensions of the states si are a function of the output dimension . 26 26 . While RNN architectures in which the state dimension is independent of the output dimension are possible , the current popular architectures , including the Simple RNN , the LSTM and the GRU do not follow this flexibility . 46 RNN ( s0 , x1 : n ) = s1 : n , y1 : n si = R ( si − 1 , xi ) yi = O ( si ) xi ∈ Rdin , yi ∈ Rdout , si ∈ Rf ( dout ) The functions R and O are the same across the sequence positions , but the RNN keeps track of the states of computation through the state vector that is kept and being passed between invocations of R . Graphically , the RNN has been traditionally presented as in Figure 5 . R , O xi yi sisi − 1 θ Figure 5 : Graphical representation of an RNN ( recursive ) . This presentation follows the recursive definition , and is correct for arbitrary long sequences . However , for a finite sized input sequence ( and all input sequences we deal with are finite ) one can unroll the recursion , resulting in the structure in Figure 6 . s0 R , O x1 y1 R , O x2 y2 s1 R , O x3 y3 s2 θ R , O x4 y4 s3 R , O x5 y5 s4 s5 Figure 6 : Graphical representation of an RNN ( unrolled ) . While not usually shown in the visualization , we include here the parameters θ in order to highlight the fact that the same parameters are shared across all time steps . Different 47 instantiations of R and O will result in different network structures , and will exhibit different properties in terms of their running times and their ability to be trained effectively using gradient-based methods . However , they all adhere to the same abstract interface . We will provide details of concrete instantiations of R and O – the Simple RNN , the LSTM and the GRU – in Section 11 . Before that , let’s consider modeling with the RNN abstraction . First , we note that the value of si is based on the entire input x1 , . . . , xi . For example , by expanding the recursion for i = 4 we get : s4 = R ( s3 , x4 ) = R ( s3 ︷ ︸ ︸ ︷ R ( s2 , x3 ) , x4 ) = R ( R ( s2 ︷ ︸ ︸ ︷ R ( s1 , x2 ) , x3 ) , x4 ) = R ( R ( R ( s1 ︷ ︸ ︸ ︷ R ( s0 , x1 ) , x2 ) , x3 ) , x4 ) Thus , sn ( as well as yn ) could be thought of as encoding the entire input sequence . 27 Is the encoding useful ? This depends on our definition of usefulness . The job of the network training is to set the parameters of R and O such that the state conveys useful information for the task we are tying to solve . 10.2 RNN Training Viewed as in Figure 6 it is easy to see that an unrolled RNN is just a very deep neural network ( or rather , a very large computation graph with somewhat complex nodes ) , in which the same parameters are shared across many parts of the computation . To train an RNN network , then , all we need to do is to create the unrolled computation graph for a given input sequence , add a loss node to the unrolled graph , and then use the backward ( backpropagation ) algorithm to compute the gradients with respect to that loss . This procedure is referred to in the RNN literature as backpropagation through time , or BPTT ( Werbos , 1990 ) . 28 There are various ways in which the supervision signal can be applied . Acceptor One option is to base the supervision signal only on the final output vector , yn . Viewed this way , the RNN is an acceptor . We observe the final state , and then decide 27 . Note that , unless R is specifically designed against this , it is likely that the later elements of the input sequence have stronger effect on sn than earlier ones . 28 . Variants of the BPTT algorithm include unrolling the RNN only for a fixed number of input symbols at each time : first unroll the RNN for inputs x1 : k , resulting in s1 : k . Compute a loss , and backpropagate the error through the network ( k steps back ) . Then , unroll the inputs xk + 1:2k , this time using sk as the initial state , and again backpropagate the error for k steps , and so on . This strategy is based on the observations that for the Simple-RNN variant , the gradients after k steps tend to vanish ( for large enough k ) , and so omitting them is negligible . This procedure allows training of arbitrarily long sequences . For RNN variants such as the LSTM or the GRU that are designed specifically to mitigate the vanishing gradients problem , this fixed size unrolling is less motivated , yet it is still being used , for example when doing language modeling over a book without breaking it into sentences . 48 on an outcome . 29 For example , consider training an RNN to read the characters of a word one by one and then use the final state to predict the part-of-speech of that word ( this is inspired by ( Ling et al . , 2015b ) ) , an RNN that reads in a sentence and , based on the final state decides if it conveys positive or negative sentiment ( this is inspired by ( Wang et al . , 2015b ) ) or an RNN that reads in a sequence of words and decides whether it is a valid noun-phrase . The loss in such cases is defined in terms of a function of yn = O ( sn ) , and the error gradients will backpropagate through the rest of the sequence ( see Figure 7 ) . 30 The loss can take any familiar form – cross entropy , hinge , margin , etc . R , O x1 s0 R , O x2 s1 R , O x3 s2 R , O x4 s3 R , O x5 s4 predict & calc loss y5 loss Figure 7 : Acceptor RNN Training Graph . Encoder Similar to the acceptor case , an encoder supervision uses only the final output vector , yn . However , unlike the acceptor , where a prediction is made solely on the basis of the final vector , here the final vector is treated as an encoding of the information in the sequence , and is used as additional information together with other signals . For example , an extractive document summarization system may first run over the document with an RNN , resulting in a vector yn summarizing the entire document . Then , yn will be used together with other features in order to select the sentences to be included in the summarization . Transducer Another option is to treat the RNN as a transducer , producing an output for each input it reads in . Modeled this way , we can compute a local loss signal Llocal ( ŷi , yi ) for each of the outputs ŷi based on a true label yi . The loss for unrolled sequence will then be : L ( ˆy1 : n , y1 : n ) = ∑ n i = 1 Llocal ( ŷi , yi ) , or using another combination rather than a sum such as an average or a weighted average ( see Figure 8 ) . One example for such a transducer is a sequence tagger , in which we take xi : n to be feature representations for the n words of a sentence , and yi as an input for predicting the tag assignment of word i based on words 1 : i . A CCG super-tagger based on such an architecture provides state-of-the art CCG super-tagging results ( Xu et al . , 2015 ) . A very natural use-case of the transduction setup is for language modeling , in which the sequence of words x1 : i is used to predict a distribution over the i + 1th word . RNN based 29 . The terminology is borrowed from Finite-State Acceptors . However , the RNN has a potentially infinite number of states , making it necessary to rely on a function other than a lookup table for mapping states to decisions . 30 . This kind of supervision signal may be hard to train for long sequences , especially so with the Simple - RNN , because of the vanishing gradients problem . It is also a generally hard learning task , as we do not tell the process on which parts of the input to focus . 49 R , O x1 s0 predict & calc loss y1 R , O x2 s1 predict & calc loss y2 R , O x3 s2 predict & calc loss y3 R , O x4 s3 predict & calc loss y4 R , O x5 s4 predict & calc loss y5 sum loss Figure 8 : Transducer RNN Training Graph . language models are shown to provide much better perplexities than traditional language models ( Mikolov et al . , 2010 ; Sundermeyer , Schlüter , & Ney , 2012 ; Mikolov , 2012 ) . Using RNNs as transducers allows us to relax the Markov assumption that is tradition - ally taken in language models and HMM taggers , and condition on the entire prediction history . The power of the ability to condition on arbitrarily long histories is demonstrated in generative character-level RNN models , in which a text is generated character by charac - ter , each character conditioning on the previous ones ( Sutskever , Martens , & Hinton , 2011 ) . The generated texts show sensitivity to properties that are not captured by n-gram language models , including line lengths and nested parenthesis balancing . For a good demonstration and analysis of the properties of RNN-based character level language models , see ( Karpathy , Johnson , & Li , 2015 ) . Encoder - Decoder Finally , an important special case of the encoder scenario is the Encoder-Decoder framework ( Cho , van Merrienboer , Bahdanau , & Bengio , 2014a ; Sutskever et al . , 2014 ) . The RNN is used to encode the sequence into a vector representation yn , and this vector representation is then used as auxiliary input to another RNN that is used as a decoder . For example , in a machine-translation setup the first RNN encodes the source sentence into a vector representation yn , and then this state vector is fed into a separate ( decoder ) RNN that is trained to predict ( using a transducer-like language modeling ob - jective ) the words of the target language sentence based on the previously predicted words as well as yn . The supervision happens only for the decoder RNN , but the gradients are propagated all the way back to the encoder RNN ( see Figure 9 ) . Such an approach was shown to be surprisingly effective for Machine Translation ( Sutskever et al . , 2014 ) using LSTM RNNs . In order for this technique to work , Sutskever et al found it effective to input the source sentence in reverse , such that xn corresponds to the first word of the sentence . In this way , it is easier for the second RNN to establish the relation be - tween the first word of the source sentence to the first word of the target sentence . Another use-case of the encoder-decoder framework is for sequence transduction . Here , in order to generate tags t1 , . . . , tn , an encoder RNN is first used to encode the sentence x1 : n into fixed sized vector . This vector is then fed as the initial state vector of another ( transducer ) RNN , which is used together with x1 : n to predict the label ti at each position i . This approach 50 RE , OE x1 se0 RE , OE x2 se1 RE , OE x3 se2 RE , OE x4 se3 RE , OE x5 se4 s e 5 RD , OD x1 sd0 predict & calc loss y1 RD , OD x2 sd1 predict & calc loss y2 RD , OD x3 sd2 predict & calc loss y3 RD , OD x4 sd3 predict & calc loss y4 RD , OD x5 sd4 predict & calc loss y5 sum loss Figure 9 : Encoder-Decoder RNN Training Graph . was used in ( Filippova , Alfonseca , Colmenares , Kaiser , & Vinyals , 2015 ) to model sentence compression by deletion . 10.3 Multi-layer ( stacked ) RNNs RNNs can be stacked in layers , forming a grid ( Hihi & Bengio , 1996 ) . Consider k RNNs , RNN1 , . . . , RNNk , where the jth RNN has states s j 1 : n and outputs y j 1 : n . The input for the first RNN are x1 : n , while the input of the jth RNN ( j ≥ 2 ) are the outputs of the RNN below it , yj − 11 : n . The output of the entire formation is the output of the last RNN , y k 1 : n . Such layered architectures are often called deep RNNs . A visual representation of a 3-layer RNN is given in Figure 10 . While it is not theoretically clear what is the additional power gained by the deeper architecture , it was observed empirically that deep RNNs work better than shallower ones on some tasks . In particular , Sutskever et al ( 2014 ) report that a 4-layers deep architec - ture was crucial in achieving good machine-translation performance in an encoder-decoder framework . Irsoy and Cardie ( 2014 ) also report improved results from moving from a one - layer BI-RNN to an architecture with several layers . Many other works report result using layered RNN architectures , but do not explicitly compare to 1-layer RNNs . 51 R1 , O1 R2 , O2 y11 s10 R3 , O3 y21 s20 s30 x1 y1 y31 R1 , O1 R2 , O2 y12 s11 R3 , O3 y22 s21 s31 x2 y2 y32 R1 , O1 R2 , O2 y13 s12 R3 , O3 y23 s22 s32 x3 y3 y33 R1 , O1 R2 , O2 y14 s13 R3 , O3 y24 s23 s33 x4 y4 y34 R1 , O1 R2 , O2 y15 s14 R3 , O3 y25 s24 s34 x5 y5 y35 s15 s25 s35 Figure 10 : A 3-layer ( “ deep ” ) RNN architecture . 10.4 BI-RNN A useful elaboration of an RNN is a bidirectional-RNN ( BI-RNN ) ( Schuster & Paliwal , 1997 ; Graves , 2008 ) . 31 Consider the task of sequence tagging over a sentence x1 , . . . , xn . An RNN allows us to compute a function of the ith word xi based on the past – the words x1 : i up to and including it . However , the following words xi : n may also be useful for prediction , as is evident by the common sliding-window approach in which the focus word is categorized based on a window of k words surrounding it . Much like the RNN relaxes the Markov assumption and allows looking arbitrarily back into the past , the BI-RNN relaxes the fixed window size assumption , allowing to look arbitrarily far at both the past and the future . Consider an input sequence x1 : n . The BI-RNN works by maintaining two separate states , sfi and s b i for each input position i . The forward state s f i is based on x1 , x2 , . . . , xi , while the backward state sbi is based on xn , xn − 1 , . . . , xi . The forward and backward states are generated by two different RNNs . The first RNN ( Rf , Of ) is fed the input sequence x1 : n as is , while the second RNN ( R b , Ob ) is fed the input sequence in reverse . The state representation si is then composed of both the forward and backward states . The output at position i is based on the concatenation of the two output vectors yi = [ y f i ; y b i ] = [ O f ( sfi ) ;O b ( sbi ) ] , taking into account both the past and the future . The vector yi can then be used directly for prediction , or fed as part of the input to a more complex network . While the two RNNs are run independently of each other , the error gra - dients at position i will flow both forward and backward through the two RNNs . A visual representation of the BI-RNN architecture is given in Figure 11 . The use of BI-RNNs for sequence tagging was introduced to the NLP community by Irsoy and Cardie ( 2014 ) . 10.5 RNNs for Representing Stacks Some algorithms in language processing , including those for transition-based parsing ( Nivre , 2008 ) , require performing feature extraction over a stack . Instead of being confined to 31 . When used with a specific RNN architecture such as an LSTM , the model is called BI-LSTM . 52 Rf , Of xthe concat yf1 sf0 Rf , Of xbrown concat yf2 sf1 Rf , Of xfox concat yf3 sf2 Rf , Of xjumped concat yf4 sf3 Rf , Of x ∗ concat yf5 sf4 s f 5 Rb , Ob s0sb0 yb1 Rb , Ob s1sb1 yb2 Rb , Ob s2sb2 yb3 Rb , Ob s3sb3 yb4 Rb , Ob s4sb4 yb5 sb5 ythe ybrown yfox yjumped y ∗ Figure 11 : BI-RNN over the sentence “ the brown fox jumped . ” . looking at the k top-most elements of the stack , the RNN framework can be used to provide a fixed-sized vector encoding of the entire stack . The main intuition is that a stack is essentially a sequence , and so the stack state can be represented by taking the stack elements and feeding them in order into an RNN , resulting in a final encoding of the entire stack . In order to do this computation efficiently ( without performing an O ( n ) stack encoding operation each time the stack changes ) , the RNN state is maintained together with the stack state . If the stack was push-only , this would be trivial : whenever a new element x is pushed into the stack , the corresponding vector x will be used together with the RNN state si in order to obtain a new state si + 1 . Dealing with pop operation is more challenging , but can be solved by using the persistent-stack data-structure ( Okasaki , 1999 ; Goldberg , Zhao , & Huang , 2013 ) . Persistent , or immutable , data-structures keep old versions of themselves intact when modified . The persistent stack construction represents a stack as a pointer to the head of a linked list . An empty stack is the empty list . The push operation appends an element to the list , returning the new head . The pop operation then returns the parent of the head , but keeping the original list intact . From the point of view of someone who held a pointer to the previous head , the stack did not change . A subsequent push operation will add a new child to the same node . Applying this procedure throughout the lifetime of the stack results in a tree , where the root is an empty stack and each path from a node to the root represents an intermediary stack state . Figure 12 provides an example of such a tree . The same process can be applied in the computation graph construction , creating an RNN with a tree structure instead of a chain structure . Backpropagating the error from a given node will then affect all the elements that participated in the stack when the node was created , in order . Figure 13 shows the computation graph for the stack-RNN corresponding to the last state in Figure 12 . This modeling approach was proposed independently by Dyer et al and Watanabe et al ( Dyer et al . , 2015 ; Watanabe & Sumita , 2015 ) for transition-based dependency parsing . 53 ⊥ a head ( 1 ) push a ⊥ a b head ( 2 ) push b ⊥ a b c head ( 3 ) push c ⊥ a b head c ( 4 ) pop ⊥ a b c d head ( 5 ) push d ⊥ a b head c d ( 6 ) pop ⊥ a head b c d ( 7 ) pop ⊥ a b c d e head ( 8 ) push e ⊥ a b c d e f head ( 9 ) push f Figure 12 : An immutable stack construction for the sequence of operations push a ; push b ; push c ; pop ; push d ; pop ; pop ; push e ; push f . so R , O ya xa R , O sa ya : b xb R , O sa : b ya : c xc sa : c R , O sa : b ya , b , d xd sa , b , d R , O sa ya , e xe R , O sa , e ya , e , f xf sa , e , f Figure 13 : The stack-RNN corresponding to the final state in Figure 12 . 10.6 A Note on Reading the Literature Unfortunately , it is often the case that inferring the exact model form from reading its description in a research paper can be quite challenging . Many aspects of the models 54 are not yet standardized , and different researchers use the same terms to refer to slightly different things . To list a few examples , the inputs to the RNN can be either one-hot vectors ( in which case the embedding matrix is internal to the RNN ) or embedded representations ; The input sequence can be padded with start-of-sequence and / or end-of-sequence symbols , or not ; While the output of an RNN is usually assumed to be a vector which is expected to be fed to additional layers followed by a softmax for prediction ( as is the case in the presentation in this tutorial ) , some papers assume the softmax to be part of the RNN itself ; In multi-layer RNN , the “ state vector ” can be either the output of the top-most layer , or a concatenation of the outputs from all layers ; When using the encoder-decoder framework , conditioning on the output of the encoder can be interpreted in various different ways ; and so on . On top of that , the LSTM architecture described in the next section has many small variants , which are all referred to under the common name LSTM . Some of these choices are made explicit in the papers , other require careful reading , and others still are not even mentioned , or are hidden behind ambiguous figures or phrasing . As a reader , be aware of these issues when reading and interpret model descriptions . As a writer , be aware of these issues as well : either fully specify your model in mathematical notation , or refer to a different source in which the model is fully specified , if such a source is available . If using the default implementation from a software package without knowing the details , be explicit of that fact and specify the software package you use . In any case , don’t rely solely on figures or natural language text when describing your model , as these are often ambiguous . 55