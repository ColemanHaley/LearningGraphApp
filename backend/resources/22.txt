Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 22 Coreference Resolution even Stigand , patriotic archbishop Canterbury , found advisable – ” ’ ‘ Found ? ’ Duck . ‘ Found , ’ Mouse replied rather crossly : ‘ course know “ ” means . ’ ‘ know “ ” means well enough , find thing , ’ Duck : ‘ gener - ally frog worm . question , archbishop find ? ’ Lewis Carroll , Alice Wonderland important component language understanding knowing talked text . Consider following passage : ( 22.1 ) Victoria Chen , CFO Megabucks Banking , saw pay jump $ 2.3 million , 38-year-old became company’s president . widely known came Megabucks rival Lotsabucks . underlined phrases passage writer refer person named Victoria Chen . call linguistic expressions like Victoria Chen mentions referring expressions , discourse entity referredmention ( Victoria Chen ) referent . ( distinguish referring expressions andreferent referents , italicize former . ) 1 Two referring expressions refer same discourse entity corefer ; thus , Victoria Chencorefer corefer ( 22.1 ) . Coreference important component natural language understanding . dialogue system just told user “ 2pm flight United 4pm Cathay Pacific ” know flight user means “ take Cathay Pacific flight ” . question answering system Wikipedia answer question Marie Curie born know sentence “ born Warsaw ” . machine translation system translating language like Spanish , pronouns dropped , coreference previous sentence decide Spanish sentence ‘ “ incanta el conocimiento ” , dice . ’ translated ‘ “ love knowledge ” , ’ , ‘ “ love knowledge ” , ’ . Indeed , example comes actual news article female professor mistranslated “ ” Google Translate inaccurate coreference resolution ( Schiebinger , 2019 ) . Natural language understanding systems ( humans ) interpret linguistic ex - pressions respect discourse model ( Karttunen , 1969 ) shown Fig . 22.1 . discoursemodel discourse model mental model system ( human hearer ) builds - crementally interprets text , containing representations entities referred text , well properties entities relations among . referent first mentioned discourse , say representation evokedevoked model . Upon subsequent mention , representation accessed theaccessed 1 convenient shorthand , sometimes speak referring expression referring referent , e.g . , saying refers Victoria Chen . , reader keep mind really mean speaker performing act referring Victoria Chen uttering . 2 CHAPTER 22 • COREFERENCE RESOLUTION V Discourse Model “ Victoria ” “ " corefer refer ( evoke ) refer ( access ) $ Lotsabucks Megabucks pay Figure 22.1 mentions evoke access discourse entities discourse model . model . Reference text entity previously introduced discourse called anaphora , referring expression ananaphora anaphor , anaphoric . 2 passage ( 22.1 ) , pronouns defi-anaphor nite NP 38-year-old anaphoric . anaphor corefers prior mention ( case Victoria Chen ) called antecedent . every refer-antecedent ring expression antecedent . entity single mention text ( like Lotsabucks ( 22.1 ) ) called singleton . singleton chapter focus task coreference resolution . Coreferencecoreferenceresolution resolution task determining two mentions corefer , mean refer same entity discourse model ( same discourse entity ) . set corefering expressions often called coreference chain cluster . coreferencechain cluster example , processing ( 22.1 ) , coreference resolution algorithm need find least four coreference chains , corresponding four entities discourse model Fig . 22.1 . 1 . { Victoria Chen , , 38-year-old , } 2 . { Megabucks Banking , company , Megabucks } 3 . { pay } 4 . { Lotsabucks } Note mentions nested ; example mention syntactically part another mention , pay , referring completely different discourse entity . Coreference resolution thus comprises two tasks ( although often per - formed jointly ) : ( 1 ) identifying mentions , ( 2 ) clustering corefer - ence chains / discourse entities . two mentions corefered associated same dis - course entity . often like go further , deciding real world entity associated discourse entity . example , mention Washington might refer state , capital city , person George Washington ; - terpretation sentence course different completely different named entity types ( Chapter 18 ) . task entity linking ( Ji Gr-entity linking ishman , 2011 ) entity resolution task mapping discourse entity real-world individual . 3 usually operationalize entity linking resolution 2 follow common NLP usage anaphor mean mention antecedent , rather narrow usage mean mentions ( like pronouns ) whose interpretation depends antecedent ( narrower interpretation , repeated names anaphors ) . 3 Computational linguistics / NLP thus differs term reference field formal semantics , words reference coreference describe relation mention real-world entity . contrast , follow functional linguistics tradition mention refers discourse entity ( Webber , 1978 ) relation discourse entity real world 3 mapping ontology : list entities world , like gazeteer ( Chapter 16 ) . Perhaps common ontology task Wikipedia ; Wikipedia page acts unique id particular entity . Thus entity linking task wiki - fication ( Mihalcea Csomai , 2007 ) task deciding Wikipedia page corresponding individual referred mention . entity linking ontology ; example ontology genes , link mentions genes text disambiguated gene name ontology . next sections introduce task coreference resolution de - tail , offer variety architectures resolution , simple deterministic baseline algorithms state-of-the-art neural models . turning algorithms , , mention important tasks touch briefly end chapter . First famous Winograd Schema problems ( so-called first pointed Terry Winograd dissertation ) . entity coreference resolution problems designed difficult solved resolution methods describe chapter , kind real-world knowledge require made kind chal - lenge task natural language understanding . example , consider task determining correct antecedent pronoun following example : ( 22.2 ) city council denied demonstrators permit . feared violence . b . advocated violence . Determining correct antecedent pronoun requires understanding second clause intended explanation first clause , city councils perhaps likely demonstrators fear violence demonstrators might likely advocate violence . Solving Winograd Schema problems requires finding way represent discover necessary real world knowledge . problem discuss chapter related task event corefer - ence , deciding two event mentions ( buy acquisition ineventcoreference two sentences ECB + corpus ) refer same event : ( 22.3 ) AMD agreed [ buy ] Markham , Ontario-based ATI around $ 5.4 billion cash stock , companies announced Monday . ( 22.4 ) [ acquisition ] turn AMD world’s largest providers graphics chips . Event mentions harder detect entity mentions , ver - bal well nominal . Once detected , same mention-pair mention-ranking models entities often applied events . even complex kind coreference discourse deixis ( Webber , 1988 ) , discourse deixis anaphor refers back discourse segment , quite hard delimit categorize , like examples ( 22.5 ) adapted Webber ( 1991 ) : ( 22.5 ) According Soleil , Beau just opened restaurant . turned lie . b . false . c . struck funny way describe situation . referent speech act ( Chapter 26 ) ( 22.5a ) , proposition ( 22.5b ) , manner description ( 22.5c ) . field awaits development robust methods interpreting types reference . individual requires additional step linking . 4 CHAPTER 22 • COREFERENCE RESOLUTION 22.1 Coreference Phenomena : Linguistic Background offer linguistic background reference phenomena . introduce four types referring expressions ( definite indefinite NPs , pronouns , names ) , describe evoke access entities discourse model , talk linguistic features anaphor / antecedent relation ( like number / gender agreement , properties verb semantics ) . 22.1.1 Types Referring Expressions Indefinite Noun Phrases : common form indefinite reference En - glish marked determiner ( ) , marked quan - tifier even determiner . Indefinite reference generally intro - duces discourse context entities new hearer . ( 22.6 ) . Mrs . Martin kind send Mrs . Goddard beautiful goose . b . gone round day bring walnuts . c . saw beautiful cauliflower today . Definite Noun Phrases : Definite reference , via NPs English article , refers entity identifiable hearer . entity identifiable hearer mentioned previously text thus already represented discourse model : ( 22.7 ) concerns white stallion sold officer . pedigree white stallion fully established . Alternatively , entity identifiable contained hearer’s set beliefs world , uniqueness object implied description itself , case evokes representation referent discourse model , ( 22.9 ) : ( 22.8 ) read New York Times . ( 22.9 ) seen car keys ? last quite common ; half definite NPs newswire texts non-anaphoric , often first time entity mentioned ( Poesio Vieira 1998 , Bean Riloff 1999 ) . Pronouns : Another form definite reference pronominalization , enti - ties extremely salient discourse , ( discuss below ) : ( 22.10 ) Emma smiled chatted cheerfully , Pronouns participate cataphora , mentioned beforecataphora referents , ( 22.11 ) . ( 22.11 ) Even saw , Dorothy thinking Emerald City every day . , pronouns occur referents introduced . Pronouns appear quantified contexts considered bound , ( 22.12 ) . bound ( 22.12 ) Every dancer brought left arm forward . relevant reading , refer woman context , behaves like variable bound quantified expression every dancer . concerned bound interpretation pronouns chapter . 22.1 • COREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 5 languages , pronouns appear clitics attached word , like lo ( ‘ ’ ) Spanish example AnCora ( Recasens Martı́ , 2010 ) : ( 22.13 ) La intención es reconocer el gran prestigio que tiene la maratón y unirlo con esta gran carrera . ‘ aim recognize great prestige Marathon join | great race . ” Demonstrative Pronouns : Demonstrative pronouns appear ei - ther alone determiners , instance , ingredient , spice : ( 22.14 ) just bought copy Thoreau’s Walden . bought five years ago . tattered ; better condition . Note NP ambiguous ; colloquial spoken English , indefinite , ( 22.6 ) , definite , ( 22.14 ) . Zero Anaphora : pronoun , languages ( including Chi - nese , Japanese , Italian ) possible anaphor lexical realization , called zero anaphor zero pronoun , following Italianzero anaphor Japanese examples Poesio et al . ( 2016 ) : ( 22.15 ) EN [ John ] went visit friends . way [ ] bought wine . [ Giovanni ] andò far visita degli amici . Per via φi comprò del vino . JA [ John ] i-wa yujin-o houmon-sita . Tochu-de φi wain-o ka-tta . Chinese example : ( 22.16 ) [ 我 ] 前 一 会 精神 上 太 紧张 。 [ 0 ] 现在 比较 平静 了 [ ] nervous ago . . . . [ 0 ] calmer . Zero anaphors complicate task mention detection languages . Names : Names ( people , locations , organizations ) refer new old entities discourse : ( 22.17 ) . Miss Woodhouse certainly justice . b . International Business Machines sought patent compensation Amazon ; IBM previously sued companies . 22.1.2 Information Status way referring expressions evoke new referents discourse ( introducing new information ) , access old entities model ( old informa - tion ) , called information status information structure . Entities beinformationstatus discourse-new discourse-old , indeed common distinguish leastdiscourse-new discourse-old three kinds entities informationally ( Prince , 1981a ) : new NPs : brand new NPs : introduce entities discourse-new hearer - new like fruit walnuts . unused NPs : introduce entities discourse-new hearer-old ( like Hong Kong , Marie Curie , New York Times . old NPs : called evoked NPs , introduce entities already dis - course model , hence discourse-old hearer-old , like “ went new restaurant . . . . ” . 6 CHAPTER 22 • COREFERENCE RESOLUTION inferrables : introduce entities neither hearer-old nor discourse-old , hearer infer existence reasoning based entities discourse . Consider following examples : ( 22.18 ) went superb restaurant yesterday . chef just opened . ( 22.19 ) Mix flour , butter water . Knead dough shiny . Neither chef nor dough discourse model based first sentence example , reader make bridging inferencebridginginference entities added discourse model associated restaurant ingredients , based world knowledge restaurants chefs dough result mixing flour liquid ( Haviland Clark 1974 , Webber Baldwin 1992 , Nissim et al . 2004 , Hou et al . 2018 ) . form NP gives strong clues information status . often talk entity’s position given-new dimension , extent refer-given-new ent ( salient discourse , easier hearer call mind , predictable hearer ) , versus new ( non-salient discourse , unpredictable ) ( Chafe 1976 , Prince 1981b , Gundel et al . 1993 ) . referent accessible ( Ariel , 2001 ) accessible i.e . , salient hearer’s mind easy call mind , referred less linguistic material . example pronouns referent high degree activation salience discourse model . 4 contrast , lesssalience salient entities , like new referent introduced discourse , need introduced longer explicit referring expression help hearer recover referent . Thus entity first introduced discourse mentions likely full names , titles roles , appositive restrictive relative clauses , introduction protagonist ( 22.1 ) : Victoria Chen , CFO Megabucks Banking . entity discussed discourse , becomes salient hearer mentions average typically becomes shorter less informative , example shortened name ( example Ms . Chen ) , definite description ( 38-year-old ) , pronoun ( ) ( Hawkins 1978 ) . , change length monotonic , sensitive discourse structure ( Grosz 1977 , Re - ichman 1985 , Fox 1993 ) . 22.1.3 Complications : Non-Referring Expressions Many noun phrases nominals referring expressions , although bear confusing superficial resemblance . example earliest computational work reference resolution , Karttunen ( 1969 ) pointed NP car following example create discourse referent : ( 22.20 ) Janet car . referred back anaphoric car : ( 22.21 ) * Toyota . ( 22.22 ) * car red . summarize four common types structures counted men - tions coreference tasks hence complicate task mention-detection : 4 Pronouns usually ( always ) refer entities introduced further two sentences back ongoing discourse , whereas definite noun phrases often refer further back . 22.1 • COREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 7 Appositives : appositional structure noun phrase appears next head noun phrase , describing head . English often appear commas , like “ unit UAL ” appearing apposition NP United , CFO Megabucks Banking apposition Victoria Chen . ( 22.23 ) Victoria Chen , CFO Megabucks Banking , saw . . . ( 22.24 ) United , unit UAL , matched fares . Appositional NPs referring expressions , functioning kind supplementary parenthetical description head NP . Nonetheless , sometimes useful link phrases entity describe , datasets like ntoNotes mark appositional relationships . Predicative Prenominal NPs : Predicative attributive NPs describe prop - erties head noun . United unit UAL , NP unit UAL describes property United , rather referring distinct entity . Thus marked mentions coreference tasks ; example NPs $ 2.3 million company’s president , attributive , describing properties pay 38-year-old ; Example ( 22.27 ) shows Chinese example predicate NP ( 中国 最大 的 城市 ; China’s biggest city ) mention . ( 22.25 ) pay jumped $ 2.3 million ( 22.26 ) 38-year-old became company’s president ( 22.27 ) 上海 是 [ 中国 最大 的 城市 ] [ Shanghai China’s biggest city ] Expletives : Many pronouns like English corresponding pronouns languages referential . expletive pleonastic cases includeexpletive raining , idioms like hit off , particular syntactic situations like cleftsclefts ( 22.28a ) extraposition ( 22.28b ) : ( 22.28 ) . Emma Goldman founded Mother Earth b . surprised herring hanging wall . Generics : Another kind expression refer back entity explic - itly evoked text generic reference . Consider ( 22.29 ) . ( 22.29 ) love mangos . tasty . , refers , particular mango set mangos , class mangos general . pronoun generically : ( 22.30 ) July San Francisco wear jacket . 22.1.4 Linguistic Properties Coreference Relation seen linguistic properties individual referring expressions turn properties antecedent / anaphor pair . Understanding properties helpful designing novel features performing error analyses . Number Agreement : Referring expressions referents generally agree number ; English / / / / / singular , / / / plu - ral , unspecified number . plural antecedent like chefs generally corefer singular anaphor like . , algorithms enforce number agreement strictly . First , semantically plural entities re - ferred : ( 22.31 ) IBM announced new machine translation product yesterday . working 20 years . 8 CHAPTER 22 • COREFERENCE RESOLUTION Second , singular become common , tosingular describe singular individuals , often useful gender neutral . Although recently increasing , singular quite old , part English many centuries . 5 Person Agreement : English distinguishes first , second , third person , pronoun’s antecedent agree pronoun person . Thus third person pronoun ( , , , , , , , , ) third person antecedent ( noun phrase ) . , phenomena like quotation cause exceptions ; example , , coreferent : ( 22.32 ) “ voted Nader aligned values , ” . Gender Noun Class Agreement : many languages , nouns grammat - ical gender noun class6 pronouns generally agree grammatical gender antecedent . English occurs third-person singular pronouns , distinguish male ( , , ) , female ( , ) , nonpersonal ( ) grammatical genders . Non-binary pronouns like ze hir occur recent texts . Knowing gender associate name text complex , require world knowledge individual . examples : ( 22.33 ) Maryam theorem . exciting . ( = Maryam , theorem ) ( 22.34 ) Maryam theorem . exciting . ( = theorem , Maryam ) Binding Theory Constraints : binding theory name syntactic con - straints relations mention antecedent same sentence ( Chomsky , 1981 ) . Oversimplifying bit , reflexive pronouns like herselfreflexive corefer subject immediate clause contains ( 22.35 ) , whereas nonreflexives corefer subject ( 22.36 ) . ( 22.35 ) Janet bought herself bottle fish sauce . [ herself = Janet ] ( 22.36 ) Janet bought bottle fish sauce . [ her6 = Janet ] Recency : Entities introduced recent utterances tend salient introduced utterances further back . Thus , ( 22.37 ) , pronoun likely refer Jim’s map doctor’s map . ( 22.37 ) doctor found old map captain’s chest . Jim found even older map hidden shelf . described island . Grammatical Role : Entities mentioned subject position salient object position , turn salient mentioned oblique positions . Thus although first sentence ( 22.38 ) ( 22.39 ) expresses roughly same propositional content , preferred referent pronoun varies subject — John ( 22.38 ) Bill ( 22.39 ) . ( 22.38 ) Billy Bones went bar Jim Hawkins . called glass rum . [ = Billy ] ( 22.39 ) Jim Hawkins went bar Billy Bones . called glass rum . [ = Jim ] 5 Here’s bound pronoun example Shakespeare’s Comedy Errors : There’s man meet doth salute well-acquainted friend 6 word “ gender ” generally languages 2 3 noun classes , like Indo - European languages ; many languages , like Bantu languages Chinese , larger number noun classes . 22.2 • COREFERENCE TASKS DATASETS 9 Verb Semantics : verbs semantically emphasize arguments , bi - asing interpretation subsequent pronouns . Compare ( 22.40 ) ( 22.41 ) . ( 22.40 ) John telephoned Bill . lost laptop . ( 22.41 ) John criticized Bill . lost laptop . examples differ verb first sentence , yet “ ” ( 22.40 ) typically resolved John , whereas “ ” ( 22.41 ) resolved Bill . due link implicit causality saliency : implicit cause “ criticizing ” event object , whereas implicit cause “ telephoning ” event subject . verbs , entity implicit cause salient . Selectional Restrictions : Many kinds semantic knowledge play role referent preference . example , selectional restrictions verb places arguments ( Chapter 20 ) help eliminate referents , ( 22.42 ) . ( 22.42 ) ate soup new bowl cooking hours two possible referents , soup bowl . verb eat , , requires direct object denote something edible , constraint rule bowl possible referent . 22.2 Coreference Tasks Datasets formulate task coreference resolution follows : text T , find entities coreference links . evaluate task com - paring links system creates human-created gold coreference annotations T . return coreference example , superscript numbers coreference chain ( cluster ) , subscript letters individual mentions clus - ter : ( 22.43 ) [ Victoria Chen ] 1a , CFO [ Megabucks Banking ] 2 , saw [ [ ] 1 b pay ] 3 jump $ 2.3 million , [ 38-year-old ] 1c became [ [ company ] 2 b’s president . widely known [ ] 1d came [ Megabucks ] 2 c rival [ Lotsabucks ] 4a . Assuming example ( 22.43 ) entirety article , chains pay Lotsabucks singleton mentions : 1 . { Victoria Chen , , 38-year-old , } 2 . { Megabucks Banking , company , Megabucks } 3 . { pay } 4 . { Lotsabucks } coreference evaluation campaigns , input system raw text articles , systems detect mentions link clusters . Solving task requires dealing pronominal anaphora ( figuring refers Victoria Chen ) , filtering non-referential pronouns like pleonastic ten years ) , dealing definite noun phrases figure 38-year-old coreferent Victoria Chen , company same Megabucks . need deal names , realize Megabucks same Megabucks Banking . 10 CHAPTER 22 • COREFERENCE RESOLUTION Exactly counts mention links annotated differs task task dataset dataset . example coreference datasets label singletons , making task simpler . Resolvers achieve higher scores corpora singletons , singletons constitute majority mentions running text , often hard distinguish non-referential NPs . tasks gold mention-detection ( i.e . system human-labeled mention boundaries task just cluster gold mentions ) , eliminates need detect segment mentions running text . Coreference usually evaluated CoNLL F1 score , combines three metrics : MUC , B3 , CEAFe ; Section 22.7 gives details . mention few characteristics popular coreference dataset , OntoNotes ( Pradhan et al . 2007 , Pradhan et al . 2007 ) , CoNLL 2012 Shared Task based ( Pradhan et al . , 2012a ) . OntoNotes contains hand-annotated Chinese En - glish coreference datasets roughly million words , consisting newswire , magazine articles , broadcast news , broadcast conversations , web data conversa - tional speech data , well 300,000 words annotated Arabic newswire . important distinguishing characteristic OntoNotes la - bel singletons , simplifying coreference task , singletons represent 60 % - 70 % entities . ways , similar coreference datasets . Referring ex - pression NPs coreferent marked mentions , generics pleonastic pronouns marked . Appositive clauses marked separate mentions , included mention . Thus NP , “ Richard Godown , president Industrial Biotechnology Association ” mention entire phrase . Prenom - inal modifiers annotated separate entities proper nouns . Thus wheat entity wheat fields , UN entity UN policy ( adjectives like American American policy ) . number corpora mark richer discourse phenomena . ISNotes corpus annotates portion OntoNotes information status , include bridging examples ( Hou et al . , 2018 ) . AnCora-CO coreference corpus ( Recasens Martı́ , 2010 ) contains 400,000 words Spanish ( AnCora-CO-Es ) Catalan ( AnCora - CO-Ca ) news data , includes labels complex phenomena like discourse deixis languages . ARRAU corpus ( Uryupina et al . , 2019 ) contains 350,000 words English marking NPs , means singleton clusters available . ARRAU includes diverse genres like dialog ( TRAINS data ) fiction ( Pear Stories ) , labels bridging references , discourse deixis , generics , - biguous anaphoric relations . 22.3 Mention Detection first stage coreference mention detection : finding spans text thatmentiondetection constitute mention . Mention detection algorithms usually liberal proposing candidate mentions ( i.e . , emphasizing recall ) , filtering later . example many systems run parsers named entity taggers text extract every span NP , possessive pronoun , named entity . sample text repeated ( 22.44 ) : ( 22.44 ) Victoria Chen , CFO Megabucks Banking , saw pay jump $ 2.3 million , 38-year-old became company’s president . widely known came Megabucks rival Lotsabucks . 22.3 • MENTION DETECTION 11 might result following list 13 potential mentions : Victoria Chen company CFO Megabucks Banking company’s president Megabucks Banking pay Megabucks $ 2.3 million Lotsabucks 38-year-old recent mention detection systems even generous ; span-based algorithm describe Section 22.6 first extracts literally N-gram spans words up N = 10 . course recall Section 22.1.3 many NPs — overwhelming majority random N-gram spans — referring expressions . mention detection systems need eventually filter pleonas - tic / expletive pronouns like , appositives like CFO Megabucks Banking Inc , predicate nominals like company’s president $ 2.3 million . filtering rules . Early rule-based systems designed regular expressions deal pleonastic , like following rules Lappin Leass ( 1994 ) dictionaries cognitive verbs ( e.g . , believe , know , antic - ipate ) capture pleonastic “ thought ketchup . . . ” , modal adjectives ( e.g . , necessary , possible , certain , important ) , , e.g . , “ likely . . . ” . rules sometimes part modern systems : Modaladjective S Modaladjective ( NP ) VP Cogv-ed S seems / appears / means / follows ( ) S Mention-detection rules sometimes designed specifically particular eval - uation campaigns . OntoNotes , example , mentions embedded larger mentions , numeric quantities annotated , rarely coref - erential . Thus OntoNotes tasks like CoNLL 2012 ( Pradhan et al . , 2012a ) , common first pass rule-based mention detection algorithm ( Lee et al . , 2013 ) : 1 . Take NPs , possessive pronouns , named entities . 2 . Remove numeric quantities ( 100 dollars , 8 % ) , mentions embedded larger mentions , adjectival forms nations , stop words ( like ) . 3 . Remove pleonastic based regular expression patterns . Rule-based systems , , generally insufficient deal mention - detection , modern systems incorporate sort learned mention detec - tion component , referentiality classifier , anaphoricity classifier — detecting NP anaphor — discourse-new classifier — detecting mention discourse-new potential antecedent future anaphor . anaphoricity detector , example , draw positive training examplesanaphoricitydetector span labeled anaphoric referring expression hand-labeled datasets like OntoNotes , ARRAU , AnCora . NP named entity marked negative training example . Anaphoricity classifiers features candidate mention head word , surrounding words , definiteness , animacy , length , position sentence / discourse , many first proposed early work Ng Cardie ( 2002a ) ; Section 22.5 features . 12 CHAPTER 22 • COREFERENCE RESOLUTION Referentiality anaphoricity detectors run filters , men - tions classified anaphoric referential passed coreference system . end result filtering mention detection system example might following filtered set 9 potential mentions : Victoria Chen pay Megabucks Bank 38-year-old Megabucks company Lotsabucks turns , , hard filtering mentions based anaphoricity referentiality classifier leads poor performance . anaphoricity classifier threshold set high , many mentions filtered recall suffers . classifier threshold set low , many pleonastic non-referential mentions included precision suffers . modern approach perform mention detection , anaphoricity , coreference jointly single end-to-end model ( Ng 2005b , Denis Baldridge 2007 , Rahman Ng 2009 ) . example mention detection Lee et al . ( 2017b ) , ( 2018 ) system based single end-to-end neural network computes score mention referential , score two mentions coreference , combines make decision , training scores single end-to-end loss . describe method detail Section 22.6 . 7 Despite advances , correctly detecting referential mentions seems still unsolved problem , systems incorrectly marking pleonastic pronouns like non-referential NPs coreferent large source errors mod - ern coreference resolution systems ( Kummerfeld Klein 2013 , Martschat Strube 2014 , Martschat Strube 2015 , Wiseman et al . 2015 , Lee et al . 2017a ) . Mention , referentiality , anaphoricity detection thus important open area investigation . sources knowledge turn helpful , especially combination unsupervised semisupervised algorithms , mit - igate expense labeled datasets . early work , example Bean Riloff ( 1999 ) learned patterns characterizing anaphoric non-anaphoric NPs ; ( ex - tracting generalizing first NPs text , guaranteed non-anaphoric ) . Chang et al . ( 2012 ) look head nouns appear frequently training data never appear gold mentions help find non-referential NPs . Bergsma et al . ( 2008 ) web counts semisupervised way augment standard features anaphoricity detection English , important task common ambiguous ; quarter half examples non-anaphoric . Consider following two examples : ( 22.45 ) make [ ] advance . [ anaphoric ] ( 22.46 ) make [ ] Hollywood . [ non-anaphoric ] make non-anaphoric , part idiom make . Bergsma et al . ( 2008 ) turn context around example patterns , like “ make * advance ” ( 22.45 ) , “ make * Hollywood ” ( 22.46 ) . Google N-grams enumerate words replace patterns . Non-anaphoric contexts tend wildcard positions , anaphoric contexts occur many NPs ( example make advance just frequent data 7 systems try avoid mention detection anaphoricity detection altogether . datasets like OntoNotes label singletons , alternative filtering non-referential mentions run coreference resolution , simply delete candidate mentions corefered another mention . likely work well explicitly modeling referentiality , solve problem detecting singletons , important tasks like entity linking . 22.4 • ARCHITECTURES COREFERENCE ALGORITHMS 13 make advance , make Hollywood occur ) . N-gram contexts features supervised anaphoricity classifier . 22.4 Architectures Coreference Algorithms Modern systems coreference based supervised neural machine learning , supervised hand-labeled datasets like OntoNotes . section overview various architecture modern systems , categorization Ng ( 2010 ) , distinguishes algorithms based make coreference deci - sion way entity-based — representing entity discourse model — mention-based — considering mention independently , ranking models directly compare potential antecedents . Afterwards , go detail state-of-the-art algorithm Section 22.6 . 22.4.1 Mention-Pair Architecture begin mention-pair architecture , simplest influentialmention-pair coreference architecture , introduces many features complex algorithms , even though architectures perform better . mention-pair ar-mention-pair chitecture based around classifier — name suggests — pair mentions , candidate anaphor candidate antecedent , makes binary classification decision : corefering . consider task classifier pronoun example , assume slightly simplified set potential antecedents Fig . 22.2 . Victoria Chen Megabucks Banking pay 37-year-old p ( coref | ” Victoria Chen ” , ” ” ) p ( coref | ” Megabucks Banking ” , ” ” ) Figure 22.2 pair mention ( like ) , potential antecedent mention ( like Victoria Chen ) , mention-pair classifier assigns probability coreference link . prior mention ( Victoria Chen , Megabucks Banking , , etc . ) , binary classifier computes probability : mention antecedent . probability high actual antecedents ( Victoria Chen , , 38-year-old ) low non-antecedents ( Megabucks Banking , pay ) . Early classifiers hand-built features ( Section 22.5 ) ; recent classifiers neural representation learning ( Section 22.6 ) training , need heuristic selecting training samples ; pairs mentions document coreferent , selecting every pair lead massive overabundance negative samples . common heuristic , ( Soon et al . , 2001 ) , choose closest antecedent positive example , pairs negative examples . formally , anaphor mention mi create • positive instance ( mi , m j ) m j closest antecedent mi , 14 CHAPTER 22 • COREFERENCE RESOLUTION • negative instance ( mi , mk ) mk m j mi Thus anaphor , choose ( , ) positive example negative examples . Similarly , anaphor company choose ( company , Megabucks ) positive example ( company , ) ( com - pany , 38-year-old ) ( company , pay ) ( company , ) negative examples . Once classifier trained , applied test sentence clustering step . mention document , classifier considers prior − 1 mentions . closest-first clustering ( Soon et al . , 2001 ) , classifier run right left ( mention − 1 down mention 1 ) first antecedent probability > . 5 linked . antecedent probably > 0.5 , antecedent selected . best-first clustering , classifier run − 1 antecedents probable preceding mention chosen antecedent . transitive closure pairwise relation taken cluster . mention-pair model advantage simplicity , two main problems . First , classifier directly compare candidate antecedents , trained decide , two likely antecedents , fact better . Second , ignores discourse model , looking mentions , entities . classifier decision made completely locally pair , able take account mentions same entity . next two models address two flaws . 22.4.2 Mention-Rank Architecture mention ranking model directly compares candidate antecedents , choosing highest-scoring antecedent anaphor . early formulations , mention , classifier decide { 1 , . . . , − 1 } prior mentions antecedent ( Denis Baldridge , 2008 ) . suppose fact anaphoric , none antecedents chosen ? model need run separate anaphoricity classifier . , turns better jointly learn anaphoricity detection coreference together single loss ( Rahman Ng , 2009 ) . modern mention-ranking systems , ith mention ( anaphor ) , associated random variable yi ranging values Y ( ) = { 1 , . . . , − 1 , ε } . value ε special dummy mention meaning antecedent ( i.e . , discourse-new starts new coref chain , non-anaphoric ) . Victoria Chen Megabucks Banking pay 37-year-old p ( ” Victoria Chen ” | ” ” ) p ( ϵ | ” ” ) ϵ high low } p ( ” pay ” | ” ) p ( ” ” | ” ) p ( ” 37-year-old ” | ” ) p ( ” Megabucks Banking ” | ” ) } Figure 22.3 candidate anaphoric mention ( like ) , mention-ranking system assigns proba - bility distribution previous mentions plus special dummy mention ε . test time , mention model computes softmax antecedents ( plus ε ) giving probability candidate antecedent ( none ) . 22.5 • CLASSIFIERS HAND-BUILT FEATURES 15 Fig . 22.3 shows example computation single candidate anaphor . Once antecedent classified anaphor , transitive closure run pairwise decisions get complete clustering . Training trickier mention-ranking model mention-pair model , anaphor know possible gold antecedents training . , best antecedent mention latent ; , mention whole cluster legal gold antecedents choose . Early work heuristics choose antecedent , example choosing closest antecedent gold antecedent non-antecedents window two sentences negative examples ( Denis Baldridge , 2008 ) . Various kinds ways model latent antecedents exist ( Fernandes et al . 2012 , Chang et al . 2013 , Durrett Klein 2013 ) . simplest way give credit legal antecedent summing , loss function optimizes likelihood correct antecedents gold clustering ( Lee et al . , 2017b ) . details Section 22.6 . Mention-ranking models implemented hand-build features neural representation learning ( might incorporate hand-built fea - tures ) . explore directions Section 22.5 Section 22.6 . 22.4.3 Entity-based Models mention-pair mention-ranking models make decisions men - tions . contrast , entity-based models link mention previous mention previous discourse entity ( cluster mentions ) . mention-ranking model turned entity-ranking model simply classifier make decisions clusters mentions rather individual mentions ( Rahman Ng , 2009 ) . traditional feature-based models , extracting features clusters . size cluster useful features , ‘ shape ’ , list types mentions cluster i.e . , sequences tokens ( P ) roper , ( D ) efinite , ( ) ndefinite , ( Pr ) onoun , cluster composed { Victoria , , 38-year-old } shape P-Pr-D ( Björkelund Kuhn , 2014 ) . entity - based model includes mention-pair classifier features aggregates mention-pair probabilities , example computing average probability coref - erence mention-pairs two clusters ( Clark Manning 2015 ) . Neural models learn representations clusters automatically , example RNN sequence cluster mentions encode state correspond - ing cluster representation ( Wiseman et al . , 2016 ) , learning distributed rep - resentations pairs clusters pooling learned representations mention pairs ( Clark Manning , 2016b ) . , although entity-based models expressive , cluster - level information practice led large gains performance , mention - ranking models still commonly . 22.5 Classifiers hand-built features Hand-designed features play important role coreference , sole input classification pre-neural classifiers , augmentations automatic 16 CHAPTER 22 • COREFERENCE RESOLUTION representation learning state-of-the-art neural systems like de - scribe Section 22.6 . section describe features commonly logistic regression , SVM , random forest classifiers coreference resolution . anaphor mention potential antecedent mention , feature based classifiers make three types features : ( ) features anaphor , ( ii ) features candidate antecedent , ( iii ) features relationship pair . Entity-based models make additional two additional classes : ( iv ) feature mentions antecedent’s entity cluster , ( v ) features relation anaphor mentions antecedent entity cluster . Figure 22.4 shows selection commonly features , shows value computed potential anaphor ” ” potential antecedent ” Victoria Chen ” example sentence , repeated below : ( 22.47 ) Victoria Chen , CFO Megabucks Banking , saw pay jump $ 2.3 million , 38-year-old became company’s president . widely known came Megabucks rival Lotsabucks . Features prior work found particularly useful exact string match , entity headword agreement , mention distance , well ( pronouns ) exact attribute match i-within-i , ( nominals proper names ) word inclusion cosine . lexical features ( like head words ) common words appear enough times ( perhaps 20 times ) , backing off parts speech rare words . crucial feature-based systems conjunctions features ; exper - iment suggested moving individual features classifier conjunctions multiple features increased F1 4 points ( Lee et al . , 2017a ) . Specific conjunc - tions designed hand ( Durrett Klein , 2013 ) , pairs features conjoined ( Bengtson Roth , 2008 ) , feature conjunctions learned auto - matically , classifiers like decision trees random forests ( ( Ng Cardie , 2002a ) , Lee et al . 2017a ) neural models take raw , unconjoined features input , automatically learn intermediate representations ( Wiseman et al . , 2015 ) . Finally , features neural models well . Neural systems kind describe next section make contextual word embeddings , benefit adding shallow features like string head match , grammatical role , mention types . features like mention length , distance mentions , genre complement contextual word embedding models nicely . 22.6 neural mention-ranking algorithm section describe neural mention-ranking system Lee et al . ( 2017b ) . end-to-end system exactly separate mention-detection step . - stead , considers every possible span text up set length ( i.e . n-grams length 1,2,3 . . . N ) possible mention . 8 8 number potential mentions makes algorithm slow unwieldy ( model’s size O ( t4 ) document length ) practice various versions algorithm find ways prune possible mentions , essentially mention score something mention-detector . 22.6 • NEURAL MENTION-RANKING ALGORITHM 17 Features Anaphor Antecedent Mention First ( last ) word Victoria / First last word ( embedding ) antecedent / anaphor Head word Victoria / Head word ( head embedding ) antecedent / anaphor Attributes Sg-F-A-3 - PER / Sg-F-A - 3-PER number , gender , animacy , person , named entity type attributes ( antecedent / anaphor ) Length 2/1 length words ( antecedent / anaphor ) Grammatical role Sub / Sub grammatical role — subject , direct object , indirect object / PP — ( antecedent / anaphor ) Mention type P / Pr Type : ( P ) roper , ( D ) efinite , ( ) ndefinite , ( Pr ) onoun ) - tecedent / anaphor Features Antecedent Entity Entity shape P-Pr-D ‘ shape ’ list types mentions antecedent entity ( cluster ) , i.e . , sequences ( P ) roper , ( D ) efinite , ( ) ndefinite , ( Pr ) onoun . Entity attributes Sg-F-A-3 - PER number , gender , animacy , person , named entity type attributes antecedent entity Antecedent cluster size 3 Number mentions antecedent cluster Features Pair Mentions Longer anaphor F True anaphor longer antecedent Pairs features Victoria / , 2/1 , Sub / Sub , P / Pr , etc . individual feature , pair type antecedent + type anaphor Sentence distance 1 number sentences antecedent anaphor Mention distance 4 number mentions antecedent anaphor i-within-i F Anaphor i-within-i relation antecedent Cosine Cosine antecedent anaphor embeddings Appositive F True anaphor syntactic apposition relation antecedent . useful even appositives mentions ( know attach appositive preceding head ) Features Pair Entities Exact String Match F True strings two mentions antecedent anaphor clusters identical . Head Word Match F True mentions antecedent cluster same headword mention anaphor cluster Word Inclusion F Words antecedent cluster includes words anaphor cluster Features Document Genre / source N document genre — ( D ) ialog , ( N ) ews , etc , Figure 22.4 common features feature-based coreference algorithms , values anaphor “ ” potential antecedent “ Victoria Chen ” . document D T words , model considers N = T ( T − 1 ) 2 text spans up length ( version Lee et al . ( 2018 ) , length 10 ) . span starts word START ( ) ends word END ( ) . task assign span antecedent yi , random variable ranging values Y ( ) = { 1 , . . . , − 1 , ε } ; previous span special dummy token ε . Choosing dummy token means antecedent , 18 CHAPTER 22 • COREFERENCE RESOLUTION discourse-new starts new coreference chain , non-anaphoric . pair spans j , system assigns score s ( , j ) corefer - ence link span span j , system learns distribution P ( yi ) antecedents span : P ( yi ) = exp ( s ( , yi ) ) ∑ y ′ ∈ Y ( ) exp ( s ( , yi ) ) ( 22.48 ) score s ( , j ) includes three factors : m ( ) ; span mention ; m ( j ) ; span j mention ; c ( j ) ; j antecedent : s ( , j ) = m ( ) + m ( j ) + c ( , j ) ( 22.49 ) dummy antecedent ε , score s ( , ε ) fixed 0 . way non - dummy scores positive , model predicts highest-scoring antecedent , scores negative abstains . scoring functions m ( ) c ( , j ) based vector gi represents span : m ( ) = wm · FFNNm ( gi ) ( 22.50 ) c ( , j ) = wc · FFNNc ( [ gi , g j , gi ◦ g j , φ ( , j ) ] ) ( 22.51 ) antecedent score c ( , j ) takes input representation spans j , element-wise similarity two spans gi ◦ g j ( ◦ element-wise multiplication ) . antecedent score c considers feature vec - tor φ ( , j ) encodes useful features like mention distances , information speaker genre . span representations gi themselves consist two parts : contextual repre - sentation first last word span , representation headword span . contextual representations first last words span . computed standard biLSTM . biLSTM takes input representation wt word , based contextual word embeddings like ELMo . ( BERT ELMo results even higher performance ( Joshi et al . , 2019 ) ) . output biLSTM word wt input ht : − → h t = LSTMforward ( − → h t − 1 , wt ) ← − h t = LSTMforward ( ← − h t + 1 , wt ) ht = [ − → h t , ← − h t ] ( 22.52 ) system independent LSTMs sentence . system attention ( Chapter 10 ) words span represent span’s head . usual attention , system learns weight vector wα , computes dot product hidden state ht transformed FFNN : αt = wα · FFNNα ( ht ) ( 22.53 ) attention score normalized distribution via softmax : ai , t = exp ( αt ) ∑ END ( ) k = START ( ) exp ( αk ) ( 22.54 ) 22.6 • NEURAL MENTION-RANKING ALGORITHM 19 attention distribution create vector hATT ( ) attention-weighted sum words span : hATT ( ) = END ( ) ∑ t = START ( ) ai , t · wt ( 22.55 ) span represented vector gi , concatenation hidden rep - resentations start end tokens span , head , feature vector containing feature : length span . gi = [ hSTART ( ) , hEND ( ) , hATT ( ) , φ ( ) ] ( 22.56 ) Fig . 22.5 Lee et al . ( 2017b ) shows computation span representation mention score . General Electric Postal Service contacted company General Electric + Electric + Postal Service + Service contacted + company + Mention score ( sm ) Span representation ( g ) Span head ( x̂ ) Bidirectional LSTM ( x ∗ ) Word & character embedding ( x ) Figure 1 : First step end-to-end coreference resolution model , computes embedding repre - sentations spans scoring potential entity mentions . Low-scoring spans pruned , manageable number spans considered coreference decisions . general , model considers possible spans up maximum width , depict small subset . General Electric Postal Service company s ( company , General Electric ) s ( company , Postal Service ) s ( company , ϵ ) = 0 Softmax ( P ( yi | D ) ) Coreference score ( s ) Antecedent score ( sa ) Mention score ( sm ) Span representation ( g ) Figure 2 : Second step model . Antecedent scores computed pairs span represen - tations . final coreference score pair spans computed summing mention scores spans pairwise antecedent score . fixing score dummy antecedent ϵ 0 , model predicts best scoring antecedent non-dummy scores positive , ab - stains negative . challenging aspect model size O ( T 4 ) document length . Section 5 , factoring enables ag - gressive pruning spans unlikely - long coreference cluster according men - tion score sm ( ) . Scoring Architecture propose end-to - end neural architecture computes scores document metadata . core model vector representa - tions gi possible span , de - scribe detail following section . span representations , scoring functions computed via standard feed-forward neural networks : sm ( ) = wm · FFNNm ( gi ) sa ( , j ) = wa · FFNNa ( [ gi , gj , gi ◦ gj , φ ( , j ) ] ) · denotes dot product , ◦ denotes element-wise multiplication , FFNN denotes feed-forward neural network computes non - linear mapping input output vectors . antecedent scoring function sa ( , j ) - cludes explicit element-wise similarity span gi ◦ gj feature vector φ ( , j ) encoding speaker genre information metadata distance two spans . Span Representations Two types infor - mation crucial accurately predicting coreference links : context surrounding mention span internal structure span . bidirectional LSTM ( Hochreiter Schmidhuber , 1997 ) en - code lexical information inside outside span . include attention mechanism words span model head words . assume vector representations word { x1 , . . . , xT } , composed fixed pre - trained word embeddings 1-dimensional con - volution neural networks ( CNN ) characters ( Section 7.1 details ) compute vector representations span , first bidirectional LSTMs encode every Mention score ( m ) Span representation ( g ) Span head ( hatt ) Bidirectional LSTM ( h ) Input word embeddings ( ELMo ) Figure 22.5 Computation span representation mention score end-to - end coreference model Lee et al . ( 2017b ) . model considers spans up maximum width ; figure shows small subset . Figure Lee et al . ( 2017b ) . Fig . 22.6 shows computation score s three possible antecedents company example sentence Fig . 22.5 . General Electric Postal Service contacted company General Electric + Electric + Postal Service + Service contacted + company + Mention score ( sm ) Span representation ( g ) Span head ( x̂ ) Bidirectional LSTM ( x ∗ ) Word & character embedding ( x ) Figure 1 : First step end-to-end coreference resolution model , computes embedding repre - sentations spans scoring potential entity mentions . Low-scoring spans pruned , manageable number spans considered coreference decisions . general , model considers possible spans up maximum width , depict small subset . General Electric Postal Service company s ( company , General Electric ) s ( company , Postal Service ) s ( company , ϵ ) = 0 Softmax ( P ( yi | D ) ) Coreference score ( s ) Antecedent score ( sa ) Mention score ( sm ) Span representation ( g ) Figure 2 : Second step model . Antecedent scores computed pairs span represen - tations . final coreference score pair spans computed summing mention scores spans pairwise antecedent score . fixing score dummy antecedent ϵ 0 , model predicts best scoring antecedent non-dummy scores positive , ab - stains negative . challenging aspect model size O ( T 4 ) document length . Section 5 , factoring enables ag - gressive pruning spans unlikely - long coreference cluster according men - tion score sm ( ) . Scoring Architecture propose end-to - end neural architecture computes scores document metadata . core model vector representa - tions gi possible span , de - scribe detail following section . span representations , scoring functions computed via standard feed-forward neural networks : sm ( ) = wm · FFNNm ( gi ) sa ( , j ) = wa · FFNNa ( [ gi , gj , gi ◦ gj , φ ( , j ) ] ) · denotes dot product , ◦ denotes element-wise multiplication , FFNN denotes feed-forward neural network computes non - linear mapping input output vectors . antecedent scoring function sa ( , j ) - cludes explicit element-wise similarity span gi ◦ gj feature vector φ ( , j ) encoding speaker genre information metadata distance two spans . Span Representations Two types infor - mation crucial accurately predicting coreference links : context surrounding mention span internal structure span . bidirectional LSTM ( Hochreiter Schmidhuber , 1997 ) en - code lexical information inside outside span . include attention mechanism words span model head words . assume vector representations word { x1 , . . . , xT } , composed fixed pre - trained word embeddings 1-dimensional con - volution neural networks ( CNN ) characters ( Section 7.1 details ) compute vector representations span , first bidirectional LSTMs encode every ( m ) ( c ) Figure 22.6 computation score s three possible antecedents com - pany example sentence Fig . 22.5 . Figure Lee et al . ( 2017b ) . inference time , method generally prune mentions ( example mention score m filter keep best few mentions function like 0.4T sentence length T ) . joint distribution 20 CHAPTER 22 • COREFERENCE RESOLUTION antecedents document computed forward pass . Finally , transitive closure antecedents create final clustering document . training , single gold antecedent mention ; coreference labeling gives entire cluster coreferent mentions , mention latent antecedent . therefor loss function maximizes sum coreference probability legal antecedents . mention possible antecedents Y ( ) , let GOLD ( ) set mentions gold cluster containing . set mentions occurring Y ( ) , set mentions gold cluster occur Y ( ) ∩ GOLD ( ) . maximize : ∑ ŷ ∈ Y ( ) ∩ GOLD ( ) P ( ŷ ) ( 22.57 ) mention gold cluster GOLD ( ) = ε . turn probability loss function , cross-entropy loss function defined Eq . ? ? Chapter 5 , taking − log probability . sum mentions , get final loss function training : L = N ∑ = 2 − log ∑ ŷ ∈ Y ( ) ∩ GOLD ( ) P ( ŷ ) ( 22.58 ) Fig . 22.7 shows example predictions model , showing attention weights , Lee et al . ( 2017b ) find correlate traditional semantic heads . Note model gets second example wrong , presumably attendants pilot likely nearby word embeddings . 1 ( fire Bangladeshi garment factory ) left least 37 people dead 100 hospitalized . deceased killed crush workers tried flee ( blaze ) four-story building . fire ( Bangladeshi garment factory ) left least 37 people dead 100 hospitalized . deceased killed crush workers tried flee blaze ( four-story building ) . 2 looking ( region central Italy bordering Adriatic Sea ) . ( area ) mostly mountainous includes Mt . Corno , highest peak Apennines . ( ) includes lot sheep , good clean-living , healthy sheep , Italian entrepreneur idea make little money . 3 ( flight attendants ) 6:00 today ratify labor concessions . ( pilots ’ ) union ground crew yesterday . 4 ( Prince Charles new wife Camilla ) jumped pond touring United States making ( ) first stop today New York . Charles ’ first opportunity showcase new wife , few Americans seem care . Here’s Jeanie Mowth . difference two decades make . ( Charles Diana ) visited JC Penney’s prince’s last official tour . Twenty years later here’s prince new wife . 5 location devices , ( ships ) smoke floats ( ) toss man overboard able smoke signals way trying , let rescuer locate ( ) . Table 4 : Examples predictions development data . row depicts single coreference cluster predicted model . Bold , parenthesized spans indicate mentions predicted cluster . redness word indicates weight head-finding attention mechanism ( ai , t Section 4 ) . 1 2 3 4 5 6 7 8 9 10 10 20 30 40 50 60 70 80 90 100 Span width % Constituency precision Head word precision Frequency Figure 4 : Indirect measure mention precision agreement gold syntax . Constituency precision : % unpruned spans matching syn - tactic constituents . Head word precision : % unpruned constituents whose syntactic head word matches attended word . Frequency : % gold spans width . high , explicit supervision syntactic heads provided . model simply learns clustering data head words useful making coreference decisions . 9.4 Qualitative Analysis qualitative analysis Table 4 highlights strengths weaknesses model . row visualization single coreference cluster predicted model . Bolded spans paren - theses belong predicted cluster , red - ness word indicates weight head - finding attention mechanism ( ai , t Section 4 ) . Strengths effectiveness attention mechanism making coreference decisions seen Example 1 . model pays attention fire span fire Bangladeshi gar - ment factory , allowing successfully predict coreference link blaze . sub - span mention , Bangladeshi garment fac - tory , model pays attention fac - tory , allowing successfully predict corefer - ence link four-story building . task-specific nature attention mecha - nism illustrated Example 4 . model generally pays attention coordinators content coordination , coordinators , , provide strong cues plurality . model capable detecting relatively long complex noun phrases , re - gion central Italy bordering Adriatic Sea Example 2 . appropriately pays atten - Figure 22.7 Sample predictions Lee et al . ( 2017b ) model , cluster per example , showing correct example mistake . Bold , parenthesized spans men - tions predicted cluster . amount red color word indicates head-finding attention weight ai , t ( 22.54 ) . Figure adapted Lee et al . ( 2017b ) . 22.7 Evaluation Coreference Resolution evaluate coreference algorithms model-theoretically , comparing set hypoth - esis chains clusters H produced system against set gold reference chains clusters R human labeling , reporting precision recall . , wide variety methods comparison . fact , 5 common metrics evaluate coreference algorithms : link based MUC ( Vilain et al . , 1995 ) BLANC ( Recasens Hovy 2011 , Luo et al . 2014 ) metrics , mention based B3 metric ( Bagga Baldwin , 1998 ) , entity based CEAF metric ( Luo , 2005 ) , link based entity aware LEA metric ( Moosavi Strube , 2016 ) . 22.8 • ENTITY LINKING 21 just explore two metrics . MUC F-measure ( Vilain et al . , 1995 ) MUCF-measure based number coreference links ( pairs mentions ) common H R . Precision number common links divided number links H . Recall number common links divided number links R ; makes MUC biased toward systems produce large chains ( fewer entities ) , ignores singletons , involve links . B3 mention-based rather link-based . mention referenceB3 chain , compute precision recall , take weighted sum N mentions document compute precision recall entire task . mention , let R reference chain includes , H hypothesis chain . set correct mentions H H ∩ R . Precision mention thus | H ∩ R | H | , recall mention thus | H ∩ R | R | . total precision weighted sum precision mention , weighted weight wi . total recall weighted sum recall mention , weighted weight wi . Equivalently : Precision = N ∑ = 1 wi # correct mentions hypothesis chain containing entityi # mentions hypothesis chain containing entityi Recall = N ∑ = 1 wi # correct mentions hypothesis chain containing entityi # mentions reference chain containing entityi weight wi entity set different values produce different versions algorithm . Following proposal Denis Baldridge ( 2009 ) , CoNLL coreference competitions scored based average MUC , CEAF-e , B3 ( Pradhan et al . 2011 , Pradhan et al . 2012b ) , common many evaluation campaigns report average 3 metrics . Luo Pradhan ( 2016 ) detailed description entire set metrics ; reference implementations rather attempting reimplement scratch ( Pradhan et al . , 2014 ) . Alternative metrics proposed deal particular coreference - mains tasks . example , consider task resolving mentions named entities ( persons , organizations , geopolitical entities ) , might useful - formation extraction knowledge base completion . hypothesis chain cor - rectly contains pronouns referring entity , version name itself , linked wrong name , useful task . might metric weights mention informative ( names informative ) ( Chen Ng , 2013 ) metric considers hypothesis match gold chain contains least variant name ( NEC F1 metric Agarwal et al . ( 2019 ) ) . 22.8 Entity Linking task entity linking ( Ji Grishman , 2011 ) , closely related coreference , entity linking associate mention text representation real-world entity ontology , list entities world , like gazeteer ( Chapter 16 ) . Perhaps common ontology task Wikipedia , Wikipedia page acts unique id particular entity . Thus entity linking task wikification ( Mihalcea Csomai , 2007 ) task deciding Wikipediawikification page corresponding individual referred mention . consider 22 CHAPTER 22 • COREFERENCE RESOLUTION task rest section , Ling et al . ( 2015 ) different linking tasks datasets . earliest systems ( Mihalcea Csomai 2007 , Cucerzan 2007 , Milne Witten 2008 ) , entity linking two stages : mention detection mention disambiguation . useful feature mention detection Mi - halcea Csomai ( 2007 ) called key phrase : mapping Wikipedia anchor texts ( hyperlinked span text associated URL , like Stanfordanchor texts University , Stanford , Governor Stanford ) Wikipedia page title links ( Stanford University , Leland Stanford ) . Prebuilt dictionaries anchor text / title page links available ( Spitkovsky Chang , 2012 ) . Mention detection steps often include various kinds query expansion , example coreference resolution current document . Mention disambiguation often supervised learning Coreference help entity linking , giving possible surface forms help link right Wikipedia page . entity linking direction , improve coreference resolution . Consider example Hajishirzi et al . ( 2013 ) : ( 22.59 ) [ Michael Eisner ] 1 [ Donald Tsang ] 2 announced grand opening [ [ Hong Kong ] 3 Disneyland ] 4 yesterday . [ Eisner ] 1 thanked [ President ] 2 welcomed [ fans ] 5 [ park ] 4 . Integrating entity linking coreference help draw encyclopedic knowl - edge ( like fact Donald Tsang president ) help disambiguate men - tion President . Ponzetto Strube ( 2006 ) ( 2007 ) Ratinov Roth ( 2012 ) showed attributes extracted Wikipedia pages build richer models entity mentions coreference . recent research shows linking coreference jointly ( Hajishirzi et al . 2013 , Zheng et al . 2013 ) even jointly named entity tagging well ( Durrett Klein 2014 ) . 22.9 Winograd Schema problems early field , researchers noted cases coreference quite difficult , seeming require world knowledge sophisticated reasoning solve . problem famously pointed Winograd ( 1972 ) following example : ( 22.60 ) city council denied demonstrators permit . feared violence . b . advocated violence . Winograd noticed antecedent readers preferred pro - noun continuation ( ) city council , ( b ) demonstrators . suggested requires understanding second clause intended explanation first clause , cultural frames suggest city councils perhaps likely demonstrators fear violence demonstrators might likely advocate violence . attempt get field NLP focus methods involving world knowledge common sense reasoning , Levesque ( 2011 ) proposed challenge task called Winograd Schema Challenge . 9 problems challenge taskWinogradschema 22.10 • GENDER BIAS COREFERENCE 23 coreference problems designed easily disambiguated human reader , hopefully solvable simple techniques selectional restrictions , basic word association methods . problems framed pair statements differ single word phrase , coreference question : ( 22.61 ) trophy fit suitcase large . Question : large ? Answer : trophy ( 22.62 ) trophy fit suitcase small . Question : small ? Answer : suitcase problems following characteristics : 1 . problems two parties 2 . pronoun preferentially refers parties , grammatically refer 3 . question asks party pronoun refers 4 . word question changed , human-preferred answer changes party kind world knowledge might needed solve problems vary . trophy / suitcase example , knowledge physical world ; bigger object fit smaller object . original Winograd sentence , stereotypes social actors like politicians protesters . examples like following , knowledge human actions like turn-taking thanking . ( 22.63 ) Bill passed gameboy John turn [ / next ] . Whose turn [ / next ] ? Answers : Bill / John ( 22.64 ) Joan made sure thank Susan help [ / received ] . [ / received ] help ? Answers : Susan / Joan . Although Winograd Schema designed require common-sense rea - soning , large percentage original set problem solved pre - trained language models , fine-tuned Winograd Schema sentences ( Kocijan et al . , 2019 ) . Large pre-trained language models encode enormous amount world common-sense knowledge ! current trend propose new datasets increasingly difficult Winograd-like coreference resolution problems like KNOWREF ( Emami et al . , 2019 ) , examples like : ( 22.65 ) Marcus undoubtedly faster Jarrett right [ ] prime gap big . end , seems likely combination language modeling knowl - edge prove fruitful ; indeed , seems knowledge-based models overfit less lexical idiosyncracies Winograd Schema training sets ( Trichelair et al . , 2018 ) , 22.10 Gender Bias Coreference aspects language processing , coreference models exhibit gender biases ( Zhao et al . 2018 , Rudinger et al . 2018 , Webster et al . 2018 ) . exam - ple WinoBias dataset ( Zhao et al . , 2018 ) variant Winograd Schema 9 Levesque’s call quickly followed up Levesque et al . ( 2012 ) Rahman Ng ( 2012 ) , competition IJCAI conference ( Davis et al . , 2017 ) , natural language inference version problem called WNLI ( Wang et al . , 2018 ) . 24 CHAPTER 22 • COREFERENCE RESOLUTION paradigm test extent coreference algorithms biased toward link - ing gendered pronouns antecedents consistent cultural stereotypes . summarized Chapter 6 , embeddings replicate societal biases training test , associating men historically sterotypical male occupations like doc - tors , women stereotypical female occupations like secretaries ( Caliskan et al . 2017 , Garg et al . 2018 ) . WinoBias sentence contain two mentions corresponding stereotypically - male stereotypically-female occupations gendered pronoun linked . sentence disambiguated gender pronoun , biased model might distracted cue . example sentence : ( 22.66 ) secretary called physiciani told himi new patient [ pro-stereotypical ] ( 22.67 ) secretary called physiciani told heri new patient [ anti-stereotypical ] Zhao et al . ( 2018 ) consider coreference system biased accu - rate linking pronouns consistent gender stereotypical occupations ( e.g . , physician ( 22.66 ) ) linking pronouns inconsistent gender-stereotypical occupations ( e.g . , physician ( 22.67 ) ) . show coreference sys - tems architectures ( rule-based , feature-based machine learned , end-to - end-neural ) show significant bias , performing average 21 F1 points worse anti-stereotypical cases . possible source bias female entities significantly un - derrepresented OntoNotes dataset , train coreference systems . Zhao et al . ( 2018 ) propose way overcome bias : generate second gender-swapped dataset male entities OntoNotes replaced female ones vice versa , retrain coreference systems combined orig - inal swapped OntoNotes data , debiased GloVE embeddings ( Boluk - basi et al . , 2016 ) . resulting coreference systems longer exhibit bias WinoBias dataset , significantly impacting OntoNotes coreference accuracy . follow-up paper , Zhao et al . ( 2019 ) show same biases exist ELMo contextualized word vector representations coref systems . showed retraining ELMo data augmentation again reduces removes bias coreference systems WinoBias . Webster et al . ( 2018 ) introduces another dataset , GAP , task Gendered Pronoun Resolution tool developing improved coreference algorithms gendered pronouns . GAP gender-balanced labeled corpus 4,454 sentences gendered ambiguous pronouns ( contrast , 20 % gendered pro - nouns English OntoNotes training data feminine ) . examples created drawing naturally occurring sentences Wikipedia pages create hard resolve cases two named entities same gender ambiguous pronoun refer person ( neither ) , like following : ( 22.68 ) , Fujisawa joined Mari Motohashi’s rink team’s skip , moving back Karuizawa Kitami spent junior days . Webster et al . ( 2018 ) show modern coreference algorithms perform signif - icantly worse resolving feminine pronouns masculine pronouns GAP . Kurita et al . ( 2019 ) shows system based BERT contextualized word repre - sentations shows similar bias . 22.11 • SUMMARY 25 22.11 Summary chapter introduced task coreference resolution . • task linking together mentions text corefer , i.e . refer same discourse entity discourse model , resulting set coreference chains ( called clusters entities ) . • Mentions definite NPs indefinite NPs , pronouns ( including zero pronouns ) names . • surface form entity mention linked information status ( new , old , inferrable ) , accessible salient entity . • NPs referring expressions , pleonastic raining . • Many corpora human-labeled coreference annotations supervised learning , including OntoNotes English , Chinese , Ara - bic , ARRAU English , AnCora Spanish Catalan . • Mention detection start nouns named entities anaphoricity classifiers referentiality classifiers filter non-mentions . • Three common architectures coreference mention-pair , mention-rank , entity-based , make feature-based neural clas - sifiers . • Modern coreference systems tend end-to-end , performing mention de - tection coreference single end-to-end architecture . • Algorithms learn representations text spans heads , learn com - pare anaphor spans candidate antecedent spans . • Coreference systems evaluated comparing gold entity labels precision / recall metrics like MUC , B3 , CEAF , BLANC , LEA . • Winograd Schema Challenge problems difficult coreference prob - lems seem require world knowledge sophisticated reasoning solve . • Coreference systems exhibit gender bias evaluated datasets like Winobias GAP . Bibliographical Historical Notes Coreference part natural language understanding 1970s ( Woods et al . 1972 , Winograd 1972 ) . discourse model entity-centric foundation coreference formulated Karttunen ( 1969 ) ( 3rd COLING confer - ence ) , playing role linguistic semantics ( Heim 1982 , Kamp 1981 ) . Bonnie Webber’s ( 1978 ) dissertation following work ( Webber 1983 ) explored model’s computational aspects , providing fundamental insights entities represented discourse model ways license subsequent reference . Many examples provided continue chal - lenge theories reference day . Hobbs algorithm10 tree-search algorithm first longHobbsalgorithm series syntax-based methods identifying reference robustly naturally occur - ring text . input Hobbs algorithm pronoun resolved , together 10 simpler two algorithms presented originally Hobbs ( 1978 ) . 26 CHAPTER 22 • COREFERENCE RESOLUTION syntactic ( constituency ) parse sentences up including cur - rent sentence . details algorithm depend grammar , understand simplified version due Kehler et al . ( 2004 ) just searches list NPs current prior sentences . simplified Hobbs algorithm searches NPs following order : “ ( ) current sentence right-to-left , starting first NP left pronoun , ( ii ) previous sentence left-to-right , ( iii ) two sentences prior left-to-right , ( iv ) current sentence left-to-right , starting first noun group right pronoun ( cataphora ) . first noun group agrees pronoun respect number , gender , person chosen antecedent ” ( Kehler et al . , 2004 ) . Lappin Leass ( 1994 ) influential entity-based system weights combine syntactic features , extended soon Kennedy Bogu - raev ( 1996 ) whose system avoids need full syntactic parses . Approximately contemporaneously centering ( Grosz et al . , 1995 ) applied pronominal anaphora resolution Brennan et al . ( 1987 ) , wide variety work followed focused centering’s coreference ( Kameyama 1986 , Di Euge - nio 1990 , Walker et al . 1994 , Di Eugenio 1996 , Strube Hahn 1996 , Kehler 1997 , Tetreault 2001 , Iida et al . 2003 ) . Kehler Rohde ( 2013 ) show centering integrated coherence-driven theories pronoun interpretation . Chap - ter 23 centering measuring discourse coherence . Coreference competitions part DARPA-sponsored MUC confer - ences provided early labeled coreference datasets ( 1995 MUC-6 1998 MUC - 7 corpora ) , set tone later work , choosing focus exclusively simplest cases identity coreference ( ignoring difficult cases like bridging , metonymy , part-whole ) drawing community toward supervised machine learning metrics like MUC metric ( Vilain et al . , 1995 ) . later ACE eval - uations produced labeled coreference corpora English , Chinese , Arabic widely model training evaluation . DARPA work influenced community toward supervised learning - ginning mid-90s ( Connolly et al . 1994 , Aone Bennett 1995 , McCarthy Lehnert 1995 ) . Soon et al . ( 2001 ) laid set basic features , extended Ng Cardie ( 2002b ) , series machine learning models followed next 15 years . often focused separately pronominal anaphora resolu - tion ( Kehler et al . 2004 , Bergsma Lin 2006 ) , full NP coreference ( Cardie Wagstaff 1999 , Ng Cardie 2002b , Ng 2005a ) definite NP reference ( Poesio Vieira 1998 , Vieira Poesio 2000 ) , well separate anaphoricity detection ( Bean Riloff 1999 , Bean Riloff 2004 , Ng Cardie 2002a , Ng 2004 ) , singleton detection ( de Marneffe et al . , 2015 ) . move mention-pair mention-ranking approaches pioneered Yang et al . ( 2003 ) Iida et al . ( 2003 ) proposed pairwise ranking methods , extended Denis Baldridge ( 2008 ) proposed ranking via soft - max prior mentions . idea mention detection , anaphoricity , coreference jointly single end-to-end model grew early proposal Ng ( 2005b ) dummy antecedent mention-ranking , allowing ‘ non-referential ’ choice coreference classifiers , Denis Baldridge’s ( 2007 ) joint sys - tem combining anaphoricity classifier probabilities coreference probabilities , Denis Baldridge ( 2008 ) ranking model , Rahman Ng ( 2009 ) proposal train two models jointly single objective . Simple rule-based systems coreference returned prominence 2010s , EXERCISES 27 partly ability encode entity-based features high-precision way ( Zhou et al . 2004 , Haghighi Klein 2009 , Raghunathan et al . 2010 , Lee et al . 2011 , Lee et al . 2013 , Hajishirzi et al . 2013 ) end suffered inability deal semantics necessary correctly handle cases common noun coreference . return supervised learning led number advances mention-ranking models extended neural architectures , example re - inforcement learning directly optimize coreference evaluation models Clark Manning ( 2016a ) , end-to-end coreference way span extraction ( Lee et al . 2017b , Zhang et al . 2018 ) . Neural models designed take advantage global entity-level information ( Clark Manning 2016b , Wiseman et al . 2016 , Lee et al . 2018 ) . coreference task introduced involves simplifying assumption relationship anaphor antecedent identity : two corefering mentions refer identical discourse referent . real texts , rela - tionship complex , different aspects discourse referent neutralized refocused . example ( 22.69 ) ( Recasens et al . , 2011 ) shows example metonymy , capital city Washington metonymicallymetonymy refer . ( 22.70-22.71 ) show examples ( Recasens et al . , 2011 ) : ( 22.69 ) strict interpretation policy requires U.S . notify foreign dictators certain coup plots . . . Washington rejected bid . . . ( 22.70 ) once crossed border Ashgh-Abad Nowruz , Persian New Year . South , everyone celebrating New Year ; North , regular day . ( 22.71 ) France , president elected term seven years , United States elected term four years . further linguistic discussions complications coreference Puste - jovsky ( 1991 ) , van Deemter Kibble ( 2000 ) , Poesio et al . ( 2006 ) , Fauconnier Turner ( 2008 ) , Versley ( 2008 ) , Barker ( 2010 ) . Ng ( 2017 ) offers useful compact history machine learning models corefer - ence resolution . three excellent book-length surveys anaphora / coreference resolution , covering different time periods : Hirst ( 1981 ) ( early work 1981 ) , Mitkov ( 2002 ) ( 1986-2001 ) , Poesio et al . ( 2016 ) ( 2001-2015 ) . Exercises 28 Chapter 22 • Coreference Resolution Agarwal , O . , Subramanian , S . , Nenkova , . , Roth , D . ( 2019 ) . Evaluation named entity coreference . Work - shop Computational Models Reference , Anaphora Coreference , 1 – 7 . Aone , C . Bennett , S . W . ( 1995 ) . Evaluating automated manual acquisition anaphora resolution strategies . ACL-95 , 122 – 129 . Ariel , M . ( 2001 ) . Accessibility theory : overview . Sanders , T . , Schilperoord , J . , Spooren , W . ( Eds . ) , Text Representation : Linguistic Psycholinguistic Aspects , 29 – 87 . Benjamins . Bagga , . Baldwin , B . ( 1998 ) . Algorithms scoring coreference chains . LREC-98 , 563 – 566 . Barker , C . ( 2010 ) . Nominals provide criteria iden - tity . Rathert , M . Alexiadou , . ( Eds . ) , Se - mantics Nominalizations Languages Frame - works , 9 – 24 . Mouton de Gruyter , Berlin . Bean , D . Riloff , E . ( 1999 ) . Corpus-based identification non-anaphoric noun phrases . ACL-99 , 373 – 380 . Bean , D . Riloff , E . ( 2004 ) . Unsupervised learning contextual role knowledge coreference resolution . HLT-NAACL-04 . Bengtson , E . Roth , D . ( 2008 ) . Understanding value features coreference resolution . EMNLP-08 , 294 – 303 . Bergsma , S . Lin , D . ( 2006 ) . Bootstrapping path-based pronoun resolution . COLING / ACL 2006 , 33 – 40 . Bergsma , S . , Lin , D . , Goebel , R . ( 2008 ) . Distributional identification non-referential pronouns . ACL-08 , 10 – 18 . Björkelund , . Kuhn , J . ( 2014 ) . Learning structured per - ceptrons coreference resolution latent antecedents non-local features . ACL 2014 , 47 – 57 . Bolukbasi , T . , Chang , K . - W . , Zou , J . , Saligrama , V . , Kalai , . T . ( 2016 ) . Man computer programmer woman homemaker ? Debiasing word embeddings . NIPS 16 , 4349 – 4357 . Brennan , S . E . , Friedman , M . W . , Pollard , C . ( 1987 ) . centering approach pronouns . ACL-87 , 155 – 162 . Caliskan , . , Bryson , J . J . , Narayanan , . ( 2017 ) . Se - mantics derived automatically language corpora con - tain human-like biases . Science , 356 ( 6334 ) , 183 – 186 . Cardie , C . Wagstaff , K . ( 1999 ) . Noun phrase coreference clustering . EMNLP / VLC-99 . Chafe , W . L . ( 1976 ) . Givenness , contrastiveness , definite - ness , subjects , topics , point view . Li , C . N . ( Ed . ) , Subject Topic , 25 – 55 . Academic Press . Chang , K . - W . , Samdani , R . , Roth , D . ( 2013 ) . con - strained latent variable model coreference resolution . EMNLP 2013 , 601 – 612 . Chang , K . - W . , Samdani , R . , Rozovskaya , . , Sammons , M . , Roth , D . ( 2012 ) . Illinois-Coref : UI system CoNLL-2012 shared task . CoNLL-12 , 113 – 117 . Chen , C . Ng , V . ( 2013 ) . Linguistically aware corefer - ence evaluation metrics . Sixth International Joint Con - ference Natural Language Processing , 1366 – 1374 . Chomsky , N . ( 1981 ) . Lectures Government Binding . Foris . Clark , K . Manning , C . D . ( 2015 ) . Entity-centric corefer - ence resolution model stacking . ACL 2015 , 1405 – 1415 . Clark , K . Manning , C . D . ( 2016a ) . Deep reinforce - ment learning mention-ranking coreference models . EMNLP 2016 . Clark , K . Manning , C . D . ( 2016b ) . Improving corefer - ence resolution learning entity-level distributed repre - sentations . ACL 2016 . Connolly , D . , Burger , J . D . , Day , D . S . ( 1994 ) . ma - chine learning approach anaphoric reference . Pro - ceedings International Conference New Methods Language Processing ( NeMLaP ) . Cucerzan , S . ( 2007 ) . Large-scale named entity disambigua - tion based Wikipedia data . EMNLP / CoNLL 2007 , 708 – 716 . Davis , E . , Morgenstern , L . , Ortiz , C . L . ( 2017 ) . first Winograd schema challenge IJCAI-16 . AI Magazine , 38 ( 3 ) , 97 – 98 . de Marneffe , M . - C . , Recasens , M . , Potts , C . ( 2015 ) . Modeling lifespan discourse entities application coreference resolution . JAIR , 52 , 445 – 475 . Denis , P . Baldridge , J . ( 2007 ) . Joint determination anaphoricity coreference resolution integer pro - gramming . NAACL-HLT 07 . Denis , P . Baldridge , J . ( 2008 ) . Specialized models ranking coreference resolution . EMNLP-08 , 660 – 669 . Denis , P . Baldridge , J . ( 2009 ) . Global joint models coreference resolution named entity classification . Procesamiento del Lenguaje Natural , 42 . Di Eugenio , B . ( 1990 ) . Centering theory Italian pronominal system . COLING-90 , 270 – 275 . Di Eugenio , B . ( 1996 ) . discourse functions Italian subjects : centering approach . COLING-96 , 352 – 357 . Durrett , G . Klein , D . ( 2013 ) . Easy victories uphill battles coreference resolution . EMNLP 2013 . Durrett , G . Klein , D . ( 2014 ) . joint model entity analysis : Coreference , typing , linking . TACL , 2 , 477 – 490 . Emami , . , Trichelair , P . , Trischler , . , Suleman , K . , Schulz , H . , Cheung , J . C . K . ( 2019 ) . KNOWREF corefer - ence corpus : Removing gender number cues diffi - cult pronominal anaphora resolution . ACL 2019 . Fauconnier , G . Turner , M . ( 2008 ) . way think : Conceptual blending mind’s hidden complexities . Basic Books . Fernandes , E . R . , dos Santos , C . N . , Milidiú , R . L . ( 2012 ) . Latent structure perceptron feature induction unrestricted coreference resolution . CoNLL-12 , 41 – 48 . Fox , B . . ( 1993 ) . Discourse Structure Anaphora : Writ - ten Conversational English . Cambridge . Garg , N . , Schiebinger , L . , Jurafsky , D . , Zou , J . ( 2018 ) . Word embeddings quantify 100 years gender eth - nic stereotypes . Proceedings National Academy Sciences , 115 ( 16 ) , E3635 – E3644 . Grosz , B . J . ( 1977 ) . Representation Focus Dialogue Understanding . Ph . D . thesis , University Cali - fornia , Berkeley . Exercises 29 Grosz , B . J . , Joshi , . K . , Weinstein , S . ( 1995 ) . Cen - tering : framework modeling local coherence discourse . Computational Linguistics , 21 ( 2 ) , 203 – 225 . Gundel , J . K . , Hedberg , N . , Zacharski , R . ( 1993 ) . Cog - nitive status form referring expressions dis - course . Language , 69 ( 2 ) , 274 – 307 . Haghighi , . Klein , D . ( 2009 ) . Simple coreference resolution rich syntactic semantic features . EMNLP-09 , 1152 – 1161 . Hajishirzi , H . , Zilles , L . , Weld , D . S . , Zettlemoyer , L . ( 2013 ) . Joint coreference resolution named-entity link - ing multi-pass sieves . EMNLP 2013 , 289 – 299 . Haviland , S . E . Clark , H . H . ( 1974 ) . new ? Acquiring new information process comprehen - sion . Journal Verbal Learning Verbal Behaviour , 13 , 512 – 521 . Hawkins , J . . ( 1978 ) . Definiteness indefiniteness : study reference grammaticality prediction . Croom Helm Ltd . Heim , . ( 1982 ) . semantics definite indefinite noun phrases . Ph . D . thesis , University Massachusetts Amherst . Hirst , G . ( 1981 ) . Anaphora Natural Language - standing : survey . . 119 Lecture notes computer science . Springer-Verlag . Hobbs , J . R . ( 1978 ) . Resolving pronoun references . Lingua , 44 , 311 – 338 . Reprinted Grosz et al . ( 1986 ) . Hou , Y . , Markert , K . , Strube , M . ( 2018 ) . Unrestricted bridging resolution . Computational Linguistics , 44 ( 2 ) , 237 – 284 . Iida , R . , Inui , K . , Takamura , H . , Matsumoto , Y . ( 2003 ) . Incorporating contextual cues trainable models coref - erence resolution . EACL Workshop Computa - tional Treatment Anaphora . Ji , H . Grishman , R . ( 2011 ) . Knowledge base popula - tion : Successful approaches challenges . ACL 2011 , 1148 – 1158 . Joshi , M . , Levy , O . , Weld , D . S . , Zettlemoyer , L . ( 2019 ) . BERT coreference resolution : Baselines analysis . EMNLP 2019 . Kameyama , M . ( 1986 ) . property-sharing constraint cen - tering . ACL-86 , 200 – 206 . Kamp , H . ( 1981 ) . theory truth semantic represen - tation . Groenendijk , J . , Janssen , T . , Stokhof , M . ( Eds . ) , Formal Methods Study Language , 189 – 222 . Mathematical Centre , Amsterdam . Karttunen , L . ( 1969 ) . Discourse referents . COLING-69 . Preprint . 70 . Kehler , . ( 1997 ) . Current theories centering pronoun interpretation : critical evaluation . Computational Lin - guistics , 23 ( 3 ) , 467 – 475 . Kehler , . , Appelt , D . E . , Taylor , L . , Simma , . ( 2004 ) . ( non ) utility predicate-argument frequencies pro - noun interpretation . HLT-NAACL-04 . Kehler , . Rohde , H . ( 2013 ) . probabilistic reconcili - ation coherence-driven centering-driven theories pronoun interpretation . Theoretical Linguistics , 39 ( 1-2 ) , 1 – 37 . Kennedy , C . Boguraev , B . K . ( 1996 ) . Anaphora ev - eryone : Pronominal anaphora resolution parser . COLING-96 , 113 – 118 . Kocijan , V . , Cretu , . - M . , Camburu , O . - M . , Yordanov , Y . , Lukasiewicz , T . ( 2019 ) . surprisingly robust trick Winograd Schema Challenge . ACL 2019 . Kummerfeld , J . K . Klein , D . ( 2013 ) . Error-driven anal - ysis challenges coreference resolution . EMNLP 2013 , 265 – 277 . Kurita , K . , Vyas , N . , Pareek , . , Black , . W . , Tsvetkov , Y . ( 2019 ) . Quantifying social biases contextual word representations . 1st ACL Workshop Gender Bias Natural Language Processing . Lappin , S . Leass , H . ( 1994 ) . algorithm pronomi - nal anaphora resolution . Computational Linguistics , 20 ( 4 ) , 535 – 561 . Lee , H . , Chang , . , Peirsman , Y . , Chambers , N . , Surdeanu , M . , Jurafsky , D . ( 2013 ) . Deterministic coreference resolution based entity-centric , precision-ranked rules . Computational Linguistics , 39 ( 4 ) , 885 – 916 . Lee , H . , Peirsman , Y . , Chang , . , Chambers , N . , Surdeanu , M . , Jurafsky , D . ( 2011 ) . Stanford’s multi-pass sieve coreference resolution system CoNLL-2011 shared task . CoNLL-11 , 28 – 34 . Lee , H . , Surdeanu , M . , Jurafsky , D . ( 2017a ) . scaffold - ing approach coreference resolution integrating statisti - cal rule-based models . Natural Language Engineering , 23 ( 5 ) , 733 – 762 . Lee , K . , , L . , Lewis , M . , Zettlemoyer , L . ( 2017b ) . End-to-end neural coreference resolution . EMNLP 2017 . Lee , K . , , L . , Zettlemoyer , L . ( 2018 ) . Higher-order coreference resolution coarse-to-fine inference . NAACL HLT 2018 . Levesque , H . ( 2011 ) . Winograd Schema Challenge . Logical Formalizations Commonsense Reasoning — Pa - pers AAAI 2011 Spring Symposium ( SS-11-06 ) . Levesque , H . , Davis , E . , Morgenstern , L . ( 2012 ) . Winograd Schema Challenge . Thirteenth International Conference Principles Knowledge Representation Reasoning . Ling , X . , Singh , S . , Weld , D . S . ( 2015 ) . Design chal - lenges entity linking . TACL , 3 , 315 – 328 . Luo , X . ( 2005 ) . coreference resolution performance met - rics . Proceedings HLT-EMNLP 2005 , 25 – 32 . Luo , X . Pradhan , S . ( 2016 ) . Evaluation metrics . Poe - sio , M . , Stuckardt , R . , Versley , Y . ( Eds . ) , Anaphora resolution : Algorithms , resources , applications , 141 – 163 . Springer . Luo , X . , Pradhan , S . , Recasens , M . , Hovy , E . H . ( 2014 ) . extension blanc system mentions . ACL 2014 , Vol . 2014 , p . 24 . Martschat , S . Strube , M . ( 2014 ) . Recall error analysis coreference resolution . EMNLP 2014 , 2070 – 2081 . Martschat , S . Strube , M . ( 2015 ) . Latent structures coreference resolution . TACL , 3 , 405 – 418 . McCarthy , J . F . Lehnert , W . G . ( 1995 ) . decision trees coreference resolution . IJCAI-95 , 1050 – 1055 . Mihalcea , R . Csomai , . ( 2007 ) . Wikify ! : linking doc - uments encyclopedic knowledge . CIKM 2007 , 233 – 242 . 30 Chapter 22 • Coreference Resolution Milne , D . Witten , . H . ( 2008 ) . Learning link wikipedia . CIKM 2008 , 509 – 518 . Mitkov , R . ( 2002 ) . Anaphora Resolution . Longman . Moosavi , N . S . Strube , M . ( 2016 ) . coreference evaluation metric trust ? proposal link-based entity aware metric . ACL 2016 , 632 – 642 . Ng , V . ( 2004 ) . Learning noun phrase anaphoricity - prove coreference resolution : Issues representation optimization . ACL-04 . Ng , V . ( 2005a ) . Machine learning coreference resolution : local classification global ranking . ACL-05 . Ng , V . ( 2005b ) . Supervised ranking pronoun resolution : recent improvements . AAAI-05 . Ng , V . ( 2010 ) . Supervised noun phrase coreference research : first fifteen years . ACL 2010 , 1396 – 1411 . Ng , V . ( 2017 ) . Machine learning entity coreference reso - lution : retrospective look two decades research . AAAI-17 . Ng , V . Cardie , C . ( 2002a ) . Identifying anaphoric non-anaphoric noun phrases improve coreference reso - lution . COLING-02 . Ng , V . Cardie , C . ( 2002b ) . Improving machine learning approaches coreference resolution . ACL-02 . Nissim , M . , Dingare , S . , Carletta , J . , Steedman , M . ( 2004 ) . annotation scheme information status dialogue . LREC-04 . Poesio , M . , Stuckardt , R . , Versley , Y . ( 2016 ) . Anaphora resolution : Algorithms , resources , applications . Springer . Poesio , M . , Sturt , P . , Artstein , R . , Filik , R . ( 2006 ) . Un - derspecification anaphora : Theoretical issues pre - liminary evidence . Discourse processes , 42 ( 2 ) , 157 – 175 . Poesio , M . Vieira , R . ( 1998 ) . corpus-based investiga - tion definite description . Computational Linguistics , 24 ( 2 ) , 183 – 216 . Ponzetto , S . P . Strube , M . ( 2006 ) . Exploiting seman - tic role labeling , WordNet Wikipedia coreference resolution . HLT-NAACL-06 , 192 – 199 . Ponzetto , S . P . Strube , M . ( 2007 ) . Knowledge derived Wikipedia computing semantic relatedness . JAIR , 30 , 181 – 212 . Pradhan , S . , Hovy , E . H . , Marcus , M . P . , Palmer , M . , Ramshaw , L . , Weischedel , R . ( 2007 ) . OntoNotes : unified relational semantic representation . Proceedings ICSC , 517 – 526 . Pradhan , S . , Luo , X . , Recasens , M . , Hovy , E . H . , Ng , V . , Strube , M . ( 2014 ) . Scoring coreference partitions predicted mentions : reference implementation . ACL 2014 . Pradhan , S . , Moschitti , . , Xue , N . , Uryupina , O . , Zhang , Y . ( 2012a ) . CoNLL-2012 shared task : Model - ing multilingual unrestricted coreference OntoNotes . CoNLL-12 . Pradhan , S . , Moschitti , . , Xue , N . , Uryupina , O . , Zhang , Y . ( 2012b ) . Conll-2012 shared task : Modeling mul - tilingual unrestricted coreference OntoNotes . CoNLL - 12 . Pradhan , S . , Ramshaw , L . , Marcus , M . P . , Palmer , M . , Weischedel , R . , Xue , N . ( 2011 ) . CoNLL-2011 shared task : Modeling unrestricted coreference OntoNotes . CoNLL-11 . Pradhan , S . , Ramshaw , L . , Weischedel , R . , MacBride , J . , Micciulla , L . ( 2007 ) . Unrestricted coreference : Identify - ing entities events OntoNotes . Proceedings ICSC 2007 , 446 – 453 . Prince , E . ( 1981a ) . Toward taxonomy given-new infor - mation . Cole , P . ( Ed . ) , Radical Pragmatics , 223 – 256 . Academic Press . Prince , E . ( 1981b ) . Toward taxonomy given-new infor - mation . Cole , P . ( Ed . ) , Radical Pragmatics , 223 – 255 . Academic Press . Pustejovsky , J . ( 1991 ) . generative lexicon . Computa - tional Linguistics , 17 ( 4 ) . Raghunathan , K . , Lee , H . , Rangarajan , S . , Chambers , N . , Surdeanu , M . , Jurafsky , D . , Manning , C . D . ( 2010 ) . multi-pass sieve coreference resolution . Proceedings EMNLP 2010 , 492 – 501 . Rahman , . Ng , V . ( 2009 ) . Supervised models coref - erence resolution . EMNLP-09 , 968 – 977 . Rahman , . Ng , V . ( 2012 ) . Resolving complex cases definite pronouns : Winograd Schema challenge . EMNLP 2012 , 777 – 789 . Ratinov , L . Roth , D . ( 2012 ) . Learning-based multi-sieve co-reference resolution knowledge . EMNLP 2012 , 1234 – 1244 . Recasens , M . Hovy , E . H . ( 2011 ) . BLANC : Implement - ing Rand index coreference evaluation . Natural Language Engineering , 17 ( 4 ) , 485 – 510 . Recasens , M . , Hovy , E . H . , Martı́ , M . . ( 2011 ) . Identity , non-identity , near-identity : Addressing complexity coreference . Lingua , 121 ( 6 ) , 1138 – 1152 . Recasens , M . Martı́ , M . . ( 2010 ) . AnCora-CO : Coref - erentially annotated corpora Spanish Catalan . Lan - guage Resources Evaluation , 44 ( 4 ) , 315 – 345 . Reichman , R . ( 1985 ) . Getting Computers Talk Like . MIT Press . Rudinger , R . , Naradowsky , J . , Leonard , B . , Van Durme , B . ( 2018 ) . Gender bias coreference resolution . NAACL HLT 2018 . Schiebinger , L . ( 2019 ) . Machine translation : Analyzing gen - der . http://genderedinnovations.stanford.edu/ case-studies / nlp . html #tabs - 2 . Soon , W . M . , Ng , H . T . , Lim , D . C . Y . ( 2001 ) . ma - chine learning approach coreference resolution noun phrases . Computational Linguistics , 27 ( 4 ) , 521 – 544 . Spitkovsky , V . . Chang , . X . ( 2012 ) . cross-lingual dictionary English Wikipedia concepts . LREC-12 . Strube , M . Hahn , U . ( 1996 ) . Functional centering . ACL-96 , 270 – 277 . Tetreault , J . R . ( 2001 ) . corpus-based evaluation cen - tering pronoun resolution . Computational Linguistics , 27 ( 4 ) , 507 – 520 . Trichelair , P . , Emami , . , Cheung , J . C . K . , Trischler , . , Suleman , K . , Diaz , F . ( 2018 ) . evaluation common-sense reasoning natural language understand - ing . NeurIPS 2018 Workshop Critiquing Cor - recting Trends Machine Learning . Exercises 31 Uryupina , O . , Artstein , R . , Bristot , . , Cavicchio , F . , Del - ogu , F . , Rodriguez , K . J . , Poesio , M . ( 2019 ) . Annotat - ing broad range anaphoric phenomena , variety genres : ARRAU corpus . Natural Language Engineer - ing , 1 – 34 . van Deemter , K . Kibble , R . ( 2000 ) . coreferring : coreference MUC related annotation schemes . Com - putational Linguistics , 26 ( 4 ) , 629 – 637 . Versley , Y . ( 2008 ) . Vagueness referential ambiguity large-scale annotated corpus . Research Language Computation , 6 ( 3-4 ) , 333 – 353 . Vieira , R . Poesio , M . ( 2000 ) . empirically based sys - tem processing definite descriptions . Computational Linguistics , 26 ( 4 ) , 539 – 593 . Vilain , M . , Burger , J . D . , Aberdeen , J . , Connolly , D . , Hirschman , L . ( 1995 ) . model-theoretic coreference scor - ing scheme . MUC-6 . Walker , M . . , Iida , M . , Cote , S . ( 1994 ) . Japanese dis - course process centering . Computational Lin - guistics , 20 ( 2 ) , 193 – 232 . Wang , . , Singh , . , Michael , J . , Hill , F . , Levy , O . , Bow - man , S . R . ( 2018 ) . Glue : multi-task benchmark anal - ysis platform natural language understanding . ICLR 2017 . Webber , B . L . ( 1978 ) . Formal Approach Discourse Anaphora . Ph . D . thesis , Harvard University . Webber , B . L . ( 1983 ) . talk ? . Brady , M . Berwick , R . C . ( Eds . ) , Computational Mod - els Discourse , 331 – 371 . MIT Press . Reprinted Grosz et al . ( 1986 ) . Webber , B . L . ( 1991 ) . Structure ostension inter - pretation discourse deixis . Language Cognitive Pro - cesses , 6 ( 2 ) , 107 – 135 . Webber , B . L . Baldwin , B . ( 1992 ) . Accommodating con - text change . ACL-92 , 96 – 103 . Webber , B . L . ( 1988 ) . Discourse deixis : Reference dis - course segments . ACL-88 , 113 – 122 . Webster , K . , Recasens , M . , Axelrod , V . , Baldridge , J . ( 2018 ) . Mind gap : balanced corpus gendered - biguous pronouns . TACL , 6 , 605 – 617 . Winograd , T . ( 1972 ) . Understanding Natural Language . Academic Press . Wiseman , S . , Rush , . M . , Shieber , S . M . ( 2016 ) . Learn - ing global features coreference resolution . NAACL HLT 2016 . Wiseman , S . , Rush , . M . , Shieber , S . M . , Weston , J . ( 2015 ) . Learning anaphoricity antecedent ranking fea - tures coreference resolution . ACL 2015 , 1416 – 1426 . Woods , W . . , Kaplan , R . M . , Nash-Webber , B . L . ( 1972 ) . lunar sciences natural language information system : Final report . Tech . rep . 2378 , BBN . Yang , X . , Zhou , G . , Su , J . , Tan , C . L . ( 2003 ) . Coref - erence resolution competition learning approach . ACL-03 , 176 – 183 . Zhang , R . , dos Santos , C . N . , Yasunaga , M . , Xiang , B . , Radev , D . ( 2018 ) . Neural coreference resolution deep biaffine attention joint mention detection mention clustering . ACL 2018 , 102 – 107 . Zhao , J . , Wang , T . , Yatskar , M . , Cotterell , R . , Ordonez , V . , Chang , K . - W . ( 2019 ) . Gender bias contextualized word embeddings . NAACL HLT 2019 , 629 – 634 . Zhao , J . , Wang , T . , Yatskar , M . , Ordonez , V . , Chang , K . - W . ( 2018 ) . Gender bias coreference resolution : Eval - uation debiasing methods . NAACL HLT 2018 . Zheng , J . , Vilnis , L . , Singh , S . , Choi , J . D . , McCallum , . ( 2013 ) . Dynamic knowledge-base alignment coref - erence resolution . CoNLL-13 , 153 – 162 . Zhou , L . , Ticrea , M . , Hovy , E . H . ( 2004 ) . Multi - document biography summarization . EMNLP 2004 .