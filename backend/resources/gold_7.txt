9 . Convolutional Layers Sometimes we are interested in making predictions based on ordered sets of items ( e.g . the sequence of words in a sentence , the sequence of sentences in a document and so on ) . Consider for example predicting the sentiment ( positive , negative or neutral ) of a sentence . Some of the sentence words are very informative of the sentiment , other words are less informative , and to a good approximation , an informative clue is informative regardless of its position in the sentence . We would like to feed all of the sentence words into a learner , and let the training process figure out the important clues . One possible solution is feeding a CBOW representation into a fully connected network such as an MLP . However , a downside of the CBOW approach is that it ignores the ordering information completely , assigning the sentences “ it was not good , it was actually quite bad ” and “ it was not bad , it was actually quite good ” the exact same representation . While the global position of the indicators “ not good ” and “ not bad ” does not matter for the classification task , the local ordering of the words ( that the word “ not ” appears right before the word “ bad ” ) is very important . A naive approach would suggest embedding word-pairs ( bi-grams ) rather than words , and building a CBOW over the embedded bigrams . While such architecture could be effective , it will result in huge embedding matrices , will not scale for longer n - grams , and will suffer from data sparsity problems as it does not share statistical strength between different n-grams ( the embedding of “ quite good ” and “ very good ” are completely independent of one another , so if the learner saw only one of them during training , it will not be able to deduce anything about the other based on its component words ) . The convolution-and-pooling ( also called convolutional neural networks , or CNNs ) architecture is an elegant and robust solution to this modeling problem . A convolutional neural network is designed to identify indicative local predictors in a large structure , and combine them to produce a fixed size vector representation of the structure , capturing these local aspects that are most informative for the prediction task at hand . Convolution-and-pooling architectures ( LeCun & Bengio , 1995 ) evolved in the neural networks vision community , where they showed great success as object detectors – recog - nizing an object from a predefined category ( “ cat ” , “ bicycles ” ) regardless of its position in the image ( Krizhevsky et al . , 2012 ) . When applied to images , the architecture is using 2 - dimensional ( grid ) convolutions . When applied to text , NLP we are mainly concerned with 1-d ( sequence ) convolutions . Convolutional networks were introduced to the NLP commu - nity in the pioneering work of Collobert , Weston and Colleagues ( 2011 ) who used them for semantic-role labeling , and later by Kalchbrenner et al ( 2014 ) and Kim ( Kim , 2014 ) who used them for sentiment and question-type classification . 9.1 Basic Convolution + Pooling The main idea behind a convolution and pooling architecture for language tasks is to apply a non-linear ( learned ) function over each instantiation of a k-word sliding window over the sentence . This function ( also called “ filter ” ) transforms a window of k words into a d dimensional vector that captures important properties of the words in the window ( each dimension is sometimes referred to in the literature as a “ channel ” ) . Then , a “ pooling ” operation is used combine the vectors resulting from the different windows into a single d-dimensional vector , by taking the max or the average value observed in each of the d 42 channels over the different windows . The intention is to focus on the most important “ features ” in the sentence , regardless of their location . The d-dimensional vector is then fed further into a network that is used for prediction . The gradients that are propagated back from the network’s loss during the training process are used to tune the parameters of the filter function to highlight the aspects of the data that are important for the task the network is trained for . Intuitively , when the sliding window is run over a sequence , the filter function learns to identify informative k-grams . More formally , consider a sequence of words x = x1 , . . . , xn , each with their correspond - ing demb dimensional word embedding v ( xi ) . A 1d convolution layer 24 of width k works by moving a sliding window of size k over the sentence , and applying the same “ filter ” to each window in the sequence ( v ( xi ) ; v ( xi + 1 ) ; . . . ; v ( xi + k − 1 ) ) . The filter function is usually a linear transformation followed by a non-linear activation function . Let the concatenated vector of the ith window be wi = v ( xi ) ; v ( xi + 1 ) ; . . . ; v ( xi + k − 1 ) , wi ∈ Rkdemb . Depending on whether we pad the sentence with k − 1 words to each side , we may get either m = n − k + 1 ( narrow convolution ) or m = n + k + 1 windows ( wide convolution ) ( Kalchbrenner et al . , 2014 ) . The result of the convolution layer is m vectors p1 , . . . , pm , pi ∈ Rdconv where : pi = g ( wiW + b ) . g is a non-linear activation function that is applied element-wise , W ∈ Rk·demb × dconv and b ∈ Rdconv are parameters of the network . Each pi is a dconv dimensional vector , encoding the information in wi . Ideally , each dimension captures a different kind of indicative infor - mation . The m vectors are then combined using a max pooling layer , resulting in a single dconv dimensional vector c . cj = max 1 < i ≤ m pi [ j ] pi [ j ] denotes the jth component of pi . The effect of the max-pooling operation is to get the most salient information across window positions . Ideally , each dimension will “ specialize ” in a particular sort of predictors , and max operation will pick on the most important predictor of each type . Figure 4 provides an illustration of the process . The resulting vector c is a representation of the sentence in which each dimension reflects the most salient information with respect to some prediction task . c is then fed into a downstream network layers , perhaps in parallel to other vectors , culminating in an output layer which is used for prediction . The training procedure of the network calculates the loss with respect to the prediction task , and the error gradients are propagated all the way back through the pooling and convolution layers , as well as the embedding layers . 25 24 . 1d here refers to a convolution operating over 1-dimensional inputs such as sequences , as opposed to 2d convolutions which are applied to images . 25 . Besides being useful for prediction , a by-product of the training procedure is a set of parameters W , B and embeddings v ( ) that can be used in a convolution and pooling architecture to encode arbitrary length sentences into fixed-size vectors , such that sentences that share the same kind of predictive information will be close to each other . 43 the quick brown quick brown fox brown fox jumped fox jumped over jumped over the over the lazy the lazy dog MUL + tanh MUL + tanh MUL + tanh MUL + tanh MUL + tanh MUL + tanh MUL + tanh W 6 × 3 the quick brown fox jumped over the lazy dog max convolution pooling Figure 4 : 1d convolution + pooling over the sentence “ the quick brown fox jumped over the lazy dog ” . This is a narrow convolution ( no padding is added to the sentence ) with a window size of 3 . Each word is translated to a 2-dim embedding vector ( not shown ) . The embedding vectors are then concatenated , resulting in 6-dim window representations . Each of the seven windows is transfered through a 6 × 3 filter ( linear transformation followed by element-wise tanh ) , resulting in seven 3-dimensional filtered representations . Then , a max-pooling operation is applied , taking the max over each dimension , resulting in a final 3-dimensional pooled vector . While max-pooling is the most common pooling operation in text applications , other pooling operations are also possible , the second most common operation being average pooling , taking the average value of each index instead of the max . 9.2 Dynamic , Hierarchical and k-max Pooling Rather than performing a single pooling operation over the entire sequence , we may want to retain some positional information based on our domain understanding of the prediction problem at hand . To this end , we can split the vectors pi into ` distinct groups , apply the pooling separately on each group , and then concatenate the ` resulting dconv vectors c1 , . . . , c ` . The division of the pis into groups is performed based on domain knowledge . For example , we may conjecture that words appearing early in the sentence are more indicative than words appearing late . We can then split the sequence into ` equally sized regions , applying a separate max-pooling to each region . For example , Johnson and Zhang ( Johnson & Zhang , 2014 ) found that when classifying documents into topics , it is useful to have 20 average-pooling regions , clearly separating the initial sentences ( where the topic is usually introduced ) from later ones , while for a sentiment classification task a single max-pooling operation over the entire sentence was optimal ( suggesting that one or two very strong signals are enough to determine the sentiment , regardless of the position in the sentence ) . 44 Similarly , in a relation extraction kind of task we may be given two words and asked to determine the relation between them . We could argue that the words before the first word , the words after the second word , and the words between them provide three different kinds of information ( Chen et al . , 2015 ) . We can thus split the pi vectors accordingly , pooling separately the windows resulting from each group . Another variation is performing hierarchical pooling , in which we have a succession of convolution and pooling layers , where each stage applies a convolution to a sequence , pools every k neighboring vectors , performs a convolution on the resulting pooled sequence , applies another convolution and so on . This architecture allows sensitivity to increasingly larger structures . Finally , ( Kalchbrenner et al . , 2014 ) introduced a k-max pooling operation , in which the top k values in each dimension are retained instead of only the best one , while preserving the order in which they appeared in the text . For example a , consider the following matrix :       1 2 3 9 6 5 2 3 1 7 8 1 3 4 1       A 1-max pooling over the column vectors will result in [ 9 8 5 ] , while a 2-max pooling will result in the following matrix : [ 9 6 3 7 8 5 ] whose rows will then be concatenated to [ 9 6 3 7 8 5 ] The k-max pooling operation makes it possible to pool the k most active indicators that may be a number of positions apart ; it preserves the order of the features , but is insensitive to their specific positions . It can also discern more finely the number of times the feature is highly activated ( Kalchbrenner et al . , 2014 ) . 9.3 Variations Rather than a single convolutional layer , several convolutional layers may be applied in parallel . For example , we may have four different convolutional layers , each with a different window size in the range 2 – 5 , capturing n-gram sequences of varying lengths . The result of each convolutional layer will then be pooled , and the resulting vectors concatenated and fed to further processing ( Kim , 2014 ) . The convolutional architecture need not be restricted into the linear ordering of a sen - tence . For example , Ma et al ( 2015 ) generalize the convolution operation to work over syntactic dependency trees . There , each window is around a node in the syntactic tree , and the pooling is performed over the different nodes . Similarly , Liu et al ( 2015 ) apply a convolutional architecture on top of dependency paths extracted from dependency trees . Le and Zuidema ( 2015 ) propose to perform max pooling over vectors representing the different derivations leading to the same chart item in a chart parser . 45