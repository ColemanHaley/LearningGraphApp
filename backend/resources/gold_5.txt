7 . Cascading and Multi-task Learning The combination of online training methods with automatic gradient computations using the computation graph abstraction allows for an easy implementation of model cascading , parameter sharing and multi-task learning . Model cascading is a powerful technique in which large networks are built by composing them out of smaller component networks . For example , we may have a feed-forward network for predicting the part of speech of a word based on its neighbouring words and / or the characters that compose it . In a pipeline approach , we would use this network for predicting parts of speech , and then feed the predictions as input features to neural network that does syntactic chunking or parsing . Instead , we could think of the hidden layers of this network as an encoding that captures the relevant information for predicting the part of speech . In a cascading approach , we take the hidden layers of this network and connect them ( and not the part of speech prediction themselves ) as the inputs for the syntactic network . We now have a larger network that takes as input sequences of words and characters , and outputs a syntactic structure . The computation graph abstraction allows us to easily propagate the error gradients from the syntactic task loss all the way back to the characters . To combat the vanishing gradient problem of deep networks , as well as to make better use of available training material , the individual component network’s parameters can be bootstrapped by training them separately on a relevant task , before plugging them in to the larger network for further tuning . For example , the part-of-speech predicting network can be trained to accurately predict parts-of-speech on a relatively large annotated corpus , before plugging its hidden layer into the syntactic parsing network for which less training data is available . In case the training data provide direct supervision for both tasks , we can make use of it during training by creating a network with two outputs , one for each task , computing a separate loss for each output , and then summing the losses into a single node from which we backpropagate the error gradients . Model cascading is very common when using convolutional , recursive and recurrent neural networks , where , for example , a recurrent network is used to encode a sentence into a fixed sized vector , which is then used as the input of another network . The supervision signal of the recurrent network comes primarily from the upper network that consumes the recurrent network’s output as it inputs . Multi-task learning is used when we have related prediction tasks that do not neces - sarily feed into one another , but we do believe that information that is useful for one type of prediction can be useful also to some of the other tasks . For example , chunking , named entity recognition ( NER ) and language modeling are examples of synergistic tasks . Infor - mation for predicting chunk boundaries , named-entity boundaries and the next word in the sentence all rely on some shared underlying syntactic-semantic representation . Instead of training a separate network for each task , we can create a single network with several out - puts . A common approach is to have a multi-layer feed-forward network , whose final hidden layer ( or a concatenation of all hidden layers ) is then passed to different output layers . This way , most of the parameters of the network are shared between the different tasks . Useful information learned from one task can then help to disambiguate other tasks . Again , the computation graph abstraction makes it very easy to construct such networks and compute 36 the gradients for them , by computing a separate loss for each available supervision signal , and then summing the losses into a single loss that is used for computing the gradients . In case we have several corpora , each with different kind of supervision signal ( e.g . we have one corpus for NER and another for chunking ) , the training procedure will shuffle all of the available training example , performing gradient computation and updates with respect to a different loss in every turn . Multi-task learning in the context of language-processing is introduced and discussed in ( Collobert et al . , 2011 ) . 37