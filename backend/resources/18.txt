Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 18 Information Extraction model modern Major-General , information vegetable , animal , mineral , know kings England , quote fights historical Marathon Waterloo , order categorical . . . Gilbert Sullivan , Pirates Penzance Imagine analyst investment firm tracks airline stocks . task determining relationship ( ) airline - nouncements fare increases behavior stocks next day . - torical data stock prices easy come , airline - nouncements ? need know least name airline , nature proposed fare hike , dates announcement , possibly response airlines . Fortunately , found news articles like : Citing high fuel prices , United Airlines Friday increased fares $ 6 per round trip flights cities served lower - cost carriers . American Airlines , unit AMR Corp . , immediately matched move , spokesman Tim Wagner . United , unit UAL Corp . , increase took effect Thursday applies routes competes against discount carriers , Chicago Dallas Denver San Francisco . chapter presents techniques extracting limited kinds semantic con - tent text . process information extraction ( IE ) , turns unstructuredinformationextraction information embedded texts structured data , example populating relational database enable further processing . begin first step IE tasks , finding proper names named entities text . task named entity recognition ( NER ) find eachnamed entityrecognition mention named entity text label type . constitutes named entity type task specific ; people , places , organizations common , gene protein names ( Cohen Demner-Fushman , 2014 ) financial asset classes might relevant tasks . Once named entities text extracted , linked together sets corresponding real-world entities , inferring , example , mentions United Airlines United refer same company . joint task coreference resolution entity linking defer til Chapter 22 . Next , turn task relation extraction : finding classifying semanticrelationextraction relations among text entities . often binary relations like child-of , em - ployment , part-whole , geospatial relations . Relation extraction close links populating relational database . Finally , discuss three tasks related events . Event extraction findingeventextraction events entities participate , like , sample text , fare increases 2 CHAPTER 18 • INFORMATION EXTRACTION United American reporting events cite . Event coreference ( Chapter 22 ) needed figure event mentions text refer same event ; running example two instances increase phrase move refer same event . figure events text happened extract temporal expres - sions like days week ( Friday Thursday ) , relative expressions like twotemporalexpression days next year times 3:30 P.M . . expressions normalized onto specific calendar dates times day situate events time . Intemporalnormalization sample task , allow link Friday time United’s announce - ment , Thursday previous day’s fare increase , produce timeline United’s announcement follows fare increase American’s announce - ment follows events . Finally , many texts describe recurring stereotypical events situations . task template filling find situations documents fill templatetemplate filling slots . slot-fillers consist text segments extracted directly text , concepts like times , amounts , ontology entities inferred text elements additional processing . airline text example kind stereotypical situation airlines often raise fares wait competitors follow . situa - tion , identify United lead airline initially raised fares , $ 6 amount , Thursday increase date , American airline followed , leading filled template like following . FARE-RAISE ATTEMPT :      LEAD AIRLINE : UNITED AIRLINES AMOUNT : $ 6 EFFECTIVE DATE : 2006-10-26 FOLLOWER : AMERICAN AIRLINES      18.1 Named Entity Recognition first step information extraction detect entities text . named entity , roughly speaking , anything referred proper name : named entity person , location , organization . term commonly extended include things entities per se , including dates , times , kinds temporal expressions , even numerical expressions like prices . Here’s sample texttemporalexpressions introduced earlier named entities marked : Citing high fuel prices , [ ORG United Airlines ] [ TIME Friday ] increased fares [ MONEY $ 6 ] per round trip flights cities served lower-cost carriers . [ ORG American Airlines ] , unit [ ORG AMR Corp . ] , immediately matched move , spokesman [ PER Tim Wagner ] . [ ORG United ] , unit [ ORG UAL Corp . ] , increase took effect [ TIME Thursday ] applies routes competes against discount carriers , [ LOC Chicago ] [ LOC Dallas ] [ LOC Denver ] [ LOC San Francisco ] . text contains 13 mentions named entities including 5 organizations , 4 loca - tions , 2 times , 1 person , 1 mention money . addition extracting events relationship par - ticipants , named entities useful many language processing tasks . 18.1 • NAMED ENTITY RECOGNITION 3 sentiment analysis might know consumer’s sentiment toward partic - ular entity . Entities useful first stage question answering , linking text information structured knowledge sources like Wikipedia . Figure 18.1 shows typical generic named entity types . Many applications need specific entity types like proteins , genes , commercial products , works art . Type Tag Sample Categories Example sentences People PER people , characters Turing giant computer science . Organization ORG companies , sports teams IPCC warned cyclone . Location LOC regions , mountains , seas Mt . Sanitas loop Sunshine Canyon . Geo-Political Entity GPE countries , states , provinces Palo Alto raising fees parking . Facility FAC bridges , buildings , airports Consider Golden Gate Bridge . Vehicles VEH planes , trains , automobiles classic Ford Falcon . Figure 18.1 list generic named entity types kinds entities refer . Named entity recognition means finding spans text constitute proper names classifying type entity . Recognition difficult partly - cause ambiguity segmentation ; need decide entity , boundaries . Another difficulty caused type ambiguity . mention JFK refer person , airport New York , number schools , bridges , streets around United States . examples kind cross-type confusion Figures 18.2 18.3 . Name Possible Categories Washington Person , Location , Political Entity , Organization , Vehicle Downing St . Location , Organization IRA Person , Organization , Monetary Instrument Louis Vuitton Person , Organization , Commercial Product Figure 18.2 Common categorical ambiguities associated various proper names . [ PER Washington ] born slavery farm James Burroughs . [ ORG Washington ] went up 2 games 1 four-game series . Blair arrived [ LOC Washington ] well last state visit . June , [ GPE Washington ] passed primary seatbelt law . [ VEH Washington ] proved leaky ship , every passage made . . . Figure 18.3 Examples type ambiguities name Washington . 18.1.1 NER Sequence Labeling standard algorithm named entity recognition word-by-word sequence labeling task , assigned tags capture boundary type . sequence classifier like MEMM / CRF , bi-LSTM , transformer trained label tokens text tags indicate presence particular kinds named entities . Consider following simplified excerpt running exam - ple . [ ORG American Airlines ] , unit [ ORG AMR Corp . ] , immediately matched move , spokesman [ PER Tim Wagner ] . 4 CHAPTER 18 • INFORMATION EXTRACTION Figure 18.4 shows same excerpt represented IOB tagging . IOB tag-IOB ging introduce tag beginning ( B ) inside ( ) entity type , tokens outside ( O ) entity . number tags thus 2n + 1 tags , n number entity types . IOB tagging represent exactly same information bracketed notation . Words IOB Label IO Label American B-ORG I-ORG Airlines I-ORG I-ORG , O O O O unit O O O O AMR B-ORG I-ORG Corp . I-ORG I-ORG , O O immediately O O matched O O O O move O O , O O spokesman O O Tim B-PER I-PER Wagner I-PER I-PER O O . O O Figure 18.4 Named entity tagging sequence model , showing IOB IO encodings . shown IO tagging , loses information eliminating B tag . B tag IO tagging unable distinguish two entities same type right next . situation arise often ( usually least punctuation deliminator ) , IO tagging sufficient , advantage n + 1 tags . following three sections introduce three standard families al - gorithms NER tagging : feature based ( MEMM / CRF ) , neural ( bi-LSTM ) , rule-based . 18.1.2 feature-based algorithm NER identity wi , identity neighboring words embeddings wi , embeddings neighboring words part speech wi , part speech neighboring words base-phrase syntactic chunk label wi neighboring words presence wi gazetteer wi contains particular prefix ( prefixes length ≤ 4 ) wi contains particular suffix ( suffixes length ≤ 4 ) wi upper case word shape wi , word shape neighboring words short word shape wi , short word shape neighboring words presence hyphen Figure 18.5 Typical features feature-based NER system . 18.1 • NAMED ENTITY RECOGNITION 5 first approach extract features train MEMM CRF sequence model type saw part-of-speech tagging Chapter 8 . Figure 18.5 lists standard features feature-based systems . seen many features context part-of-speech tagging , particularly tagging un - known words . surprising , many unknown words fact named entities . Word shape features thus particularly important context NER . Recall word shape features represent abstract letter pattern ofword shape word mapping lower-case letters ‘ x ’ , upper-case ‘ X ’ , numbers ’ d ’ , retaining punctuation . Thus example I.M.F map X.X.X . DC10-30 map XXdd-dd . second class shorter word shape features . features consecutive character types removed , DC10-30 mapped Xd-d I.M.F still map X.X.X . feature itself accounts considerable part success feature-based NER systems English news text . Shape features particularly important recognizing names proteins genes biological texts . example named entity token L’Occitane generate following non-zero valued feature values : prefix ( wi ) = L suffix ( wi ) = tane prefix ( wi ) = L ’ suffix ( wi ) = ane prefix ( wi ) = L’O suffix ( wi ) = ne prefix ( wi ) = L’Oc suffix ( wi ) = e word-shape ( wi ) = X’Xxxxxxxx short-word-shape ( wi ) = X’Xx gazetteer list place names , often providing millions entries lo-gazetteer cations detailed geographical political information . 1 related resource name-lists ; United States Census Bureau provides extensive lists first names surnames derived decadal census U.S . 2 Similar lists cor - porations , commercial products , manner things biological mineral available variety sources . Gazetteer name features typically implemented binary feature name list . Unfortunately , lists difficult create maintain , usefulness varies considerably . gazetteers quite effective , lists persons organizations always helpful ( Mikheev et al . , 1999 ) . Feature effectiveness depends application , genre , media , language . example , shape features , critical English newswire texts , little automatic speech recognition transcripts , non-edited informally - edited sources , languages like Chinese orthographic case . features Fig . 18.5 thought starting point . Figure 18.6 illustrates result adding part-of-speech tags , syntactic base - phrase chunk tags , shape information earlier example . training set , sequence classifier like MEMM trained label new sentences . Figure 18.7 illustrates operation sequence labeler point token Corp . next labeled . assume context win - dow includes two preceding following words , features available classifier shown boxed area . 1 www.geonames.org 2 www.census.gov 6 CHAPTER 18 • INFORMATION EXTRACTION Word POS Chunk Short shape Label American NNP B-NP Xx B-ORG Airlines NNPS I-NP Xx I-ORG , , O , O DT B-NP x O unit NN I-NP x O B-PP x O AMR NNP B-NP X B-ORG Corp . NNP I-NP Xx . I-ORG , , O , O immediately RB B-ADVP x O matched VBD B-VP x O DT B-NP x O move NN I-NP x O , , O , O spokesman NN B-NP x O Tim NNP I-NP Xx B-PER Wagner NNP I-NP Xx I-PER VBD B-VP x O . , O . O Figure 18.6 Word-by-word feature encoding NER . Classifier NNP NNP RB unit ofa . . . x B-PP . . . AMR Corp . immediately B-NP X I-NP X . B-ADVP x O B-ORG ? . . . . . . , , O , matched Figure 18.7 Named entity recognition sequence labeling . features available classifier training classification boxed area . 18.1.3 neural algorithm NER standard neural algorithm NER based bi-LSTM introduced Chap - ter 9 . Recall model , word character embeddings computed input word wi . passed left-to-right LSTM right-to-left LSTM , whose outputs concatenated ( otherwise combined ) produce sin - gle output layer position . simplest method , layer directly passed onto softmax creates probability distribution NER tags , likely tag chosen ti . named entity tagging greedy approach decoding insufficient , allow impose strong constraints neighboring tokens ( e.g . , tag I-PER follow another I-PER B-PER ) . CRF layer normally top bi-LSTM output , Viterbi decoding algorithm decode . Fig . 18.8 shows sketch algorithm 18.1 • NAMED ENTITY RECOGNITION 7 Mark Watney visits Mars LSTM1 LSTM1 LSTM1 LSTM1 LSTM2 LSTM2 LSTM2 LSTM2 Concatenation Right-to-left LSTM Left-to-right LSTM B-PER I-PER O B-LOCCRF Layer Char LSTM Char LSTM Char LSTM Char LSTM GloVe GloVe GloVe GloVe Char + GloVe Embeddings Figure 18.8 Putting together : character embeddings words together bi-LSTM sequence model . Lample et al . ( 2016 ) . 18.1.4 Rule-based NER machine learned ( neural MEMM / CRF ) sequence models norm academic research , commercial approaches NER often based pragmatic combinations lists rules , smaller amount supervised machine learning ( Chiticariu et al . , 2013 ) . example IBM System T text understand - ing architecture user specifies complex declarative constraints tagging tasks formal query language includes regular expressions , dictionaries , se - mantic constraints , NLP operators , table structures , system compiles efficient extractor ( Chiticariu et al . , 2018 ) . common approach make repeated rule-based passes text , allow - ing results pass influence next . stages typically first involve rules extremely high precision low recall . Subsequent stages employ error-prone statistical methods take output first pass account . 1 . First , high-precision rules tag unambiguous entity mentions . 2 . , search substring matches previously detected names . 3 . Consult application-specific name lists identify likely name entity mentions domain . 4 . Finally , apply probabilistic sequence labeling techniques make tags previous stages additional features . intuition behind staged approach twofold . First , entity mentions text clearly indicative entity’s class others . Second , once unambiguous entity mention introduced text , likely subsequent shortened versions refer same entity ( thus same type entity ) . 18.1.5 Evaluation Named Entity Recognition familiar metrics recall , precision , F1 measure evaluate NER systems . Remember recall ratio number correctly labeled re - sponses total labeled ; precision ratio num - 8 CHAPTER 18 • INFORMATION EXTRACTION ARTIFACT GENERAL AFFILIATION ORG AFFILIATION PART - WHOLE PERSON - SOCIAL PHYSICAL Located Near Business Family Lasting Personal Citizen - Resident - Ethnicity - Religion Org-Location - Origin Founder Employment Membership Ownership Student-Alum Investor User-Owner-Inventor - Manufacturer Geographical Subsidiary Sports-Affiliation Figure 18.9 17 relations ACE relation extraction task . ber correctly labeled responses total labeled ; F-measure harmonic mean two . named entities , entity rather word unit response . Thus example Fig . 18.6 , two entities Tim Wagner AMR Corp . non-entity count single response . fact named entity tagging segmentation component present tasks like text categorization part-of-speech tagging causes prob - lems evaluation . example , system labeled American American Airlines organization cause two errors , false positive O false negative I-ORG . addition , entities unit response words unit training means mismatch training test conditions . 18.2 Relation Extraction Next list tasks discern relationships exist among detected entities . return sample airline text : Citing high fuel prices , [ ORG United Airlines ] [ TIME Friday ] increased fares [ MONEY $ 6 ] per round trip flights cities served lower-cost carriers . [ ORG American Airlines ] , unit [ ORG AMR Corp . ] , immediately matched move , spokesman [ PER Tim Wagner ] . [ ORG United ] , unit [ ORG UAL Corp . ] , increase took effect [ TIME Thursday ] applies routes competes against discount carriers , [ LOC Chicago ] [ LOC Dallas ] [ LOC Denver ] [ LOC San Francisco ] . text tells , example , Tim Wagner spokesman American Airlines , United unit UAL Corp . , American unit AMR . binary relations instances generic relations part-of employs fairly frequent news-style texts . Figure 18.9 lists 17 relations ACE relation extraction evaluations Fig . 18.10 shows sample relations . might extract domain-specific relation notion airline route . example text conclude United routes Chicago , Dallas , Denver , San Francisco . 18.2 • RELATION EXTRACTION 9 Relations Types Examples Physical-Located PER-GPE Tennessee Part-Whole-Subsidiary ORG-ORG XYZ , parent company ABC Person-Social-Family PER-PER Yoko’s husband John Org-AFF-Founder PER-ORG Steve Jobs , co-founder Apple . . . Figure 18.10 Semantic relations examples named entity types involve . Domain D = { , b , c , d , e , f , g , h , } United , UAL , American Airlines , AMR , b , c , d Tim Wagner e Chicago , Dallas , Denver , San Francisco f , g , h , Classes United , UAL , American , AMR organizations Org = { , b , c , d } Tim Wagner person Pers = { e } Chicago , Dallas , Denver , San Francisco places Loc = { f , g , h , } Relations United unit UAL PartOf = { 〈 , b 〉 , 〈 c , d 〉 } American unit AMR Tim Wagner works American Airlines OrgAff = { 〈 c , e 〉 } United serves Chicago , Dallas , Denver , San Francisco Serves = { 〈 , f 〉 , 〈 , g 〉 , 〈 , h 〉 , 〈 , 〉 } Figure 18.11 model-based view relations entities sample text . relations correspond nicely model-theoretic notions introduced Chapter 16 ground meanings logical forms . , relation consists set ordered tuples elements domain . standard information - extraction applications , domain elements correspond named entities occur text , underlying entities result co-reference resolution , entities selected domain ontology . Figure 18.11 shows model-based view set entities relations extracted running example . Notice model-theoretic view subsumes NER task well ; named entity recognition corresponds identification class unary relations . Sets relations defined many domains well . example UMLS , Unified Medical Language System National Library Medicine network defines 134 broad subject categories , entity types , 54 relations entities , following : Entity Relation Entity Injury disrupts Physiological Function Bodily Location location-of Biologic Function Anatomical Structure part-of Organism Pharmacologic Substance causes Pathological Function Pharmacologic Substance treats Pathologic Function medical sentence like : ( 18.1 ) Doppler echocardiography diagnose left anterior descending artery stenosis patients type 2 diabetes thus extract UMLS relation : Echocardiography , Doppler Diagnoses Acquired stenosis Wikipedia offers large supply relations , drawn infoboxes , struc-infoboxes tured tables associated certain Wikipedia articles . example , Wikipedia 10 CHAPTER 18 • INFORMATION EXTRACTION infobox Stanford includes structured facts like state = " California " president = " Mark Tessier-Lavigne " . facts turned rela - tions like president-of located-in . relations metalanguage called RDFRDF ( Resource Description Framework ) . RDF triple tuple entity-relation-RDF triple entity , called subject-predicate-object expression . Here’s sample RDF triple : subject predicate object Golden Gate Park location San Francisco example crowdsourced DBpedia ( Bizer et al . , 2009 ) ontology derived Wikipedia containing 2 billion RDF triples . Another dataset Wikipedia infoboxes , Freebase ( Bollacker et al . , 2008 ) , part WikidataFreebase ( Vrandečić Krötzsch , 2014 ) , relations like : people / person / nationality location / location / contains WordNet ontologies offer useful ontological relations express hier - archical relations words concepts . example WordNet is-a oris-a hypernym relation classes , hypernym Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate . . . WordNet Instance-of relation individuals classes , example San Francisco Instance-of relation city . Extracting relations important step extending building ontologies . five main classes algorithms relation extraction : handwritten patterns , supervised machine learning , semi-supervised ( via bootstrapping via distant supervision ) , unsupervised . introduce next sections . 18.2.1 Patterns Extract Relations earliest still common algorithm relation extraction lexico-syntactic patterns , first developed Hearst ( 1992a ) . Consider following sentence : Agar substance prepared mixture red algae , Ge - lidium , laboratory industrial . Hearst points human readers know Gelidium , readily infer kind ( hyponym ) red algae , whatever . suggests following lexico-syntactic pattern NP0 NP1 { , NP2 . . . , ( | ) NPi } , ≥ 1 ( 18.2 ) implies following semantics ∀ NPi , ≥ 1 , hyponym ( NPi , NP0 ) ( 18.3 ) allowing infer hyponym ( Gelidium , red algae ) ( 18.4 ) Figure 18.12 shows five patterns Hearst ( 1992a , 1998 ) suggested inferring hyponym relation ; shown NPH parent / hyponym . Modern versions pattern-based approach extend adding named entity constraints . example goal answer questions “ holds office organization ? ” , patterns like following : 18.2 • RELATION EXTRACTION 11 NP { , NP } * { , } ( | ) NPH temples , treasuries , important civic buildings NPH { NP , } * { ( | ) } NP red algae Gelidium NPH { NP , } * { ( | ) } NP authors Herrick , Goldsmith , Shakespeare NPH { , } including { NP , } * { ( | ) } NP common-law countries , including Canada England NPH { , } especially { NP } * { ( | ) } NP European countries , especially France , England , Spain Figure 18.12 Hand-built lexico-syntactic patterns finding hypernyms , { } mark optionality ( Hearst 1992a , Hearst 1998 ) . PER , POSITION ORG : George Marshall , Secretary State United States PER ( named | appointed | chose | etc . ) PER Prep ? POSITION Truman appointed Marshall Secretary State PER [ ] ? ( named | appointed | etc . ) Prep ? ORG POSITION George Marshall named Secretary State Hand-built patterns advantage high-precision tailored specific domains . hand , often low-recall , lot work create possible patterns . 18.2.2 Relation Extraction via Supervised Learning Supervised machine learning approaches relation extraction follow scheme familiar . fixed set relations entities chosen , training corpus hand-annotated relations entities , annotated texts train classifiers annotate unseen test set . straightforward approach three steps , illustrated Fig . 18.13 . Step find pairs named entities ( usually same sentence ) . step two , filtering classifier trained make binary decision pair named entities related ( relation ) . Positive examples extracted directly relations annotated corpus , negative examples generated within-sentence entity pairs annotated relation . step 3 , classi - fier trained assign label relations found step 2 . filtering classifier speed up final classification allows distinct feature-sets appropriate task . two classifiers , standard classification techniques ( logistic regression , neural network , SVM , etc . ) function FINDRELATIONS ( words ) returns relations relations ← nil entities ← FINDENTITIES ( words ) forall entity pairs 〈 e1 , e2 〉 entities RELATED ? ( e1 , e2 ) relations ← relations + CLASSIFYRELATION ( e1 , e2 ) Figure 18.13 Finding classifying relations among entities text . feature-based classifiers like logistic regression random forests important step identify useful features . consider features clas - 12 CHAPTER 18 • INFORMATION EXTRACTION sifying relationship American Airlines ( Mention 1 , M1 ) Tim Wagner ( Mention 2 , M2 ) sentence : ( 18.5 ) American Airlines , unit AMR , immediately matched move , spokesman Tim Wagner Useful word features include • headwords M1 M2 concatenation Airlines Wagner Airlines-Wagner • Bag-of-words bigrams M1 M2 American , Airlines , Tim , Wagner , American Airlines , Tim Wagner • Words bigrams particular positions M2 : - 1 spokesman M2 : + 1 • Bag words bigrams M1 M2 : , AMR , , immediately , matched , move , spokesman , , unit • Stemmed versions same Embeddings represent words features . Useful named entity features include • Named-entity types concatenation ( M1 : ORG , M2 : PER , M1M2 : ORG-PER ) • Entity Level M1 M2 ( set NAME , NOMINAL , PRONOUN ) M1 : NAME [ PRONOUN ] M2 : NAME [ company NOMINAL ] • Number entities arguments ( case 1 , AMR ) syntactic structure sentence signal relationships among entities . Syntax often featured strings representing syntactic paths : ( dependency constituency ) path traversed tree getting entity . • Base syntactic chunk sequence M1 M2 NP NP PP VP NP NP • Constituent paths M1 M2 NP ↑ NP ↑ S ↑ S ↓ NP • Dependency-tree paths Airlines ← sub j matched ← comp → sub j Wagner Figure 18.14 summarizes many features discussed classifying relationship American Airlines Tim Wagner example text . Neural models relation extraction similarly treat task supervised clas - sification . option similar architecture saw named entity tagging : bi-LSTM model word embeddings inputs single softmax classification sentence output 1-of-N relation label . relations often hold entities far part sentence ( sentences ) , possible get higher performance algorithms like convolutional nets ( dos Santos et al . , 2015 ) chain tree LSTMS ( Miwa Bansal 2016 , Peng et al . 2017 ) . general , test set similar enough training set , enough hand-labeled data , supervised relation extraction systems get high ac - curacies . labeling large training set extremely expensive supervised 18.2 • RELATION EXTRACTION 13 M1 headword airlines ( word token embedding ) M2 headword Wagner Word ( s ) M1 NONE Word ( s ) M2 Bag words { , unit , , AMR , Inc . , immediately , matched , , move , spokesman } M1 type ORG M2 type PERS Concatenated types ORG-PERS Constituent path NP ↑ NP ↑ S ↑ S ↓ NP Base phrase path NP → NP → PP → NP → V P → NP → NP Typed-dependency path Airlines ← sub j matched ← comp → sub j Wagner Figure 18.14 Sample features extracted classification < American Airlines , Tim Wagner > tuple ; M1 first mention , M2 second . models brittle : generalize well different text genres . rea - son , research relation extraction focused semi-supervised unsupervised approaches turn next . 18.2.3 Semisupervised Relation Extraction via Bootstrapping Supervised machine learning assumes lots labeled data . Unfortu - nately , expensive . suppose just few high-precision seed pat - terns , like Section 18.2.1 , perhaps few seed tuples . That’s enoughseed patterns seed tuples bootstrap classifier ! Bootstrapping proceeds taking entities seed bootstrapping pair , finding sentences ( web , whatever dataset ) contain entities . sentences , extract generalize context around entities learn new patterns . Fig . 18.15 sketches basic algorithm . function BOOTSTRAP ( Relation R ) returns new relation tuples tuples ← Gather set seed tuples relation R iterate sentences ← find sentences contain entities tuples patterns ← generalize context around entities sentences newpairs ← patterns grep tuples newpairs ← newpairs high confidence tuples ← tuples + newpairs return tuples Figure 18.15 Bootstrapping seed entity pairs learn relations . Suppose , example , need create list airline / hub pairs , know Ryanair hub Charleroi . seed fact discover new patterns finding mentions relation corpus . search terms Ryanair , Charleroi hub proximity . Perhaps find following set sentences : ( 18.6 ) Budget airline Ryanair , Charleroi hub , scrapped weekend flights airport . ( 18.7 ) flights Ryanair’s Belgian hub Charleroi airport grounded Friday . . . 14 CHAPTER 18 • INFORMATION EXTRACTION ( 18.8 ) spokesman Charleroi , main hub Ryanair , estimated 8000 passengers already affected . results , context words entity mentions , words mention , word mention two , named entity types two mentions , perhaps features , extract general patterns following : / [ ORG ] , [ LOC ] hub / / [ ORG ] ’ s hub [ LOC ] / / [ LOC ] main hub [ ORG ] / new patterns search additional tuples . Bootstrapping systems assign confidence values new tuples avoid se-confidencevalues mantic drift . semantic drift , erroneous pattern leads introduction ofsemantic drift erroneous tuples , , turn , lead creation problematic patterns meaning extracted relations ‘ drifts ’ . Consider following example : ( 18.9 ) Sydney ferry hub Circular Quay . accepted positive example , expression lead incorrect - troduction tuple 〈 Sydney , CircularQuay 〉 . Patterns based tuple propagate further errors database . Confidence values patterns based balancing two factors : pattern’s performance respect current set tuples pattern’s productivity terms number matches produces document collection . formally , document collection D , current set tuples T , proposed pattern p , need track two factors : • hits : set tuples T p matches looking D • f inds : total set tuples p finds D following equation balances considerations ( Riloff Jones , 1999 ) . Conf RlogF ( p ) = hitsp findsp × log ( findsp ) ( 18.10 ) metric generally normalized produce probability . assess confidence proposed new tuple combining evidence supporting patterns P ′ match tuple D ( Agichtein Gravano , 2000 ) . way combine evidence noisy-or technique . noisy-or Assume tuple supported subset patterns P , own confidence assessed . noisy-or model , make two basic assumptions . First , proposed tuple false , supporting patterns error , second , sources individual failures independent . loosely treat confidence measures probabilities , probability individual pattern p failing 1 − Conf ( p ) ; probability supporting patterns tuple wrong product individual failure probabilities , leaving following equation confidence new tuple . Conf ( t ) = 1 − ∏ p ∈ P ′ ( 1 − Conf ( p ) ) ( 18.11 ) Setting conservative confidence thresholds acceptance new patterns tuples bootstrapping process helps prevent system drifting away targeted relation . 18.2 • RELATION EXTRACTION 15 18.2.4 Distant Supervision Relation Extraction Although text hand-labeled relation labels extremely expensive produce , ways find indirect sources training data . distant supervision method Mintz et al . ( 2009 ) combines advantages bootstrappingdistantsupervision supervised learning . just handful seeds , distant supervision large database acquire huge number seed examples , creates lots noisy pattern features examples combines supervised classifier . example suppose trying learn place-of-birth relationship - tween people birth cities . seed-based approach , might 5 examples start . Wikipedia-based databases like DBPedia Freebase tens thousands examples many relations ; including 100,000 ex - amples place-of-birth , ( < Edwin Hubble , Marshfield > , < Albert Einstein , Ulm > , etc . , ) . next step run named entity taggers large amounts text — Mintz et al . ( 2009 ) 800,000 articles Wikipedia — extract sentences two named entities match tuple , like following : . . . Hubble born Marshfield . . . . . . Einstein , born ( 1879 ) , Ulm . . . . . . Hubble’s birthplace Marshfield . . . Training instances extracted data , training instance identical tuple < relation , entity1 , entity2 > . Thus training instance : < born-in , Edwin Hubble , Marshfield > < born-in , Albert Einstein , Ulm > < born-year , Albert Einstein , 1879 > . apply feature-based neural classification . feature-based clas - sification , standard supervised relation extraction features like named entity la - bels two mentions , words dependency paths mentions , neighboring words . tuple features collected many training instances ; feature vector single training instance like ( < born-in , Albert Einstein , Ulm > lexical syntactic features many different sen - tences mention Einstein Ulm . distant supervision large training sets , able rich features conjunctions individual features . extract thousands patterns conjoin entity types intervening words dependency paths like : PER born LOC PER , born ( XXXX ) , LOC PER’s birthplace LOC return running example , sentence : ( 18.12 ) American Airlines , unit AMR , immediately matched move , spokesman Tim Wagner learn rich conjunction features like : M1 = ORG & M2 = PER & nextword = “ ” & path = NP ↑ NP ↑ S ↑ S ↓ NP result supervised classifier huge rich set features detecting relations . every test sentence training 16 CHAPTER 18 • INFORMATION EXTRACTION relations , classifier need able label example no-relation . label trained randomly selecting entity pairs appear Freebase relation , extracting features , building feature vector tuple . final algorithm sketched Fig . 18.16 . function DISTANT SUPERVISION ( Database D , Text T ) returns relation classifier C foreach relation R foreach tuple ( e1 , e2 ) entities relation R D sentences ← Sentences T contain e1 e2 f ← Frequent features sentences observations ← observations + new training tuple ( e1 , e2 , f , R ) C ← Train supervised classifier observations return C Figure 18.16 distant supervision algorithm relation extraction . neural classifier might need feature set f . Distant supervision shares advantages methods exam - ined . Like supervised classification , distant supervision classifier lots features , supervised detailed hand-created knowledge . Like pattern-based classifiers , make high-precision evidence relation en - tities . Indeed , distance supervision systems learn patterns just like hand-built patterns early relation extractors . example is-a hypernym extraction system Snow et al . ( 2005 ) hypernym / hyponym NP pairs WordNet distant supervision , learned new patterns large amounts text . system induced exactly original 5 template patterns Hearst ( 1992a ) , 70,000 additional patterns including four : NPH like NP Many hormones like leptin . . . NPH called NP . . . markup language called XHTML NP NPH Ruby programming language . . . NP , NPH IBM , company long . . . ability large number features simultaneously means , un - like iterative expansion patterns seed-based systems , there’s semantic drift . Like unsupervised classification , labeled training corpus texts , sensitive genre issues training corpus , relies large amounts unlabeled data . Distant supervision advantage create training tuples neural classifiers , features required . distant supervision help extracting relations large enough database already exists . extract new relations datasets , rela - tions new domains , purely unsupervised methods . 18.2.5 Unsupervised Relation Extraction goal unsupervised relation extraction extract relations web labeled training data , even list relations . task often called open information extraction Open IE . Open IE , relations open information extraction simply strings words ( usually beginning verb ) . example , ReVerb system ( Fader et al . , 2011 ) extracts relation sentence s 4 steps : 18.2 • RELATION EXTRACTION 17 1 . Run part-of-speech tagger entity chunker s 2 . verb s , find longest sequence words w start verb satisfy syntactic lexical constraints , merging adjacent matches . 3 . phrase w , find nearest noun phrase x left relative pronoun , wh-word existential “ ” . Find nearest noun phrase y right . 4 . Assign confidence c relation r = ( x , w , y ) confidence classifier return . relation accepted meets syntactic lexical constraints . syntactic constraints ensure verb-initial sequence might include nouns ( relations begin light verbs like make , , often express core relation noun , like hub ) : V | VP | VW * P V = verb particle ? adv ? W = ( noun | adj | adv | pron | det ) P = ( prep | particle | inf . marker ) lexical constraints based dictionary D prune rare , long relation strings . intuition eliminate candidate relations oc - cur sufficient number distinct argument types likely bad examples . system first runs relation extraction algorithm offline 500 million web sentences extracts list relations occur nor - malizing ( removing inflection , auxiliary verbs , adjectives , adverbs ) . relation r added dictionary occurs least 20 different arguments . Fader et al . ( 2011 ) dictionary 1.7 million normalized relations . Finally , confidence value computed relation logistic re - gression classifier . classifier trained taking 1000 random web sentences , running extractor , hand labelling extracted relation correct incor - rect . confidence classifier trained hand-labeled data , features relation surrounding words . Fig . 18.17 shows sample features classification . ( x , r , y ) covers words s last preposition r last preposition r len ( s ) ≤ 10 coordinating conjunction left r s r matches lone V syntactic constraints preposition left x s NP right y s Figure 18.17 Features classifier assigns confidence relations extracted Open Information Extraction system REVERB ( Fader et al . , 2011 ) . example following sentence : ( 18.13 ) United hub Chicago , headquarters United Continental Holdings . relation phrases hub headquarters ( , longer phrases preferred ) . Step 3 finds United left Chicago right hub , skips find Chicago left headquarters . final output : 18 CHAPTER 18 • INFORMATION EXTRACTION r1 : < United , hub , Chicago > r2 : < Chicago , headquarters , United Continental Holdings > great advantage unsupervised relation extraction ability handle huge number relations specify advance . disad - vantage need map large sets strings canonical form adding databases knowledge sources . Current methods focus heavily relations expressed verbs , miss many relations expressed nominally . 18.2.6 Evaluation Relation Extraction Supervised relation extraction systems evaluated test sets human - annotated , gold-standard relations computing precision , recall , F-measure . Labeled precision recall require system classify relation correctly , whereas unlabeled methods simply measure system’s ability detect entities related . Semi-supervised unsupervised methods difficult evalu - ate , extract totally new relations web large text . methods large amounts text , generally possible run solely small labeled test set , result possible pre-annotate gold set correct instances relations . methods possible approximate ( ) precision drawing random sample relations output , human check accuracy relations . Usually approach focuses tuples extracted body text rather relation mentions ; systems need detect every mention relation scored correctly . , evaluation based set tuples occupying database system finished . , know system discover Ryanair hub Charleroi ; really care many times discovers . estimated precision P̂ P̂ = # correctly extracted relation tuples sample total # extracted relation tuples sample . ( 18.14 ) Another approach gives little bit information recall com - pute precision different levels recall . Assuming system able rank relations produces ( probability , confidence ) separately com - pute precision top 1000 new relations , top 10,000 new relations , top 100,000 , . case take random sample set . show precision curve behaves extract tuples . way directly evaluate recall . 18.3 Extracting Times Times dates particularly important kind named entity play role question answering , calendar personal assistant applications . order reason times dates , extract temporal expressions normalized — converted standard format reason . section consider extraction normalization temporal expressions . 18.3 • EXTRACTING TIMES 19 18.3.1 Temporal Expression Extraction Temporal expressions refer absolute points time , relative times , durations , sets . Absolute temporal expressions beabsolute mapped directly calendar dates , times day , . Relative temporal expres-relative sions map particular times reference point ( week last Tuesday ) . Finally , durations denote spans time varying levels granular-duration ity ( seconds , minutes , days , weeks , centuries , etc . ) . Figure 18.18 lists sample temporal expressions categories . Absolute Relative Durations April 24 , 1916 yesterday four hours summer ’ 77 next semester three weeks 10:15 two weeks yesterday six days 3rd quarter 2006 last quarter last three quarters Figure 18.18 Examples absolute , relational durational temporal expressions . Temporal expressions grammatical constructions temporal lexical triggers heads . Lexical triggers might nouns , proper nouns , adjectives , lexical triggers adverbs ; full temporal expressions consist phrasal projections : noun phrases , adjective phrases , adverbial phrases . Figure 18.19 provides examples . Category Examples Noun morning , noon , night , winter , dusk , dawn Proper Noun January , Monday , Ides , Easter , Rosh Hashana , Ramadan , Tet Adjective recent , past , annual , former Adverb hourly , daily , monthly , yearly Figure 18.19 Examples temporal lexical triggers . look TimeML annotation scheme , temporal expressions annotated XML tag , TIMEX3 , various attributes tag ( Pustejovsky et al . 2005 , Ferro et al . 2005 ) . following example illustrates basic scheme ( defer discussion attributes Section 18.3.2 ) . fare increase initiated < TIMEX3 > last week < / TIMEX3 > UAL Corp’s United Airlines matched competitors < TIMEX3 > weekend < / TIMEX3 > , marking second successful fare increase < TIMEX3 > two weeks < / TIMEX3 > . temporal expression recognition task consists finding start end text spans correspond temporal expressions . Rule-based ap - proaches temporal expression recognition cascades automata recognize patterns increasing levels complexity . Tokens first part-of-speech tagged , larger larger chunks recognized results previous stages , based patterns containing trigger words ( e.g . , February ) classes ( e.g . , MONTH ) . Figure 18.20 gives fragment rule-based system . Sequence-labeling approaches follow same IOB scheme named - entity tags , marking words inside , outside beginning TIMEX3-delimited temporal expression , O , B tags follows : O fare O increase O initiated O last B week O UAL O Corp’s . . . O 20 CHAPTER 18 • INFORMATION EXTRACTION # yesterday / today / tomorrow $ string = ˜ s / ( ( ( $ OT + $ CT + \ s + ) ? $ OT + day $ CT + \ s + $ OT + ( | ) $ CT + \ s + ) ? $ OT + $ TERelDayExpr $ CT + ( \ s + $ OT + ( morning | afternoon | evening | night ) $ CT + ) ? ) / < TIMEX $ tever TYPE =\ " DATE \ " > $ 1 < \ / TIMEX $ tever > / gio ; $ string = ˜ s / ( $ OT + \ w + $ CT + \ s + ) < TIMEX $ tever TYPE =\ " DATE \ " [ ˆ > ] * > ( $ OT + ( Today | Tonight ) $ CT + ) < \ / TIMEX $ tever > / $ 1 $ 4 / gso ; # ( morning / afternoon / evening ) $ string = ˜ s / ( ( $ OT + ( early | late ) $ CT + \ s + ) ? $ OT + $ CT + \ s * $ OT + ( morning | afternoon | evening ) $ CT + ) / < TIMEX $ tever TYPE =\ " DATE \ " > $ 1 < \ / TIMEX $ tever > / gosi ; $ string = ˜ s / ( ( $ OT + ( early | late ) $ CT + \ s + ) ? $ OT + last $ CT + \ s * $ OT + night $ CT + ) / < TIMEX $ tever TYPE =\ " DATE \ " > $ 1 < \ / TIMEX $ tever > / gsio ; Figure 18.20 Perl fragment GUTime temporal tagging system Tarsqi ( Verhagen et al . , 2005 ) . Features extracted token context , statistical sequence labeler trained ( sequence model ) . Figure 18.21 lists standard features temporal tagging . Feature Explanation Token target token labeled Tokens window Bag tokens window around target Shape Character shape features POS Parts speech target window words Chunk tags Base-phrase chunk tag target words window Lexical triggers Presence list temporal terms Figure 18.21 Typical features train IOB-style temporal expression taggers . Temporal expression recognizers evaluated usual recall , precision , F-measures . major difficulty lexicalized approaches avoiding expressions trigger false positives : ( 18.15 ) 1984 tells story Winston Smith . . . ( 18.16 ) . . . U2 ’ s classic Sunday Bloody Sunday 18.3.2 Temporal Normalization Temporal normalization process mapping temporal expression eithertemporalnormalization specific point time duration . Points time correspond calendar dates , times day , . Durations primarily consist lengths time include information start end points . Normalized times represented VALUE attribute ISO 8601 standard encoding temporal values ( ISO8601 , 2004 ) . Fig . 18.22 reproduces earlier example value attributes added . < TIMEX3 d = ’ ’ t 1 ’ ’ t y p e = ” DATE ” v l u e = ” 2007 − 07 − 02 ” f u n c t o n n D o c u m e n t = ” CREATION TIME ” > J u l y 2 , 2007 < / TIMEX3 > f r e n c r e s e n t t e d < TIMEX3 d = ” t 2 ” t y p e = ” DATE ” v l u e = ” 2007 − W26 ” anchorTimeID = ” t 1 ” > l s t week < / TIMEX3 > Un t ed r l n e s matched c o m p e t t o r s ove r < TIMEX3 d = ” t 3 ” t y p e = ” DURATION ” v l u e = ” P1WE ” anchorTimeID = ” t 1 ” > t h e weekend < / TIMEX3 > , marking t h e second s u c c e s s f u l f r e n c r e s e n < TIMEX3 d = ” t 4 ” t y p e = ” DURATION ” v l u e = ” P2W ” anchorTimeID = ” t 1 ” > two weeks < / TIMEX3 > . Figure 18.22 TimeML markup including normalized values temporal expressions . dateline , document date , text July 2 , 2007 . ISO repre - sentation kind expression YYYY-MM-DD , case , 2007-07-02 . 18.3 • EXTRACTING TIMES 21 encodings temporal expressions sample text follow date , shown values VALUE attribute . first temporal expression text proper refers particular week year . ISO standard , weeks numbered 01 53 , first week year first Thursday year . weeks represented template YYYY-Wnn . ISO week document date week 27 ; thus value last week represented “ 2007-W26 ” . next temporal expression weekend . ISO weeks begin Monday ; thus , weekends occur end week fully contained single week . Weekends treated durations , value VALUE attribute length . Durations represented according pattern Pnx , n integer denoting length x represents unit , P3Y three years P2D two days . example , weekend captured P1WE . case , sufficient information anchor particular weekend part particular week . information encoded ANCHORTIMEID attribute . Finally , phrase two weeks denotes duration captured P2W . lot various temporal annotation standards — far cover . Figure 18.23 describes basic ways times durations represented . Consult ISO8601 ( 2004 ) , Ferro et al . ( 2005 ) , Pustejovsky et al . ( 2005 ) details . Unit Pattern Sample Value Fully specified dates YYYY-MM-DD 1991-09-28 Weeks YYYY-Wnn 2007-W27 Weekends PnWE P1WE 24-hour clock times HH : MM :SS 11:13:45 Dates times YYYY-MM-DDTHH : MM :SS 1991-09-28T11:00:00 Financial quarters Qn 1999-Q3 Figure 18.23 Sample ISO patterns representing various times durations . current approaches temporal normalization rule-based ( Chang Manning 2012 , Strötgen Gertz 2013 ) . Patterns match temporal expres - sions associated semantic analysis procedures . compositional rule-to-rule approach introduced Chapter 17 , meaning constituent com - puted meaning parts method specific constituent , al - though semantic composition rules involve temporal arithmetic rather λ - calculus attachments . Fully qualified date expressions contain year , month , day con-fully qualified ventional form . units expression detected placed correct place corresponding ISO pattern . following pattern normalizes expressions like April 24 , 1916 . FQTE → Month Date , Year { Year . val − Month . val − Date . val } non-terminals Month , Date , Year represent constituents already recognized assigned semantic values , accessed * . val notation . value FQE constituent , turn , accessed FQTE . val further processing . Fully qualified temporal expressions fairly rare real texts . temporal expressions news articles incomplete implicitly anchored , - ten respect dateline article , refer document’s 22 CHAPTER 18 • INFORMATION EXTRACTION temporal anchor . values temporal expressions today , yesterday , ortemporalanchor tomorrow computed respect temporal anchor . semantic procedure today simply assigns anchor , attachments tomorrow yesterday add day subtract day anchor , respectively . course , cyclic nature representations months , weeks , days , times day , temporal arithmetic procedures modulo arithmetic appropriate time unit . Unfortunately , even simple expressions weekend Wednesday - troduce fair amount complexity . current example , weekend clearly refers weekend week immediately precedes document date . always case , illustrated following example . ( 18.17 ) Random security checks began yesterday Sky Harbor continue least weekend . case , expression weekend refers weekend week anchoring date part ( i.e . , coming weekend ) . information signals meaning comes tense continue , verb governing weekend . Relative temporal expressions handled temporal arithmetic similar today yesterday . document date indicates example article ISO week 27 , expression last week normalizes current week minus 1 . resolve ambiguous next last expressions consider distance anchoring date nearest unit . Next Friday refer immediately next Friday Friday following , closer document date Friday , likely phrase skip nearest . ambiguities handled encoding language domain-specific heuristics temporal attachments . 18.4 Extracting Events Times task event extraction identify mentions events texts . theeventextraction purposes task , event mention expression denoting event state assigned particular point , interval , time . following markup sample text page 19 shows events text . [ EVENT Citing ] high fuel prices , United Airlines [ EVENT ] Fri - day [ EVENT increased ] fares $ 6 per round trip flights cities served lower-cost carriers . American Airlines , unit AMR Corp . , immediately [ EVENT matched ] [ EVENT move ] , spokesman Tim Wagner [ EVENT ] . United , unit UAL Corp . , [ EVENT ] [ EVENT increase ] took effect Thursday [ EVENT applies ] routes [ EVENT competes ] against discount carriers , Chicago Dallas Denver San Francisco . English , event mentions correspond verbs , verbs introduce events . , example , always case . Events introduced noun phrases , move increase , verbs fail introduce events , phrasal verb took effect , refers event began rather event itself . Similarly , light verbs make , take , often fail denote events ; light verbs event often expressed nominal direct object ( took flight ) , light verbs just provide syntactic structure noun’s arguments . 18.4 • EXTRACTING EVENTS TIMES 23 Various versions event extraction task exist , depending goal . example TempEval shared tasks ( Verhagen et al . 2009 ) goal extract events aspects like aspectual temporal properties . Events classified actions , states , reporting events ( say , report , tell , explain ) , perceptionreportingevents events , . aspect , tense , modality event needs extracted . Thus example various events sample text annotated ( class = REPORTING , tense = PAST , aspect = PERFECTIVE ) . Event extraction generally modeled via supervised learning , detecting events via sequence models IOB tagging , assigning event classes attributes multi-class classifiers . Feature-based models surface information like parts speech , lexical items , verb tense information ; Fig . 18.24 . Feature Explanation Character affixes Character-level prefixes suffixes target word Nominalization suffix Character-level suffixes nominalizations ( e.g . , - tion ) Part speech Part speech target word Light verb Binary feature indicating target governed light verb Subject syntactic category Syntactic category subject sentence Morphological stem Stemmed version target word Verb root Root form verb basis nominalization WordNet hypernyms Hypernym set target Figure 18.24 Features commonly rule-based machine learning approaches event detec - tion . 18.4.1 Temporal Ordering Events events temporal expressions text detected , next logical task information fit events complete timeline . timeline useful applications question answering summarization . ambitious task subject considerable current research beyond capabilities current systems . somewhat simpler , still useful , task impose partial ordering events temporal expressions mentioned text . ordering provide many same benefits true timeline . example partial ordering determination fare increase American Airlines came fare increase United sample text . Determining ordering viewed binary relation detection classification task similar described earlier Section 18.2 . temporal relation events classified standard set Allen relations shown Fig . 18.25 ( Allen , 1984 ) , feature-Allen relations based classifiers Section 18.2 , trained TimeBank corpus features like words / embeddings , parse paths , tense aspect . TimeBank corpus consists text annotated informationTimeBank discussing throughout section ( Pustejovsky et al . , 2003b ) . Time - Bank 1.2 consists 183 news articles selected variety sources , including Penn TreeBank PropBank collections . article TimeBank corpus temporal expressions event mentions explicitly annotated TimeML annotation ( Pustejovsky et al . , 2003a ) . addition temporal expressions events , TimeML annotation provides temporal links events temporal expressions specify nature relation . Consider following sample sentence 24 CHAPTER 18 • INFORMATION EXTRACTION B B B B B B Time B B overlaps B B overlaps ' meets B B meets ' equals B ( B equals ) starts B B starts ' finishes B B finishes ' B B B ' Figure 18.25 13 temporal relations Allen ( 1984 ) . < TIMEX3 tid = " t57 " type = " DATE " value = " 1989-10-26 " functionInDocument = " CREATION_TIME " > 10/26/89 < / TIMEX3 > Delta Air Lines earnings < EVENT eid = " e1 " class = " OCCURRENCE " > soared < / EVENT > 33 % record < TIMEX3 tid = " t58 " type = " DATE " value = " 1989-Q1 " anchorTimeID = " t57 " > fiscal first quarter < / TIMEX3 > , < EVENT eid = " e3 " class = " OCCURRENCE " > bucking < / EVENT > industry trend toward < EVENT eid = " e4 " class = " OCCURRENCE " > declining < / EVENT > profits . Figure 18.26 Example TimeBank corpus . corresponding markup shown Fig . 18.26 , selected TimeBank documents . ( 18.18 ) Delta Air Lines earnings soared 33 % record fiscal first quarter , bucking industry trend toward declining profits . annotated , text includes three events two temporal expressions . events occurrence class unique identifiers fur - ther annotations . temporal expressions include creation time article , serves document time , single temporal expression text . addition annotations , TimeBank provides four links capture temporal relations events times text , Allen relations Fig . 18.25 . following within-sentence temporal relations annotated example . 18.5 • TEMPLATE FILLING 25 • Soaringe1 included fiscal first quartert58 • Soaringe1 1989-10-26t57 • Soaringe1 simultaneous buckinge3 • Declininge4 includes soaringe1 18.5 Template Filling Many texts contain reports events , possibly sequences events , often correspond fairly common , stereotypical situations world . abstract situations stories , related called scripts ( Schank Abel-scripts son , 1977 ) , consist prototypical sequences sub-events , participants , roles . strong expectations provided scripts facilitate proper classification entities , assignment entities roles relations , critically , drawing inferences fill things left unsaid . simplest form , scripts represented templates consisting fixedtemplates sets slots take values slot-fillers belonging particular classes . task template filling find documents invoke particular scripts fill thetemplate filling slots associated templates fillers extracted text . slot-fillers consist text segments extracted directly text , consist concepts inferred text elements additional pro - cessing . filled template original airline story might look like following . FARE-RAISE ATTEMPT :      LEAD AIRLINE : UNITED AIRLINES AMOUNT : $ 6 EFFECTIVE DATE : 2006-10-26 FOLLOWER : AMERICAN AIRLINES      template four slots ( LEAD AIRLINE , AMOUNT , EFFECTIVE DATE , FOL - LOWER ) . next section describes standard sequence-labeling approach filling slots . Section 18.5.2 describes older system based cascades finite-state transducers designed address complex template-filling task current learning-based systems yet address . 18.5.1 Machine Learning Approaches Template Filling standard paradigm template filling , training documents text spans annotated pre-defined templates slot fillers . goal create template event input , filling slots text spans . task generally modeled training two separate supervised systems . first system decides template present particular sentence . task called template recognition sometimes , perhaps confusing bit oftemplaterecognition terminology , event recognition . Template recognition treated text classi - fication task , features extracted every sequence words labeled training documents filling slot template detected . usual set features : tokens , embeddings , word shapes , part-of-speech tags , syntactic chunk tags , named entity tags . second system job role-filler extraction . separate classifier isrole-fillerextraction trained detect role ( LEAD-AIRLINE , AMOUNT , ) . 26 CHAPTER 18 • INFORMATION EXTRACTION binary classifier run every noun-phrase parsed input sentence , sequence model run sequences words . role classifier trained labeled data training set . Again , usual set features , trained individual noun phrase fillers single slot . Multiple non-identical text segments might labeled same slot la - bel . example sample text , strings United United Airlines might labeled LEAD AIRLINE . incompatible choices corefer - ence resolution techniques introduced Chapter 22 provide path solution . variety annotated collections evaluate style ap - proach template filling , including sets job announcements , conference calls papers , restaurant guides , biological texts . Recent work focuses extracting templates cases training data even predefined templates , inducing templates sets linked events ( Chambers Jurafsky , 2011 ) . 18.5.2 Earlier Finite-State Template-Filling Systems templates relatively simple . consider task producing template contained information text like ( Grishman Sundheim , 1995 ) : Bridgestone Sports Co . Friday set up joint venture Taiwan local concern Japanese trading house produce golf clubs shipped Japan . joint venture , Bridgestone Sports Taiwan Co . , capital - ized 20 million new Taiwan dollars , start production January 1990 production 20,000 iron “ metal wood ” clubs month . MUC-5 ‘ joint venture ’ task ( Message Understanding Conferences series U.S . government-organized information-extraction evaluations ) produce hierarchically linked templates describing joint ventures . Figure 18.27 shows structure produced FASTUS system ( Hobbs et al . , 1997 ) . Note filler ACTIVITY slot TIE-UP template itself template slots . Tie-up-1 Activity-1 : RELATIONSHIP tie-up COMPANY Bridgestone Sports Taiwan Co . ENTITIES Bridgestone Sports Co . PRODUCT iron “ metal wood ” clubs local concern START DATE : January 1990 Japanese trading house JOINT VENTURE Bridgestone Sports Taiwan Co . ACTIVITY Activity-1 AMOUNT NT $ 20000000 Figure 18.27 templates produced FASTUS input text page 26 . Early systems dealing complex templates based cascades transducers based handwritten rules , sketched Fig . 18.28 . first four stages handwritten regular expression grammar rules basic tokenization , chunking , parsing . Stage 5 recognizes entities events FST-based recognizer inserts recognized objects ap - propriate slots templates . FST recognizer based hand-built regular expressions like following ( NG indicates Noun-Group VG Verb-Group ) , matches first sentence news story . NG ( Company / ies ) VG ( Set-up ) NG ( Joint-Venture ) NG ( Company / ies ) VG ( Produce ) NG ( Product ) 18.6 • SUMMARY 27 . Step Description 1 Tokens Tokenize input stream characters 2 Complex Words Multiword phrases , numbers , proper names . 3 Basic phrases Segment sentences noun verb groups 4 Complex phrases Identify complex noun groups verb groups 5 Semantic Patterns Identify entities events , insert templates . 6 Merging Merge references same entity event Figure 18.28 Levels processing FASTUS ( Hobbs et al . , 1997 ) . level extracts specific type information passed next higher level . result processing two sentences five draft templates ( Fig . 18.29 ) merged single hierarchical structure shown Fig . 18.27 . merging algorithm , performing coreference resolution , merges two activi - ties likely describing same events . # Template / Slot Value 1 RELATIONSHIP : TIE-UP ENTITIES : Bridgestone Co . , local concern , Japanese trading house 2 ACTIVITY : PRODUCTION PRODUCT : “ golf clubs ” 3 RELATIONSHIP : TIE-UP JOINT VENTURE : “ Bridgestone Sports Taiwan Co . ” AMOUNT : NT $ 20000000 4 ACTIVITY : PRODUCTION COMPANY : “ Bridgestone Sports Taiwan Co . ” STARTDATE : : January 1990 5 ACTIVITY : PRODUCTION PRODUCT : “ iron “ metal wood ” clubs ” Figure 18.29 five partial templates produced stage 5 FASTUS . templates merged stage 6 produce final template shown Fig . 18.27 page 26 . 18.6 Summary chapter explored techniques extracting limited forms semantic con - tent texts . • Named entities recognized classified featured-based neural sequence labeling techniques . • Relations among entities extracted pattern-based approaches , su - pervised learning methods annotated training data available , lightly supervised bootstrapping methods small numbers seed tuples seed patterns available , distant supervision database relations available , unsupervised Open IE methods . • Reasoning time facilitated detection normalization temporal expressions combination statistical learning rule - based methods . • Events detected ordered time sequence models classi - fiers trained temporally - event-labeled data like TimeBank corpus . 28 CHAPTER 18 • INFORMATION EXTRACTION • Template-filling applications recognize stereotypical situations texts assign elements text roles represented fixed sets slots . Bibliographical Historical Notes earliest work information extraction addressed template-filling task context Frump system ( DeJong , 1982 ) . Later work stimulated U.S . government-sponsored MUC conferences ( Sundheim 1991 , Sundheim 1992 , Sund - heim 1993 , Sundheim 1995 ) . Early MUC systems like CIRCUS system ( Lehnert et al . , 1991 ) SCISOR ( Jacobs Rau , 1990 ) quite influential inspired later systems like FASTUS ( Hobbs et al . , 1997 ) . Chinchor et al . ( 1993 ) describe MUC evaluation techniques . Due difficulty porting systems domain another , attention shifted machine learning approaches . Early supervised learning approaches IE ( Cardie 1993 , Cardie 1994 , Riloff 1993 , Soderland et al . 1995 , Huffman 1996 ) focused automating knowledge acqui - sition process , mainly finite-state rule-based systems . success , earlier success HMM-based speech recognition , led sequence la - beling ( HMMs : Bikel et al . 1997 ; MEMMs McCallum et al . 2000 ; CRFs : Laf - ferty et al . 2001 ) , wide exploration features ( Zhou et al . , 2005 ) . Neural approaches NER mainly follow pioneering results Collobert et al . ( 2011 ) , applied CRF top convolutional net . BiLSTMs word character-based embeddings input followed shortly became standard neural algorithm NER ( Huang et al . 2015 , Ma Hovy 2016 , Lample et al . 2016 ) . Neural algorithms relation extraction often explore architectures handle entities far apart sentence : recursive networks ( Socher et al . , 2012 ) , convolutional nets ( dos Santos et al . , 2015 ) , chain tree LSTMS ( Miwa Bansal 2016 , Peng et al . 2017 ) . Progress area continues stimulated formal evaluations shared benchmark datasets , including Automatic Content Extraction ( ACE ) evaluations 2000-2007 named entity recognition , relation extraction , temporal ex - pressions3 , KBP ( Knowledge Base Population ) evaluations ( Ji et al . 2010 , Sur-KBP deanu 2013 ) relation extraction tasks like slot filling ( extracting attributes ( ‘ slots ’ ) slot filling like age , birthplace , spouse entity ) series SemEval work - shops ( Hendrickx et al . , 2009 ) . Semisupervised relation extraction first proposed Hearst ( 1992b ) , extended systems like AutoSlog-TS ( Riloff , 1996 ) , DIPRE ( Brin , 1998 ) , SNOW - BALL ( Agichtein Gravano , 2000 ) , ( Jones et al . , 1999 ) . distant super - vision algorithm describe drawn Mintz et al . ( 2009 ) , coined term ‘ distant supervision ’ , similar ideas occurred earlier systems like Craven Kumlien ( 1999 ) Morgan et al . ( 2004 ) name weakly labeled data , well Snow et al . ( 2005 ) Wu Weld ( 2007 ) . Among many exten - sions Wu Weld ( 2010 ) , Riedel et al . ( 2010 ) , Ritter et al . ( 2013 ) . Open IE systems include KNOWITALL Etzioni et al . ( 2005 ) , TextRunner ( Banko et al . , 2007 ) , REVERB ( Fader et al . , 2011 ) . Riedel et al . ( 2013 ) universal schema combines advantages distant supervision Open IE . 3 www.nist.gov/speech/tests/ace/ EXERCISES 29 HeidelTime ( Strötgen Gertz , 2013 ) SUTime ( Chang Manning , 2012 ) downloadable temporal extraction normalization systems . 2013 TempE - val challenge described UzZaman et al . ( 2013 ) ; Chambers ( 2013 ) Bethard ( 2013 ) give typical approaches . Exercises 18.1 Develop set regular expressions recognize character shape features described page 5 . 18.2 IOB labeling scheme chapter possible . example , E tag might added mark end entities , B tag reserved situations ambiguity exists adjacent entities . Propose new set IOB tags NER system . Experiment compare performance scheme presented chapter . 18.3 Names works art ( books , movies , video games , etc . ) quite different kinds named entities discussed chapter . Collect list names works art particular category Web-based source ( e.g . , gutenberg.org , amazon.com , imdb.com , etc . ) . Analyze list give examples ways names likely problematic techniques described chapter . 18.4 Develop NER system specific category names collected last exercise . Evaluate system collection text likely contain instances named entities . 18.5 Acronym expansion , process associating phrase acronym , accomplished simple form relational analysis . Develop system based relation analysis approaches described chapter populate database acronym expansions . focus English Three Letter Acronyms ( TLAs ) evaluate system’s performance comparing Wikipedia’s TLA page . 18.6 useful functionality newer email calendar applications ability associate temporal expressions connected events email ( doctor’s appointments , meeting planning , party invitations , etc . ) specific calendar entries . Collect corpus email containing temporal expressions related event planning . expressions compare kinds expressions commonly found news text discussing chapter ? 18.7 Acquire CMU seminar corpus develop template-filling system techniques mentioned Section 18.5 . Analyze well system performs compared state-of-the-art results corpus . 30 Chapter 18 • Information Extraction Agichtein , E . Gravano , L . ( 2000 ) . Snowball : Extract - ing relations large plain-text collections . Proceed - ings 5th ACM International Conference Digital Libraries . Allen , J . ( 1984 ) . general theory action time . Artificial Intelligence , 23 ( 2 ) , 123 – 154 . Banko , M . , Cafarella , M . , Soderland , S . , Broadhead , M . , Etzioni , O . ( 2007 ) . Open information extraction web . IJCAI , 2670 – 2676 . Bethard , S . ( 2013 ) . ClearTK-TimeML : minimalist ap - proach TempEval 2013 . SemEval-13 , 10 – 14 . Bikel , D . M . , Miller , S . , Schwartz , R . , Weischedel , R . ( 1997 ) . Nymble : high-performance learning name - finder . ANLP 1997 , 194 – 201 . Bizer , C . , Lehmann , J . , Kobilarov , G . , Auer , S . , Becker , C . , Cyganiak , R . , Hellmann , S . ( 2009 ) . DBpedia — crys - tallization point Web Data . Web Semantics : sci - ence , services agents world wide web , 7 ( 3 ) , 154 – 165 . Bollacker , K . , Evans , C . , Paritosh , P . , Sturge , T . , Tay - lor , J . ( 2008 ) . Freebase : collaboratively created graph database structuring human knowledge . SIGMOD 2008 , 1247 – 1250 . Brin , S . ( 1998 ) . Extracting patterns relations World Wide Web . Proceedings World Wide Web Databases International Workshop , Number 1590 LNCS , 172 – 183 . Springer . Cardie , C . ( 1993 ) . case-based approach knowledge ac - quisition domain specific sentence analysis . AAAI - 93 , 798 – 803 . AAAI Press . Cardie , C . ( 1994 ) . Domain-Specific Knowledge Acquisition Conceptual Sentence Analysis . Ph . D . thesis , University Massachusetts , Amherst , MA . Available CMPSCI Technical Report 94-74 . Chambers , N . ( 2013 ) . NavyTime : Event time ordering raw text . SemEval-13 , 73 – 77 . Chambers , N . Jurafsky , D . ( 2011 ) . Template-based - formation extraction templates . ACL 2011 . Chang , . X . Manning , C . D . ( 2012 ) . SUTime : li - brary recognizing normalizing time expressions . . LREC-12 , 3735 – 3740 . Chinchor , N . , Hirschman , L . , Lewis , D . L . ( 1993 ) . Eval - uating Message Understanding systems : analysis third Message Understanding Conference . Computational Linguistics , 19 ( 3 ) , 409 – 449 . Chiticariu , L . , Danilevsky , M . , Li , Y . , Reiss , F . , Zhu , H . ( 2018 ) . SystemT : Declarative text understanding enter - prise . NAACL HLT 2018 , Vol . 3 , 76 – 83 . Chiticariu , L . , Li , Y . , Reiss , F . R . ( 2013 ) . Rule-Based Information Extraction Dead ! Long Live Rule-Based - formation Extraction Systems ! . EMNLP 2013 , 827 – 832 . Cohen , K . B . Demner-Fushman , D . ( 2014 ) . Biomedical natural language processing . Benjamins . Collobert , R . , Weston , J . , Bottou , L . , Karlen , M . , Kavukcuoglu , K . , Kuksa , P . ( 2011 ) . Natural language processing ( almost ) scratch . JMLR , 12 , 2493 – 2537 . Craven , M . Kumlien , J . ( 1999 ) . Constructing biolog - ical knowledge bases extracting information text sources . ISMB-99 , 77 – 86 . DeJong , G . F . ( 1982 ) . overview FRUMP system . Lehnert , W . G . Ringle , M . H . ( Eds . ) , Strategies Natural Language Processing , 149 – 176 . LEA . dos Santos , C . N . , Xiang , B . , Zhou , B . ( 2015 ) . Clas - sifying relations ranking convolutional neural net - works . ACL 2015 . Etzioni , O . , Cafarella , M . , Downey , D . , Popescu , . - M . , Shaked , T . , Soderland , S . , Weld , D . S . , Yates , . ( 2005 ) . Unsupervised named-entity extraction web : experimental study . Artificial Intelligence , 165 ( 1 ) , 91 – 134 . Fader , . , Soderland , S . , Etzioni , O . ( 2011 ) . Identifying relations open information extraction . EMNLP-11 , 1535 – 1545 . Ferro , L . , Gerber , L . , Mani , . , Sundheim , B . , Wilson , G . ( 2005 ) . Tides 2005 standard annotation temporal expressions . Tech . rep . , MITRE . Grishman , R . Sundheim , B . ( 1995 ) . Design MUC - 6 evaluation . MUC-6 , 1 – 11 . Hearst , M . . ( 1992a ) . Automatic acquisition hyponyms large text corpora . COLING-92 . Hearst , M . . ( 1992b ) . Automatic acquisition hyponyms large text corpora . COLING-92 . COLING . Hearst , M . . ( 1998 ) . Automatic discovery WordNet re - lations . Fellbaum , C . ( Ed . ) , WordNet : Electronic Lexical Database . MIT Press . Hendrickx , . , Kim , S . N . , Kozareva , Z . , Nakov , P . , Ó Séaghdha , D . , Padó , S . , Pennacchiotti , M . , Romano , L . , Szpakowicz , S . ( 2009 ) . Semeval-2010 task 8 : Multi - way classification semantic relations pairs nominals . Proceedings Workshop Semantic Evaluations : Recent Achievements Future Directions , 94 – 99 . Hobbs , J . R . , Appelt , D . E . , Bear , J . , Israel , D . , Kameyama , M . , Stickel , M . E . , Tyson , M . ( 1997 ) . FASTUS : cascaded finite-state transducer extracting information natural-language text . Roche , E . Schabes , Y . ( Eds . ) , Finite-State Language Processing , 383 – 406 . MIT Press . Huang , Z . , Xu , W . , Yu , K . ( 2015 ) . Bidirectional LSTM - CRF models sequence tagging . arXiv preprint arXiv : 1508.01991 . Huffman , S . ( 1996 ) . Learning information extraction pat - terns examples . Wertmer , S . , Riloff , E . , Scheller , G . ( Eds . ) , Connectionist , Statistical , Sym - bolic Approaches Learning Natural Language Process - ing , 246 – 260 . Springer . ISO8601 ( 2004 ) . Data elements interchange formats — information interchange — representation dates times . Tech . rep . , International Organization Standards ( ISO ) . Jacobs , P . S . Rau , L . F . ( 1990 ) . SCISOR : system extracting information on-line news . CACM , 33 ( 11 ) , 88 – 97 . Ji , H . , Grishman , R . , Dang , H . T . ( 2010 ) . Overview tac 2011 knowledge base population track . TAC-11 . Jones , R . , McCallum , . , Nigam , K . , Riloff , E . ( 1999 ) . Bootstrapping text learning tasks . IJCAI-99 Work - shop Text Mining : Foundations , Techniques Appli - cations . Exercises 31 Lafferty , J . D . , McCallum , . , Pereira , F . C . N . ( 2001 ) . Conditional random fields : Probabilistic models seg - menting labeling sequence data . ICML 2001 . Lample , G . , Ballesteros , M . , Subramanian , S . , Kawakami , K . , Dyer , C . ( 2016 ) . Neural architectures named entity recognition . NAACL HLT 2016 . Lehnert , W . G . , Cardie , C . , Fisher , D . , Riloff , E . , Williams , R . ( 1991 ) . Description CIRCUS system MUC-3 . Sundheim , B . ( Ed . ) , MUC-3 , 223 – 233 . Ma , X . Hovy , E . H . ( 2016 ) . End-to-end sequence label - ing via bi-directional LSTM-CNNs-CRF . ACL 2016 . McCallum , . , Freitag , D . , Pereira , F . C . N . ( 2000 ) . Maximum entropy Markov models information extrac - tion segmentation . ICML 2000 , 591 – 598 . Mikheev , . , Moens , M . , Grover , C . ( 1999 ) . Named entity recognition gazetteers . EACL-99 , 1 – 8 . Mintz , M . , Bills , S . , Snow , R . , Jurafsky , D . ( 2009 ) . Distant supervision relation extraction labeled data . ACL IJCNLP 2009 . Miwa , M . Bansal , M . ( 2016 ) . End-to-end relation ex - traction lstms sequences tree structures . ACL 2016 , 1105 – 1116 . Morgan , . . , Hirschman , L . , Colosimo , M . , Yeh , . S . , Colombe , J . B . ( 2004 ) . Gene name identification normalization model organism database . Journal Biomedical Informatics , 37 ( 6 ) , 396 – 410 . Peng , N . , Poon , H . , Quirk , C . , Toutanova , K . , Yih , W . - t . ( 2017 ) . Cross-sentence n-ary relation extraction graph LSTMs . TACL , 5 , 101 – 115 . Pustejovsky , J . , Castaño , J . , Ingria , R . , Saurı́ , R . , Gaizauskas , R . , Setzer , . , Katz , G . ( 2003a ) . TimeML : robust spec - ification event temporal expressions text . Pro - ceedings 5th International Workshop Computa - tional Semantics ( IWCS-5 ) . Pustejovsky , J . , Hanks , P . , Saurı́ , R . , , . , Gaizauskas , R . , Setzer , . , Radev , D . , Sundheim , B . , Day , D . S . , Ferro , L . , Lazo , M . ( 2003b ) . TIMEBANK corpus . Proceedings Corpus Linguistics 2003 Conference , 647 – 656 . UCREL Technical Paper number 16 . Pustejovsky , J . , Ingria , R . , Saurı́ , R . , Castaño , J . , Littman , J . , Gaizauskas , R . , Setzer , . , Katz , G . , Mani , . ( 2005 ) . Specification Language TimeML , chap . 27 . Oxford . Riedel , S . , Yao , L . , McCallum , . ( 2010 ) . Modeling re - lations mentions labeled text . Machine Learning Knowledge Discovery Databases , 148 – 163 . Springer . Riedel , S . , Yao , L . , McCallum , . , Marlin , B . M . ( 2013 ) . Relation extraction matrix factorization universal schemas . NAACL HLT 2013 . Riloff , E . ( 1993 ) . Automatically constructing dictionary information extraction tasks . AAAI-93 , 811 – 816 . Riloff , E . ( 1996 ) . Automatically generating extraction pat - terns untagged text . AAAI-96 , 117 – 124 . Riloff , E . Jones , R . ( 1999 ) . Learning dictionaries information extraction multi-level bootstrapping . AAAI-99 , 474 – 479 . Ritter , . , Zettlemoyer , L . , Mausam , Etzioni , O . ( 2013 ) . Modeling missing data distant supervision informa - tion extraction . . TACL , 1 , 367 – 378 . Schank , R . C . Abelson , R . P . ( 1977 ) . Scripts , Plans , Goals Understanding . Lawrence Erlbaum . Snow , R . , Jurafsky , D . , Ng , . Y . ( 2005 ) . Learning syn - tactic patterns automatic hypernym discovery . Saul , L . K . , Weiss , Y . , Bottou , L . ( Eds . ) , NIPS 17 , 1297 – 1304 . MIT Press . Socher , R . , Huval , B . , Manning , C . D . , Ng , . Y . ( 2012 ) . Semantic compositionality recursive matrix-vector spaces . EMNLP 2012 , 1201 – 1211 . Soderland , S . , Fisher , D . , Aseltine , J . , Lehnert , W . G . ( 1995 ) . CRYSTAL : Inducing conceptual dictionary . IJCAI-95 , 1134 – 1142 . Strötgen , J . Gertz , M . ( 2013 ) . Multilingual cross - domain temporal tagging . Language Resources Eval - uation , 47 ( 2 ) , 269 – 298 . Sundheim , B . ( Ed . ) . ( 1991 ) . Proceedings MUC-3 . Sundheim , B . ( Ed . ) . ( 1992 ) . Proceedings MUC-4 . Sundheim , B . ( Ed . ) . ( 1993 ) . Proceedings MUC-5 , Balti - , MD . Sundheim , B . ( Ed . ) . ( 1995 ) . Proceedings MUC-6 . Surdeanu , M . ( 2013 ) . Overview TAC2013 Knowledge Base Population evaluation : English slot filling tempo - ral slot filling . TAC-13 . UzZaman , N . , Llorens , H . , Derczynski , L . , Allen , J . , Ver - hagen , M . , Pustejovsky , J . ( 2013 ) . Semeval-2013 task 1 : Tempeval-3 : Evaluating time expressions , events , temporal relations . SemEval-13 , 1 – 9 . Verhagen , M . , Gaizauskas , R . , Schilder , F . , Hepple , M . , Moszkowicz , J . , Pustejovsky , J . ( 2009 ) . tempeval challenge : identifying temporal relations text . Language Resources Evaluation , 43 ( 2 ) , 161 – 179 . Verhagen , M . , Mani , . , Sauri , R . , Knippen , R . , Jang , S . B . , Littman , J . , Rumshisky , . , Phillips , J . , Pustejovsky , J . ( 2005 ) . Automating temporal annotation tarsqi . ACL-05 , 81 – 84 . Vrandečić , D . Krötzsch , M . ( 2014 ) . Wikidata : free collaborative knowledge base . CACM , 57 ( 10 ) , 78 – 85 . Wu , F . Weld , D . S . ( 2007 ) . Autonomously semantifying Wikipedia . CIKM-07 , 41 – 50 . Wu , F . Weld , D . S . ( 2010 ) . Open information extraction Wikipedia . ACL 2010 , 118 – 127 . Zhou , G . , Su , J . , Zhang , J . , Zhang , M . ( 2005 ) . Explor - ing various knowledge relation extraction . ACL-05 , 427 – 434 .