Chapter 2 Linear text classification We begin with the problem of text classification : given a text document , assign it a dis - crete label y ∈ Y , where Y is the set of possible labels . Text classification has many ap - plications , from spam filtering to the analysis of electronic health records . This chapter describes some of the most well known and effective algorithms for text classification , from a mathematical perspective that should help you understand what they do and why they work . Text classification is also a building block in more elaborate natural language processing tasks . For readers without a background in machine learning or statistics , the material in this chapter will take more time to digest than most of the subsequent chap - ters . But this investment will pay off as the mathematical principles behind these basic classification algorithms reappear in other contexts throughout the book . 2.1 The bag of words To perform text classification , the first question is how to represent each document , or instance . A common approach is to use a column vector of word counts , e.g . , x = [ 0 , 1 , 1 , 0 , 0 , 2 , 0 , 1 , 13 , 0 . . . ] > , where xj is the count of word j . The length of x is V , | V | , where V is the set of possible words in the vocabulary . In linear classification , the classi - fication decision is based on a weighted sum of individual feature counts , such as word counts . The object x is a vector , but it is often called a bag of words , because it includes only information about the count of each word , and not the order in which the words appear . With the bag of words representation , we are ignoring grammar , sentence boundaries , paragraphs — everything but the words . Yet the bag of words model is surprisingly effective for text classification . If you see the word whale in a document , is it fiction or non - fiction ? What if you see the word molybdenum ? For many labeling problems , individual words can be strong predictors . 13 14 CHAPTER 2 . LINEAR TEXT CLASSIFICATION To predict a label from a bag-of-words , we can assign a score to each word in the vo - cabulary , measuring the compatibility with the label . For example , for the label FICTION , we might assign a positive score to the word whale , and a negative score to the word molybdenum . These scores are called weights , and they are arranged in a column vector θ . Suppose that you want a multiclass classifier , where K , | Y | > 2 . For example , you might want to classify news stories about sports , celebrities , music , and business . The goal is to predict a label ŷ , given the bag of words x , using the weights θ . For each label y ∈ Y , we compute a score Ψ ( x , y ) , which is a scalar measure of the compatibility between the bag-of-words x and the label y . In a linear bag-of-words classifier , this score is the vector inner product between the weights θ and the output of a feature function f ( x , y ) , Ψ ( x , y ) = θ · f ( x , y ) = ∑ j θjfj ( x , y ) . [ 2.1 ] As the notation suggests , f is a function of two arguments , the word counts x and the label y , and it returns a vector output . For example , given arguments x and y , element j of this feature vector might be , fj ( x , y ) = { xwhale , if y = FICTION 0 , otherwise [ 2.2 ] This function returns the count of the word whale if the label is FICTION , and it returns zero otherwise . The index j depends on the position of whale in the vocabulary , and of FICTION in the set of possible labels . The corresponding weight θj then scores the compatibility of the word whale with the label FICTION . 1 A positive score means that this word makes the label more likely . The output of the feature function can be formalized as a vector : f ( x , y = 1 ) = [ x ; 0 ; 0 ; . . . ; 0 ︸ ︷ ︷ ︸ ( K − 1 ) × V ] [ 2.3 ] f ( x , y = 2 ) = [ 0 ; 0 ; . . . ; 0 ︸ ︷ ︷ ︸ V ; x ; 0 ; 0 ; . . . ; 0 ︸ ︷ ︷ ︸ ( K − 2 ) × V ] [ 2.4 ] f ( x , y = K ) = [ 0 ; 0 ; . . . ; 0 ︸ ︷ ︷ ︸ ( K − 1 ) × V ; x ] , [ 2.5 ] where [ 0 ; 0 ; . . . ; 0 ︸ ︷ ︷ ︸ ( K − 1 ) × V ] is a column vector of ( K − 1 ) × V zeros , and the semicolon indicates vertical concatenation . For each of the K possible labels , the feature function returns a 1In practice , both f and θ may be implemented as a dictionary rather than vectors , so that it is not necessary to explicitly identify j . In such an implementation , the tuple ( whale , FICTION ) acts as a key in both dictionaries ; the values in f are feature counts , and the values in θ are weights . Jacob Eisenstein . Draft of October 15 , 2018 . 2.1 . THE BAG OF WORDS 15 vector that is mostly zeros , with a column vector of word counts x inserted in a location that depends on the specific label y . This arrangement is shown in Figure 2.1 . The notation may seem awkward at first , but it generalizes to an impressive range of learning settings , particularly structure prediction , which is the focus of Chapters 7-11 . Given a vector of weights , θ ∈ RV K , we can now compute the score Ψ ( x , y ) by Equa - tion 2.1 . This inner product gives a scalar measure of the compatibility of the observation xwith label y . 2 For any document x , we predict the label ŷ , ŷ = argmax y ∈ Y Ψ ( x , y ) [ 2.6 ] Ψ ( x , y ) = θ · f ( x , y ) . [ 2.7 ] This inner product notation gives a clean separation between the data ( x and y ) and the parameters ( θ ) . While vector notation is used for presentation and analysis , in code the weights and feature vector can be implemented as dictionaries . The inner product can then be com - puted as a loop . In python : def compute_score ( x , y , weights ) : total = 0 for feature , count in feature_function ( x , y ) . items ( ) : total + = weights [ feature ] * count return total This representation is advantageous because it avoids storing and iterating over the many features whose counts are zero . It is common to add an offset feature at the end of the vector of word counts x , which is always 1 . We then have to also add an extra zero to each of the zero vectors , to make the vector lengths match . This gives the entire feature vector f ( x , y ) a length of ( V + 1 ) × K . The weight associated with this offset feature can be thought of as a bias for or against each label . For example , if we expect most emails to be spam , then the weight for the offset feature for y = SPAM should be larger than the weight for the offset feature for y = NOT-SPAM . Returning to the weights θ , where do they come from ? One possibility is to set them by hand . If we wanted to distinguish , say , English from Spanish , we can use English and Spanish dictionaries , and set the weight to one for each word that appears in the 2Only V × ( K − 1 ) features and weights are necessary . By stipulating that Ψ ( x , y = K ) = 0 regardless of x , it is possible to implement any classification rule that can be achieved with V × K features and weights . This is the approach taken in binary classification rules like y = Sign ( β · x + a ) , where β is a vector of weights , a is an offset , and the label set is Y = { − 1 , 1 } . However , for multiclass classification , it is more concise to write θ · f ( x , y ) for all y ∈ Y . Under contract with MIT Press , shared under CC-BY-NC-ND license . 16 CHAPTER 2 . LINEAR TEXT CLASSIFICATION It was the best of times , it was the worst of times . . . x it was the best worst times 1 2 of2 2 2 2 1 0 . . . 0 0 . . . 0 . . . 0 . . . 0 . . . 0 . . . 0 . . . 0 . . . 0 1 x 0 0 0 f ( x , y = News ) y = Fiction y = News y = Gossip y = Sports Bag of words Feature vectorOriginal text < OFFSET > aardvark zyxt Figure 2.1 : The bag-of-words and feature vector representations , for a hypothetical text classification task . associated dictionary . For example , 3 θ ( E , bicycle ) = 1 θ ( S , bicycle ) = 0 θ ( E , bicicleta ) = 0 θ ( S , bicicleta ) = 1 θ ( E , con ) = 1 θ ( S , con ) = 1 θ ( E , ordinateur ) = 0 θ ( S , ordinateur ) = 0 . Similarly , if we want to distinguish positive and negative sentiment , we could use posi - tive and negative sentiment lexicons ( see subsection 4.1.2 ) , which are defined by social psychologists ( Tausczik and Pennebaker , 2010 ) . But it is usually not easy to set classification weights by hand , due to the large number of words and the difficulty of selecting exact numerical weights . Instead , we will learn the weights from data . Email users manually label messages as SPAM ; newspapers label their own articles as BUSINESS or STYLE . Using such instance labels , we can automatically acquire weights using supervised machine learning . This chapter will discuss several machine learning approaches for classification . The first is based on probability . For a review of probability , consult Appendix A . 3In this notation , each tuple ( language , word ) indexes an element in θ , which remains a vector . Jacob Eisenstein . Draft of October 15 , 2018 . 2.2 . NAÏVE BAYES 17 2.2 Naı̈ve Bayes The joint probability of a bag of words x and its true label y is written p ( x , y ) . Suppose we have a dataset of N labeled instances , { ( x ( i ) , y ( i ) ) } Ni = 1 , which we assume are indepen - dent and identically distributed ( IID ) ( see section A . 3 ) . Then the joint probability of the entire dataset , written p ( x ( 1 : N ) , y ( 1 : N ) ) , is equal to ∏ N i = 1 pX , Y ( x ( i ) , y ( i ) ) . 4 What does this have to do with classification ? One approach to classification is to set the weights θ so as to maximize the joint probability of a training set of labeled docu - ments . This is known as maximum likelihood estimation : θ̂ = argmax θ p ( x ( 1 : N ) , y ( 1 : N ) ; θ ) [ 2.8 ] = argmax θ N ∏ i = 1 p ( x ( i ) , y ( i ) ; θ ) [ 2.9 ] = argmax θ N ∑ i = 1 log p ( x ( i ) , y ( i ) ; θ ) . [ 2.10 ] The notation p ( x ( i ) , y ( i ) ; θ ) indicates that θ is a parameter of the probability function . The product of probabilities can be replaced by a sum of log-probabilities because the log func - tion is monotonically increasing over positive arguments , and so the same θ will maxi - mize both the probability and its logarithm . Working with logarithms is desirable because of numerical stability : on a large dataset , multiplying many probabilities can underflow to zero . 5 The probability p ( x ( i ) , y ( i ) ; θ ) is defined through a generative model — an idealized random process that has generated the observed data . 6 Algorithm 1 describes the gener - ative model underlying the Naı̈ve Bayes classifier , with parameters θ = { µ , φ } . • The first line of this generative model encodes the assumption that the instances are mutually independent : neither the label nor the text of document i affects the label or text of document j . 7 Furthermore , the instances are identically distributed : the 4The notation p X , Y ( x ( i ) , y ( i ) ) indicates the joint probability that random variables X and Y take the specific values x ( i ) and y ( i ) respectively . The subscript will often be omitted when it is clear from context . For a review of random variables , see Appendix A . 5Throughout this text , you may assume all logarithms and exponents are base 2 , unless otherwise indi - cated . Any reasonable base will yield an identical classifier , and base 2 is most convenient for working out examples by hand . 6Generative models will be used throughout this text . They explicitly define the assumptions underlying the form of a probability distribution over observed and latent variables . For a readable introduction to generative models in statistics , see Blei ( 2014 ) . 7Can you think of any cases in which this assumption is too strong ? Under contract with MIT Press , shared under CC-BY-NC-ND license . 18 CHAPTER 2 . LINEAR TEXT CLASSIFICATION Algorithm 1 Generative process for the Naı̈ve Bayes classification model for Instance i ∈ { 1 , 2 , . . . , N } do : Draw the label y ( i ) ∼ Categorical ( µ ) ; Draw the word counts x ( i ) | y ( i ) ∼ Multinomial ( φy ( i ) ) . distributions over the label y ( i ) and the text x ( i ) ( conditioned on y ( i ) ) are the same for all instances i . In other words , we make the assumption that every document has the same distribution over labels , and that each document’s distribution over words depends only on the label , and not on anything else about the document . We also assume that the documents don’t affect each other : if the word whale appears in document i = 7 , that does not make it any more or less likely that it will appear again in document i = 8 . • The second line of the generative model states that the random variable y ( i ) is drawn from a categorical distribution with parameter µ . Categorical distributions are like weighted dice : the column vector µ = [ µ1 ; µ2 ; . . . ; µK ] gives the probabilities of each label , so that the probability of drawing label y is equal to µy . For example , if Y = { POSITIVE , NEGATIVE , NEUTRAL } , we might have µ = [ 0.1 ; 0.7 ; 0.2 ] . We require ∑ y ∈ Y µy = 1 and µy ≥ 0 , ∀ y ∈ Y : each label’s probability is non-negative , and the sum of these probabilities is equal to one . 8 • The third line describes how the bag-of-words counts x ( i ) are generated . By writing x ( i ) | y ( i ) , this line indicates that the word counts are conditioned on the label , so that the joint probability is factored using the chain rule , pX , Y ( x ( i ) , y ( i ) ) = pX | Y ( x ( i ) | y ( i ) ) × pY ( y ( i ) ) . [ 2.11 ] The specific distribution pX | Y is the multinomial , which is a probability distribu - tion over vectors of non-negative counts . The probability mass function for this distribution is : pmult ( x ; φ ) = B ( x ) V ∏ j = 1 φ xj j [ 2.12 ] B ( x ) = ( ∑ V j = 1 xj ) ! ∏ V j = 1 ( xj ! ) . [ 2.13 ] 8Formally , we require µ ∈ ∆ K − 1 , where ∆ K − 1 is the K − 1 probability simplex , the set of all vectors of K nonnegative numbers that sum to one . Because of the sum-to-one constraint , there are K − 1 degrees of freedom for a vector of size K . Jacob Eisenstein . Draft of October 15 , 2018 . 2.2 . NAÏVE BAYES 19 As in the categorical distribution , the parameter φj can be interpreted as a probabil - ity : specifically , the probability that any given token in the document is the word j . The multinomial distribution involves a product over words , with each term in the product equal to the probability φj , exponentiated by the count xj . Words that have zero count play no role in this product , because φ0j = 1 . The term B ( x ) is called the multinomial coefficient . It doesn’t depend on φ , and can usually be ignored . Can you see why we need this term at all ? 9 The notation p ( x | y ; φ ) indicates the conditional probability of word counts x given label y , with parameter φ , which is equal to pmult ( x ; φy ) . By specifying the multinomial distribution , we describe the multinomial Naı̈ve Bayes classifier . Why “ naı̈ve ” ? Because the multinomial distribution treats each word token indepen - dently , conditioned on the class : the probability mass function factorizes across the counts . 10 2.2.1 Types and tokens A slight modification to the generative model of Naı̈ve Bayes is shown in Algorithm 2 . Instead of generating a vector of counts of types , x , this model generates a sequence of tokens , w = ( w1 , w2 , . . . , wM ) . The distinction between types and tokens is critical : xj ∈ { 0 , 1 , 2 , . . . , M } is the count of word type j in the vocabulary , e.g . , the number of times the word cannibal appears ; wm ∈ V is the identity of token m in the document , e.g . wm = cannibal . The probability of the sequencew is a product of categorical probabilities . Algorithm 2 makes a conditional independence assumption : each tokenw ( i ) m is independent of all other tokens w ( i ) n6 = m , conditioned on the label y ( i ) . This is identical to the “ naı̈ve ” independence assumption implied by the multinomial distribution , and as a result , the optimal parame - ters for this model are identical to those in multinomial Naı̈ve Bayes . For any instance , the probability assigned by this model is proportional to the probability under multinomial Naı̈ve Bayes . The constant of proportionality is the multinomial coefficientB ( x ) . Because B ( x ) ≥ 1 , the probability for a vector of counts x is at least as large as the probability for a list of words w that induces the same counts : there can be many word sequences that correspond to a single vector of counts . For example , man bites dog and dog bites man correspond to an identical count vector , { bites : 1 , dog : 1 , man : 1 } , and B ( x ) is equal to the total number of possible word orderings for count vector x . 9Technically , a multinomial distribution requires a second parameter , the total number of word counts in x . In the bag-of-words representation is equal to the number of words in the document . However , this parameter is irrelevant for classification . 10You can plug in any probability distribution to the generative story and it will still be Naı̈ve Bayes , as long as you are making the “ naı̈ve ” assumption that the features are conditionally independent , given the label . For example , a multivariate Gaussian with diagonal covariance is naı̈ve in exactly the same sense . Under contract with MIT Press , shared under CC-BY-NC-ND license . 20 CHAPTER 2 . LINEAR TEXT CLASSIFICATION Algorithm 2 Alternative generative process for the Naı̈ve Bayes classification model for Instance i ∈ { 1 , 2 , . . . , N } do : Draw the label y ( i ) ∼ Categorical ( µ ) ; for Token m ∈ { 1 , 2 , . . . , Mi } do : Draw the token w ( i ) m | y ( i ) ∼ Categorical ( φy ( i ) ) . Sometimes it is useful to think of instances as counts of types , x ; other times , it is better to think of them as sequences of tokens , w . If the tokens are generated from a model that assumes conditional independence , then these two views lead to probability models that are identical , except for a scaling factor that does not depend on the label or the parameters . 2.2.2 Prediction The Naı̈ve Bayes prediction rule is to choose the label y which maximizes log p ( x , y ; µ , φ ) : ŷ = argmax y log p ( x , y ; µ , φ ) [ 2.14 ] = argmax y log p ( x | y ; φ ) + log p ( y ; µ ) [ 2.15 ] Now we can plug in the probability distributions from the generative story . log p ( x | y ; φ ) + log p ( y ; µ ) = log   B ( x ) V ∏ j = 1 φ xj y , j   + logµy [ 2.16 ] = logB ( x ) + V ∑ j = 1 xj log φy , j + logµy [ 2.17 ] = logB ( x ) + θ · f ( x , y ) , [ 2.18 ] where θ = [ θ ( 1 ) ; θ ( 2 ) ; . . . ; θ ( K ) ] [ 2.19 ] θ ( y ) = [ log φy , 1 ; log φy , 2 ; . . . ; log φy , V ; logµy ] [ 2.20 ] The feature function f ( x , y ) is a vector of V word counts and an offset , padded by zeros for the labels not equal to y ( see Equations 2.3-2.5 , and Figure 2.1 ) . This construction ensures that the inner product θ · f ( x , y ) only activates the features whose weights are in θ ( y ) . These features and weights are all we need to compute the joint log-probability log p ( x , y ) for each y . This is a key point : through this notation , we have converted the problem of computing the log-likelihood for a document-label pair ( x , y ) into the compu - tation of a vector inner product . Jacob Eisenstein . Draft of October 15 , 2018 . 2.2 . NAÏVE BAYES 21 2.2.3 Estimation The parameters of the categorical and multinomial distributions have a simple interpre - tation : they are vectors of expected frequencies for each possible event . Based on this interpretation , it is tempting to set the parameters empirically , φy , j = count ( y , j ) ∑ V j ′ = 1 count ( y , j ′ ) = ∑ i : y ( i ) = y x ( i ) j ∑ V j ′ = 1 ∑ i : y ( i ) = y x ( i ) j ′ , [ 2.21 ] where count ( y , j ) refers to the count of word j in documents with label y . Equation 2.21 defines the relative frequency estimate for φ . It can be justified as a maximum likelihood estimate : the estimate that maximizes the probability p ( x ( 1 : N ) , y ( 1 : N ) ; θ ) . Based on the generative model in Algorithm 1 , the log-likelihood is , L ( φ , µ ) = N ∑ i = 1 log pmult ( x ( i ) ; φy ( i ) ) + log pcat ( y ( i ) ; µ ) , [ 2.22 ] which is now written as a function L of the parameters φ and µ . Let’s continue to focus on the parameters φ . Since p ( y ) is constant with respect to φ , we can drop it : L ( φ ) = N ∑ i = 1 log pmult ( x ( i ) ; φy ( i ) ) = N ∑ i = 1 logB ( x ( i ) ) + V ∑ j = 1 x ( i ) j log φy ( i ) , j , [ 2.23 ] where B ( x ( i ) ) is constant with respect to φ . Maximum-likelihood estimation chooses φ to maximize the log-likelihood L . How - ever , the solution must obey the following constraints : V ∑ j = 1 φy , j = 1 ∀ y [ 2.24 ] These constraints can be incorporated by adding a set of Lagrange multipliers to the objec - tive ( see Appendix B for more details ) . To solve for each θy , we maximize the Lagrangian , ` ( φy ) = ∑ i : y ( i ) = y V ∑ j = 1 x ( i ) j log φy , j − λ ( V ∑ j = 1 φy , j − 1 ) . [ 2.25 ] Differentiating with respect to the parameter φy , j yields , ∂ ` ( φy ) ∂ φy , j = ∑ i : y ( i ) = y x ( i ) j / φy , j − λ . [ 2.26 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 22 CHAPTER 2 . LINEAR TEXT CLASSIFICATION The solution is obtained by setting each element in this vector of derivatives equal to zero , λφy , j = ∑ i : y ( i ) = y x ( i ) j [ 2.27 ] φy , j ∝ ∑ i : y ( i ) = y x ( i ) j = N ∑ i = 1 δ ( y ( i ) = y ) x ( i ) j = count ( y , j ) , [ 2.28 ] where δ ( y ( i ) = y ) is a delta function , also sometimes called an indicator function , which returns one if y ( i ) = y . The symbol ∝ indicates that φy , j is proportional to the right-hand side of the equation . Equation 2.28 shows three different notations for the same thing : a sum over the word counts for all documents i such that the label y ( i ) = y . This gives a solution for each φy up to a constant of proportionality . Now recall the constraint ∑ V j = 1φy , j = 1 , which arises because φy represents a vector of probabilities for each word in the vocabulary . This constraint leads to an exact solution , which does not depend on λ : φy , j = count ( y , j ) ∑ V j ′ = 1 count ( y , j ′ ) . [ 2.29 ] This is equal to the relative frequency estimator from Equation 2.21 . A similar derivation gives µy ∝ ∑ N i = 1 δ ( y ( i ) = y ) . 2.2.4 Smoothing With text data , there are likely to be pairs of labels and words that never appear in the training set , leaving φy , j = 0 . For example , the word molybdenum may have never yet appeared in a work of fiction . But choosing a value of φFICTION , molybdenum = 0 would allow this single feature to completely veto a label , since p ( FICTION | x ) = 0 if xmolybdenum > 0 . This is undesirable , because it imposes high variance : depending on what data hap - pens to be in the training set , we could get vastly different classification rules . One so - lution is to smooth the probabilities , by adding a “ pseudocount ” of α to each count , and then normalizing . φy , j = α + count ( y , j ) V α + ∑ V j ′ = 1 count ( y , j ′ ) [ 2.30 ] This is called Laplace smoothing . 11 The pseudocount α is a hyperparameter , because it controls the form of the log-likelihood function , which in turn drives the estimation of φ . 11Laplace smoothing has a Bayesian justification , in which the generative model is extended to include φ as a random variable . The resulting distribution over φ depends on both the data ( x and y ) and the prior probability p ( φ ; α ) . The corresponding estimate of φ is called maximum a posteriori , or MAP . This is in contrast with maximum likelihood , which depends only on the data . Jacob Eisenstein . Draft of October 15 , 2018 . 2.2 . NAÏVE BAYES 23 Smoothing reduces variance , but moves us away from the maximum likelihood esti - mate : it imposes a bias . In this case , the bias points towards uniform probabilities . Ma - chine learning theory shows that errors on heldout data can be attributed to the sum of bias and variance ( Mohri et al . , 2012 ) . In general , techniques for reducing variance often increase the bias , leading to a bias-variance tradeoff . • Unbiased classifiers may overfit the training data , yielding poor performance on unseen data . • But if the smoothing is too large , the resulting classifier can underfit instead . In the limit of α → ∞ , there is zero variance : you get the same classifier , regardless of the data . However , the bias is likely to be large . Similar issues arise throughout machine learning . Later in this chapter we will encounter regularization , which controls the bias-variance tradeoff for logistic regression and large - margin classifiers ( subsection 2.5.1 ) ; subsection 3.3.2 describes techniques for controlling variance in deep learning ; chapter 6 describes more elaborate methods for smoothing empirical probabilities . 2.2.5 Setting hyperparameters Returning to Naı̈ve Bayes , how should we choose the best value of hyperparameters like α ? Maximum likelihood will not work : the maximum likelihood estimate of α on the training set will always be α = 0 . In many cases , what we really want is accuracy : the number of correct predictions , divided by the total number of predictions . ( Other measures of classification performance are discussed in section 4.4 . ) As we will see , it is hard to optimize for accuracy directly . But for scalar hyperparameters like α , tun - ing can be performed by a simple heuristic called grid search : try a set of values ( e.g . , α ∈ { 0.001 , 0.01 , 0.1 , 1 , 10 } ) , compute the accuracy for each value , and choose the setting that maximizes the accuracy . The goal is to tune α so that the classifier performs well on unseen data . For this reason , the data used for hyperparameter tuning should not overlap the training set , where very small values of α will be preferred . Instead , we hold out a development set ( also called a tuning set ) for hyperparameter selection . This development set may consist of a small fraction of the labeled data , such as 10 % . We also want to predict the performance of our classifier on unseen data . To do this , we must hold out a separate subset of data , called the test set . It is critical that the test set not overlap with either the training or development sets , or else we will overestimate the performance that the classifier will achieve on unlabeled data in the future . The test set should also not be used when making modeling decisions , such as the form of the feature function , the size of the vocabulary , and so on ( these decisions are reviewed in chapter 4 . ) Under contract with MIT Press , shared under CC-BY-NC-ND license . 24 CHAPTER 2 . LINEAR TEXT CLASSIFICATION The ideal practice is to use the test set only once — otherwise , the test set is used to guide the classifier design , and test set accuracy will diverge from accuracy on truly unseen data . Because annotated data is expensive , this ideal can be hard to follow in practice , and many test sets have been used for decades . But in some high-impact applications like machine translation and information extraction , new test sets are released every year . When only a small amount of labeled data is available , the test set accuracy can be unreliable . K-fold cross-validation is one way to cope with this scenario : the labeled data is divided into K folds , and each fold acts as the test set , while training on the other folds . The test set accuracies are then aggregated . In the extreme , each fold is a single data point ; this is called leave-one-out cross-validation . To perform hyperparameter tuning in the context of cross-validation , another fold can be used for grid search . It is important not to repeatedly evaluate the cross-validated accuracy while making design decisions about the classifier , or you will overstate the accuracy on truly unseen data . 2.3 Discriminative learning Naı̈ve Bayes is easy to work with : the weights can be estimated in closed form , and the probabilistic interpretation makes it relatively easy to extend . However , the assumption that features are independent can seriously limit its accuracy . Thus far , we have defined the feature function f ( x , y ) so that it corresponds to bag-of-words features : one feature per word in the vocabulary . In natural language , bag-of-words features violate the as - sumption of conditional independence — for example , the probability that a document will contain the word naı̈ve is surely higher given that it also contains the word Bayes — but this violation is relatively mild . However , good performance on text classification often requires features that are richer than the bag-of-words : • To better handle out-of-vocabulary terms , we want features that apply to multiple words , such as prefixes and suffixes ( e.g . , anti - , un - , - ing ) and capitalization . • We also want n-gram features that apply to multi-word units : bigrams ( e.g . , not good , not bad ) , trigrams ( e.g . , not so bad , lacking any decency , never before imagined ) , and beyond . These features flagrantly violate the Naı̈ve Bayes independence assumption . Consider what happens if we add a prefix feature . Under the Naı̈ve Bayes assumption , the joint probability of a word and its prefix are computed with the following approximation : 12 Pr ( word = unfit , prefix = un - | y ) ≈ Pr ( prefix = un - | y ) × Pr ( word = unfit | y ) . 12The notation Pr ( · ) refers to the probability of an event , and p ( · ) refers to the probability density or mass for a random variable ( see Appendix A ) . Jacob Eisenstein . Draft of October 15 , 2018 . 2.3 . DISCRIMINATIVE LEARNING 25 To test the quality of the approximation , we can manipulate the left-hand side by applying the chain rule , Pr ( word = unfit , prefix = un - | y ) = Pr ( prefix = un - | word = unfit , y ) [ 2.31 ] × Pr ( word = unfit | y ) [ 2.32 ] But Pr ( prefix = un - | word = unfit , y ) = 1 , since un - is guaranteed to be the prefix for the word unfit . Therefore , Pr ( word = unfit , prefix = un - | y ) = 1 × Pr ( word = unfit | y ) [ 2.33 ] � Pr ( prefix = un - | y ) × Pr ( word = unfit | y ) , [ 2.34 ] because the probability of any given word starting with the prefix un - is much less than one . Naı̈ve Bayes will systematically underestimate the true probabilities of conjunctions of positively correlated features . To use such features , we need learning algorithms that do not rely on an independence assumption . The origin of the Naı̈ve Bayes independence assumption is the learning objective , p ( x ( 1 : N ) , y ( 1 : N ) ) , which requires modeling the probability of the observed text . In clas - sification problems , we are always given x , and are only interested in predicting the label y . In this setting , modeling the probability of the text x seems like a difficult and unnec - essary task . Discriminative learning algorithms avoid this task , and focus directly on the problem of predicting y . 2.3.1 Perceptron In Naı̈ve Bayes , the weights can be interpreted as parameters of a probabilistic model . But this model requires an independence assumption that usually does not hold , and limits our choice of features . Why not forget about probability and learn the weights in an error - driven way ? The perceptron algorithm , shown in Algorithm 3 , is one way to do this . The algorithm is simple : if you make a mistake , increase the weights for features that are active with the correct label y ( i ) , and decrease the weights for features that are active with the guessed label ŷ . Perceptron is an online learning algorithm , since the classifier weights change after every example . This is different from Naı̈ve Bayes , which is a batch learning algorithm : it computes statistics over the entire dataset , and then sets the weights in a single operation . Algorithm 3 is vague about when this online learning procedure terminates . We will return to this issue shortly . The perceptron algorithm may seem like an unprincipled heuristic : Naı̈ve Bayes has a solid foundation in probability , but the perceptron is just adding and subtracting constants from the weights every time there is a mistake . Will this really work ? In fact , there is some nice theory for the perceptron , based on the concept of linear separability . Informally , a dataset with binary labels ( y ∈ { 0 , 1 } ) is linearly separable if it is possible to draw a Under contract with MIT Press , shared under CC-BY-NC-ND license . 26 CHAPTER 2 . LINEAR TEXT CLASSIFICATION Algorithm 3 Perceptron learning algorithm 1 : procedure PERCEPTRON ( x ( 1 : N ) , y ( 1 : N ) ) 2 : t ← 0 3 : θ ( 0 ) ← 0 4 : repeat 5 : t ← t + 1 6 : Select an instance i 7 : ŷ ← argmaxy θ ( t − 1 ) · f ( x ( i ) , y ) 8 : if ŷ 6 = y ( i ) then 9 : θ ( t ) ← θ ( t − 1 ) + f ( x ( i ) , y ( i ) ) − f ( x ( i ) , ŷ ) 10 : else 11 : θ ( t ) ← θ ( t − 1 ) 12 : until tired 13 : return θ ( t ) hyperplane ( a line in many dimensions ) , such that on each side of the hyperplane , all instances have the same label . This definition can be formalized and extended to multiple labels : Definition 1 ( Linear separability ) . The dataset D = { ( x ( i ) , y ( i ) ) } Ni = 1 is linearly separable iff ( if and only if ) there exists some weight vector θ and some margin ρ such that for every instance ( x ( i ) , y ( i ) ) , the inner product of θ and the feature function for the true label , θ · f ( x ( i ) , y ( i ) ) , is at least ρ greater than inner product of θ and the feature function for every other possible label , θ · f ( x ( i ) , y ′ ) . ∃ θ , ρ > 0 : ∀ ( x ( i ) , y ( i ) ) ∈ D , θ · f ( x ( i ) , y ( i ) ) ≥ ρ + max y ′ 6 = y ( i ) θ · f ( x ( i ) , y ′ ) . [ 2.35 ] Linear separability is important because of the following guarantee : if your data is linearly separable , then the perceptron algorithm will find a separator ( Novikoff , 1962 ) . 13 So while the perceptron may seem heuristic , it is guaranteed to succeed , if the learning problem is easy enough . How useful is this proof ? Minsky and Papert ( 1969 ) famously proved that the simple logical function of exclusive-or is not separable , and that a perceptron is therefore inca - pable of learning this function . But this is not just an issue for the perceptron : any linear classification algorithm , including Naı̈ve Bayes , will fail on this task . Text classification problems usually involve high dimensional feature spaces , with thousands or millions of 13It is also possible to prove an upper bound on the number of training iterations required to find the separator . Proofs like this are part of the field of machine learning theory ( Mohri et al . , 2012 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 2.4 . LOSS FUNCTIONS AND LARGE-MARGIN CLASSIFICATION 27 features . For these problems , it is very likely that the training data is indeed separable . And even if the dataset is not separable , it is still possible to place an upper bound on the number of errors that the perceptron algorithm will make ( Freund and Schapire , 1999 ) . 2.3.2 Averaged perceptron The perceptron iterates over the data repeatedly — until “ tired ” , as described in Algo - rithm 3 . If the data is linearly separable , the perceptron will eventually find a separator , and we can stop once all training instances are classified correctly . But if the data is not linearly separable , the perceptron can thrash between two or more weight settings , never converging . In this case , how do we know that we can stop training , and how should we choose the final weights ? An effective practical solution is to average the perceptron weights across all iterations . This procedure is shown in Algorithm 4 . The learning algorithm is nearly identical , but we also maintain a vector of the sum of the weights , m . At the end of the learning procedure , we divide this sum by the total number of updates t , to compute the average weights , θ . These average weights are then used for prediction . In the algorithm sketch , the average is computed from a running sum , m ← m + θ . However , this is inefficient , because it requires | θ | operations to update the running sum . When f ( x , y ) is sparse , | θ | � | f ( x , y ) | for any individual ( x , y ) . This means that computing the running sum will be much more expensive than computing of the update to θ itself , which requires only 2 × | f ( x , y ) | operations . One of the exercises is to sketch a more efficient algorithm for computing the averaged weights . Even if the dataset is not separable , the averaged weights will eventually converge . One possible stopping criterion is to check the difference between the average weight vectors after each pass through the data : if the norm of the difference falls below some predefined threshold , we can stop training . Another stopping criterion is to hold out some data , and to measure the predictive accuracy on this heldout data . When the accuracy on the heldout data starts to decrease , the learning algorithm has begun to overfit the training set . At this point , it is probably best to stop ; this stopping criterion is known as early stopping . Generalization is the ability to make good predictions on instances that are not in the training data . Averaging can be proven to improve generalization , by computing an upper bound on the generalization error ( Freund and Schapire , 1999 ; Collins , 2002 ) . 2.4 Loss functions and large-margin classification Naı̈ve Bayes chooses the weights θ by maximizing the joint log-likelihood log p ( x ( 1 : N ) , y ( 1 : N ) ) . By convention , optimization problems are generally formulated as minimization of a loss function . The input to a loss function is the vector of weights θ , and the output is a Under contract with MIT Press , shared under CC-BY-NC-ND license . 28 CHAPTER 2 . LINEAR TEXT CLASSIFICATION Algorithm 4 Averaged perceptron learning algorithm 1 : procedure AVG-PERCEPTRON ( x ( 1 : N ) , y ( 1 : N ) ) 2 : t ← 0 3 : θ ( 0 ) ← 0 4 : repeat 5 : t ← t + 1 6 : Select an instance i 7 : ŷ ← argmaxy θ ( t − 1 ) · f ( x ( i ) , y ) 8 : if ŷ 6 = y ( i ) then 9 : θ ( t ) ← θ ( t − 1 ) + f ( x ( i ) , y ( i ) ) − f ( x ( i ) , ŷ ) 10 : else 11 : θ ( t ) ← θ ( t − 1 ) 12 : m ← m + θ ( t ) 13 : until tired 14 : θ ← 1tm 15 : return θ non-negative number , measuring the performance of the classifier on a training instance . Formally , the loss ` ( θ ; x ( i ) , y ( i ) ) is then a measure of the performance of the weights θ on the instance ( x ( i ) , y ( i ) ) . The goal of learning is to minimize the sum of the losses across all instances in the training set . We can trivially reformulate maximum likelihood as a loss function , by defining the loss function to be the negative log-likelihood : log p ( x ( 1 : N ) , y ( 1 : N ) ; θ ) = N ∑ i = 1 log p ( x ( i ) , y ( i ) ; θ ) [ 2.36 ] ` NB ( θ ; x ( i ) , y ( i ) ) = − log p ( x ( i ) , y ( i ) ; θ ) [ 2.37 ] θ̂ = argmin θ N ∑ i = 1 ` NB ( θ ; x ( i ) , y ( i ) ) [ 2.38 ] = argmax θ N ∑ i = 1 log p ( x ( i ) , y ( i ) ; θ ) . [ 2.39 ] The problem of minimizing ` NB is thus identical to maximum-likelihood estimation . Loss functions provide a general framework for comparing learning objectives . For example , an alternative loss function is the zero-one loss , ` 0-1 ( θ ; x ( i ) , y ( i ) ) = { 0 , y ( i ) = argmaxy θ · f ( x ( i ) , y ) 1 , otherwise [ 2.40 ] Jacob Eisenstein . Draft of October 15 , 2018 . 2.4 . LOSS FUNCTIONS AND LARGE-MARGIN CLASSIFICATION 29 The zero-one loss is zero if the instance is correctly classified , and one otherwise . The sum of zero-one losses is proportional to the error rate of the classifier on the training data . Since a low error rate is often the ultimate goal of classification , this may seem ideal . But the zero-one loss has several problems . One is that it is non-convex , 14 which means that there is no guarantee that gradient-based optimization will be effective . A more serious problem is that the derivatives are useless : the partial derivative with respect to any parameter is zero everywhere , except at the points where θ·f ( x ( i ) , y ) = θ·f ( x ( i ) , ŷ ) for some ŷ . At those points , the loss is discontinuous , and the derivative is undefined . The perceptron optimizes a loss function that has better properties for learning : ` PERCEPTRON ( θ ; x ( i ) , y ( i ) ) = max y ∈ Y θ · f ( x ( i ) , y ) − θ · f ( x ( i ) , y ( i ) ) , [ 2.41 ] When ŷ = y ( i ) , the loss is zero ; otherwise , it increases linearly with the gap between the score for the predicted label ŷ and the score for the true label y ( i ) . Plotting this loss against the input maxy ∈ Y θ · f ( x ( i ) , y ) − θ · f ( x ( i ) , y ( i ) ) gives a hinge shape , motivating the name hinge loss . To see why this is the loss function optimized by the perceptron , take the derivative with respect to θ , ∂ ∂ θ ` PERCEPTRON ( θ ; x ( i ) , y ( i ) ) = f ( x ( i ) , ŷ ) − f ( x ( i ) , y ( i ) ) . [ 2.42 ] At each instance , the perceptron algorithm takes a step of magnitude one in the opposite direction of this gradient , ∇ θ ` PERCEPTRON = ∂ ∂ θ ` PERCEPTRON ( θ ; x ( i ) , y ( i ) ) . As we will see in section 2.6 , this is an example of the optimization algorithm stochastic gradient descent , applied to the objective in Equation 2.41 . * Breaking ties with subgradient descent 15 Careful readers will notice the tacit assump - tion that there is a unique ŷ that maximizes θ · f ( x ( i ) , y ) . What if there are two or more labels that maximize this function ? Consider binary classification : if the maximizer is y ( i ) , then the gradient is zero , and so is the perceptron update ; if the maximizer is ŷ 6 = y ( i ) , then the update is the difference f ( x ( i ) , y ( i ) ) − f ( x ( i ) , ŷ ) . The underlying issue is that the perceptron loss is not smooth , because the first derivative has a discontinuity at the hinge point , where the score for the true label y ( i ) is equal to the score for some other label ŷ . At this point , there is no unique gradient ; rather , there is a set of subgradients . A vector v is 14A function f is convex iff αf ( xi ) + ( 1 − α ) f ( xj ) ≥ f ( αxi + ( 1 − α ) xj ) , for all α ∈ [ 0 , 1 ] and for all xi and xj on the domain of the function . In words , any weighted average of the output of f applied to any two points is larger than the output of f when applied to the weighted average of the same two points . Convexity implies that any local minimum is also a global minimum , and there are many effective techniques for optimizing convex functions ( Boyd and Vandenberghe , 2004 ) . See Appendix B for a brief review . 15Throughout this text , advanced topics will be marked with an asterisk . Under contract with MIT Press , shared under CC-BY-NC-ND license . 30 CHAPTER 2 . LINEAR TEXT CLASSIFICATION a subgradient of the function g at u0 iff g ( u ) − g ( u0 ) ≥ v · ( u − u0 ) for all u . Graphically , this defines the set of hyperplanes that include g ( u0 ) and do not intersect g at any other point . As we approach the hinge point from the left , the gradient is f ( x , ŷ ) − f ( x , y ) ; as we approach from the right , the gradient is 0 . At the hinge point , the subgradients include all vectors that are bounded by these two extremes . In subgradient descent , any subgradient can be used ( Bertsekas , 2012 ) . Since both 0 and f ( x , ŷ ) − f ( x , y ) are subgradients at the hinge point , either one can be used in the perceptron update . This means that if multiple labels maximize θ · f ( x ( i ) , y ) , any of them can be used in the perceptron update . Perceptron versus Naı̈ve Bayes The perceptron loss function has some pros and cons with respect to the negative log-likelihood loss implied by Naı̈ve Bayes . • Both ` NB and ` PERCEPTRON are convex , making them relatively easy to optimize . How - ever , ` NB can be optimized in closed form , while ` PERCEPTRON requires iterating over the dataset multiple times . • ` NB can suffer infinite loss on a single example , since the logarithm of zero probability is negative infinity . Naı̈ve Bayes will therefore overemphasize some examples , and underemphasize others . • The Naı̈ve Bayes classifier assumes that the observed features are conditionally in - dependent , given the label , and the performance of the classifier depends on the extent to which this assumption holds . The perceptron requires no such assump - tion . • ` PERCEPTRON treats all correct answers equally . Even if θ only gives the correct answer by a tiny margin , the loss is still zero . 2.4.1 Online large margin classification This last comment suggests a potential problem with the perceptron . Suppose a test ex - ample is very close to a training example , but not identical . If the classifier only gets the correct answer on the training example by a small amount , then it may give a different answer on the nearby test instance . To formalize this intuition , define the margin as , γ ( θ ; x ( i ) , y ( i ) ) = θ · f ( x ( i ) , y ( i ) ) − max y 6 = y ( i ) θ · f ( x ( i ) , y ) . [ 2.43 ] The margin represents the difference between the score for the correct label y ( i ) , and the score for the highest-scoring incorrect label . The intuition behind large margin clas - sification is that it is not enough to label the training data correctly — the correct label should be separated from other labels by a comfortable margin . This idea can be encoded Jacob Eisenstein . Draft of October 15 , 2018 . 2.4 . LOSS FUNCTIONS AND LARGE-MARGIN CLASSIFICATION 31 − 2 − 1 0 1 2 θ · f ( x ( i ) , y ( i ) ) − θ · f ( x ( i ) , ŷ ) 0 1 2 3 lo ss 0/1 loss margin loss logistic loss Figure 2.2 : Margin , zero-one , and logistic loss functions . into a loss function , ` MARGIN ( θ ; x ( i ) , y ( i ) ) = { 0 , γ ( θ ; x ( i ) , y ( i ) ) ≥ 1 , 1 − γ ( θ ; x ( i ) , y ( i ) ) , otherwise [ 2.44 ] = ( 1 − γ ( θ ; x ( i ) , y ( i ) ) ) + , [ 2.45 ] where ( x ) + = max ( 0 , x ) . The loss is zero if there is a margin of at least 1 between the score for the true label and the best-scoring alternative ŷ . This is almost identical to the perceptron loss , but the hinge point is shifted to the right , as shown in Figure 2.2 . The margin loss is a convex upper bound on the zero-one loss . The margin loss can be minimized using an online learning rule that is similar to per - ceptron . We will call this learning rule the online support vector machine , for reasons that will be discussed in the derivation . Let us first generalize the notion of a classifica - tion error with a cost function c ( y ( i ) , y ) . We will focus on the simple cost function , c ( y ( i ) , y ) = { 1 , y ( i ) 6 = ŷ 0 , otherwise , [ 2.46 ] but it is possible to design specialized cost functions that assign heavier penalties to espe - cially undesirable errors ( Tsochantaridis et al . , 2004 ) . This idea is revisited in chapter 7 . Using the cost function , we can now define the online support vector machine as the Under contract with MIT Press , shared under CC-BY-NC-ND license . 32 CHAPTER 2 . LINEAR TEXT CLASSIFICATION following classification rule : ŷ = argmax y ∈ Y θ · f ( x ( i ) , y ) + c ( y ( i ) , y ) [ 2.47 ] θ ( t ) ← ( 1 − λ ) θ ( t − 1 ) + f ( x ( i ) , y ( i ) ) − f ( x ( i ) , ŷ ) [ 2.48 ] This update is similar in form to the perceptron , with two key differences . • Rather than selecting the label ŷ that maximizes the score of the current classifi - cation model , the argmax searches for labels that are both strong , as measured by θ · f ( x ( i ) , y ) , and wrong , as measured by c ( y ( i ) , y ) . This maximization is known as cost-augmented decoding , because it augments the maximization objective to favor high-cost labels . If the highest-scoring label is y = y ( i ) , then the margin loss for this instance is zero , and no update is needed . If not , then an update is required to reduce the margin loss — even if the current model classifies the instance correctly . Cost augmentation is only done while learning ; it is not applied when making pre - dictions on unseen data . • The previous weights θ ( t − 1 ) are scaled by ( 1 − λ ) , with λ ∈ ( 0 , 1 ) . The effect of this term is to cause the weights to “ decay ” back towards zero . In the support vector machine , this term arises from the minimization of a specific form of the margin , as described below . However , it can also be viewed as a form of regularization , which can help to prevent overfitting ( see subsection 2.5.1 ) . In this sense , it plays a role that is similar to smoothing in Naı̈ve Bayes ( see subsection 2.2.4 ) . 2.4.2 * Derivation of the online support vector machine The derivation of the online support vector machine is somewhat involved , but gives further intuition about why the method works . Begin by returning the idea of linear sep - arability ( Definition 1 ) : if a dataset is linearly separable , then there is some hyperplane θ that correctly classifies all training instances with margin ρ . This margin can be increased to any desired value by multiplying the weights by a constant . Now , for any datapoint ( x ( i ) , y ( i ) ) , the geometric distance to the separating hyper - plane is given by γ ( θ ; x ( i ) , y ( i ) ) | | θ | | 2 , where the denominator is the norm of the weights , | | θ | | 2 = √ ∑ j θ 2 j . The geometric distance is sometimes called the geometric margin , in contrast to the functional margin γ ( θ ; x ( i ) , y ( i ) ) . Both are shown in Figure 2.3 . The geometric margin is a good measure of the robustness of the separator : if the functional margin is large , but the norm | | θ | | 2 is also large , then a small change in x ( i ) could cause it to be misclassified . We therefore seek to maximize the minimum geometric margin across the dataset , subject Jacob Eisenstein . Draft of October 15 , 2018 . 2.4 . LOSS FUNCTIONS AND LARGE-MARGIN CLASSIFICATION 33 functional margin geometric margin Figure 2.3 : Functional and geometric margins for a binary classification problem . All separators that satisfy the margin constraint are shown . The separator with the largest geometric margin is shown in bold . to the constraint that the margin loss is always zero : max θ min i = 1,2 , . . . N γ ( θ ; x ( i ) , y ( i ) ) | | θ | | 2 s.t . γ ( θ ; x ( i ) , y ( i ) ) ≥ 1 , ∀ i . [ 2.49 ] This is a constrained optimization problem , where the second line describes constraints on the space of possible solutions θ . In this case , the constraint is that the functional margin always be at least one , and the objective is that the minimum geometric margin be as large as possible . Constrained optimization is reviewed in Appendix B . In this case , further manipula - tion yields an unconstrained optimization problem . First , note that the norm | | θ | | 2 scales linearly : | | aθ | | 2 = a | | θ | | 2 . Furthermore , the functional margin γ is a linear function of θ , so that γ ( aθ , x ( i ) , y ( i ) ) = aγ ( θ , x ( i ) , y ( i ) ) . As a result , any scaling factor on θ will cancel in the numerator and denominator of the geometric margin . If the data is linearly separable at any ρ > 0 , it is always possible to rescale the functional margin to 1 by multiplying θ by a scalar constant . We therefore need only minimize the denominator | | θ | | 2 , subject to the constraint on the functional margin . The minimizer of | | θ | | 2 is also the minimizer of 1 2 | | θ | | 22 = 12 ∑ θ2j , which is easier to work with . This yields a simpler optimization prob - Under contract with MIT Press , shared under CC-BY-NC-ND license . 34 CHAPTER 2 . LINEAR TEXT CLASSIFICATION lem : min θ . 1 2 | | θ | | 22 s.t . γ ( θ ; x ( i ) , y ( i ) ) ≥ 1 , ∀ i . [ 2.50 ] This problem is a quadratic program : the objective is a quadratic function of the pa - rameters , and the constraints are all linear inequalities . One solution to this problem is to incorporate the constraints through Lagrange multipliers αi ≥ 0 , i = 1 , 2 , . . . , N . The instances for which αi > 0 are called support vectors ; other instances are irrelevant to the classification boundary . This motivates the name support vector machine . Thus far we have assumed linear separability , but many datasets of interest are not linearly separable . In this case , there is no θ that satisfies the margin constraint . To add more flexibility , we can introduce a set of slack variables ξi ≥ 0 . Instead of requiring that the functional margin be greater than or equal to one , we require that it be greater than or equal to 1 − ξi . Ideally there would not be any slack , so the slack variables are penalized in the objective function : min θ , ξ 1 2 | | θ | | 22 + C N ∑ i = 1 ξi s.t . γ ( θ ; x ( i ) , y ( i ) ) + ξi ≥ 1 , ∀ i ξi ≥ 0 , ∀ i . [ 2.51 ] The hyperparameter C controls the tradeoff between violations of the margin con - straint and the preference for a low norm of θ . As C → ∞ , slack is infinitely expensive , and there is only a solution if the data is separable . As C → 0 , slack becomes free , and there is a trivial solution at θ = 0 . Thus , C plays a similar role to the smoothing parameter in Naı̈ve Bayes ( subsection 2.2.4 ) , trading off between a close fit to the training data and better generalization . Like the smoothing parameter of Naı̈ve Bayes , C must be set by the user , typically by maximizing performance on a heldout development set . To solve the constrained optimization problem defined in Equation 2.51 , we can first solve for the slack variables , ξi ≥ ( 1 − γ ( θ ; x ( i ) , y ( i ) ) ) + . [ 2.52 ] The inequality is tight : the optimal solution is to make the slack variables as small as possible , while still satisfying the constraints ( Ratliff et al . , 2007 ; Smith , 2011 ) . By plugging in the minimum slack variables back into Equation 2.51 , the problem can be transformed into the unconstrained optimization , min θ λ 2 | | θ | | 22 + N ∑ i = 1 ( 1 − γ ( θ ; x ( i ) , y ( i ) ) ) + , [ 2.53 ] Jacob Eisenstein . Draft of October 15 , 2018 . 2.5 . LOGISTIC REGRESSION 35 where each ξi has been substituted by the right-hand side of Equation 2.52 , and the factor of C on the slack variables has been replaced by an equivalent factor of λ = 1C on the norm of the weights . Equation 2.53 can be rewritten by expanding the margin , min θ λ 2 | | θ | | 22 + N ∑ i = 1 ( max y ∈ Y ( θ · f ( x ( i ) , y ) + c ( y ( i ) , y ) ) − θ · f ( x ( i ) , y ( i ) ) ) + , [ 2.54 ] where c ( y , y ( i ) ) is the cost function defined in Equation 2.46 . We can now differentiate with respect to the weights , ∇ θLSVM = λθ + N ∑ i = 1 f ( x ( i ) , ŷ ) − f ( x ( i ) , y ( i ) ) , [ 2.55 ] where LSVM refers to minimization objective in Equation 2.54 and ŷ = argmaxy ∈ Y θ · f ( x ( i ) , y ) + c ( y ( i ) , y ) . The online support vector machine update arises from the appli - cation of stochastic gradient descent ( described in subsection 2.6.2 ) to this gradient . 2.5 Logistic regression Thus far , we have seen two broad classes of learning algorithms . Naı̈ve Bayes is a prob - abilistic method , where learning is equivalent to estimating a joint probability distribu - tion . The perceptron and support vector machine are discriminative , error-driven algo - rithms : the learning objective is closely related to the number of errors on the training data . Probabilistic and error-driven approaches each have advantages : probability makes it possible to quantify uncertainty about the predicted labels , but the probability model of Naı̈ve Bayes makes unrealistic independence assumptions that limit the features that can be used . Logistic regression combines advantages of discriminative and probabilistic classi - fiers . Unlike Naı̈ve Bayes , which starts from the joint probability pX , Y , logistic regression defines the desired conditional probability pY | X directly . Think of θ · f ( x , y ) as a scoring function for the compatibility of the base features x and the label y . To convert this score into a probability , we first exponentiate , obtaining exp ( θ · f ( x , y ) ) , which is guaranteed to be non-negative . Next , we normalize , dividing over all possible labels y ′ ∈ Y . The resulting conditional probability is defined as , p ( y | x ; θ ) = exp ( θ · f ( x , y ) ) ∑ y ′ ∈ Y exp ( θ · f ( x , y ′ ) ) . [ 2.56 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 36 CHAPTER 2 . LINEAR TEXT CLASSIFICATION Given a dataset D = { ( x ( i ) , y ( i ) ) } Ni = 1 , the weights θ are estimated by maximum condi - tional likelihood , log p ( y ( 1 : N ) | x ( 1 : N ) ; θ ) = N ∑ i = 1 log p ( y ( i ) | x ( i ) ; θ ) [ 2.57 ] = N ∑ i = 1 θ · f ( x ( i ) , y ( i ) ) − log ∑ y ′ ∈ Y exp ( θ · f ( x ( i ) , y ′ ) ) . [ 2.58 ] The final line is obtained by plugging in Equation 2.56 and taking the logarithm . 16 Inside the sum , we have the ( additive inverse of the ) logistic loss , ` LOGREG ( θ ; x ( i ) , y ( i ) ) = − θ · f ( x ( i ) , y ( i ) ) + log ∑ y ′ ∈ Y exp ( θ · f ( x ( i ) , y ′ ) ) [ 2.59 ] The logistic loss is shown in Figure 2.2 on page 31 . A key difference from the zero-one and hinge losses is that logistic loss is never zero . This means that the objective function can always be improved by assigning higher confidence to the correct label . 2.5.1 Regularization As with the support vector machine , better generalization can be obtained by penalizing the norm of θ . This is done by adding a multiple of the squared norm λ2 | | θ | | 22 to the minimization objective . This is called L2 regularization , because | | θ | | 22 is the squared L2 norm of the vector θ . Regularization forces the estimator to trade off performance on the training data against the norm of the weights , and this can help to prevent overfitting . Consider what would happen to the unregularized weight for a base feature j that is active in only one instance x ( i ) : the conditional log-likelihood could always be improved by increasing the weight for this feature , so that θ ( j , y ( i ) ) → ∞ and θ ( j , ỹ 6 = y ( i ) ) → − ∞ , where ( j , y ) is the index of feature associated with x ( i ) j and label y in f ( x ( i ) , y ) . In subsection 2.2.4 ( footnote 11 ) , we saw that smoothing the probabilities of a Naı̈ve Bayes classifier can be justified as a form of maximum a posteriori estimation , in which the parameters of the classifier are themselves random variables , drawn from a prior dis - tribution . The same justification applies to L2 regularization . In this case , the prior is a zero-mean Gaussian on each term of θ . The log-likelihood under a zero-mean Gaussian is , logN ( θj ; 0 , σ 2 ) ∝ − 1 2σ2 θ2j , [ 2.60 ] 16The log-sum-exp term is a common pattern in machine learning . It is numerically unstable , because it will underflow if the inner product is small , and overflow if the inner product is large . Scientific computing libraries usually contain special functions for computing logsumexp , but with some thought , you should be able to see how to create an implementation that is numerically stable . Jacob Eisenstein . Draft of October 15 , 2018 . 2.6 . OPTIMIZATION 37 so that the regularization weight λ is equal to the inverse variance of the prior , λ = 1 σ2 . 2.5.2 Gradients Logistic loss is minimized by optimization along the gradient . Specific algorithms are de - scribed in the next section , but first let’s compute the gradient with respect to the logistic loss of a single example : ` LOGREG = − θ · f ( x ( i ) , y ( i ) ) + log ∑ y ′ ∈ Y exp ( θ · f ( x ( i ) , y ′ ) ) [ 2.61 ] ∂ ` ∂ θ = − f ( x ( i ) , y ( i ) ) + 1 ∑ y ′ ′ ∈ Y exp ( θ · f ( x ( i ) , y ′ ′ ) ) × ∑ y ′ ∈ Y exp ( θ · f ( x ( i ) , y ′ ) ) × f ( x ( i ) , y ′ ) [ 2.62 ] = − f ( x ( i ) , y ( i ) ) + ∑ y ′ ∈ Y exp ( θ · f ( x ( i ) , y ′ ) ) ∑ y ′ ′ ∈ Y exp ( θ · f ( x ( i ) , y ′ ′ ) ) × f ( x ( i ) , y ′ ) [ 2.63 ] = − f ( x ( i ) , y ( i ) ) + ∑ y ′ ∈ Y p ( y ′ | x ( i ) ; θ ) × f ( x ( i ) , y ′ ) [ 2.64 ] = − f ( x ( i ) , y ( i ) ) + EY | X [ f ( x ( i ) , y ) ] . [ 2.65 ] The final step employs the definition of a conditional expectation ( section A . 5 ) . The gra - dient of the logistic loss is equal to the difference between the expected counts under the current model , EY | X [ f ( x ( i ) , y ) ] , and the observed feature counts f ( x ( i ) , y ( i ) ) . When these two vectors are equal for a single instance , there is nothing more to learn from it ; when they are equal in sum over the entire dataset , there is nothing more to learn from the dataset as a whole . The gradient of the hinge loss is nearly identical , but it involves the features of the predicted label under the current model , f ( x ( i ) , ŷ ) , rather than the expected features EY | X [ f ( x ( i ) , y ) ] under the conditional distribution p ( y | x ; θ ) . The regularizer contributes λθ to the overall gradient : LLOGREG = λ 2 | | θ | | 22 − N ∑ i = 1   θ · f ( x ( i ) , y ( i ) ) − log ∑ y ′ ∈ Y expθ · f ( x ( i ) , y ′ )   [ 2.66 ] ∇ θLLOGREG = λθ − N ∑ i = 1 ( f ( x ( i ) , y ( i ) ) − Ey | x [ f ( x ( i ) , y ) ] ) . [ 2.67 ] 2.6 Optimization Each of the classification algorithms in this chapter can be viewed as an optimization problem : Under contract with MIT Press , shared under CC-BY-NC-ND license . 38 CHAPTER 2 . LINEAR TEXT CLASSIFICATION • In Naı̈ve Bayes , the objective is the joint likelihood log p ( x ( 1 : N ) , y ( 1 : N ) ) . Maximum likelihood estimation yields a closed-form solution for θ . • In the support vector machine , the objective is the regularized margin loss , LSVM = λ 2 | | θ | | 22 + N ∑ i = 1 ( max y ∈ Y ( θ · f ( x ( i ) , y ) + c ( y ( i ) , y ) ) − θ · f ( x ( i ) , y ( i ) ) ) + , [ 2.68 ] There is no closed-form solution , but the objective is convex . The perceptron algo - rithm minimizes a similar objective . • In logistic regression , the objective is the regularized negative log-likelihood , LLOGREG = λ 2 | | θ | | 22 − N ∑ i = 1   θ · f ( x ( i ) , y ( i ) ) − log ∑ y ∈ Y exp ( θ · f ( x ( i ) , y ) )   [ 2.69 ] Again , there is no closed-form solution , but the objective is convex . These learning algorithms are distinguished by what is being optimized , rather than how the optimal weights are found . This decomposition is an essential feature of con - temporary machine learning . The domain expert’s job is to design an objective function — or more generally , a model of the problem . If the model has certain characteristics , then generic optimization algorithms can be used to find the solution . In particular , if an objective function is differentiable , then gradient-based optimization can be employed ; if it is also convex , then gradient-based optimization is guaranteed to find the globally optimal solution . The support vector machine and logistic regression have both of these properties , and so are amenable to generic convex optimization techniques ( Boyd and Vandenberghe , 2004 ) . 2.6.1 Batch optimization In batch optimization , each update to the weights is based on a computation involving the entire dataset . One such algorithm is gradient descent , which iteratively updates the weights , θ ( t + 1 ) ← θ ( t ) − η ( t ) ∇ θL , [ 2.70 ] where ∇ θL is the gradient computed over the entire training set , and η ( t ) is the learning rate at iteration t . If the objective L is a convex function of θ , then this procedure is guaranteed to terminate at the global optimum , for appropriate schedule of learning rates , η ( t ) . 17 17Convergence proofs typically require the learning rate to satisfy the following conditions : ∑ ∞ t = 1 η ( t ) = ∞ and ∑ ∞ t = 1 ( η ( t ) ) 2 < ∞ ( Bottou et al . , 2016 ) . These properties are satisfied by any learning rate schedule η ( t ) = η ( 0 ) t − α for α ∈ [ 1 , 2 ] . Jacob Eisenstein . Draft of October 15 , 2018 . 2.6 . OPTIMIZATION 39 In practice , gradient descent can be slow to converge , as the gradient can become infinitesimally small . Faster convergence can be obtained by second-order Newton opti - mization , which incorporates the inverse of the Hessian matrix , Hi , j = ∂ 2L ∂ θi ∂ θj [ 2.71 ] The size of the Hessian matrix is quadratic in the number of features . In the bag-of-words representation , this is usually too big to store , let alone invert . Quasi-Network optimiza - tion techniques maintain a low-rank approximation to the inverse of the Hessian matrix . Such techniques usually converge more quickly than gradient descent , while remaining computationally tractable even for large feature sets . A popular quasi-Newton algorithm is L-BFGS ( Liu and Nocedal , 1989 ) , which is implemented in many scientific computing environments , such as SCIPY and MATLAB . For any gradient-based technique , the user must set the learning rates η ( t ) . While con - vergence proofs usually employ a decreasing learning rate , in practice , it is common to fix η ( t ) to a small constant , like 10 − 3 . The specific constant can be chosen by experimentation , although there is research on determining the learning rate automatically ( Schaul et al . , 2013 ; Wu et al . , 2018 ) . 2.6.2 Online optimization Batch optimization computes the objective on the entire training set before making an up - date . This may be inefficient , because at early stages of training , a small number of train - ing examples could point the learner in the correct direction . Online learning algorithms make updates to the weights while iterating through the training data . The theoretical basis for this approach is a stochastic approximation to the true objective function , N ∑ i = 1 ` ( θ ; x ( i ) , y ( i ) ) ≈ N × ` ( θ ; x ( j ) , y ( j ) ) , ( x ( j ) , y ( j ) ) ∼ { ( x ( i ) , y ( i ) ) } Ni = 1 , [ 2.72 ] where the instance ( x ( j ) , y ( j ) ) is sampled at random from the full dataset . In stochastic gradient descent , the approximate gradient is computed by randomly sampling a single instance , and an update is made immediately . This is similar to the perceptron algorithm , which also updates the weights one instance at a time . In mini - batch stochastic gradient descent , the gradient is computed over a small set of instances . A typical approach is to set the minibatch size so that the entire batch fits in memory on a graphics processing unit ( GPU ; Neubig et al . , 2017 ) . It is then possible to speed up learn - ing by parallelizing the computation of the gradient over each instance in the minibatch . Algorithm 5 offers a generalized view of gradient descent . In standard gradient de - scent , the batcher returns a single batch with all the instances . In stochastic gradient de - Under contract with MIT Press , shared under CC-BY-NC-ND license . 40 CHAPTER 2 . LINEAR TEXT CLASSIFICATION Algorithm 5 Generalized gradient descent . The function BATCHER partitions the train - ing set into B batches such that each instance appears in exactly one batch . In gradient descent , B = 1 ; in stochastic gradient descent , B = N ; in minibatch stochastic gradient descent , 1 < B < N . 1 : procedure GRADIENT-DESCENT ( x ( 1 : N ) , y ( 1 : N ) , L , η ( 1 . . . ∞ ) , BATCHER , Tmax ) 2 : θ ← 0 3 : t ← 0 4 : repeat 5 : ( b ( 1 ) , b ( 2 ) , . . . , b ( B ) ) ← BATCHER ( N ) 6 : for n ∈ { 1 , 2 , . . . , B } do 7 : t ← t + 1 8 : θ ( t ) ← θ ( t − 1 ) − η ( t ) ∇ θL ( θ ( t − 1 ) ; x ( b ( n ) 1 , b ( n ) 2 , . . . ) , y ( b ( n ) 1 , b ( n ) 2 , . . . ) ) 9 : if Converged ( θ ( 1,2 , . . . , t ) ) then 10 : return θ ( t ) 11 : until t ≥ Tmax 12 : return θ ( t ) scent , it returns N batches with one instance each . In mini-batch settings , the batcher returns B minibatches , 1 < B < N . There are many other techniques for online learning , and research in this area is on - going ( Bottou et al . , 2016 ) . Some algorithms use an adaptive learning rate , which can be different for every feature ( Duchi et al . , 2011 ) . Features that occur frequently are likely to be updated frequently , so it is best to use a small learning rate ; rare features will be updated infrequently , so it is better to take larger steps . The AdaGrad ( adaptive gradient ) algorithm achieves this behavior by storing the sum of the squares of the gradients for each feature , and rescaling the learning rate by its inverse : gt = ∇ θL ( θ ( t ) ; x ( i ) , y ( i ) ) [ 2.73 ] θ ( t + 1 ) j ← θ ( t ) j − η ( t ) √ ∑ t t ′ = 1 g 2 t , j gt , j , [ 2.74 ] where j iterates over features in f ( x , y ) . In most cases , the number of active features for any instance is much smaller than the number of weights . If so , the computation cost of online optimization will be dominated by the update from the regularization term , λθ . The solution is to be “ lazy ” , updating each θj only as it is used . To implement lazy updating , store an additional parameter τj , which is the iteration at which θj was last updated . If θj is needed at time t , the t − τ regularization updates can be performed all at once . This strategy is described in detail by Kummerfeld et al . ( 2015 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 2.7 . * ADDITIONAL TOPICS IN CLASSIFICATION 41 2.7 * Additional topics in classification This section presents some additional topics in classification that are particularly relevant for natural language processing , especially for understanding the research literature . 2.7.1 Feature selection by regularization In logistic regression and large-margin classification , generalization can be improved by regularizing the weights towards 0 , using the L2 norm . But rather than encouraging weights to be small , it might be better for the model to be sparse : it should assign weights of exactly zero to most features , and only assign non-zero weights to features that are clearly necessary . This idea can be formalized by the L0 norm , L0 = | | θ | | 0 = ∑ j δ ( θj 6 = 0 ) , which applies a constant penalty for each non-zero weight . This norm can be thought of as a form of feature selection : optimizing the L0-regularized conditional likelihood is equivalent to trading off the log-likelihood against the number of active features . Reduc - ing the number of active features is desirable because the resulting model will be fast , low-memory , and should generalize well , since irrelevant features will be pruned away . Unfortunately , the L0 norm is non-convex and non-differentiable . Optimization under L0 regularization is NP-hard , meaning that it can be solved efficiently only if P = NP ( Ge et al . , 2011 ) . A useful alternative is the L1 norm , which is equal to the sum of the absolute values of the weights , | | θ | | 1 = ∑ j | θj | . The L1 norm is convex , and can be used as an approxima - tion to L0 ( Tibshirani , 1996 ) . Conveniently , the L1 norm also performs feature selection , by driving many of the coefficients to zero ; it is therefore known as a sparsity inducing regularizer . The L1 norm does not have a gradient at θj = 0 , so we must instead optimize the L1-regularized objective using subgradient methods . The associated stochastic sub - gradient descent algorithms are only somewhat more complex than conventional SGD ; Sra et al . ( 2012 ) survey approaches for estimation under L1 and other regularizers . Gao et al . ( 2007 ) compare L1 and L2 regularization on a suite of NLP problems , finding that L1 regularization generally gives similar accuracy to L2 regularization , but that L1 regularization produces models that are between ten and fifty times smaller , because more than 90 % of the feature weights are set to zero . 2.7.2 Other views of logistic regression In binary classification , we can dispense with the feature function , and choose y based on the inner product of θ · x . The conditional probability pY | X is obtained by passing this Under contract with MIT Press , shared under CC-BY-NC-ND license . 42 CHAPTER 2 . LINEAR TEXT CLASSIFICATION inner product through a logistic function , σ ( a ) , exp ( a ) 1 + exp ( a ) = ( 1 + exp ( − a ) ) − 1 [ 2.75 ] p ( y | x ; θ ) = σ ( θ · x ) . [ 2.76 ] This is the origin of the name “ logistic regression . ” Logistic regression can be viewed as part of a larger family of generalized linear models ( GLMs ) , in which various other link functions convert between the inner product θ · x and the parameter of a conditional probability distribution . Logistic regression and related models are sometimes referred to as log-linear , be - cause the log-probability is a linear function of the features . But in the early NLP liter - ature , logistic regression was often called maximum entropy classification ( Berger et al . , 1996 ) . This name refers to an alternative formulation , in which the goal is to find the max - imum entropy probability function that satisfies moment-matching constraints . These constraints specify that the empirical counts of each feature should match the expected counts under the induced probability distribution pY | X ; θ , N ∑ i = 1 fj ( x ( i ) , y ( i ) ) = N ∑ i = 1 ∑ y ∈ Y p ( y | x ( i ) ; θ ) fj ( x ( i ) , y ) , ∀ j [ 2.77 ] The moment-matching constraint is satisfied exactly when the derivative of the condi - tional log-likelihood function ( Equation 2.65 ) is equal to zero . However , the constraint can be met by many values of θ , so which should we choose ? The entropy of the conditional probability distribution pY | X is , H ( pY | X ) = − ∑ x ∈ X pX ( x ) ∑ y ∈ Y pY | X ( y | x ) log pY | X ( y | x ) , [ 2.78 ] where X is the set of all possible feature vectors , and pX ( x ) is the probability of observing the base features x . The distribution pX is unknown , but it can be estimated by summing over all the instances in the training set , H̃ ( pY | X ) = − 1 N N ∑ i = 1 ∑ y ∈ Y pY | X ( y | x ( i ) ) log pY | X ( y | x ( i ) ) . [ 2.79 ] If the entropy is large , the likelihood function is smooth across possible values of y ; if it is small , the likelihood function is sharply peaked at some preferred value ; in the limiting case , the entropy is zero if p ( y | x ) = 1 for some y . The maximum-entropy cri - terion chooses to make the weakest commitments possible , while satisfying the moment - matching constraints from Equation 2.77 . The solution to this constrained optimization problem is identical to the maximum conditional likelihood ( logistic-loss ) formulation that was presented in section 2.5 . Jacob Eisenstein . Draft of October 15 , 2018 . 2.8 . SUMMARY OF LEARNING ALGORITHMS 43 2.8 Summary of learning algorithms It is natural to ask which learning algorithm is best , but the answer depends on what characteristics are important to the problem you are trying to solve . Naı̈ve Bayes Pros : easy to implement ; estimation is fast , requiring only a single pass over the data ; assigns probabilities to predicted labels ; controls overfitting with smooth - ing parameter . Cons : often has poor accuracy , especially with correlated features . Perceptron Pros : easy to implement ; online ; error-driven learning means that accuracy is typically high , especially after averaging . Cons : not probabilistic ; hard to know when to stop learning ; lack of margin can lead to overfitting . Support vector machine Pros : optimizes an error-based metric , usually resulting in high accuracy ; overfitting is controlled by a regularization parameter . Cons : not proba - bilistic . Logistic regression Pros : error-driven and probabilistic ; overfitting is controlled by a reg - ularization parameter . Cons : batch learning requires black-box optimization ; logistic loss can “ overtrain ” on correctly labeled examples . One of the main distinctions is whether the learning algorithm offers a probability over labels . This is useful in modular architectures , where the output of one classifier is the input for some other system . In cases where probability is not necessary , the sup - port vector machine is usually the right choice , since it is no more difficult to implement than the perceptron , and is often more accurate . When probability is necessary , logistic regression is usually more accurate than Naı̈ve Bayes . Additional resources A machine learning textbook will offer more classifiers and more details ( e.g . , Murphy , 2012 ) , although the notation will differ slightly from what is typical in natural language processing . Probabilistic methods are surveyed by Hastie et al . ( 2009 ) , and Mohri et al . ( 2012 ) emphasize theoretical considerations . Bottou et al . ( 2016 ) surveys the rapidly mov - ing field of online learning , and Kummerfeld et al . ( 2015 ) empirically review several opti - mization algorithms for large-margin learning . The python toolkit SCIKIT-LEARN includes implementations of all of the algorithms described in this chapter ( Pedregosa et al . , 2011 ) . Appendix B describes an alternative large-margin classifier , called passive-aggressive . Passive-aggressive is an online learner that seeks to make the smallest update that satisfies the margin constraint at the current instance . It is closely related to MIRA , which was used widely in NLP in the 2000s ( Crammer and Singer , 2003 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 44 CHAPTER 2 . LINEAR TEXT CLASSIFICATION Exercises There will be exercises at the end of each chapter . In this chapter , the exercises are mostly mathematical , matching the subject material . In other chapters , the exercises will empha - size linguistics or programming . 1 . Let x be a bag-of-words vector such that ∑ V j = 1 xj = 1 . Verify that the multinomial probability pmult ( x ; φ ) , as defined in Equation 2.12 , is identical to the probability of the same document under a categorical distribution , pcat ( w ; φ ) . 2 . Suppose you have a single feature x , with the following conditional distribution : p ( x | y ) =            α , X = 0 , Y = 0 1 − α , X = 1 , Y = 0 1 − β , X = 0 , Y = 1 β , X = 1 , Y = 1 . [ 2.80 ] Further suppose that the prior is uniform , Pr ( Y = 0 ) = Pr ( Y = 1 ) = 12 , and that both α > 12 and β > 1 2 . Given a Naı̈ve Bayes classifier with accurate parameters , what is the probability of making an error ? 3 . Derive the maximum-likelihood estimate for the parameter µ in Naı̈ve Bayes . 4 . The classification models in the text have a vector of weights for each possible label . While this is notationally convenient , it is overdetermined : for any linear classifier that can be obtained withK × V weights , an equivalent classifier can be constructed using ( K − 1 ) × V weights . a ) Describe how to construct this classifier . Specifically , if given a set of weights θ and a feature function f ( x , y ) , explain how to construct alternative weights and feature function θ ′ and f ′ ( x , y ) , such that , ∀ y , y ′ ∈ Y , θ · f ( x , y ) − θ · f ( x , y ′ ) = θ ′ · f ′ ( x , y ) − θ ′ · f ′ ( x , y ′ ) . [ 2.81 ] b ) Explain how your construction justifies the well-known alternative form for binary logistic regression , Pr ( Y = 1 | x ; θ ) = 11 + exp ( − θ ′ · x ) = σ ( θ ′ · x ) , where σ is the sigmoid function . 5 . Suppose you have two labeled datasets D1 and D2 , with the same features and la - bels . • Let θ ( 1 ) be the unregularized logistic regression ( LR ) coefficients from training on dataset D1 . Jacob Eisenstein . Draft of October 15 , 2018 . 2.8 . SUMMARY OF LEARNING ALGORITHMS 45 • Let θ ( 2 ) be the unregularized LR coefficients ( same model ) from training on dataset D2 . • Let θ ∗ be the unregularized LR coefficients from training on the combined dataset D1 ∪ D2 . Under these conditions , prove that for any feature j , θ ∗ j ≥ min ( θ ( 1 ) j , θ ( 2 ) j ) θ ∗ j ≤ max ( θ ( 1 ) j , θ ( 2 ) j ) . 6 . Let θ̂ be the solution to an unregularized logistic regression problem , and let θ ∗ be the solution to the same problem , with L2 regularization . Prove that | | θ ∗ | | 22 ≤ | | θ̂ | | 22 . 7 . As noted in the discussion of averaged perceptron in subsection 2.3.2 , the computa - tion of the running summ ← m + θ is unnecessarily expensive , requiringK × V op - erations . Give an alternative way to compute the averaged weights θ , with complex - ity that is independent of V and linear in the sum of feature sizes ∑ N i = 1 | f ( x ( i ) , y ( i ) ) | . 8 . Consider a dataset that is comprised of two identical instances x ( 1 ) = x ( 2 ) with distinct labels y ( 1 ) 6 = y ( 2 ) . Assume all features are binary , xj ∈ { 0 , 1 } for all j . Now suppose that the averaged perceptron always trains on the instance ( xi ( t ) , yi ( t ) ) , where i ( t ) = 2 − ( t mod 2 ) , which is 1 when the training iteration t is odd , and 2 when t is even . Further suppose that learning terminates under the following con - dition : � ≥ max j ∣ ∣ ∣ ∣ ∣ 1 t ∑ t θ ( t ) j − 1 t − 1 ∑ t θ ( t − 1 ) j ∣ ∣ ∣ ∣ ∣ . [ 2.82 ] In words , the algorithm stops when the largest change in the averaged weights is less than or equal to � . Compute the number of iterations before the averaged per - ceptron terminates . 9 . Prove that the margin loss is convex in θ . Use this definition of the margin loss : L ( θ ) = − θ · f ( x , y ∗ ) + max y θ · f ( x , y ) + c ( y ∗ , y ) , [ 2.83 ] where y ∗ is the gold label . As a reminder , a function f is convex iff , f ( αx1 + ( 1 − α ) x2 ) ≤ αf ( x1 ) + ( 1 − α ) f ( x2 ) , [ 2.84 ] for any x1 , x2 and α ∈ [ 0 , 1 ] . Under contract with MIT Press , shared under CC-BY-NC-ND license . 46 CHAPTER 2 . LINEAR TEXT CLASSIFICATION 10 . If a function f is m-strongly convex , then for some m > 0 , the following inequality holds for all x and x ′ on the domain of the function : f ( x ′ ) ≤ f ( x ) + ( ∇ xf ) · ( x ′ − x ) + m 2 | | x ′ − x | | 22 . [ 2.85 ] Let f ( x ) = L ( θ ( t ) ) , representing the loss of the classifier at iteration t of gradient descent ; let f ( x ′ ) = L ( θ ( t + 1 ) ) . Assuming the loss function is m-convex , prove that L ( θ ( t + 1 ) ) ≤ L ( θ ( t ) ) for an appropriate constant learning rate η , which will depend on m . Explain why this implies that gradient descent converges when applied to an m-strongly convex loss function with a unique minimum . Jacob Eisenstein . Draft of October 15 , 2018 .