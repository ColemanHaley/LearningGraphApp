4 . Feed-forward Neural Networks A Brain-inspired metaphor As the name suggest , neural-networks are inspired by the brain’s computation mechanism , which consists of computation units called neurons . In the metaphor , a neuron is a computational unit that has scalar inputs and outputs . Each input has an associated weight . The neuron multiplies each input by its weight , and then sums4 them , applies a non-linear function to the result , and passes it to its output . The neurons are connected to each other , forming a network : the output of a neuron may feed into the inputs of one or more neurons . Such networks were shown to be very capable computational devices . If the weights are set correctly , a neural network with enough neurons and a non - linear activation function can approximate a very wide range of mathematical functions ( we will be more precise about this later ) . x1 x2 x3 x4Input layer ∫ ∫ ∫ ∫ ∫ ∫ Hidden layer ∫ ∫ ∫ ∫ ∫ Hidden layer y1 y2 y3 Output layer Figure 2 : Feed-forward neural network with two hidden layers . A typical feed-forward neural network may be drawn as in Figure 2 . Each circle is a neuron , with incoming arrows being the neuron’s inputs and outgoing arrows being the neu - ron’s outputs . Each arrow carries a weight , reflecting its importance ( not shown ) . Neurons are arranged in layers , reflecting the flow of information . The bottom layer has no incom - ing arrows , and is the input to the network . The top-most layer has no outgoing arrows , and is the output of the network . The other layers are considered “ hidden ” . The sigmoid shape inside the neurons in the middle layers represent a non-linear function ( typically a 1 / ( 1 + e − x ) ) that is applied to the neuron’s value before passing it to the output . In the figure , each neuron is connected to all of the neurons in the next layer – this is called a fully-connected layer or an affine layer . 4 . While summing is the most common operation , other functions , such as a max , are also possible 11 While the brain metaphor is sexy and intriguing , it is also distracting and cumbersome to manipulate mathematically . We therefore switch to using more concise mathematic no - tation . The values of each row of neurons in the network can be thought of as a vector . In Figure 2 the input layer is a 4 dimensional vector ( x ) , and the layer above it is a 6 dimen - sional vector ( h1 ) . The fully connected layer can be thought of as a linear transformation from 4 dimensions to 6 dimensions . A fully-connected layer implements a vector-matrix multiplication , h = xW where the weight of the connection from the ith neuron in the input row to the jth neuron in the output row is Wij . 5 The values of h are then trans - formed by a non-linear function g that is applied to each value before being passed on to the next input . The whole computation from input to output can be written as : ( g ( xW1 ) ) W2 where W1 are the weights of the first layer and W2 are the weights of the second one . In Mathematical Notation From this point on , we will abandon the brain metaphor and describe networks exclusively in terms of vector-matrix operations . The simplest neural network is the perceptron , which is a linear function of its inputs : NNPerceptron ( x ) = xW + b x ∈ Rdin , W ∈ Rdin × dout , b ∈ Rdout W is the weight matrix , and b is a bias term . 6 In order to go beyond linear functions , we introduce a non-linear hidden layer ( the network in Figure 2 has two such layers ) , resulting in the 1-layer Multi Layer Perceptron ( MLP1 ) . A one-layer feed-forward neural network has the form : NNMLP1 ( x ) = g ( xW 1 + b1 ) W2 + b2 x ∈ Rdin , W1 ∈ Rdin × d1 , b1 ∈ Rd1 , W2 ∈ Rd1 × d2 , b2 ∈ Rd2 Here W1 and b1 are a matrix and a bias term for the first linear transformation of the input , g is a non-linear function that is applied element-wise ( also called a non-linearity or an activation function ) , and W2 and b2 are the matrix and bias term for a second linear transform . Breaking it down , xW1 + b1 is a linear transformation of the input x from din dimensions to d1 dimensions . g is then applied to each of the d1 dimensions , and the matrix W 2 together with bias vector b2 are then used to transform the result into the d2 dimensional output vector . The non-linear activation function g has a crucial role in the network’s ability to represent complex functions . Without the non-linearity in g , the neural network can only represent linear transformations of the input . 7 We can add additional linear-transformations and non-linearities , resulting in a 2-layer MLP ( the network in Figure 2 is of this form ) : NNMLP2 ( x ) = ( g 2 ( g1 ( xW1 + b1 ) W2 + b2 ) ) W3 5 . To see why this is the case , denote the weight of the ith input of the jth neuron in h as wij . The value of hj is then hj = ∑ 4 i = 1 xi · wij . 6 . The network in figure 2 does not include bias terms . A bias term can be added to a layer by adding to it an additional neuron that does not have any incoming connections , whose value is always 1 . 7 . To see why , consider that a sequence of linear transformations is still a linear transformation . 12 It is perhaps clearer to write deeper networks like this using intermediary variables : NNMLP2 ( x ) = y h1 = g1 ( xW1 + b1 ) h2 = g2 ( h1W2 + b2 ) y = h2W3 The vector resulting from each linear transform is referred to as a layer . The outer-most linear transform results in the output layer and the other linear transforms result in hidden layers . Each hidden layer is followed by a non-linear activation . In some cases , such as in the last layer of our example , the bias vectors are forced to 0 ( “ dropped ” ) . Layers resulting from linear transformations are often referred to as fully connected , or affine . Other types of architectures exist . In particular , image recognition problems benefit from convolutional and pooling layers . Such layers have uses also in language processing , and will be discussed in Section 9 . Networks with more than one hidden layer are said to be deep networks , hence the name deep learning . When describing a neural network , one should specify the dimensions of the layers and the input . A layer will expect a din dimensional vector as its input , and transform it into a dout dimensional vector . The dimensionality of the layer is taken to be the dimensionality of its output . For a fully connected layer l ( x ) = xW + b with input dimensionality din and output dimensionality dout , the dimensions of x is 1 × din , of W is din × dout and of b is 1 × dout . The output of the network is a dout dimensional vector . In case dout = 1 , the network’s output is a scalar . Such networks can be used for regression ( or scoring ) by considering the value of the output , or for binary classification by consulting the sign of the output . Networks with dout = k > 1 can be used for k-class classification , by associating each dimension with a class , and looking for the dimension with maximal value . Similarly , if the output vector entries are positive and sum to one , the output can be interpreted as a distribution over class assignments ( such output normalization is typically achieved by applying a softmax transformation on the output layer , see Section 4.3 ) . The matrices and the bias terms that define the linear transformations are the parame - ters of the network . It is common to refer to the collection of all parameters as θ . Together with the input , the parameters determine the network’s output . The training algorithm is responsible for setting their values such that the network’s predictions are correct . Training is discussed in Section 6 . 4.1 Representation Power In terms of representation power , it was shown by ( Hornik , Stinchcombe , & White , 1989 ; Cybenko , 1989 ) that MLP1 is a universal approximator – it can approximate with any desired non-zero amount of error a family of functions8 that include all continuous functions 8 . Specifically , a feed-forward network with linear output layer and at least one hidden layer with a “ squash - ing ” activation function can approximate any Borel measurable function from one finite dimensional space to another . 13 on a closed and bounded subset of Rn , and any function mapping from any finite dimensional discrete space to another . This may suggest there is no reason to go beyond MLP1 to more complex architectures . However , the theoretical result does not state how large the hidden layer should be , nor does it say anything about the learnability of the neural network ( it states that a representation exists , but does not say how easy or hard it is to set the parameters based on training data and a specific learning algorithm ) . It also does not guarantee that a training algorithm will find the correct function generating our training data . Since in practice we train neural networks on relatively small amounts of data , using a combination of the backpropagation algorithm and variants of stochastic gradient descent , and use hidden layers of relatively modest sizes ( up to several thousands ) , there is benefit to be had in trying out more complex architectures than MLP1 . In many cases , however , MLP1 does indeed provide very strong results . For further discussion on the representation power of feed-forward neural networks , see ( Bengio et al . , 2015 , Section 6.5 ) . 4.2 Common Non-linearities The non-linearity g can take many forms . There is currently no good theory as to which non-linearity to apply in which conditions , and choosing the correct non-linearity for a given task is for the most part an empirical question . I will now go over the common non - linearities from the literature : the sigmoid , tanh , hard tanh and the rectified linear unit ( ReLU ) . Some NLP researchers also experimented with other forms of non-linearities such as cube and tanh-cube . Sigmoid The sigmoid activation function σ ( x ) = 1 / ( 1 + e − x ) is an S-shaped function , transforming each value x into the range [ 0 , 1 ] . Hyperbolic tangent ( tanh ) The hyperbolic tangent tanh ( x ) = e 2x − 1 e2x + 1 activation func - tion is an S-shaped function , transforming the values x into the range [ − 1 , 1 ] . Hard tanh The hard-tanh activation function is an approximation of the tanh function which is faster to compute and take derivatives of : hardtanh ( x ) =      − 1 x < − 1 1 x > 1 x otherwise Rectifier ( ReLU ) The Rectifier activation function ( Glorot , Bordes , & Bengio , 2011 ) , also known as the rectified linear unit is a very simple activation function that is easy to work with and was shown many times to produce excellent results . 9 The ReLU unit clips each value x < 0 at 0 . Despite its simplicity , it performs well for many tasks , especially when combined with the dropout regularization technique ( see Section 6.4 ) . 9 . The technical advantages of the ReLU over the sigmoid and tanh activation functions is that it does not involve expensive-to-compute functions , and more importantly that it does not saturate . The sigmoid and tanh activation are capped at 1 , and the gradients at this region of the functions are near zero , driving the entire gradient near zero . The ReLU activation does not have this problem , making it especially suitable for networks with multiple layers , which are susceptible to the vanishing gradients problem when trained with the saturating units . 14 ReLU ( x ) = max ( 0 , x ) = { 0 x < 0 x otherwise As a rule of thumb , ReLU units work better than tanh , and tanh works better than sigmoid . 10 4.3 Output Transformations In many cases , the output layer vector is also transformed . A common transformation is the softmax : x = x1 , . . . , xk softmax ( xi ) = exi ∑ k j = 1 e xj The result is a vector of non-negative real numbers that sum to one , making it a discrete probability distribution over k possible outcomes . The softmax output transformation is used when we are interested in modeling a prob - ability distribution over the possible output classes . To be effective , it should be used in conjunction with a probabilistic training objective such as cross-entropy ( see Section 4.5 below ) . When the softmax transformation is applied to the output of a network without a hidden layer , the result is the well known multinomial logistic regression model , also known as a maximum-entropy classifier . 4.4 Embedding Layers Up until now , the discussion ignored the source of x , treating it as an arbitrary vector . In an NLP application , x is usually composed of various embeddings vectors . We can be explicit about the source of x , and include it in the network’s definition . We introduce c ( · ) , a function from core features to an input vector . It is common for c to extract the embedding vector associated with each feature , and concatenate them : 10 . In addition to these activation functions , recent works from the NLP community experiment with and reported success with other forms of non-linearities . The Cube activation function , g ( x ) = ( x ) 3 , was suggested by ( Chen & Manning , 2014 ) , who found it to be more effective than other non-linearities in a feed-forward network that was used to predict the actions in a greedy transition-based dependency parser . The tanh cube activation function g ( x ) = tanh ( ( x ) 3 + x ) was proposed by ( Pei et al . , 2015 ) , who found it to be more effective than other non-linearities in a feed-forward network that was used as a component in a structured-prediction graph-based dependency parser . The cube and tanh-cube activation functions are motivated by the desire to better capture interac - tions between different features . While these activation functions are reported to improve performance in certain situations , their general applicability is still to be determined . 15 x = c ( f1 , f2 , f3 ) =[ v ( f1 ) ; v ( f2 ) ; v ( f3 ) ] NNMLP1 ( x ) = NNMLP1 ( c ( f1 , f2 , f3 ) ) = NNMLP1 ( [ v ( f1 ) ; v ( f2 ) ; v ( f3 ) ] ) =( g ( [ v ( f1 ) ; v ( f2 ) ; v ( f3 ) ] W 1 + b1 ) ) W2 + b2 Another common choice is for c to sum the embedding vectors ( this assumes the em - bedding vectors all share the same dimensionality ) : x = c ( f1 , f2 , f3 ) = v ( f1 ) + v ( f2 ) + v ( f3 ) NNMLP1 ( x ) = NNMLP1 ( c ( f1 , f2 , f3 ) ) = NNMLP1 ( v ( f1 ) + v ( f2 ) + v ( f3 ) ) =( g ( ( v ( f1 ) + v ( f2 ) + v ( f3 ) ) W 1 + b1 ) ) W2 + b2 The form of c is an essential part of the network’s design . In many papers , it is common to refer to c as part of the network , and likewise treat the word embeddings v ( fi ) as resulting from an “ embedding layer ” or “ lookup layer ” . Consider a vocabulary of | V | words , each embedded as a d dimensional vector . The collection of vectors can then be thought of as a | V | × d embedding matrix E in which each row corresponds to an embedded feature . Let fi be a | V | - dimensional vector , which is all zeros except from one index , corresponding to the value of the ith feature , in which the value is 1 ( this is called a one-hot vector ) . The multiplication fiE will then “ select ” the corresponding row of E . Thus , v ( fi ) can be defined in terms of E and fi : v ( fi ) = fiE And similarly : CBOW ( f1 , . . . , fk ) = k ∑ i = 1 ( fiE ) = ( k ∑ i = 1 fi ) E The input to the network is then considered to be a collection of one-hot vectors . While this is elegant and well defined mathematically , an efficient implementation typically involves a hash-based data structure mapping features to their corresponding embedding vectors , without going through the one-hot representation . In this tutorial , we take c to be separate from the network architecture : the network’s inputs are always dense real-valued input vectors , and c is applied before the input is passed the network , similar to a “ feature function ” in the familiar linear-models terminology . How - ever , when training a network , the input vector x does remember how it was constructed , and can propagate error gradients back to its component embedding vectors , as appropriate . A note on notation When describing network layers that get concatenated vectors x , y and z as input , some authors use explicit concatenation ( [ x ; y ; z ] W + b ) while others use an affine transformation ( xU + yV + zW + b ) . If the weight matrices U , V , W in the affine transformation are different than one another , the two notations are equivalent . 16 A note on sparse vs . dense features Consider a network which uses a “ traditional ” sparse representation for its input vectors , and no embedding layer . Assuming the set of all available features is V and we have k “ on ” features f1 , . . . , fk , fi ∈ V , the network’s input is : x = k ∑ i = 1 fi x ∈ N | V | + and so the first layer ( ignoring the non-linear activation ) is : xW + b = ( k ∑ i = 1 fi ) W W ∈ R | V | × d , b ∈ Rd This layer selects rows of W corresponding to the input features in x and sums them , then adding a bias term . This is very similar to an embedding layer that produces a CBOW representation over the features , where the matrix W acts as the embedding matrix . The main difference is the introduction of the bias vector b , and the fact that the embedding layer typically does not undergo a non-linear activation but rather passed on directly to the first layer . Another difference is that this scenario forces each feature to receive a separate vector ( row in W ) while the embedding layer provides more flexibility , allowing for example for the features “ next word is dog ” and “ previous word is dog ” to share the same vector . However , these differences are small and subtle . When it comes to multi-layer feed-forward networks , the difference between dense and sparse inputs is smaller than it may seem at first sight . 4.5 Loss Functions When training a neural network ( more on training in Section 6 below ) , much like when training a linear classifier , one defines a loss function L ( ŷ , y ) , stating the loss of predicting ŷ when the true output is y . The training objective is then to minimize the loss across the different training examples . The loss L ( ŷ , y ) assigns a numerical score ( a scalar ) for the network’s output ŷ given the true expected output y . 11 The loss is always positive , and should be zero only for cases where the network’s output is correct . The parameters of the network ( the matrices Wi , the biases bi and commonly the em - beddings E ) are then set in order to minimize the loss L over the training examples ( usually , it is the sum of the losses over the different training examples that is being minimized ) . The loss can be an arbitrary function mapping two vectors to a scalar . For practical purposes of optimization , we restrict ourselves to functions for which we can easily compute gradients ( or sub-gradients ) . In most cases , it is sufficient and advisable to rely on a common loss function rather than defining your own . For a detailed discussion on loss functions for neural networks see ( LeCun , Chopra , Hadsell , Ranzato , & Huang , 2006 ; LeCun & Huang , 2005 ; Bengio et al . , 2015 ) . We now discuss some loss functions that are commonly used in neural networks for NLP . 11 . In our notation , both the model’s output and the expected output are vectors , while in many cases it is more natural to think of the expected output as a scalar ( class assignment ) . In such cases , y is simply the corresponding one-hot vector . 17 Hinge ( binary ) For binary classification problems , the network’s output is a single scalar ŷ and the intended output y is in { + 1 , − 1 } . The classification rule is sign ( ŷ ) , and a classification is considered correct if y · ŷ > 0 , meaning that y and ŷ share the same sign . The hinge loss , also known as margin loss or SVM loss , is defined as : Lhinge ( binary ) ( ŷ , y ) = max ( 0 , 1 − y · ŷ ) The loss is 0 when y and ŷ share the same sign and | ŷ | ≥ 1 . Otherwise , the loss is linear . In other words , the binary hinge loss attempts to achieve a correct classification , with a margin of at least 1 . Hinge ( multiclass ) The hinge loss was extended to the multiclass setting by Crammer and Singer ( 2002 ) . Let ŷ = ŷ1 , . . . , ŷn be the network’s output vector , and y be the one-hot vector for the correct output class . The classification rule is defined as selecting the class with the highest score : prediction = arg max i ŷi , Denote by t = arg maxi yi the correct class , and by k = arg maxi 6 = t ŷi the highest scoring class such that k 6 = t . The multiclass hinge loss is defined as : Lhinge ( multiclass ) ( ŷ , y ) = max ( 0 , 1 − ( ŷt − ŷk ) ) The multiclass hinge loss attempts to score the correct class above all other classes with a margin of at least 1 . Both the binary and multiclass hinge losses are intended to be used with a linear output layer . The hinge losses are useful whenever we require a hard decision rule , and do not attempt to model class membership probability . Log loss The log loss is a common variation of the hinge loss , which can be seen as a “ soft ” version of the hinge loss with an infinite margin ( LeCun et al . , 2006 ) . Llog ( ŷ , y ) = log ( 1 + exp ( − ( ŷt − ŷk ) ) Categorical cross-entropy loss The categorical cross-entropy loss ( also referred to as negative log likelihood ) is used when a probabilistic interpretation of the scores is desired . Let y = y1 , . . . , yn be a vector representing the true multinomial distribution over the labels 1 , . . . , n , and let ŷ = ŷ1 , . . . , ŷn be the network’s output , which was transformed by the softmax activation function , and represent the class membership conditional distribution ŷi = P ( y = i | x ) . The categorical cross entropy loss measures the dissimilarity between the true label distribution y and the predicted label distribution ŷ , and is defined as cross entropy : Lcross − entropy ( ŷ , y ) = − ∑ i yi log ( ŷi ) 18 For hard classification problems in which each training example has a single correct class assignment , y is a one-hot vector representing the true class . In such cases , the cross entropy can be simplified to : Lcross − entropy ( hard classification ) ( ŷ , y ) = − log ( ŷt ) where t is the correct class assignment . This attempts to set the probability mass assigned to the correct class t to 1 . Because the scores ŷ have been transformed using the softmax function and represent a conditional distribution , increasing the mass assigned to the correct class means decreasing the mass assigned to all the other classes . The cross-entropy loss is very common in the neural networks literature , and produces a multi-class classifier which does not only predict the one-best class label but also predicts a distribution over the possible labels . When using the cross-entropy loss , it is assumed that the network’s output is transformed using the softmax transformation . Ranking losses In some settings , we are not given supervision in term of labels , but rather as pairs of correct and incorrect items x and x ′ , and our goal is to score correct items above incorrect ones . Such training situations arise when we have only positive examples , and generate negative examples by corrupting a positive example . A useful loss in such scenarios is the margin-based ranking loss , defined for a pair of correct and incorrect examples : Lranking ( margin ) ( x , x ′ ) = max ( 0 , 1 − ( NN ( x ) − NN ( x ′ ) ) ) where NN ( x ) is the score assigned by the network for input vector x . The objective is to score ( rank ) correct inputs over incorrect ones with a margin of at least 1 . A common variation is to use the log version of the ranking loss : Lranking ( log ) ( x , x ′ ) = log ( 1 + exp ( − ( NN ( x ) − NN ( x ′ ) ) ) ) Examples using the ranking hinge loss in language tasks include training with the aux - iliary tasks used for deriving pre-trained word embeddings ( see section 5 ) , in which we are given a correct word sequence and a corrupted word sequence , and our goal is to score the correct sequence above the corrupt one ( Collobert & Weston , 2008 ) . Similarly , Van de Cruys ( 2014 ) used the ranking loss in a selectional-preferences task , in which the net - work was trained to rank correct verb-object pairs above incorrect , automatically derived ones , and ( Weston , Bordes , Yakhnenko , & Usunier , 2013 ) trained a model to score correct ( head , relation , trail ) triplets above corrupted ones in an information-extraction setting . An example of using the ranking log loss can be found in ( Gao et al . , 2014 ) . A variation of the ranking log loss allowing for a different margin for the negative and positive class is given in ( dos Santos et al . , 2015 ) . 19