12 . Modeling Trees – Recursive Neural Networks The RNN is very useful for modeling sequences . In language processing , it is often natural and desirable to work with tree structures . The trees can be syntactic trees , discourse trees , or even trees representing the sentiment expressed by various parts of a sentence ( Socher et al . , 2013 ) . We may want to predict values based on specific tree nodes , predict values based on the root nodes , or assign a quality score to a complete tree or part of a tree . In other cases , we may not care about the tree structure directly but rather reason about spans in the sentence . In such cases , the tree is merely used as a backbone structure which help guide the encoding process of the sequence into a fixed size vector . The recursive neural network ( RecNN ) abstraction ( Pollack , 1990 ) , popularized in NLP by Richard Socher and colleagues ( Socher , Manning , & Ng , 2010 ; Socher , Lin , Ng , & Man - ning , 2011 ; Socher et al . , 2013 ; Socher , 2014 ) is a generalization of the RNN from sequences to ( binary ) trees . 37 Much like the RNN encodes each sentence prefix as a state vector , the RecNN encodes each tree-node as a state vector in Rd . We can then use these state vectors either to predict values of the corresponding nodes , assign quality values to each node , or as a semantic representation of the spans rooted at the nodes . The main intuition behind the recursive neural networks is that each subtree is repre - sented as a d dimensional vector , and the representation of a node p with children c1 and c2 is a function of the representation of the nodes : vec ( p ) = f ( vec ( c1 ) , vec ( c2 ) ) , where f is a composition function taking two d-dimensional vectors and returning a single d-dimensional vector . Much like the RNN state si is used to encode the entire sequence x1 : i , the RecNN state associated with a tree node p encodes the entire subtree rooted at p . See Figure 14 for an illustration . 12.1 Formal Definition Consider a binary parse tree T over an n-word sentence . As a reminder , an ordered , unlabeled tree over a string x1 , . . . , xn can be represented as a unique set of triplets ( i , k , j ) , s.t . i ≤ k ≤ j . Each such triplet indicates that a node spanning words xi : j is parent of the nodes spanning xi : k and xk + 1 : j . Triplets of the form ( i , i , i ) correspond to terminal symbols at the tree leaves ( the words xi ) . Moving from the unlabeled case to the labeled one , we can represent a tree as a set of 6-tuples ( A → B , C , i , k , j ) , whereas i , k and j indicate the spans as before , and A , B and C are the node labels of of the nodes spanning xi : j , xi : k and xk + 1 : j respectively . Here , leaf nodes have the form ( A → A , A , i , i , i ) , where A is a pre-terminal symbol . We refer to such tuples as production rules . For an example , consider the syntactic tree for the sentence “ the boy saw her duck ” . 37 . While presented in terms of binary parse trees , the concepts easily transfer to general recursively-defined data structures , with the major technical challenge is the definition of an effective form for R , the combination function . 60 V = NP1 = combine VP = NP2 = combine S = Figure 14 : Illustration of a recursive neural network . The representations of V and NP1 are combined to form the representation of VP . The representations of VP and NP2 are then combined to form the representation of S . S VP NP Noun duck Det her Verb saw NP Noun boy Det the Its corresponding unlabeled and labeled representations are : Unlabeled Labeled Corresponding Span ( 1,1,1 ) ( Det , Det , Det , 1 , 1 , 1 ) x1:1 the ( 2,2,2 ) ( Noun , Noun , Noun , 2 , 2 , 2 ) x2:2 boy ( 3,3,3 ) ( Verb , Verb , Verb , 3 , 3 , 3 ) saw ( 4,4,4 ) ( Det , Det , Det , 4 , 4 , 4 ) her ( 5,5,5 ) ( Noun , Noun , Noun , 5 , 5 , 5 ) duck ( 4,4,5 ) ( NP , Det , Noun , 4 , 4 , 5 ) her duck ( 3,3,5 ) ( VP , Verb , NP , 3 , 3 , 5 ) saw her duck ( 1,1,2 ) ( NP , Det , Noun , 1 , 1 , 2 ) the boy ( 1,2,5 ) ( S , NP , VP , 1 , 2 , 5 ) the boy saw her duck The set of production rules above can be uniquely converted to a set tree nodes qAi : j ( indicating a node with symbol A over the span xi : j ) by simply ignoring the elements 61 ( B , C , k ) in each production rule . We are now in position to define the Recursive Neural Network . A Recursive Neural Network ( RecNN ) is a function that takes as input a parse tree over an n-word sentence x1 , . . . , xn . Each of the sentence’s words is represented as a d - dimensional vector xi , and the tree is represented as a set T of production rules ( A → B , C , i , j , k ) . Denote the nodes of T by qAi : j . The RecNN returns as output a correspond - ing set of inside state vectors sAi : j , where each inside state vector s A i : j ∈ Rd represents the corresponding tree node qAi : j , and encodes the entire structure rooted at that node . Like the sequence RNN , the tree shaped RecNN is defined recursively using a function R , where the inside vector of a given node is defined as a function of the inside vectors of its direct children . 38 Formally : RecNN ( x1 , . . . , xn , T ) ={ sAi : j ∈ Rd | qAi : j ∈ T } sAi : i = v ( xi ) sAi : j = R ( A , B , C , s B i : k , s C k + 1 : j ) q B i : k ∈ T , qCk + 1 : j ∈ T The function R usually takes the form of a simple linear transformation , which may or may not be followed by a non-linear activation function g : R ( A , B , C , sBi : k , s C k + 1 : j ) = g ( [ s B i : k ; s C k + 1 : j ] W ) This formulation of R ignores the tree labels , using the same matrix W ∈ R2d × d for all combinations . This may be a useful formulation in case the node labels do not exist ( e.g . when the tree does not represent a syntactic structure with clearly defined labels ) or when they are unreliable . However , if the labels are available , it is generally useful to include them in the composition function . One approach would be to introduce label embeddings v ( A ) mapping each non-terminal symbol to a dnt dimensional vector , and change R to include the embedded symbols in the combination function : R ( A , B , C , sBi : k , s C k + 1 : j ) = g ( [ s B i : k ; s C k + 1 : j ; v ( A ) ; v ( B ) ] W ) ( here , W ∈ R2d + 2dnt × d ) . Such approach is taken by ( Qian , Tian , Huang , Liu , Zhu , & Zhu , 2015 ) . An alternative approach , due to ( Socher et al . , 2013 ) is to untie the weights according to the non-terminals , using a different composition matrix for each B , C pair of symbols : 39 R ( A , B , C , sBi : k , s C k + 1 : j ) = g ( [ s B i : k ; s C k + 1 : j ] W BC ) 38 . Le and Zuidema ( 2014 ) extend the RecNN definition such that each node has , in addition to its inside state vector , also an outside state vector representing the entire structure around the subtree rooted at that node . Their formulation is based on the recursive computation of the classic inside-outside algorithm , and can be thought of as the BI-RNN counterpart of the tree RecNN . For details , see ( Le & Zuidema , 2014 ) . 39 . While not explored in the literature , a trivial extension would condition the transformation matrix also on A . 62 This formulation is useful when the number of non-terminal symbols ( or the number of possible symbol combinations ) is relatively small , as is usually the case with phrase-structure parse trees . A similar model was also used by ( Hashimoto et al . , 2013 ) to encode subtrees in semantic-relation classification task . 12.2 Extensions and Variations As all of the definitions of R above suffer from the vanishing gradients problem of the Simple RNN , several authors sought to replace it with functions inspired by the Long Short - Term Memory ( LSTM ) gated architecture , resulting in Tree-shaped LSTMs ( Tai , Socher , & Manning , 2015 ; Zhu , Sobhani , & Guo , 2015b ) . The question of optimal tree representation is still very much an open research question , and the vast space of possible combination functions R is yet to be explored . Other proposed variants on tree-structured RNNs includes a recursive matrix-vector model ( Socher , Huval , Manning , & Ng , 2012 ) and recursive neural tensor network ( Socher et al . , 2013 ) . In the first variant , each word is represented as a combination of a vector and a matrix , where the vector defines the word’s static semantic content as before , while the matrix acts as a learned “ operator ” for the word , allowing more subtle semantic compositions than the addition and weighted averaging implied by the concatenation followed by linear transformation function . In the second variant , words are associated with vectors as usual , but the composition function becomes more expressive by basing it on tensor instead of matrix operations . 12.3 Training Recursive Neural Networks The training procedure for a recursive neural network follows the same recipe as training other forms of networks : define a loss , spell out the computation graph , compute gradients using backpropagation40 , and train the parameters using SGD . With regard to the loss function , similar to the sequence RNN one can associate a loss either with the root of the tree , with any given node , or with a set of nodes , in which case the individual node’s losses are combined , usually by summation . The loss function is based on the labeled training data which associates a label or other quantity with different tree nodes . Additionally , one can treat the RecNN as an Encoder , whereas the inside-vector associ - ated with a node is taken to be an encoding of the tree rooted at that node . The encoding can potentially be sensitive to arbitrary properties of the structure . The vector is then passed as input to another network . For further discussion on recursive neural networks and their use in natural language tasks , refer to the PhD thesis of Richard Socher ( 2014 ) . 40 . Before the introduction of the computation graph abstraction , the specific backpropagation procedure for computing the gradients in a RecNN as defined above was referred to as the Back-propagation trough Structure ( BPTS ) algorithm ( Goller & Küchler , 1996 ) . 63