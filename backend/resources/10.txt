Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 16 , 2019 . CHAPTER 10 Encoder-Decoder Models , At-tention , Contextual Em - beddings well good copy sees , better draw remains one’s memory . transformation imagination memory collaborate . Edgar Degas Chapter 9 explored recurrent neural networks com - mon cases , including language modeling , contextual generation , sequence labeling . common thread applications notion transduction — input sequences transformed output sequences one-to-one fash - ion . , explore approach extends models provides greater flexibility range applications . Specifically , introduce encoder - decoder networks , sequence-to-sequence models , capable generating contextually appropriate , arbitrary length , output sequences . Encoder-decoder net - works applied wide range applications including machine translation , summarization , question answering , dialogue modeling . key idea underlying networks encoder network takes input sequence creates contextualized representation . rep - resentation passed decoder generates task-specific output se - quence . encoder decoder networks typically implemented same architecture , often recurrent networks kind studied Chapter 9 . deep networks introduced , encoder-decoder architecture allows networks trained end-to-end fashion application . 10.1 Neural Language Models Generation Revisited understand design encoder-decoder networks return neural language models notion autoregressive generation . Recall simple recurrent network , value f hidden state particular point time function previous hidden state current input ; network output function new hidden state . ht = g ( Uht − 1 + Wxt ) yt = f ( V ht ) , U , V , W weight matrices adjusted training , g suitable non-linear activation function tanh ReLU , common case classification f softmax set possible outputs . practice , gated networks LSTMs GRUs place simple RNNs . 2 CHAPTER 10 • ENCODER-DECODER MODELS , ATTENTION CONTEXTUAL EMBEDDINGS lived hobbit lived hobbit < / s > Sampled Words Softmax Embeddings Prefix groundtheinhole Autogenerated completion RNN Figure 10.1 RNN generate completion input phrase . reflect , abstract away details specific RNN simply specify inputs computation based . earlier equations expressed follows understanding suitable RNN underneath . ht = g ( ht − 1 , xt ) yt = f ( ht ) create RNN-based language model , train network predict next word sequence corpus representative text . Language models trained fashion referred autoregressive models . trained model , ask network generate novel sequences first randomly sam - pling appropriate word beginning sequence . condition generation subsequent words hidden state previous time step well embedding word just generated , again sampling distri - bution provided softmax . specifically , generation softmax output point time provides probability every word vocabulary preceding context , P ( yi | y < ) ∀ ∈ V ; sample particular word , ŷi , distribution condition subsequent generation . process continues end sentence token < \ s > generated . , consider simple variation scheme . language model generate sentence scratch , complete sequence specified prefix . specifically , first pass specified prefix language model forward inference produce sequence hidden states , ending hidden state corresponding last word prefix . begin generating earlier , final hidden state prefix starting point . result process novel output sequence reasonable completion prefix input . Fig . 10.1 illustrates basic scheme . portion network left processes provided prefix , right side executes subsequent auto - regressive generation . Note goal lefthand portion network generate series hidden states input ; outputs 10.1 • NEURAL LANGUAGE MODELS GENERATION REVISITED 3 associated part process reach end prefix . , consider ingenious extension idea world machine translation ( MT ) , task automatically translating sentences language another . primary resources train modern translation systems known parallel texts , bitexts . large text collections consisting pairsbitexts sentences different languages translations another . Tradition - ally MT , text translated referred source translation output called target . extend language models autoregressive generation machine transla - tion , first add end-of-sentence marker end bitext’s source sentence simply concatenate corresponding target . concate - nated source-target pairs serve training data combined language model . Training proceeds RNN-based language model . network trained autoregressively predict next word set sequences comprised concatenated source-target bitexts , shown Fig . 10.2 . translate source text trained model , run network performing forward inference generate hidden states get end source . begin autoregressive generation , asking word context hidden layer end source input well end-of-sentence marker . Subsequent words conditioned previous hidden state embedding last word generated . vivait un < / s > hobbit vivait un hobbit < / s > Source hobbita livedthere Target < / s > lived hobbita Figure 10.2 Training setup neural language model approach machine translation . Source-target bi - texts concatenated train language model . Early efforts clever approach demonstrated surprisingly good results standard datasets led series innovations basis net - works discussed remainder chapter . Chapter 11 provides in-depth discussion fundamental issues translation well current state-of - the-art approaches MT . , focus powerful models arose early efforts . 4 CHAPTER 10 • ENCODER-DECODER MODELS , ATTENTION CONTEXTUAL EMBEDDINGS 10.2 Encoder-Decoder Networks Fig . 10.3 abstracts away specifics machine translation illustrates basic encoder-decoder architecture . elements network left pro-encoder-decoder cess input sequence comprise encoder , entire purpose generate contextualized representation input . network , represen - tation embodied final hidden state encoder , hn , turn feeds first hidden state decoder . decoder network right takes state autoregressively generates sequence outputs . y1 y2 y3 ym Encoder xnx2x1 Decoder hn … … … … Figure 10.3 Basic RNN-based encoder-decoder architecture . final hidden state encoder RNN serves context decoder role h0 decoder RNN . basic architecture consistent original applications neural mod - els machine translation . , embodies number design choices less optimal . Among major ones encoder decoder assumed same internal structure ( RNNs case ) , final state encoder context available decoder , finally context available decoder initial hidden state . Abstracting away choices , say encoder-decoder networks consist three components : 1 . encoder accepts input sequence , xn1 , generates corresponding sequence contextualized representations , hn1 . 2 . context vector , c , function hn1 , conveys essence input decoder . 3 . decoder , accepts c input generates arbitrary length sequence hidden states hm1 , corresponding sequence output states ym1 , obtained . Fig . 10.4 illustrates abstracted architecture . explore pos - sibilities components . 10.2 • ENCODER-DECODER NETWORKS 5 y1 y2 ym xnx2x1 … Encoder Decoder Context … Figure 10.4 Basic architecture abstract encoder-decoder network . context function vector contextualized input representations decoder variety ways . Encoder Simple RNNs , LSTMs , GRUs , convolutional networks , well transformer net - works ( discussed later chapter ) , employed encoders . simplicity , figures show single network layer encoder , , stacked architectures norm , output states top layer stack taken final representation . widely encoder design makes stacked Bi-LSTMs hidden states top layers forward backward passes concatenated described Chapter 9 provide contex - tualized representations time step . Decoder decoder , autoregressive generation produce output sequence , element time , end-of-sequence marker generated . incremen - tal process guided context provided encoder well items generated earlier states decoder . Again , typical approach LSTM GRU-based RNN context consists final hidden state encoder , initialize first hidden state decoder . ( help keep things straight , superscripts e d needed distinguish hidden states encoder decoder . ) Generation proceeds described earlier hidden state conditioned previous hidden state - put generated previous state . c = hen hd0 = c hdt = g ( ŷt − 1 , h d t − 1 ) zt = f ( hdt ) yt = softmax ( zt ) Recall , g stand-in flavor RNN ŷt − 1 embedding output sampled softmax previous step . weakness approach context vector , c , directly avail - able beginning process influence wane output se - quence generated . solution make context vector c available step 6 CHAPTER 10 • ENCODER-DECODER MODELS , ATTENTION CONTEXTUAL EMBEDDINGS decoding process adding parameter computation current hidden state . hdt = g ( ŷt − 1 , h d t − 1 , c ) common approach calculation output layer y base solely newly computed hidden state . cleanly separates underlying recurrence output generation task , makes difficult keep track already generated hasn’t . alternative approach condition output newly generated hidden state , output generated previous state , encoder context . yt = softmax ( ŷt − 1 , zt , c ) Finally , shown earlier , output y time consists softmax computa - tion set possible outputs ( vocabulary case language models ) . distribution task-dependent , critical re - currence depends choosing particular output , ŷ , softmax condition next step decoding . already seen several possible options . neural generation , trying generate novel outputs , sim - ply sample softmax distribution . , applications like MT looking specific output sequence , random sampling appropriate likely lead strange output . alternative choose likely output time step taking argmax softmax output : ŷ = argmaxP ( yi | y < ) easy implement seen several times sequence labeling , independently choosing argmax sequence reliable way arriving good output guarantee individual choices made make sense together combine coherent whole . sequence labeling addressed CRF-layer output token types combined Viterbi - style dynamic programming search . Unfortunately , approach viable dynamic programming invariant hold . Beam Search viable alternative view decoding problem heuristic state-space search systematically explore space possible outputs . key ap - proach controlling exponential growth search space . accomplish , technique called beam search . Beam search operates combin-Beam Search ing breadth-first-search strategy heuristic filter scores option prunes search space stay fixed-size memory footprint , called beam width . first step decoding , select B-best options softmax output y , B size beam . option scored corresponding probability softmax output decoder . initial outputs constitute search frontier . refer sequence partial outputs generated search paths hypotheses . subsequent steps , hypothesis frontier extended incrementally passed distinct decoders , again generate softmax entire vocabulary . provide necessary inputs decoders , hypothesis include words generated thus far context vector , 10.2 • ENCODER-DECODER NETWORKS 7 hidden state previous step . New hypotheses representing every possible extension current ones generated added frontier . new hypotheses scored P ( yi | y < ) , product probability current word choice multiplied probability path led . control exponential growth frontier , pruned contain top B hypotheses . process continues < \ s > generated indicating complete - didate output found . point , completed hypothesis removed frontier size beam reduced . search continues beam reduced 0 . Leaving B hypotheses consider . Fig . 10.5 illustrates process beam width 3 . y_ { i-1 } c h_ { i-1 } EOS EOS EOS EOS y_i Decoder Beam search beam width = 4 . 0 1 2 3 4 5 6 7 Figure 10.5 Beam decoding beam width 4 . initial step , frontier filled best 4 options initial state decoder . breadth-first fashion , state frontier passed decoder computes softmax entire vocabulary attempts enter new state frontier subject constraint better worst state already . completed sequences discovered recorded removed frontier beam width reduced 1 . final complication arises fact completed hypotheses different lengths . Unfortunately , due probabilistic nature scoring scheme , longer hypotheses naturally look worse shorter ones just based length . issue earlier steps decoding ; due breadth-first nature beam search hypotheses compared same length . usual solution apply form length normalization hypotheses . normalization , B hypotheses select best , pass subset downstream application respective scores . Context defined context vector c function hidden states encoder , , c = f ( hn1 ) . Unfortunately , number hidden states varies size input , making difficult just directly context decode . basic approach described earlier avoids issue c just final hidden state 8 CHAPTER 10 • ENCODER-DECODER MODELS , ATTENTION CONTEXTUAL EMBEDDINGS function BEAMDECODE ( c , beam width ) returns best paths y0 , h0 ← 0 path ← ( ) complete paths ← ( ) state ← ( c , y0 , h0 , path ) ; initial state frontier ← 〈 state 〉 ; initial frontier frontier contains incomplete paths beamwidth > 0 extended frontier ← 〈 〉 state ∈ frontier y ← DECODE ( state ) word ∈ Vocabulary successor ← NEWSTATE ( state , , yi ) new agenda ← ADDTOBEAM ( successor , extended frontier , beam width ) state extended frontier state complete complete paths ← APPEND ( complete paths , state ) extended frontier ← REMOVE ( extended frontier , state ) beam width ← beam width - 1 frontier ← extended frontier return completed paths function NEWSTATE ( state , word , word prob ) returns new state function ADDTOBEAM ( state , frontier , width ) returns updated frontier LENGTH ( frontier ) < width frontier ← INSERT ( state , frontier ) else SCORE ( state ) > SCORE ( WORSTOF ( frontier ) ) frontier ← REMOVE ( WORSTOF ( frontier ) ) frontier ← INSERT ( state , frontier ) return frontier Figure 10.6 Beam search decoding . encoder . approach advantage simple reducing context fixed length vector . , final hidden state inevitably focused latter parts input sequence , rather input whole . solution problem Bi-RNNs , context function end state forward backward passes . described Chapter 9 , straightforward approach concatenate final states forward backward passes . alternative simply sum average encoder hidden states produce context vector . Unfortunately , approach loses useful information individual encoder states might prove useful decoding . 10.3 • ATTENTION 9 10.3 Attention overcome deficiencies simple approaches context , need mechanism take entire encoder context account , dynamically updates course decoding , embodied fixed-size vector . Taken together , refer approach attention mechanism . attentionmechanism first step replace static context vector dynamically derived encoder hidden states point decoding . context vector , ci , generated anew decoding step takes encoder hidden states account derivation . make context available decoding conditioning computation current decoder state , prior hidden state previous output generated decoder . hdi = g ( ŷi − 1 , h d − 1 , ci ) first step computing ci compute vector scores capture relevance encoder hidden state decoder state captured hdi − 1 . , state decoding compute score ( hdi − 1 , h e j ) encoder state j . , assume score provides measure similar decoder hidden state encoder hidden state . implement similarity score , begin straightforward approach introduced Chapter 6 dot product vectors . score ( hdi − 1 , h e j ) = h d − 1 · hej result dot product scalar reflects degree similarity two vectors . vector scores encoder hidden states gives relevance encoder state current step decoder . simple dot product effective , static measure facilitate adaptation course training fit characteristics applications . robust similarity score obtained parameterizing score own set weights , Ws . score ( hdi − 1 , h e j ) = h d t − 1Wsh e j introducing Ws score , giving network ability learn aspects similarity decoder encoder states important current application . make scores , next normalize softmax create vector weights , αi j , tells proportional relevance encoder hidden state j current decoder state , . αi j = softmax ( score ( hdi − 1 , h e j ) ∀ j ∈ e ) = exp ( score ( hdi − 1 , h e j ) ∑ k exp ( score ( h d − 1 , h e k ) ) Finally , distribution α , compute fixed-length context vector current decoder state taking weighted average encoder hidden states . ci = ∑ j αi jhej ( 10.1 ) 10 CHAPTER 10 • ENCODER-DECODER MODELS , ATTENTION CONTEXTUAL EMBEDDINGS , finally fixed-length context vector takes account information entire encoder state dynamically update reflect needs decoder step decoding . Fig . 10.7 illustrates encoder - decoder network attention . yi-1 yi Encoder xnx2x1 Decoder hnh1 hi-1 hi … … ci … ↵ , j = softmax ( hi � 1 , h̄j ) < latexit sha1_base64 = " DLuRVqL15PhhLqsypSJaKREG2C0 = " > AAACE3icbVDLSsNAFJ34rPUVdelmsAgKWhIp6EYounFZwVbBlHAznTTTziRhZiKWkM9w46 + 4EXGhgh / g3zhpu / FxYeBwzhnuPSdIOVPacb6smdm5 + YXFylJ1eWV1bd3e2OyoJJOEtknCE3kTgKKcxbStmeb0JpUURMDpdTA8L / XrOyoVS + IrPUppV0A / ZiEjoA3l2w0PeBqBn7ODQYFPsSdAR1LkKgm1gPtiLzLSoVscYC8AmUf + oNiv + nbNqTvjwX + BOwU1NJ2Wb795vYRkgsaacFDq1nVS3c1BakY4LapepmgKZAh9mo8zFXjXUD0cJtK8WOMx + 8MHQqmRCIyzPFn91kryP + 020 + FJN2dxmmkak8miMONYJ7gsCPeYpETzkQFAJDMXYhKBBKJNjWV093fQv6BzVHcNvmzUmmfTEipoG + 2gPeSiY9REF6iF2oigR / SM3tGH9WA9WS / W68Q6Y03 / bKEfY31 + AwCBnVk = < / latexit >< latexit sha1_base64 = " DLuRVqL15PhhLqsypSJaKREG2C0 = " > AAACE3icbVDLSsNAFJ34rPUVdelmsAgKWhIp6EYounFZwVbBlHAznTTTziRhZiKWkM9w46 + 4EXGhgh / g3zhpu / FxYeBwzhnuPSdIOVPacb6smdm5 + YXFylJ1eWV1bd3e2OyoJJOEtknCE3kTgKKcxbStmeb0JpUURMDpdTA8L / XrOyoVS + IrPUppV0A / ZiEjoA3l2w0PeBqBn7ODQYFPsSdAR1LkKgm1gPtiLzLSoVscYC8AmUf + oNiv + nbNqTvjwX + BOwU1NJ2Wb795vYRkgsaacFDq1nVS3c1BakY4LapepmgKZAh9mo8zFXjXUD0cJtK8WOMx + 8MHQqmRCIyzPFn91kryP + 020 + FJN2dxmmkak8miMONYJ7gsCPeYpETzkQFAJDMXYhKBBKJNjWV093fQv6BzVHcNvmzUmmfTEipoG + 2gPeSiY9REF6iF2oigR / SM3tGH9WA9WS / W68Q6Y03 / bKEfY31 + AwCBnVk = < / latexit >< latexit sha1_base64 = " DLuRVqL15PhhLqsypSJaKREG2C0 = " > AAACE3icbVDLSsNAFJ34rPUVdelmsAgKWhIp6EYounFZwVbBlHAznTTTziRhZiKWkM9w46 + 4EXGhgh / g3zhpu / FxYeBwzhnuPSdIOVPacb6smdm5 + YXFylJ1eWV1bd3e2OyoJJOEtknCE3kTgKKcxbStmeb0JpUURMDpdTA8L / XrOyoVS + IrPUppV0A / ZiEjoA3l2w0PeBqBn7ODQYFPsSdAR1LkKgm1gPtiLzLSoVscYC8AmUf + oNiv + nbNqTvjwX + BOwU1NJ2Wb795vYRkgsaacFDq1nVS3c1BakY4LapepmgKZAh9mo8zFXjXUD0cJtK8WOMx + 8MHQqmRCIyzPFn91kryP + 020 + FJN2dxmmkak8miMONYJ7gsCPeYpETzkQFAJDMXYhKBBKJNjWV093fQv6BzVHcNvmzUmmfTEipoG + 2gPeSiY9REF6iF2oigR / SM3tGH9WA9WS / W68Q6Y03 / bKEfY31 + AwCBnVk = < / latexit >< latexit sha1_base64 = " DLuRVqL15PhhLqsypSJaKREG2C0 = " > AAACE3icbVDLSsNAFJ34rPUVdelmsAgKWhIp6EYounFZwVbBlHAznTTTziRhZiKWkM9w46 + 4EXGhgh / g3zhpu / FxYeBwzhnuPSdIOVPacb6smdm5 + YXFylJ1eWV1bd3e2OyoJJOEtknCE3kTgKKcxbStmeb0JpUURMDpdTA8L / XrOyoVS + IrPUppV0A / ZiEjoA3l2w0PeBqBn7ODQYFPsSdAR1LkKgm1gPtiLzLSoVscYC8AmUf + oNiv + nbNqTvjwX + BOwU1NJ2Wb795vYRkgsaacFDq1nVS3c1BakY4LapepmgKZAh9mo8zFXjXUD0cJtK8WOMx + 8MHQqmRCIyzPFn91kryP + 020 + FJN2dxmmkak8miMONYJ7gsCPeYpETzkQFAJDMXYhKBBKJNjWV093fQv6BzVHcNvmzUmmfTEipoG + 2gPeSiY9REF6iF2oigR / SM3tGH9WA9WS / W68Q6Y03 / bKEfY31 + AwCBnVk = < / latexit > … Figure 10.7 Encoder-decoder network attention . Computing value hi based previous hidden state , previous word generated , current context vector ci . context vector derived attention computation based comparing previous hidden state encoder hidden states . 10.4 Applications Encoder-Decoder Networks addition attention basic encoder-decoder networks led rapid improve - ment performance wide-range applications including summarization , sentence simplification , question answering image captioning . 10.5 Self-Attention Transformer Networks 10.6 • SUMMARY 11 10.6 Summary • Encoder-decoder networks • Attention • Transformers Bibliographical Historical Notes 12 Chapter 10 • Encoder-Decoder Models , Attention Contextual Embeddings Bahdanau , D . , Cho , K . , Bengio , Y . ( 2015 ) . Neural ma - chine translation jointly learning align translate . 3rd International Conference Learning Representa - tions , ICLR 2015 , San Diego , CA , USA , 7-9 , 2015 , Conference Track Proceedings . Cho , K . , van Merriënboer , B . , Gulcehre , C . , Bahdanau , D . , Bougares , F . , Schwenk , H . , Bengio , Y . ( 2014 ) . Learn - ing phrase representations RNN encoder – decoder statistical machine translation . EMNLP 2014 , 1724 – 1734 . Graves , . ( 2013 ) . Generating sequences recurrent neu - ral networks . . Graves , . , Fernández , S . , Gomez , F . , Schmidhuber , J . ( 2006 ) . Connectionist temporal classification : La - belling unsegmented sequence data recurrent neural networks . Proceedings 23rd International Con - ference Machine Learning , ICML ’ 06 , 369 – 376 . Graves , . , Fernández , S . , Liwicki , M . , Bunke , H . , Schmidhuber , J . ( 2007 ) . Unconstrained on-line handwrit - ing recognition recurrent neural networks . NIPS 2007 , 577 – 584 . Kalchbrenner , N . Blunsom , P . ( 2013 ) . Recurrent contin - uous translation models . Proceedings 2013 Con - ference Empirical Methods Natural Language Pro - cessing . Association Computational Linguistics . Luong , T . , Pham , H . , Manning , C . D . ( 2015 ) . Effective approaches attention-based neural machine translation . Proceedings Conference Empirical Methods Natural Language Processing , 1412 – 1421 . Sutskever , . , Vinyals , O . , Le , Q . V . ( 2014 ) . Sequence sequence learning neural networks . Advances Neural Information Processing Systems 27 : Annual Con - ference Neural Information Processing Systems 2014 , December 8-13 2014 , Montreal , Quebec , Canada , 3104 – 3112 .