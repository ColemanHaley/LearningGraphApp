Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 14 Statistical Constituency Pars-ing characters Damon Runyon’s short stories willing bet “ propo - sition whatever ” , Runyon Sky Masterson Idyll Miss Sarah Brown , probability getting aces back-to-back odds against man able throw peanut second base home plate . moral language processing : enough knowledge figure probability just anything . last two chapters introduced models syntactic constituency structure parsing . , show possible build probabilistic models syntactic knowledge efficient probabilistic parsers . crucial probabilistic parsing solve problem disambigua - tion . Recall Chapter 13 sentences average tend syntactically ambiguous phenomena like coordination ambiguity attachment ambiguity . CKY parsing algorithm represent ambiguities effi - cient way equipped resolve . probabilistic parser offers solution problem : compute probability interpretation choose probable interpretation . commonly probabilistic constituency gram - mar formalism probabilistic context-free grammar ( PCFG ) , probabilistic augmentation context-free grammars rule associated prob - ability . introduce PCFGs next section , showing trained Treebank grammars parsed probabilistic version CKY algorithm Chapter 13 . show number ways improve basic probabil - ity model ( PCFGs trained Treebank grammars ) , modifying set non-terminals ( making specific general ) , adding sophisticated conditioning factors like subcategorization dependencies . Heav - ily lexicalized grammar formalisms Lexical-Functional Grammar ( LFG ) ( Bresnan , 1982 ) , Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard Sag , 1994 ) , Tree-Adjoining Grammar ( TAG ) ( Joshi , 1985 ) , Combinatory Categorial Grammar ( CCG ) pose additional problems probabilistic parsers . Section 14.7 introduces task supertagging heuristic search methods based * algorithm context CCG parsing . Finally , describe standard techniques metrics evaluating parsers . 14.1 Probabilistic Context-Free Grammars simplest augmentation context-free grammar Probabilistic Context - Free Grammar ( PCFG ) , known Stochastic Context-Free GrammarPCFG ( SCFG ) , first proposed Booth ( 1969 ) . Recall context-free grammar G isSCFG defined four parameters ( N , Σ , R , S ) ; probabilistic context-free grammar defined four parameters , slight augmentation rules R : 2 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING N set non-terminal symbols ( variables ) Σ set terminal symbols ( disjoint N ) R set rules productions , form → β [ p ] , non-terminal , β string symbols infinite set strings ( Σ ∪ N ) ∗ , p number 0 1 expressing P ( β | ) S designated start symbol , PCFG differs standard CFG augmenting rule R conditional probability : → β [ p ] ( 14.1 ) p expresses probability non-terminal expanded sequence β . , p conditional probability expansion β left-hand-side ( LHS ) non-terminal . represent probability P ( → β ) P ( → β | ) P ( RHS | LHS ) Thus , consider possible expansions non-terminal , sum probabilities 1 : ∑ β P ( → β ) = 1 Figure 14.1 shows PCFG : probabilistic augmentation L1 miniature En - glish CFG grammar lexicon . Note probabilities expansions non-terminal sum 1 . note probabilities made up pedagogical purposes . real grammar great many rules non-terminal ; hence , probabilities particular rule tend smaller . PCFG consistent sum probabilities sentencesconsistent language equals 1 . Certain kinds recursive rules cause grammar inconsistent causing infinitely looping derivations sentences . ex - ample , rule S → S probability 1 lead lost probability mass due derivations never terminate . Booth Thompson ( 1973 ) details consistent inconsistent grammars . PCFGs ? PCFG estimate number useful probabilities concerning sentence parse tree ( s ) , including probability particular parse tree ( useful disambiguation ) probability sentence piece sentence ( useful language modeling ) . works . 14.1.1 PCFGs Disambiguation PCFG assigns probability parse tree T ( i.e . , derivation ) sen - tence S . attribute useful disambiguation . example , consider two parses sentence “ Book dinner flight ” shown Fig . 14.2 . sensible 14.1 • PROBABILISTIC CONTEXT-FREE GRAMMARS 3 Grammar Lexicon S → NP VP [ . 80 ] Det → [ . 10 ] | [ . 30 ] | [ . 60 ] S → Aux NP VP [ . 15 ] Noun → book [ . 10 ] | flight [ . 30 ] S → VP [ . 05 ] | meal [ . 05 ] | money [ . 05 ] NP → Pronoun [ . 35 ] | flight [ . 40 ] | dinner [ . 10 ] NP → Proper-Noun [ . 30 ] Verb → book [ . 30 ] | include [ . 30 ] NP → Det Nominal [ . 20 ] | prefer [ . 40 ] NP → Nominal [ . 15 ] Pronoun → [ . 40 ] | [ . 05 ] Nominal → Noun [ . 75 ] | [ . 15 ] | [ . 40 ] Nominal → Nominal Noun [ . 20 ] Proper-Noun → Houston [ . 60 ] Nominal → Nominal PP [ . 05 ] | NWA [ . 40 ] VP → Verb [ . 35 ] Aux → [ . 60 ] | [ . 40 ] VP → Verb NP [ . 20 ] Preposition → [ . 30 ] | [ . 30 ] VP → Verb NP PP [ . 10 ] | [ . 20 ] | near [ . 15 ] VP → Verb PP [ . 15 ] | [ . 05 ] VP → Verb NP NP [ . 05 ] VP → VP PP [ . 15 ] PP → Preposition NP [ 1.0 ] Figure 14.1 PCFG probabilistic augmentation L1 miniature English CFG grammar lexicon Fig . ? ? . probabilities made up pedagogical purposes based corpus ( real corpus many rules , true probabilities rule smaller ) . parse left means “ Book flight serves dinner ” . nonsensical parse right , , mean something like “ Book flight behalf ‘ dinner ” ’ just structurally similar sentence like “ book John flight ? ” means something like “ book flight behalf John ? ” probability particular parse T defined product probabil - ities n rules expand n non-terminal nodes parse tree T , rule expressed LHSi → RHSi : P ( T , S ) = n ∏ = 1 P ( RHSi | LHSi ) ( 14.2 ) resulting probability P ( T , S ) joint probability parse sentence probability parse P ( T ) . true ? First , definition joint probability : P ( T , S ) = P ( T ) P ( S | T ) ( 14.3 ) parse tree includes words sentence , P ( S | T ) 1 . Thus , P ( T , S ) = P ( T ) P ( S | T ) = P ( T ) ( 14.4 ) compute probability trees Fig . 14.2 multiplying probabilities rules derivation . example , proba - bility left tree Fig . 14.2a ( call Tle f t ) right tree ( Fig . 14.2b Tright ) computed follows : P ( Tle f t ) = . 05 ∗ . 20 ∗ . 20 ∗ . 20 ∗ . 75 ∗ . 30 ∗ . 60 ∗ . 10 ∗ . 40 = 2.2 × 10 − 6 P ( Tright ) = . 05 ∗ . 10 ∗ . 20 ∗ . 15 ∗ . 75 ∗ . 75 ∗ . 30 ∗ . 60 ∗ . 10 ∗ . 40 = 6.1 × 10 − 7 4 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING S VP NP Nominal Noun flight Nominal Noun dinner Det Verb Book S VP NP Nominal Noun flight NP Nominal Noun dinner Det Verb Book Rules P Rules P S → VP . 05 S → VP . 05 VP → Verb NP . 20 VP → Verb NP NP . 10 NP → Det Nominal . 20 NP → Det Nominal . 20 Nominal → Nominal Noun . 20 NP → Nominal . 15 Nominal → Noun . 75 Nominal → Noun . 75 Nominal → Noun . 75 Verb → book . 30 Verb → book . 30 Det → . 60 Det → . 60 Noun → dinner . 10 Noun → dinner . 10 Noun → flight . 40 Noun → flight . 40 Figure 14.2 Two parse trees ambiguous sentence . parse left corresponds sensible meaning “ Book flight serves dinner ” , parse right corre - sponds nonsensical meaning “ Book flight behalf ‘ dinner ’ ” . left tree Fig . 14.2 higher probability tree right . Thus , parse correctly chosen disambiguation algorithm selects parse highest PCFG probability . formalize intuition picking parse highest probability correct way disambiguation . Consider possible parse trees sentence S . string words S called yield parse tree S . yield Thus , parse trees yield S , disambiguation algorithm picks parse tree probable S : T̂ ( S ) = argmax T s.t.S = yield ( T ) P ( T | S ) ( 14.5 ) definition , probability P ( T | S ) rewritten P ( T , S ) / P ( S ) , thus leading T̂ ( S ) = argmax T s.t.S = yield ( T ) P ( T , S ) P ( S ) ( 14.6 ) maximizing parse trees same sentence , P ( S ) 14.1 • PROBABILISTIC CONTEXT-FREE GRAMMARS 5 constant tree , eliminate : T̂ ( S ) = argmax T s.t.S = yield ( T ) P ( T , S ) ( 14.7 ) Furthermore , showed P ( T , S ) = P ( T ) , final equation choosing likely parse neatly simplifies choosing parse high - est probability : T̂ ( S ) = argmax T s.t.S = yield ( T ) P ( T ) ( 14.8 ) 14.1.2 PCFGs Language Modeling second attribute PCFG assigns probability string words constituting sentence . important language modeling , speech recognition , machine translation , spelling correction , augmentative com - munication , applications . probability unambiguous sentence P ( T , S ) = P ( T ) just probability single parse tree sentence . probability ambiguous sentence sum probabilities parse trees sentence : P ( S ) = ∑ T s.t.S = yield ( T ) P ( T , S ) ( 14.9 ) = ∑ T s.t.S = yield ( T ) P ( T ) ( 14.10 ) additional feature PCFGs useful language modeling ability assign probability substrings sentence . example , suppose know probability next word wi sentence words seen far w1 , . . . , wi − 1 . general formula P ( wi | w1 , w2 , . . . , wi − 1 ) = P ( w1 , w2 , . . . , wi − 1 , wi ) P ( w1 , w2 , . . . , wi − 1 ) ( 14.11 ) saw Chapter 3 simple approximation probability N-grams , conditioning last word two entire context ; thus , bigram approximation give P ( wi | w1 , w2 , . . . , wi − 1 ) ≈ P ( wi − 1 , wi ) P ( wi − 1 ) ( 14.12 ) fact N-gram model make couple words context means ignoring potentially useful prediction cues . Consider predicting word following sentence Chelba Jelinek ( 2000 ) : ( 14.13 ) contract ended loss 7 cents trading low 9 cents trigram grammar predict words 7 cents , seems clear verb ended subject contract useful predictors PCFG - based parser help make . Indeed , turns PCFGs allow condition entire previous context w1 , w2 , . . . , wi − 1 shown Eq . 14.11 . summary , section previous shown PCFGs applied disambiguation syntactic parsing word prediction lan - guage modeling . applications require able compute probability parse tree T sentence S . next few sections introduce algorithms computing probability . 6 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING 14.2 Probabilistic CKY Parsing PCFGs parsing problem PCFGs produce most-likely parse T̂ sentence S , , T̂ ( S ) = argmax T s.t.S = yield ( T ) P ( T ) ( 14.14 ) algorithms computing likely parse simple extensions standard algorithms parsing ; modern probabilistic parsers based probabilistic CKY algorithm , first described Ney ( 1991 ) . probabilistic CKYprobabilisticCKY algorithm assumes PCFG Chomsky normal form . Recall page ? ? CNF , right-hand side rule expand two non-terminals single terminal , i.e . , rules form → B C , → w . CKY algorithm , represented sentence indices words . Thus , example sentence like ( 14.15 ) Book flight Houston . assume following indices word : ( 14.16 ) 0 © Book 1 © 2 © flight 3 © 4 © Houston 5 © indices , constituent CKY parse tree encoded two-dimensional matrix . Specifically , sentence length n grammar contains V non-terminals , upper-triangular portion ( n + 1 ) × ( n + 1 ) matrix . CKY , cell table [ , j ] contained list constituents span sequence words j . probabilistic CKY , slightly simpler think constituents cell constituting third dimension maximum length V . third dimension corresponds non-terminal placed cell , value cell probability non - terminal / constituent rather list constituents . summary , cell [ , j , ] ( n + 1 ) × ( n + 1 ) × V matrix probability constituent type spans positions j input . Figure 14.3 gives probabilistic CKY algorithm . function PROBABILISTIC-CKY ( words , grammar ) returns probable parse probability j ← 1 LENGTH ( words ) { | → words [ j ] ∈ grammar } table [ j − 1 , j , ] ← P ( → words [ j ] ) ← j − 2 downto 0 k ← + 1 j − 1 { | → BC ∈ grammar , table [ , k , B ] > 0 table [ k , j , C ] > 0 } ( table [ , j , ] < P ( → BC ) × table [ , k , B ] × table [ k , j , C ] ) table [ , j , ] ← P ( → BC ) × table [ , k , B ] × table [ k , j , C ] back [ , j , ] ← { k , B , C } return BUILD TREE ( back [ 1 , LENGTH ( words ) , S ] ) , table [ 1 , LENGTH ( words ) , S ] Figure 14.3 probabilistic CKY algorithm finding maximum probability parse string num words words PCFG grammar num rules rules Chomsky normal form . back array backpointers recover best parse . build tree function left exercise reader . 14.2 • PROBABILISTIC CKY PARSING PCFGS 7 Like basic CKY algorithm Fig . ? ? , probabilistic CKY algorithm re - quires grammar Chomsky normal form . Converting probabilistic grammar CNF requires modify probabilities probability parse remains same new CNF grammar . Exercise 14.2 asks modify algorithm conversion CNF Chapter 13 correctly han - dles rule probabilities . practice , generalized CKY algorithm handles unit productions directly typically . Recall Exercise 13.3 asked make change CKY ; Exercise 14.3 asks extend change probabilistic CKY . example probabilistic CKY chart , following mini - grammar , already CNF : S → NP VP . 80 Det → . 40 NP → Det N . 30 Det → . 40 V P → V NP . 20 N → meal . 01 V → includes . 05 N → f light . 02 grammar , Fig . 14.4 shows first steps probabilistic CKY parse sentence “ flight includes meal ” . flight [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] Det : . 40 includes meal [ 3,4 ] [ 4,5 ] N : . 02 V : . 05 NP : . 30 * . 40 * . 02 = . 0024 [ 0,4 ] [ 1,4 ] [ 2,4 ] [ 3,5 ] [ 2,5 ] [ 1,5 ] [ 0,5 ] Det : . 40 N : . 01 Figure 14.4 beginning probabilistic CKY matrix . Filling rest chart left Exercise 14.4 reader . 8 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING 14.3 Ways Learn PCFG Rule Probabilities PCFG rule probabilities come ? two ways learn proba - bilities rules grammar . simplest way treebank , corpus already parsed sentences . Recall introduced Chapter 12 idea treebanks commonly Penn Treebank ( Marcus et al . , 1993 ) , collec - tion parse trees English , Chinese , languages distributed Linguistic Data Consortium . treebank , compute probability expansion non-terminal counting number times expansion occurs normalizing . P ( α → β | α ) = Count ( α → β ) ∑ γ Count ( α → γ ) = Count ( α → β ) Count ( α ) ( 14.17 ) treebank ( non-probabilistic ) parser , generate counts need computing PCFG rule probabilities first parsing corpus sentences parser . sentences unambiguous , simple : parse corpus , increment counter every rule parse , normalize get probabilities . wait ! sentences ambiguous , , multiple parses , know parse count rules . , need keep separate count parse sentence weight partial counts probability parse appears . get parse probabilities weight rules , need already probabilistic parser . intuition solving chicken-and-egg problem incrementally - prove estimates beginning parser equal rule probabilities , parse sentence , compute probability parse , probabilities weight counts , re-estimate rule probabilities , , proba - bilities converge . standard algorithm computing solution called inside-outside algorithm ; proposed Baker ( 1979 ) generalization theinside-outside forward-backward algorithm HMMs . Like forward-backward , inside-outside special case Expectation Maximization ( EM ) algorithm , hence two steps : expectation step , maximization step . Lari Young ( 1990 ) Manning Schütze ( 1999 ) algorithm . 14.4 Problems PCFGs probabilistic context-free grammars natural extension context-free grammars , two main problems probability estimators : Poor independence assumptions : CFG rules impose independence assumption probabilities leads poor modeling structural dependencies parse tree . Lack lexical conditioning : CFG rules model syntactic facts specific words , leading problems subcategorization ambiguities , preposition attachment , coordinate structure ambiguities . problems , probabilistic constituent parsing models augmented version PCFGs , modify Treebank-based grammar way . 14.4 • PROBLEMS PCFGS 9 next few sections discussing problems detail introduce augmentations . 14.4.1 Independence Assumptions Miss Rule Dependencies look problems detail . Recall CFG expansion non-terminal independent context , , nearby non - terminals parse tree . Similarly , PCFG , probability particular rule like NP → Det N independent rest tree . definition , probability group independent events product probabilities . two facts explain why PCFG compute probability tree just multiplying probabilities non-terminal expansion . Unfortunately , CFG independence assumption results poor probability estimates . English choice node expands depend location node parse tree . example , English turns NPs syntactic subjects far likely pronouns , NPs syntactic objects far likely non-pronominal ( e.g . , proper noun determiner noun sequence ) , shown statistics NPs Switchboard corpus ( Francis et al . , 1999 ) : 1 Pronoun Non-Pronoun Subject 91 % 9 % Object 34 % 66 % Unfortunately , way represent contextual difference prob - abilities PCFG . Consider two expansions non-terminal NP pronoun determiner + noun . shall set probabilities two rules ? set probabilities overall probability Switchboard corpus , two rules equal probability . NP → DT NN . 28 NP → PRP . 25 PCFGs allow rule probability conditioned surrounding context , equal probability get ; way capture fact subject position , probability NP → PRP go up . 91 , object position , probability NP → DT NN go up . 66 . dependencies captured probability expanding NP pronoun ( e.g . , NP → PRP ) versus lexical NP ( e.g . , NP → DT NN ) condi - tioned NP subject object . Section 14.5 introduces technique parent annotation adding kind conditioning . 14.4.2 Lack Sensitivity Lexical Dependencies second class problems PCFGs lack sensitivity words parse tree . Words play role PCFGs parse probability includes probability word part-of-speech ( e.g . , rules like V → sleep , NN → book , etc . ) . 1 Distribution subjects 31,021 declarative sentences ; distribution objects 7,489 sen - tences . tendency caused subject position realize topic old information sentence ( Givón , 1990 ) . Pronouns way talk old information , non-pronominal ( “ lexical ” ) noun-phrases often introduce new referents ( Chapter 22 ) . 10 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING turns lexical information useful places grammar , resolving prepositional phrase ( PP ) attachment ambiguities . prepo - sitional phrases English modify noun phrase verb phrase , parser finds prepositional phrase , decide attach tree . Consider following example : ( 14.18 ) Workers dumped sacks bin . Figure 14.5 shows two possible parse trees sentence ; left correct parse ; Fig . 14.6 shows another perspective preposition attachment problem , demonstrating resolving ambiguity Fig . 14.5 equivalent deciding attach prepositional phrase rest tree NP VP nodes ; say correct parse requires VP attachment , theVP attachment incorrect parse implies NP attachment.NP attachment S VP PP NP NN bin DT P NP NNS sacks VBD dumped NP NNS workers S VP NP PP NP NN bin DT P NP NNS sacks VBD dumped NP NNS workers Figure 14.5 Two possible parse trees prepositional phrase attachment ambiguity . left parse sensible , “ bin ” describes resulting location sacks . right incorrect parse , sacks dumped ones already “ bin ” , whatever might mean . Why PCFG already deal PP attachment ambiguities ? Note two parse trees Fig . 14.5 almost exactly same rules ; differ left-hand parse rule : V P → V BD NP PP right-hand parse : V P → V BD NP NP → NP PP Depending probabilities set , PCFG always prefer NP attachment VP attachment . happens , NP attachment slightly common English , trained rule probabilities corpus , might always prefer NP attachment , causing misparse sentence . suppose set probabilities prefer VP attachment sen - tence . misparse following , requires NP attachment : 14.5 • IMPROVING PCFGS SPLITTING NON-TERMINALS 11 S VP NP NNS sacks VBD dumped NP NNS workers PP NP NN bin DT P Figure 14.6 Another view preposition attachment problem . PP right attach VP NP nodes partial parse tree left ? ( 14.19 ) fishermen caught tons herring information input sentence know ( 14.19 ) requires NP attachment ( 14.18 ) requires VP attachment ? preferences come identities verbs , nouns , prepositions . affinity verb dumped preposition greater affinity noun sacks preposition , thus leading VP attachment . hand , ( 14.19 ) affinity tons greater caught , leading NP attachment . Thus , get correct parse kinds examples , need model somehow augments PCFG probabilities deal lexical dependency statistics different verbs prepositions . lexicaldependency Coordination ambiguities another case lexical dependencies key choosing proper parse . Figure 14.7 shows example Collins ( 1999 ) two parses phrase dogs houses cats . dogs semantically better conjunct cats houses ( dogs fit inside cats ) , parse [ dogs [ NP houses cats ] ] intuitively unnatural dispreferred . two parses Fig . 14.7 , , exactly same PCFG rules , thus PCFG assign same probability . summary , shown section previous probabilistic context-free grammars incapable modeling important structural lexical dependencies . next two sections sketch current methods augmenting PCFGs deal issues . 14.5 Improving PCFGs Splitting Non-Terminals start first two problems PCFGs mentioned : inability model structural dependencies , like fact NPs subject position tend pronouns , whereas NPs object position tend full lexical ( non - pronominal ) form . augment PCFG correctly model fact ? idea split NP non-terminal two versions : sub-split jects , objects . two nodes ( e.g . , NPsubject NPobject ) allow correctly model different distributional properties , 12 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING NP NP Noun cats Conj NP PP NP Noun houses Prep NP Noun dogs NP PP NP NP Noun cats Conj NP Noun houses Prep NP Noun dogs Figure 14.7 instance coordination ambiguity . Although left structure intu - itively correct , PCFG assign identical probabilities structures exactly same set rules . Collins ( 1999 ) . different probabilities rule NPsubject → PRP rule NPobject → PRP . way implement intuition splits parent annotation ( John-parentannotation son , 1998 ) , annotate node parent parse tree . Thus , NP node subject sentence hence parent S anno - tated NPˆS , direct object NP whose parent VP annotated NPˆVP . Figure 14.8 shows example tree produced grammar parent-annotates phrasal non-terminals ( like NP VP ) . ) S VP NP NN flight DT VBD need NP PRP b ) S VPˆS NPˆVP NN flight DT VBD need NPˆS PRP Figure 14.8 standard PCFG parse tree ( ) parent annotation nodes pre-terminal ( b ) . non-terminal nodes ( except pre-terminal part-of-speech nodes ) parse ( b ) annotated identity parent . addition splitting phrasal nodes , improve PCFG splitting pre-terminal part-of-speech nodes ( Klein Manning , 2003b ) . ex - ample , different kinds adverbs ( RB ) tend occur different syntactic positions : common adverbs ADVP parents , VP parents n’t , NP parents just . Thus , adding tags like RBˆADVP , RBˆVP , RBˆNP useful improving PCFG modeling . Similarly , Penn Treebank tag mark wide variety parts-of-speech , including subordinating conjunctions ( , , ) , complementizers ( , ) , prepositions ( , , ) . differences captured parent 14.6 • PROBABILISTIC LEXICALIZED CFGS 13 annotation ( subordinating conjunctions occur S , prepositions PP ) , others require splitting pre-terminal nodes . Figure 14.9 shows example Klein Manning ( 2003b ) even parent-annotated grammar incorrectly parses works noun advertising works . Splitting pre-terminals allow prefer sentential complement results correct verbal parse . Node-splitting problems ; increases size grammar hence reduces amount training data available grammar rule , leading overfitting . Thus , important split just correct level granularity particular training set . early models employed handwritten rules try find optimal number non-terminals ( Klein Manning , 2003b ) , modern models automatically search optimal splits . split merge algorithm Petrovsplit merge et al . ( 2006 ) , example , starts simple X-bar grammar , alternately splits non-terminals , merges non-terminals , finding set annotated nodes maximizes likelihood training set treebank . 14.6 Probabilistic Lexicalized CFGs previous section showed simple probabilistic CKY algorithm pars - ing raw PCFGs achieve extremely high parsing accuracy grammar rule symbols redesigned automatic splits merges . section , discuss alternative family models modifying grammar rules , modify probabilistic model parser allow lexicalized rules . resulting family lexicalized parsers includes Collins parser ( Collins , 1999 ) Charniak parser ( Charniak , 1997 ) . saw Section ? ? syntactic constituents associated lexi - cal head , defined lexicalized grammar non-terminal thelexicalizedgrammar tree annotated lexical head , rule like V P → V BD NP PP VPˆS VPˆVP PPˆVP NPˆPP NNS works NN advertising VB VPˆS VPˆVP SBARˆVP SˆSBAR VPˆS VBZˆVP works NPˆS NNˆNP advertising INˆSBAR VBˆVP TOˆVP Figure 14.9 incorrect parse even parent-annotated parse ( left ) . correct parse ( right ) , produced grammar pre-terminal nodes split , allowing probabilistic grammar capture fact prefers sentential complements . Adapted Klein Manning ( 2003b ) . 14 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING extended VP ( dumped ) → VBD ( dumped ) NP ( sacks ) PP ( ) ( 14.20 ) standard type lexicalized grammar , actually make further exten - sion , associate head tag , part-of-speech tags headwords , head tag non-terminal symbols well . rule thus lexicalized headword head tag constituent resulting format lexicalized rules like VP ( dumped , VBD ) → VBD ( dumped , VBD ) NP ( sacks , NNS ) PP ( , P ) ( 14.21 ) show lexicalized parse tree head tags Fig . 14.10 , extended Fig . ? ? . TOP S ( dumped , VBD ) VP ( dumped , VBD ) PP ( , P ) NP ( bin , NN ) NN ( bin , NN ) bin DT ( , DT ) P ( , P ) NP ( sacks , NNS ) NNS ( sacks , NNS ) sacks VBD ( dumped , VBD ) dumped NP ( workers , NNS ) NNS ( workers , NNS ) workers Internal Rules Lexical Rules TOP → S ( dumped , VBD ) NNS ( workers , NNS ) → workers S ( dumped , VBD ) → NP ( workers , NNS ) VP ( dumped , VBD ) VBD ( dumped , VBD ) → dumped NP ( workers , NNS ) → NNS ( workers , NNS ) NNS ( sacks , NNS ) → sacks VP ( dumped , VBD ) → VBD ( dumped , VBD ) NP ( sacks , NNS ) PP ( , P ) P ( , P ) → PP ( , P ) → P ( , P ) NP ( bin , NN ) DT ( , DT ) → NP ( bin , NN ) → DT ( , DT ) NN ( bin , NN ) NN ( bin , NN ) → bin Figure 14.10 lexicalized tree , including head tags , WSJ sentence , adapted Collins ( 1999 ) . Below show PCFG rules needed parse tree , internal rules left , lexical rules right . generate lexicalized tree , PCFG rule augmented iden - tify right-hand constituent head daughter . headword node set headword head daughter , head tag part-of-speech tag headword . Recall gave Fig . ? ? set handwritten rules identifying heads particular constituents . natural way think lexicalized grammar parent annotation , , simple context-free grammar many copies rule , copy possible headword / head tag constituent . Thinking probabilistic lexicalized CFG way lead set simple PCFG rules shown below tree Fig . 14.10 . Note Fig . 14.10 shows two kinds rules : lexical rules , expresslexical rules expansion pre-terminal word , internal rules , express theinternal rules 14.6 • PROBABILISTIC LEXICALIZED CFGS 15 rule expansions . need distinguish kinds rules lexicalized grammar associated different kinds probabilities . lexical rules deterministic , , probability 1.0 lexicalized pre-terminal like NN ( bin , NN ) expand word bin . internal rules , need estimate probabilities . Suppose treat probabilistic lexicalized CFG like really big CFG just happened lots complex non-terminals estimate probabilities rule maximum likelihood estimates . Thus , according Eq . 14.17 , MLE estimate probability rule P ( VP ( dumped , VBD ) → VBD ( dumped , VBD ) NP ( sacks , NNS ) PP ( , P ) ) Count ( VP ( dumped , VBD ) → VBD ( dumped , VBD ) NP ( sacks , NNS ) PP ( , P ) ) Count ( VP ( dumped , VBD ) ) ( 14.22 ) there’s way get good estimates counts like ( 14.22 ) specific : unlikely many ( even ) instances sentence verb phrase headed dumped NP argument headed sacks PP argument headed . words , counts fully lexicalized PCFG rules like far sparse , rule probabilities come 0 . idea lexicalized parsing make further independence assump - tions break down rule estimate probability P ( VP ( dumped , VBD ) → VBD ( dumped , VBD ) NP ( sacks , NNS ) PP ( , P ) ) product smaller independent probability estimates ac - quire reasonable counts . next section summarizes method , Collins parsing method . 14.6.1 Collins Parser Statistical parsers differ exactly independence assumptions make . look assumptions simplified version Collins parser . first intuition Collins parser think right-hand side every ( internal ) CFG rule consisting head non-terminal , together non-terminals left head non-terminals right head . abstract , think rules follows : LHS → Ln Ln − 1 . . . L1 H R1 . . . Rn − 1 Rn ( 14.23 ) lexicalized grammar , symbols like L1 R3 H LHS actually complex symbol representing category head head tag , like VP ( dumped , VP ) NP ( sacks , NNS ) . , computing single MLE probability rule , going break down rule via neat generative story , slight simplification called Collins Model 1 . new generative story left-hand side , first generate head rule generate dependents head , , inside . steps own probability . add special STOP non-terminal left right edges rule ; non-terminal allows model know stop generating dependents side . generate dependents left side head generated STOP left side head , point move right side head start generating dependents generate STOP . 16 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING generating rule augmented follows : P ( VP ( dumped , VBD ) → ( 14.24 ) STOP VBD ( dumped , VBD ) NP ( sacks , NNS ) PP ( , P ) STOP ) generative story augmented rule . make three kinds probabilities : PH generating heads , PL generating dependents left , PR generating dependents right . 1 . Generate head VBD ( dumped , VBD ) probability P ( H | LHS ) = P ( VBD ( dumped , VBD ) | VP ( dumped , VBD ) ) VP ( dumped , VBD ) VBD ( dumped , VBD ) 2 . Generate left dependent ( STOP , ) probability P ( STOP | VP ( dumped , VBD ) VBD ( dumped , VBD ) ) VP ( dumped , VBD ) VBD ( dumped , VBD ) STOP 3 . Generate right dependent NP ( sacks , NNS ) probability Pr ( NP ( sacks , NNS | VP ( dumped , VBD ) , VBD ( dumped , VBD ) ) VP ( dumped , VBD ) NP ( sacks , NNS ) VBD ( dumped , VBD ) STOP 4 . Generate right dependent PP ( , P ) probability Pr ( PP ( , P ) | VP ( dumped , VBD ) , VBD ( dumped , VBD ) ) VP ( dumped , VBD ) PP ( , P ) NP ( sacks , NNS ) VBD ( dumped , VBD ) STOP 5 ) Generate right dependent STOP probability Pr ( STOP | VP ( dumped , VBD ) , VBD ( dumped , VBD ) ) VP ( dumped , VBD ) STOPPP ( , P ) NP ( sacks , NNS ) VBD ( dumped , VBD ) STOP summary , probability rule P ( VP ( dumped , VBD ) → ( 14.25 ) VBD ( dumped , VBD ) NP ( sacks , NNS ) PP ( , P ) ) estimated PH ( VBD | VP , dumped ) × PL ( STOP | VP , VBD , dumped ) ( 14.26 ) × PR ( NP ( sacks , NNS ) | VP , VBD , dumped ) × PR ( PP ( , P ) | VP , VBD , dumped ) × PR ( STOP | VP , VBD , dumped ) probabilities estimated smaller amounts data full probability ( 14.25 ) . example , maximum likelihood estimate component probability PR ( NP ( sacks , NNS ) | VP , VBD , dumped ) Count ( VP ( dumped , VBD ) NNS ( sacks ) daughter somewhere right ) Count ( VP ( dumped , VBD ) ) ( 14.27 ) 14.7 • PROBABILISTIC CCG PARSING 17 counts less subject sparsity problems complex counts like ( 14.25 ) . generally , H head head word hw head tag ht , lw / lt rw / rt word / tag left right respectively , P parent , probability entire rule expressed follows : 1 . Generate head phrase H ( hw , ht ) probability : PH ( H ( hw , ht ) | P , hw , ht ) 2 . Generate modifiers left head total probability n + 1 ∏ = 1 PL ( Li ( lwi , lti ) | P , H , hw , ht ) Ln + 1 ( lwn + 1 , ltn + 1 ) = STOP , stop generating once gen - erated STOP token . 3 . Generate modifiers right head total probability : n + 1 ∏ = 1 PR ( Ri ( rwi , rti ) | P , H , hw , ht ) Rn + 1 ( rwn + 1 , rtn + 1 ) = STOP , stop generating once gen - erated STOP token . parsing algorithm Collins model extension probabilistic CKY . Extending CKY algorithm handle basic lexicalized probabilities left Exercises 14.5 14.6 reader . 14.7 Probabilistic CCG Parsing Lexicalized grammar frameworks CCG pose problems phrase - based methods discussing particularly well-suited . quickly review , CCG consists three major parts : set categories , lexicon asso - ciates words categories , set rules govern categories combine context . Categories atomic elements , S NP , functions ( S \ NP ) / NP specifies transitive verb category . Rules specify functions , arguments , functions combine . example , following rule templates , forward backward function application , specify way functions apply arguments . X / Y Y ⇒ X Y X \ Y ⇒ X first rule applies function argument right , second looks left argument . result applying rules category specified value function applied . purposes discussion , rely two rules forward backward composition rules type-raising , described Chapter 12 . 18 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING 14.7.1 Ambiguity CCG always case parsing , managing ambiguity key successful CCG parsing . difficulties CCG parsing arise ambiguity caused large number complex lexical categories combined general nature grammatical rules . ways ambiguity arises categorial framework , consider following example . ( 14.28 ) United diverted flight Reno . grasp role flight example depends prepo - sitional phrase Reno taken modifier flight , modifier entire verb phrase , potential second argument verb divert . context-free grammar approach , ambiguity manifest itself choice among fol - lowing rules grammar . Nominal → Nominal PP VP → VP PP VP → Verb NP PP phrase-structure approach simply assign word cate - gory P allowing combine Reno form prepositional phrase . sub - sequent choice grammar rules dictate ultimate derivation . categorial approach , associate distinct categories reflect ways might interact elements sentence . fairly abstract combinatoric rules sort derivations possible . , source ambiguity arises grammar rather lexicon . works considering several possible derivations example . capture case prepositional phrase Reno modifies flight , assign preposition category ( NP \ NP ) / NP , gives rise following derivation . United diverted flight Reno NP ( S \ NP ) / NP NP / N N ( NP \ NP ) / NP NP > > NP NP \ NP < NP > S \ NP < S , category assigned expects find two arguments : right traditional preposition , left corresponds NP modified . Alternatively , assign category ( S \ S ) / NP , permits following derivation Reno modifies preceding verb phrase . United diverted flight Reno NP ( S \ NP ) / NP NP / N N ( S \ S ) / NP NP > > NP S \ S > S \ NP < B S \ NP < S 14.7 • PROBABILISTIC CCG PARSING 19 third possibility view divert ditransitive verb assigning category ( ( S \ NP ) / PP ) / NP , treating Reno simple prepositional phrase . United diverted flight Reno NP ( ( S \ NP ) / PP ) / NP NP / N N PP / NP NP > > NP PP > ( S \ NP ) / PP > S \ NP < S CCG parsers still subject ambiguity arising choice grammar rules , including kind spurious ambiguity discussed Chapter 12 , clear choice lexical categories primary problem addressed CCG parsing . 14.7.2 CCG Parsing Frameworks rules combinatory grammars binary unary , bottom-up , tabular approach based CKY algorithm directly applicable CCG parsing . Recall Fig . 14.3 PCKY employs table records location , category probability valid constituents discovered input . appropriate probability model CCG derivations , same kind approach work CCG parsing . Unfortunately , large number lexical categories available word , combined promiscuity CCG’s combinatoric rules , leads explosion number ( mostly useless ) constituents added parsing table . key managing explosion zombie constituents accurately assess ex - ploit likely lexical categories possible word — process called supertagging . following sections describe two approaches CCG parsing make supertags . Section 14.7.4 , presents approach structures parsing process heuristic search * algorithm . following section briefly describes traditional maximum entropy approach manages search space complexity adaptive supertagging — process iteratively considers tags parse found . 14.7.3 Supertagging Chapter 8 introduced task part-of-speech tagging , process assigning correct lexical category word sentence . Supertagging correspond-supertagging ing task highly lexicalized grammar frameworks , assigned tags often dictate derivation sentence . CCG supertaggers rely treebanks CCGbank provide - set lexical categories well allowable category assignments word lexicon . CCGbank includes 1000 lexical categories , , practice , supertaggers limit tagsets tags occur least 10 times training corpus . results overall total around 425 lexical categories available lexicon . Note even smaller number large contrast 45 POS types Penn Treebank tagset . 20 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING traditional part-of-speech tagging , standard approach building CCG supertagger supervised machine learning build sequence classi - fier labeled training data . common approach maximum entropy Markov model ( MEMM ) , described Chapter 8 , find likely sequence tags sentence . features model consist current word wi , surrounding words l words wi + li − l , well k previously assigned supertags t − 1i − k . type model summarized following equation Chapter 8 . Training maximizing log-likelihood training corpus decod - ing via Viterbi algorithm same described Chapter 8 . T̂ = argmax T P ( T | W ) = argmax T ∏ P ( ti | wi + li − l , t − 1 − k ) = argmax T ∏ exp ( ∑ wi fi ( ti , wi + li − l , t − 1 − k ) ) ∑ t ′ ∈ tagset exp ( ∑ wi fi ( t ′ , wi + li − l , t − 1 − k ) ) ( 14.29 ) Word tag-based features k l set 2 provides reasonable results sufficient training data . Additional features POS tags short char - acter suffixes commonly improve performance . Unfortunately , even additional features large number possible su - pertags combined high per-word ambiguity leads error rates high practical parser . specifically , single best tag sequence T̂ typically contain many incorrect tags effective parsing take place . overcome , return probability distribution possible supertags word input . following table illustrates example dis - tribution simple example sentence . table , column represents probability supertag word context input sentence . “ . . . ” represent remaining supertags possible word . United serves Denver N / N : 0.4 ( S \ NP ) / NP : 0.8 NP : 0.9 NP : 0.3 N : 0.1 N / N : 0.05 S / S : 0.1 . . . . . . S \ S : . 05 . . . MEMM framework , probability optimal tag sequence defined Eq . 14.29 efficiently computed suitably modified version Viterbi algorithm . , Viterbi finds single best tag sequence provide exactly need ; need know probability pos - sible word / tag pair . probability tag word sum probabilities supertag sequences contain tag location . table representing values computed efficiently version forward-backward algorithm HMMs . same result achieved recurrent neural network ( RNN ) sequence models , advantage embeddings represent inputs allow representations span entire sentence , opposed size-limited sliding 14.7 • PROBABILISTIC CCG PARSING 21 windows . RNN approaches avoid high-level features , part speech tags , helpful errors tag assignment propagate errors supertags . forward-backward algorithm , RNN-based methods provide probability distribution lexical categories word input . 14.7.4 CCG Parsing * Algorithm * algorithm heuristic search method employs agenda find optimal solution . Search states representing partial solutions added agenda based cost function , least-cost option selected further ex - ploration iteration . state representing complete solution first selected agenda , guaranteed optimal search terminates . * cost function , f ( n ) , efficiently guide search solution . f - cost two components : g ( n ) , exact cost partial solution repre - sented state n , h ( n ) heuristic approximation cost solution makes n . h ( n ) satisfies criteria overestimating actual cost , * find optimal solution . surprisingly , closer heuristic get actual cost , effective * finding solution explore significant portion solution space . applied parsing , search states correspond edges representing com - pleted constituents . PCKY algorithm , edges specify constituent’s start end positions , grammatical category , f - cost . , g component represents current cost edge h component represents estimate cost complete derivation makes edge . * phrase structure parsing originated ( Klein Manning , 2003a ) , CCG approach presented based ( Lewis Steedman , 2014 ) . information supertagger , agenda parse table initial - ized states representing possible lexical categories word input , f - costs . main loop removes lowest cost edge agenda tests complete derivation . reflects complete derivation selected best solution loop terminates . Otherwise , new states based applicable CCG rules generated , assigned costs , entered agenda await further processing . loop continues complete derivation discovered , agenda exhausted , indicating failed parse . algorithm Fig . 14.11 . Heuristic Functions define heuristic function * search , need decide assess quality CCG derivations . generic PCFG model , defined probability tree product probability rules made up tree . CCG’s lexical nature , make simplifying assumption probability CCG derivation just product probability supertags assigned words derivation , ignoring rules derivation . formally , sentence S derivation D contains supertag sequence T , : P ( D , S ) = P ( T , S ) ( 14.30 ) = n ∏ = 1 P ( ti | si ) ( 14.31 ) 22 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING function CCG-ASTAR-PARSE ( words ) returns table failure supertags ← SUPERTAGGER ( words ) ← 1 LENGTH ( words ) { | ( words [ ] , , score ) ∈ supertags } edge ← MAKEEDGE ( − 1 , , , score ) table ← INSERTEDGE ( table , edge ) agenda ← INSERTEDGE ( agenda , edge ) loop EMPTY ? ( agenda ) return failure current ← POP ( agenda ) COMPLETEDPARSE ? ( current ) return table table ← INSERTEDGE ( chart , edge ) rule APPLICABLERULES ( edge ) successor ← APPLY ( rule , edge ) successor ∈ agenda chart agenda ← INSERTEDGE ( agenda , successor ) else successor ∈ agenda higher cost agenda ← REPLACEEDGE ( agenda , successor ) Figure 14.11 * - based CCG parsing . better fit traditional * approach , prefer states scored cost function lower better ( i.e . , trying minimize cost derivation ) . achieve , negative log probabilities score deriva - tions ; results following equation , score completed CCG derivations . P ( D , S ) = P ( T , S ) ( 14.32 ) = n ∑ = 1 − logP ( ti | si ) ( 14.33 ) model , define f - cost follows . f - cost edge sum two components : g ( n ) , cost span represented edge , h ( n ) , estimate cost complete derivation containing edge ( often referred inside outside costs ) . define g ( n ) edge Equation 14.33 . , just sum costs supertags comprise span . h ( n ) , need score approximates never overestimates actual cost final derivation . simple heuristic meets requirement assumes words outside span assigned probable su - pertag . tags final derivation , score equal heuristic . tags final derivation f - cost higher new tags higher costs , thus guaranteeing overestimate . Putting together , arrive following definition suitable f - cost 14.8 • EVALUATING PARSERS 23 edge . f ( wi , j , ti , j ) = g ( wi , j ) + h ( wi , j ) ( 14.34 ) = j ∑ k = − logP ( tk | wk ) + − 1 ∑ k = 1 min t ∈ tags ( − logP ( t | wk ) ) + N ∑ k = j + 1 min t ∈ tags ( − logP ( t | wk ) ) example , consider edge representing word serves supertag N following example . ( 14.35 ) United serves Denver . g-cost edge just negative log probability tag , − log10 ( 0.1 ) , 1 . outside h-cost consists optimistic supertag assignments United Denver , N / N NP respectively . resulting f - cost edge 1.443 . Example Fig . 14.12 shows initial agenda progress complete parse example . initializing agenda parse table information supertagger , selects best edge agenda — entry United tag N / N f - cost 0.591 . edge constitute complete parse generate new states applying relevant grammar rules . case , applying forward application United : N / N serves : N results creation edge United serves : N [ 0,2 ] , 1.795 agenda . Skipping ahead , third iteration edge representing complete deriva - tion United serves Denver , S [ 0,3 ] , . 716 added agenda . , algo - rithm terminate point cost edge ( . 716 ) place top agenda . , edge representing Denver category NP popped . leads addition another edge agenda ( type-raising Denver ) . edge popped dealt earlier state repre - senting complete derivation rise top agenda popped , goal tested , returned solution . effectiveness * approach reflected coloring states Fig . 14.12 well final parsing table . edges shown blue ( includ - ing initial lexical category assignments explicitly shown ) reflect states search space never made top agenda , , never contributed edges final table . contrast PCKY approach parser systematically fills parse table possible constituents possible spans input , filling table myriad constituents contribute final analysis . 14.8 Evaluating Parsers standard techniques evaluating parsers grammars called PAR - SEVAL measures ; proposed Black et al . ( 1991 ) based same ideas signal-detection theory saw earlier chapters . 24 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING United serves : N [ 0,2 ] 1.795 United : N / N . 591 Denver : N / N 2.494 Denver : N 1.795 serves : N 1.494 United : S \ S 1.494 United : S / S 1.1938 United : NP . 716 Denver : NP . 591 serves : ( S \ NP ) / NP . 591 serves Denver : S \ NP [ 1,3 ] . 591 United serves Denver : S [ 0,3 ] . 716 Denver : S / ( S \ NP ) [ 0,1 ] . 591 1 2 3 4 5 6 Initial Agenda Goal State … S : 0.716 S / NP : 0.591 United serves [ 0,1 ] [ 0,2 ] [ 0,3 ] [ 1,2 ] [ 1,3 ] [ 2,3 ] N / N : 0.591 NP : 0.716 S / S : 1.1938 S \ S : 1.494 … Denver ( S \ NP ) / NP : 0.591 N : 1.494 … NP : 0.591 N : 1.795 N / N : 2.494 … N : 1.795 Figure 14.12 Example * search example “ United serves Denver ” . circled numbers blue boxes indicate order states popped agenda . costs state based f-costs negative log10 probabilities . intuition PARSEVAL metric measure constituents hypothesis parse tree look like constituents hand-labeled , gold-reference parse . PARSEVAL thus assumes human-labeled “ gold standard ” parse tree sentence test set ; generally draw gold-standard parses treebank like Penn Treebank . gold-standard reference parses test set , constituent hypothesis parse Ch sentence s labeled “ correct ” constituent reference parse Cr same starting point , ending point , non-terminal symbol . measure precision recall just chunking previous chapter . 14.9 • SUMMARY 25 labeled recall : = # correct constituents hypothesis parse s # correct constituents reference parse s labeled precision : = # correct constituents hypothesis parse s # total constituents hypothesis parse s precision recall , often report combination two , F-measure ( van Rijsbergen , 1975 ) , , saw Chapter 4 , isF-measure defined : Fβ = ( β 2 + 1 ) PR β 2P + R Values β > 1 favor recall values β < 1 favor precision . β = 1 , precision recall equally balanced ; called Fβ = 1 just F1 : F1 = 2PR P + R ( 14.36 ) additionally new metric , crossing brackets , sentence s : cross-brackets : number constituents reference parse bracketing ( ( B ) C ) hypothesis parse bracketing ( ( B C ) ) . comparing parsers different grammars , PARSEVAL metric - cludes canonicalization algorithm removing information likely grammar - specific ( auxiliaries , pre-infinitival “ ” , etc . ) computing simplified score ( Black et al . , 1991 ) . canonical implementation PARSEVAL metrics called evalb ( Sekine Collins , 1997 ) . evalb Nonetheless , phrasal constituents always appropriate unit parser evaluation . lexically-oriented grammars , CCG LFG , ultimate goal extract appropriate predicate-argument relations grammatical dependen - cies , rather specific derivation . relations directly relevant further semantic processing . purposes , alternative evalua - tion metrics based precision recall labeled dependencies whose labels indicate grammatical relations ( Lin 1995 , Carroll et al . 1998 , Collins et al . 1999 ) . Finally , might wonder why evaluate parsers measuring many sentences parsed correctly measuring component accuracy form constituents dependencies . reason components gives fine-grained metric . especially true long sentences , parsers get perfect parse . just measured sentence accuracy , able distinguish parse got parts wrong just got part wrong . 14.9 Summary chapter sketched basics probabilistic parsing , concentrating probabilistic context-free grammars probabilistic lexicalized context-free grammars . • Probabilistic grammars assign probability sentence string words attempting capture sophisticated syntactic information N-gram grammars Chapter 3 . 26 CHAPTER 14 • STATISTICAL CONSTITUENCY PARSING • probabilistic context-free grammar ( PCFG ) context-free grammar every rule annotated probability rule chosen . PCFG rule treated conditionally inde - pendent ; thus , probability sentence computed multiplying probabilities rule parse sentence . • probabilistic CKY ( Cocke-Kasami-Younger ) algorithm probabilistic version CKY parsing algorithm . probabilistic versions parsers like Earley algorithm . • PCFG probabilities learned counting parsed corpus pars - ing corpus . inside-outside algorithm way dealing fact sentences parsed ambiguous . • Raw PCFGs suffer poor independence assumptions among rules lack sensitivity lexical dependencies . • way deal problem split merge non-terminals ( auto - matically hand ) . • Probabilistic lexicalized CFGs another solution problem basic PCFG model augmented lexical head rule . probability rule conditioned lexical head nearby heads . • Parsers lexicalized PCFGs ( like Charniak Collins parsers ) based extensions probabilistic CKY parsing . • Parsers evaluated three metrics : labeled recall , labeled precision , cross-brackets . Bibliographical Historical Notes Many formal properties probabilistic context-free grammars first worked Booth ( 1969 ) Salomaa ( 1969 ) . Baker ( 1979 ) proposed inside - outside algorithm unsupervised training PCFG probabilities , CKY - style parsing algorithm compute inside probabilities . Jelinek Lafferty ( 1991 ) extended CKY algorithm compute probabilities prefixes . Stolcke ( 1995 ) adapted Earley algorithm PCFGs . number researchers starting early 1990s worked adding lexical de - pendencies PCFGs making PCFG rule probabilities sensitive sur - rounding syntactic structure . example , Schabes et al . ( 1988 ) Schabes ( 1990 ) presented early work heads . Many papers lexical depen - dencies first presented DARPA Speech Natural Language Workshop June 1990 . paper Hindle Rooth ( 1990 ) applied lexical dependencies problem attaching prepositional phrases ; question session later paper , Ken Church suggested applying method full parsing ( Marcus , 1990 ) . Early work probabilistic CFG parsing augmented probabilistic depen - dency information includes Magerman Marcus ( 1991 ) , Black et al . ( 1992 ) , Bod ( 1993 ) , Jelinek et al . ( 1994 ) , addition Collins ( 1996 ) , Charniak ( 1997 ) , Collins ( 1999 ) discussed . recent PCFG parsing models include Klein Manning ( 2003a ) Petrov et al . ( 2006 ) . early lexical probabilistic work led initially work focused solving specific parsing problems like preposition-phrase attachment methods - EXERCISES 27 cluding transformation-based learning ( TBL ) ( Brill Resnik , 1994 ) , maximum entropy ( Ratnaparkhi et al . , 1994 ) , memory-based learning ( Zavrel Daelemans , 1997 ) , log-linear models ( Franz , 1997 ) , decision trees semantic distance heads ( computed WordNet ) ( Stetina Nagao , 1997 ) , boosting ( Abney et al . , 1999 ) . Another direction extended lexical probabilistic parsing work build probabilistic formulations grammars PCFGs , probabilistic TAG grammar ( Resnik 1992 , Schabes 1992 ) , based TAG gram - mars discussed Chapter 12 , probabilistic LR parsing ( Briscoe Carroll , 1993 ) , probabilistic link grammar ( Lafferty et al . , 1992 ) . supertagging approach saw CCG developed TAG grammars ( Bangalore Joshi 1999 , Joshi Srinivas 1994 ) , based lexicalized TAG grammars Schabes et al . ( 1988 ) . Exercises 14.1 Implement CKY algorithm . 14.2 Modify algorithm conversion CNF Chapter 13 correctly handle rule probabilities . Make sure resulting CNF assigns same total probability parse tree . 14.3 Recall Exercise 13.3 asked update CKY algorithm han - dle unit productions directly rather converting CNF . Extend change probabilistic CKY . 14.4 Fill rest probabilistic CKY chart Fig . 14.4 . 14.5 Sketch CKY algorithm augmented handle lexi - calized probabilities . 14.6 Implement lexicalized extension CKY algorithm . 14.7 Implement PARSEVAL metrics described Section 14.8 . Next , treebank create own hand-checked parsed test set . CFG ( ) parser grammar , parse test set compute labeled recall , labeled precision , cross-brackets . 28 Chapter 14 • Statistical Constituency Parsing Abney , S . P . , Schapire , R . E . , Singer , Y . ( 1999 ) . Boosting applied tagging PP attachment . EMNLP / VLC-99 , 38 – 45 . Baker , J . K . ( 1979 ) . Trainable grammars speech recog - nition . Klatt , D . H . Wolf , J . J . ( Eds . ) , Speech Com - munication Papers 97th Meeting Acoustical Society America , 547 – 550 . Bangalore , S . Joshi , . K . ( 1999 ) . Supertagging : approach almost parsing . Computational Linguistics , 25 ( 2 ) , 237 – 265 . Black , E . , Abney , S . P . , Flickinger , D . , Gdaniec , C . , Grish - man , R . , Harrison , P . , Hindle , D . , Ingria , R . , Jelinek , F . , Klavans , J . L . , Liberman , M . Y . , Marcus , M . P . , Roukos , S . , Santorini , B . , Strzalkowski , T . ( 1991 ) . procedure quantitatively comparing syntactic coverage En - glish grammars . Proceedings DARPA Speech Natu - ral Language Workshop , 306 – 311 . Black , E . , Jelinek , F . , Lafferty , J . D . , Magerman , D . M . , Mercer , R . L . , Roukos , S . ( 1992 ) . history - based grammars : richer models probabilistic parsing . Proceedings DARPA Speech Natural Lan - guage Workshop , 134 – 139 . Bod , R . ( 1993 ) . annotated corpus stochastic grammar . EACL-93 , 37 – 44 . Booth , T . L . ( 1969 ) . Probabilistic representation formal languages . IEEE Conference Record 1969 Tenth Annual Symposium Switching Automata Theory , 74 – 81 . Booth , T . L . Thompson , R . . ( 1973 ) . Applying prob - ability measures abstract languages . IEEE Transactions Computers , C-22 ( 5 ) , 442 – 450 . Bresnan , J . ( Ed . ) . ( 1982 ) . Mental Representation Grammatical Relations . MIT Press . Brill , E . Resnik , P . ( 1994 ) . rule-based approach prepositional phrase attachment disambiguation . COLING-94 , 1198 – 1204 . Briscoe , T . Carroll , J . ( 1993 ) . Generalized probabilistic LR parsing natural language ( corpora ) unification - based grammars . Computational Linguistics , 19 ( 1 ) , 25 – 59 . Carroll , J . , Briscoe , T . , Sanfilippo , . ( 1998 ) . Parser evaluation : survey new proposal . LREC-98 , 447 – 454 . Charniak , E . ( 1997 ) . Statistical parsing context-free grammar word statistics . AAAI-97 , 598 – 603 . Chelba , C . Jelinek , F . ( 2000 ) . Structured language mod - eling . Computer Speech Language , 14 , 283 – 332 . Collins , M . ( 1996 ) . new statistical parser based bigram lexical dependencies . ACL-96 , 184 – 191 . Collins , M . ( 1999 ) . Head-Driven Statistical Models Nat - ural Language Parsing . Ph . D . thesis , University Penn - sylvania , Philadelphia . Collins , M . , Hajič , J . , Ramshaw , L . . , Tillmann , C . ( 1999 ) . statistical parser Czech . ACL-99 , 505 – 512 . Francis , H . S . , Gregory , M . L . , Michaelis , L . . ( 1999 ) . lexical subjects deviant ? . CLS-99 . University Chicago . Franz , . ( 1997 ) . Independence assumptions considered harmful . ACL / EACL-97 , 182 – 189 . Givón , T . ( 1990 ) . Syntax : Functional Typological Intro - duction . John Benjamins . Hindle , D . Rooth , M . ( 1990 ) . Structural ambiguity lexical relations . Proceedings DARPA Speech Nat - ural Language Workshop , 257 – 262 . Hindle , D . Rooth , M . ( 1991 ) . Structural ambiguity lexical relations . ACL-91 , 229 – 236 . Jelinek , F . Lafferty , J . D . ( 1991 ) . Computation probability initial substring generation stochastic context-free grammars . Computational Linguistics , 17 ( 3 ) , 315 – 323 . Jelinek , F . , Lafferty , J . D . , Magerman , D . M . , Mercer , R . L . , Ratnaparkhi , . , Roukos , S . ( 1994 ) . Decision tree pars - ing hidden derivation model . ARPA Human Lan - guage Technologies Workshop , 272 – 277 . Johnson , M . ( 1998 ) . PCFG models linguistic tree repre - sentations . Computational Linguistics , 24 ( 4 ) , 613 – 632 . Joshi , . K . ( 1985 ) . Tree adjoining grammars : context-sensitivity required provide reasonable struc - tural descriptions ? . Dowty , D . R . , Karttunen , L . , Zwicky , . ( Eds . ) , Natural Language Parsing , 206 – 250 . Cambridge University Press . Joshi , . K . Srinivas , B . ( 1994 ) . Disambiguation super parts speech ( supertags ) : Almost parsing . COLING-94 , 154 – 160 . Klein , D . Manning , C . D . ( 2001 ) . Parsing hyper - graphs . IWPT-01 , 123 – 134 . Klein , D . Manning , C . D . ( 2003a ) . * parsing : Fast exact Viterbi parse selection . HLT-NAACL-03 . Klein , D . Manning , C . D . ( 2003b ) . Accurate unlexical - ized parsing . HLT-NAACL-03 . Lafferty , J . D . , Sleator , D . , Temperley , D . ( 1992 ) . Gram - matical trigrams : probabilistic model link grammar . Proceedings 1992 AAAI Fall Symposium Prob - abilistic Approaches Natural Language . Lari , K . Young , S . J . ( 1990 ) . estimation stochastic context-free grammars Inside-Outside algorithm . Computer Speech Language , 4 , 35 – 56 . Lewis , M . Steedman , M . ( 2014 ) . * ccg parsing supertag-factored model . . EMNLP , 990 – 1000 . Lin , D . ( 1995 ) . dependency-based method evaluating broad-coverage parsers . IJCAI-95 , 1420 – 1425 . Magerman , D . M . Marcus , M . P . ( 1991 ) . Pearl : prob - abilistic chart parser . EACL-91 . Manning , C . D . Schütze , H . ( 1999 ) . Foundations Sta - tistical Natural Language Processing . MIT Press . Marcus , M . P . ( 1990 ) . Summary session 9 : Automatic acquisition linguistic structure . Proceedings DARPA Speech Natural Language Workshop , 249 – 250 . Marcus , M . P . , Santorini , B . , Marcinkiewicz , M . . ( 1993 ) . Building large annotated corpus English : Penn treebank . Computational Linguistics , 19 ( 2 ) , 313 – 330 . Ney , H . ( 1991 ) . Dynamic programming parsing context - free grammars continuous speech recognition . IEEE Transactions Signal Processing , 39 ( 2 ) , 336 – 340 . Petrov , S . , Barrett , L . , Thibaux , R . , Klein , D . ( 2006 ) . Learning accurate , compact , interpretable tree annota - tion . COLING / ACL 2006 , 433 – 440 . Exercises 29 Pollard , C . Sag , . . ( 1994 ) . Head-Driven Phrase Struc - ture Grammar . University Chicago Press . Ratnaparkhi , . , Reynar , J . C . , Roukos , S . ( 1994 ) . maximum entropy model prepositional phrase attach - ment . ARPA Human Language Technologies Workshop , 250 – 255 . Resnik , P . ( 1992 ) . Probabilistic tree-adjoining grammar framework statistical natural language processing . COLING-92 , 418 – 424 . Salomaa , . ( 1969 ) . Probabilistic weighted grammars . Information Control , 15 , 529 – 544 . Schabes , Y . ( 1990 ) . Mathematical Computational - pects Lexicalized Grammars . Ph . D . thesis , University Pennsylvania , Philadelphia , PA . Schabes , Y . ( 1992 ) . Stochastic lexicalized tree-adjoining grammars . COLING-92 , 426 – 433 . Schabes , Y . , Abeillé , . , Joshi , . K . ( 1988 ) . Pars - ing strategies ‘ lexicalized ’ grammars : Applications Tree Adjoining Grammars . COLING-88 , 578 – 583 . Sekine , S . Collins , M . ( 1997 ) . evalb software . http://cs.nyu.edu/cs/projects/ proteus / evalb . Stetina , J . Nagao , M . ( 1997 ) . Corpus based PP attach - ment ambiguity resolution semantic dictionary . Zhou , J . Church , K . W . ( Eds . ) , Proceedings Fifth Workshop Large Corpora , 66 – 80 . Stolcke , . ( 1995 ) . efficient probabilistic context-free parsing algorithm computes prefix probabilities . Com - putational Linguistics , 21 ( 2 ) , 165 – 202 . van Rijsbergen , C . J . ( 1975 ) . Information Retrieval . Butter - worths . Zavrel , J . Daelemans , W . ( 1997 ) . Memory-based learn - ing : similarity smoothing . ACL / EACL-97 , 436 – 443 .