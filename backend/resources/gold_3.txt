5 . Word Embeddings A main component of the neural-network approach is the use of embeddings – representing each feature as a vector in a low dimensional space . But where do the vectors come from ? This section will survey the common approaches . 5.1 Random Initialization When enough supervised training data is available , one can just treat the feature embeddings the same as the other model parameters : initialize the embedding vectors to random values , and let the network-training procedure tune them into “ good ” vectors . Some care has to be taken in the way the random initialization is performed . The method used by the effective word2vec implementation ( Mikolov et al . , 2013 ; Mikolov , Sutskever , Chen , Corrado , & Dean , 2013 ) is to initialize the word vectors to uniformly sampled random numbers in the range [ − 12d , 12d ] where d is the number of dimensions . Another option is to use xavier initialization ( see Section 6.3 ) and initialize with uniformly sampled values from [ − √ 6 √ d , √ 6 √ d ] . In practice , one will often use the random initialization approach to initialize the em - bedding vectors of commonly occurring features , such as part-of-speech tags or individual letters , while using some form of supervised or unsupervised pre-training to initialize the potentially rare features , such as features for individual words . The pre-trained vectors can then either be treated as fixed during the network training process , or , more commonly , treated like the randomly-initialized vectors and further tuned to the task at hand . 5.2 Supervised Task-specific Pre-training If we are interested in task A , for which we only have a limited amount of labeled data ( for example , syntactic parsing ) , but there is an auxiliary task B ( say , part-of-speech tagging ) for which we have much more labeled data , we may want to pre-train our word vectors so that they perform well as predictors for task B , and then use the trained vectors for training task A . In this way , we can utilize the larger amounts of labeled data we have for task B . When training task A we can either treat the pre-trained vectors as fixed , or tune them further for task A . Another option is to train jointly for both objectives , see Section 7 for more details . 5.3 Unsupervised Pre-training The common case is that we do not have an auxiliary task with large enough amounts of annotated data ( or maybe we want to help bootstrap the auxiliary task training with better vectors ) . In such cases , we resort to “ unsupervised ” methods , which can be trained on huge amounts of unannotated text . The techniques for training the word vectors are essentially those of supervised learning , but instead of supervision for the task that we care about , we instead create practically 20 unlimited number of supervised training instances from raw text , hoping that the tasks that we created will match ( or be close enough to ) the final task we care about . 12 The key idea behind the unsupervised approaches is that one would like the embedding vectors of “ similar ” words to have similar vectors . While word similarity is hard to define and is usually very task-dependent , the current approaches derive from the distributional hypothesis ( Harris , 1954 ) , stating that words are similar if they appear in similar contexts . The different methods all create supervised training instances in which the goal is to either predict the word from its context , or predict the context from the word . An important benefit of training word embeddings on large amounts of unannotated data is that it provides vector representations for words that do not appear in the super - vised training set . Ideally , the representations for these words will be similar to those of related words that do appear in the training set , allowing the model to generalize better on unseen events . It is thus desired that the similarity between word vectors learned by the un - supervised algorithm captures the same aspects of similarity that are useful for performing the intended task of the network . Common unsupervised word-embedding algorithms include word2vec 13 ( Mikolov et al . , 2013 , 2013 ) , GloVe ( Pennington , Socher , & Manning , 2014 ) and the Collobert and Weston ( 2008 , 2011 ) embeddings algorithm . These models are inspired by neural networks and are based on stochastic gradient training . However , they are deeply connected to another family of algorithms which evolved in the NLP and IR communities , and that are based on matrix factorization ( see ( Levy & Goldberg , 2014b ; Levy et al . , 2015 ) for a discussion ) . Arguably , the choice of auxiliary problem ( what is being predicted , based on what kind of context ) affects the resulting vectors much more than the learning method that is being used to train them . We thus focus on the different choices of auxiliary problems that are available , and only skim over the details of the training methods . Several software packages for deriving word vectors are available , including word2vec14 and Gensim15 implementing the word2vec models with word-windows based contexts , word2vecf16 which is a modified version of word2vec allowing the use of arbitrary contexts , and GloVe17 implementing the GloVe model . Many pre-trained word vectors are also available for download on the web . While beyond the scope of this tutorial , it is worth noting that the word embeddings derived by unsupervised training algorithms have a wide range of applications in NLP beyond using them for initializing the word-embeddings layer of a neural-network model . 5.4 Training Objectives Given a word w and its context c , different algorithms formulate different auxiliary tasks . In all cases , each word is represented as a d-dimensional vector which is initialized to a random value . Training the model to perform the auxiliary tasks well will result in good 12 . The interpretation of creating auxiliary problems from raw text is inspired by Ando and Zhang ( Ando & Zhang , 2005a , 2005b ) . 13 . While often treated as a single algorithm , word2vec is actually a software package including various training objectives , optimization methods and other hyperparameters . See ( Rong , 2014 ; Levy , Goldberg , & Dagan , 2015 ) for a discussion . 14 . https://code.google.com/p/word2vec/ 15 . https://radimrehurek.com/gensim/ 16 . https://bitbucket.org/yoavgo/word2vecf 17 . http://nlp.stanford.edu/projects/glove/ 21 word embeddings for relating the words to the contexts , which in turn will result in the embedding vectors for similar words to be similar to each other . Language-modeling inspired approaches such as those taken by ( Mikolov et al . , 2013 ; Mnih & Kavukcuoglu , 2013 ) as well as GloVe ( Pennington et al . , 2014 ) use auxiliary tasks in which the goal is to predict the word given its context . This is posed in a probabilistic setup , trying to model the conditional probability P ( w | c ) . Other approaches reduce the problem to that of binary classification . In addition to the set D of observed word-context pairs , a set D̄ is created from random words and context pairings . The binary classification problem is then : does the given ( w , c ) pair come from D or not ? The approaches differ in how the set D̄ is constructed , what is the structure of the classifier , and what is the objective being optimized . Collobert and Weston ( 2008 , 2011 ) take a margin-based binary ranking approach , training a feed-forward neural network to score correct ( w , c ) pairs over incorrect ones . Mikolov et al ( 2013 , 2014 ) take instead a probabilistic version , training a log-bilinear model to predict the probability P ( ( w , c ) ∈ D | w , c ) that the pair come from the corpus rather than the random sample . 5.5 The Choice of Contexts In most cases , the contexts of a word are taken to be other words that appear in its surrounding , either in a short window around it , or within the same sentence , paragraph or document . In some cases the text is automatically parsed by a syntactic parser , and the contexts are derived from the syntactic neighbourhood induced by the automatic parse trees . Sometimes , the definitions of words and context change to include also parts of words , such as prefixes or suffixes . Neural word embeddings originated from the world of language modeling , in which a network is trained to predict the next word based on a sequence of preceding words ( Bengio et al . , 2003 ) . There , the text is used to create auxiliary tasks in which the aim is to predict a word based on a context the k previous words . While training for the language modeling auxiliary prediction problems indeed produce useful embeddings , this approach is needlessly restricted by the constraints of the language modeling task , in which one is allowed to look only at the previous words . If we do not care about language modeling but only about the resulting embeddings , we may do better by ignoring this constraint and taking the context to be a symmetric window around the focus word . 5.5.1 Window Approach The most common approach is a sliding window approach , in which auxiliary tasks are created by looking at a sequence of 2k + 1 words . The middle word is callled the focus word and the k words to each side are the contexts . Then , either a single task is created in which the goal is to predict the focus word based on all of the context words ( represented either using CBOW ( Mikolov et al . , 2013 ) or vector concatenation ( Collobert & Weston , 2008 ) ) , or 2k distinct tasks are created , each pairing the focus word with a different context word . The 2k tasks approach , popularized by ( Mikolov et al . , 2013 ) is referred to as a skip-gram model . Skip-gram based approaches are shown to be robust and efficient to train ( Mikolov et al . , 2013 ; Pennington et al . , 2014 ) , and often produce state of the art results . 22 Effect of Window Size The size of the sliding window has a strong effect on the re - sulting vector similarities . Larger windows tend to produce more topical similarities ( i.e . “ dog ” , “ bark ” and “ leash ” will be grouped together , as well as “ walked ” , “ run ” and “ walk - ing ” ) , while smaller windows tend to produce more functional and syntactic similarities ( i.e . “ Poodle ” , “ Pitbull ” , “ Rottweiler ” , or “ walking ” , “ running ” , “ approaching ” ) . Positional Windows When using the CBOW or skip-gram context representations , all the different context words within the window are treated equally . There is no distinction between context words that are close to the focus words and those that are farther from it , and likewise there is no distinction between context words that appear before the focus words to context words that appear after it . Such information can easily be factored in by using positional contexts : indicating for each context word also its relative position to the focus words ( i.e . instead of the context word being “ the ” it becomes “ the : + 2 ” , indicating the word appears two positions to the right of the focus word ) . The use of positional context together with smaller windows tend to produce similarities that are more syntactic , with a strong tendency of grouping together words that share a part of speech , as well as being functionally similar in terms of their semantics . Positional vectors were shown by ( Ling , Dyer , Black , & Trancoso , 2015a ) to be more effective than window-based vectors when used to initialize networks for part-of-speech tagging and syntactic dependency parsing . Variants Many variants on the window approach are possible . One may lemmatize words before learning , apply text normalization , filter too short or too long sentences , or remove capitalization ( see , e.g . , the pre-processing steps described in ( dos Santos & Gatti , 2014 ) . One may sub-sample part of the corpus , skipping with some probability the creation of tasks from windows that have too common or too rare focus words . The window size may be dynamic , using a different window size at each turn . One may weigh the different positions in the window differently , focusing more on trying to predict correctly close word-context pairs than further away ones . Each of these choices will effect the resulting vectors . Some of these hyperparameters ( and others ) are discussed in ( Levy et al . , 2015 ) . 5.5.2 Sentences , Paragraphs or Documents Using a skip-grams ( or CBOW ) approach , one can consider the contexts of a word to be all the other words that appear with it in the same sentence , paragraph or document . This is equivalent to using very large window sizes , and is expected to result in word vectors that capture topical similarity ( words from the same topic , i.e . words that one would expect to appear in the same document , are likely to receive similar vectors ) . 5.5.3 Syntactic Window Some work replace the linear context within a sentence with a syntactic one ( Levy & Goldberg , 2014a ; Bansal , Gimpel , & Livescu , 2014 ) . The text is automatically parsed using a dependency parser , and the context of a word is taken to be the words that are in its proximity in the parse tree , together with the syntactic relation by which they are connected . Such approaches produce highly functional similarities , grouping together words than can fill the same role in a sentence ( e.g . colors , names of schools , verbs of movement ) . 23 The grouping is also syntactic , grouping together words that share an inflection ( Levy & Goldberg , 2014a ) . 5.5.4 Multilingual Another option is using multilingual , translation based contexts ( Hermann & Blunsom , 2014 ; Faruqui & Dyer , 2014 ) . For example , given a large amount of sentence-aligned parallel text , one can run a bilingual alignment model such as the IBM model 1 or model 2 ( i.e . using the GIZA + + software ) , and then use the produced alignments to derive word contexts . Here , the context of a word instance are the foreign language words that are aligned to it . Such alignments tend to result in synonym words receiving similar vectors . Some authors work instead on the sentence alignment level , without relying on word alignments . An appealing method is to mix a monolingual window-based approach with a multilingual approach , creating both kinds of auxiliary tasks . This is likely to produce vectors that are similar to the window-based approach , but reducing the somewhat undesired effect of the window-based approach in which antonyms ( e.g . hot and cold , high and low ) tend to receive similar vectors ( Faruqui & Dyer , 2014 ) . 5.5.5 Character-based and Sub-word Representations An interesting line of work attempts to derive the vector representation of a word from the characters that compose it . Such approaches are likely to be particularly useful for tasks which are syntactic in nature , as the character patterns within words are strongly related to their syntactic function . These approaches also have the benefit of producing very small model sizes ( only one vector for each character in the alphabet together with a handful of small matrices needs to be stored ) , and being able to provide an embedding vector for every word that may be encountered . dos Santos and Gatti ( 2014 ) and dos Santos and Zadrozny ( 2014 ) model the embedding of a word using a convolutional network ( see Section 9 ) over the characters . Ling et al ( 2015b ) model the embedding of a word using the concatenation of the final states of two RNN ( LSTM ) encoders ( Section 10 ) , one reading the characters from left to right , and the other from right to left . Both produce very strong results for part-of-speech tagging . The work of Ballesteros et al ( 2015 ) show that the two-LSTMs encoding of ( Ling et al . , 2015b ) is beneficial also for representing words in dependency parsing of morphologically rich languages . Deriving representations of words from the representations of their characters is moti - vated by the unknown words problem – what do you do when you encounter a word for which you do not have an embedding vector ? Working on the level of characters alleviates this problem to a large extent , as the vocabulary of possible characters is much smaller than the vocabulary of possible words . However , working on the character level is very challenging , as the relationship between form ( characters ) and function ( syntax , semantics ) in language is quite loose . Restricting oneself to stay on the character level may be an unnecessarily hard constraint . Some researchers propose a middle-ground , in which a word is represented as a combination of a vector for the word itself with vectors of sub-word units that comprise it . The sub-word embeddings then help in sharing information between different words with similar forms , as well as allowing back-off to the subword level when the word is not observed . At the same time , the models are not forced to rely solely on 24 form when enough observations of the word are available . Botha and Blunsom ( 2014 ) sug - gest to model the embedding vector of a word as a sum of the word-specific vector if such vector is available , with vectors for the different morphological components that comprise it ( the components are derived using Morfessor ( Creutz & Lagus , 2007 ) , an unsupervised morphological segmentation method ) . Gao et al ( Gao et al . , 2014 ) suggest using as core features not only the word form itself but also a unique feature ( hence a unique embedding vector ) for each of the letter-trigrams in the word . 25