3 . Feature Representation Before discussing the network structure in more depth , it is important to pay attention to how features are represented . For now , we can think of a feed-forward neural network as a function NN ( x ) that takes as input a din dimensional vector x and produces a dout dimensional output vector . The function is often used as a classifier , assigning the input x a degree of membership in one or more of dout classes . The function can be complex , and is almost always non-linear . Common structures of this function will be discussed in Section 4 . Here , we focus on the input , x . When dealing with natural language , the input x encodes features such as words , part-of-speech tags or other linguistic information . Perhaps the biggest jump when moving from sparse-input linear models to neural-network based models is to stop representing each feature as a unique dimension ( the so called one-hot representation ) and representing them instead as dense vectors . That is , each core feature is embedded into a d dimensional space , and represented as a vector in that space . 1 The embeddings ( the vector representation of each core feature ) can then be trained like the other parameter of the function NN . Figure 1 shows the two approaches to feature representation . The feature embeddings ( the values of the vector entries for each feature ) are treated as model parameters that need to be trained together with the other components of the network . Methods of training ( or obtaining ) the feature embeddings will be discussed later . For now , consider the feature embeddings as given . The general structure for an NLP classification system based on a feed-forward neural network is thus : 1 . Extract a set of core linguistic features f1 , . . . , fk that are relevant for predicting the output class . 2 . For each feature fi of interest , retrieve the corresponding vector v ( fi ) . 3 . Combine the vectors ( either by concatenation , summation or a combination of both ) into an input vector x . 4 . Feed x into a non-linear classifier ( feed-forward neural network ) . The biggest change in the input , then , is the move from sparse representations in which each feature is its own dimension , to a dense representation in which each feature is mapped to a vector . Another difference is that we extract only core features and not feature com - binations . We will elaborate on both these changes briefly . Dense Vectors vs . One-hot Representations What are the benefits of representing our features as vectors instead of as unique IDs ? Should we always represent features as dense vectors ? Let’s consider the two kinds of representations : One Hot Each feature is its own dimension . • Dimensionality of one-hot vector is same as number of distinct features . 1 . Different feature types may be embedded into different spaces . For example , one may represent word features using 100 dimensions , and part-of-speech features using 20 dimensions . 5 Figure 1 : Sparse vs . dense feature representations . Two encodings of the informa - tion : current word is “ dog ” ; previous word is “ the ” ; previous pos-tag is “ DET ” . ( a ) Sparse feature vector . Each dimension represents a feature . Feature combi - nations receive their own dimensions . Feature values are binary . Dimensionality is very high . ( b ) Dense , embeddings-based feature vector . Each core feature is represented as a vector . Each feature corresponds to several input vector en - tries . No explicit encoding of feature combinations . Dimensionality is low . The feature-to-vector mappings come from an embedding table . • Features are completely independent from one another . The feature “ word is ‘ dog ’ ” is as dis-similar to “ word is ‘ thinking ’ ” than it is to “ word is ‘ cat ’ ” . Dense Each feature is a d-dimensional vector . • Dimensionality of vector is d . • Similar features will have similar vectors – information is shared between similar features . One benefit of using dense and low-dimensional vectors is computational : the majority of neural network toolkits do not play well with very high-dimensional , sparse vectors . However , this is just a technical obstacle , which can be resolved with some engineering effort . The main benefit of the dense representations is in generalization power : if we believe some features may provide similar clues , it is worthwhile to provide a representation that is able to capture these similarities . For example , assume we have observed the word ‘ dog ’ many times during training , but only observed the word ‘ cat ’ a handful of times , or not at 6 all . If each of the words is associated with its own dimension , occurrences of ‘ dog ’ will not tell us anything about the occurrences of ‘ cat ’ . However , in the dense vectors representation the learned vector for ‘ dog ’ may be similar to the learned vector from ‘ cat ’ , allowing the model to share statistical strength between the two events . This argument assumes that “ good ” vectors are somehow given to us . Section 5 describes ways of obtaining such vector representations . In cases where we have relatively few distinct features in the category , and we believe there are no correlations between the different features , we may use the one-hot representa - tion . However , if we believe there are going to be correlations between the different features in the group ( for example , for part-of-speech tags , we may believe that the different verb inflections VB and VBZ may behave similarly as far as our task is concerned ) it may be worthwhile to let the network figure out the correlations and gain some statistical strength by sharing the parameters . It may be the case that under some circumstances , when the feature space is relatively small and the training data is plentiful , or when we do not wish to share statistical information between distinct words , there are gains to be made from using the one-hot representations . However , this is still an open research question , and there are no strong evidence to either side . The majority of work ( pioneered by ( Collobert & Weston , 2008 ; Collobert et al . , 2011 ; Chen & Manning , 2014 ) ) advocate the use of dense , trainable embedding vectors for all features . For work using neural network architecture with sparse vector encodings see ( Johnson & Zhang , 2015 ) . Finally , it is important to note that representing features as dense vectors is an integral part of the neural network framework , and that consequentially the differences between using sparse and dense feature representations are subtler than they may appear at first . In fact , using sparse , one-hot vectors as input when training a neural network amounts to dedicating the first layer of the network to learning a dense embedding vector for each feature based on the training data . We touch on this in Section 4.4 . Variable Number of Features : Continuous Bag of Words Feed-forward networks assume a fixed dimensional input . This can easily accommodate the case of a feature - extraction function that extracts a fixed number of features : each feature is represented as a vector , and the vectors are concatenated . This way , each region of the resulting input vector corresponds to a different feature . However , in some cases the number of features is not known in advance ( for example , in document classification it is common that each word in the sentence is a feature ) . We thus need to represent an unbounded number of features using a fixed size vector . One way of achieving this is through a so - called continuous bag of words ( CBOW ) representation ( Mikolov , Chen , Corrado , & Dean , 2013 ) . The CBOW is very similar to the traditional bag-of-words representation in which we discard order information , and works by either summing or averaging the embedding vectors of the corresponding features : 2 2 . Note that if the v ( fi ) s were one-hot vectors rather than dense feature representations , the CBOW and WCBOW equations above would reduce to the traditional ( weighted ) bag-of-words representations , which is in turn equivalent to a sparse feature-vector representation in which each binary indicator feature corresponds to a unique “ word ” . 7 CBOW ( f1 , . . . , fk ) = 1 k k ∑ i = 1 v ( fi ) A simple variation on the CBOW representation is weighted CBOW , in which different vectors receive different weights : WCBOW ( f1 , . . . , fk ) = 1 ∑ k i = 1 ai k ∑ i = 1 aiv ( fi ) Here , each feature fi has an associated weight ai , indicating the relative importance of the feature . For example , in a document classification task , a feature fi may correspond to a word in the document , and the associated weight ai could be the word’s TF-IDF score . Distance and Position Features The linear distance in between two words in a sentence may serve as an informative feature . For example , in an event extraction task3 we may be given a trigger word and a candidate argument word , and asked to predict if the argument word is indeed an argument of the trigger . The distance ( or relative position ) between the trigger and the argument is a strong signal for this prediction task . In the “ traditional ” NLP setup , distances are usually encoded by binning the distances into several groups ( i.e . 1 , 2 , 3 , 4 , 5 – 10 , 10 + ) and associating each bin with a one-hot vector . In a neural architecture , where the input vector is not composed of binary indicator features , it may seem natural to allocate a single input vector entry to the distance feature , where the numeric value of that entry is the distance . However , this approach is not taken in practice . Instead , distance features are encoded similarly to the other feature types : each bin is associated with a d-dimensional vector , and these distance-embedding vectors are then trained as regular parameters in the network ( Zeng et al . , 2014 ; dos Santos et al . , 2015 ; Zhu et al . , 2015a ; Nguyen & Grishman , 2015 ) . Feature Combinations Note that the feature extraction stage in the neural-network settings deals only with extraction of core features . This is in contrast to the traditional linear-model-based NLP systems in which the feature designer had to manually specify not only the core features of interests but also interactions between them ( e.g . , introducing not only a feature stating “ word is X ” and a feature stating “ tag is Y ” but also combined feature stating “ word is X and tag is Y ” or sometimes even “ word is X , tag is Y and previous word is Z ” ) . The combination features are crucial in linear models because they introduce more dimensions to the input , transforming it into a space where the data-points are closer to being linearly separable . On the other hand , the space of possible combinations is very large , and the feature designer has to spend a lot of time coming up with an effective set of feature combinations . One of the promises of the non-linear neural network models is that one needs to define only the core features . The non-linearity of the classifier , as defined by the network structure , is expected to take care of finding the indicative feature combinations , alleviating the need for feature combination engineering . 3 . The event extraction task involves identification of events from a predefined set of event types . For example identification of “ purchase ” events or “ terror-attack ” events . Each event type can be triggered by various triggering words ( commonly verbs ) , and has several slots ( arguments ) that needs to be filled ( i.e . who purchased ? what was purchased ? at what amount ? ) . 8 Kernel methods ( Shawe-Taylor & Cristianini , 2004 ) , and in particular polynomial kernels ( Kudo & Matsumoto , 2003 ) , also allow the feature designer to specify only core features , leaving the feature combination aspect to the learning algorithm . In contrast to neural - network models , kernels methods are convex , admitting exact solutions to the optimization problem . However , the classification efficiency in kernel methods scales linearly with the size of the training data , making them too slow for most practical purposes , and not suitable for training with large datasets . On the other hand , neural network classification efficiency scales linearly with the size of the network , regardless of the training data size . Dimensionality How many dimensions should we allocate for each feature ? Unfortu - nately , there are no theoretical bounds or even established best-practices in this space . Clearly , the dimensionality should grow with the number of the members in the class ( you probably want to assign more dimensions to word embeddings than to part-of-speech embed - dings ) but how much is enough ? In current research , the dimensionality of word-embedding vectors range between about 50 to a few hundreds , and , in some extreme cases , thousands . Since the dimensionality of the vectors has a direct effect on memory requirements and processing time , a good rule of thumb would be to experiment with a few different sizes , and choose a good trade-off between speed and task accuracy . Vector Sharing Consider a case where you have a few features that share the same vocabulary . For example , when assigning a part-of-speech to a given word , we may have a set of features considering the previous word , and a set of features considering the next word . When building the input to the classifier , we will concatenate the vector representation of the previous word to the vector representation of the next word . The classifier will then be able to distinguish the two different indicators , and treat them differently . But should the two features share the same vectors ? Should the vector for “ dog : previous-word ” be the same as the vector of “ dog : next-word ” ? Or should we assign them two distinct vectors ? This , again , is mostly an empirical question . If you believe words behave differently when they appear in different positions ( e.g . , word X behaves like word Y when in the previous position , but X behaves like Z when in the next position ) then it may be a good idea to use two different vocabularies and assign a different set of vectors for each feature type . However , if you believe the words behave similarly in both locations , then something may be gained by using a shared vocabulary for both feature types . Network’s Output For multi-class classification problems with k classes , the network’s output is a k-dimensional vector in which every dimension represents the strength of a particular output class . That is , the output remains as in the traditional linear models – scalar scores to items in a discrete set . However , as we will see in Section 4 , there is a d × k matrix associated with the output layer . The columns of this matrix can be thought of as d dimensional embeddings of the output classes . The vector similarities between the vector representations of the k classes indicate the model’s learned similarities between the output classes . Historical Note Representing words as dense vectors for input to a neural network was introduced by Bengio et al ( Bengio et al . , 2003 ) in the context of neural language modeling . It was introduced to NLP tasks in the pioneering work of Collobert , Weston and colleagues 9 ( 2008 , 2011 ) . Using embeddings for representing not only words but arbitrary features was popularized following Chen and Manning ( 2014 ) . 10