11 . Concrete RNN Architectures We now turn to present three different instantiations of the abstract RNN architecture discussed in the previous section , providing concrete definitions of the functions R and O . These are the Simple RNN ( SRNN ) , the Long Short-Term Memory ( LSTM ) and the Gated Recurrent Unit ( GRU ) . 11.1 Simple RNN The simplest RNN formulation , known as an Elman Network or Simple-RNN ( S-RNN ) , was proposed by Elman ( 1990 ) and explored for use in language modeling by Mikolov ( 2012 ) . The S-RNN takes the following form : si = RSRNN ( si − 1 , xi ) = g ( xiW x + si − 1W s + b ) yi = OSRNN ( si ) = si si , yi ∈ Rds , xi ∈ Rdx , Wx ∈ Rdx × ds , Ws ∈ Rds × ds , b ∈ Rds That is , the state at position i is a linear combination of the input at position i and the previous state , passed through a non-linear activation ( commonly tanh or ReLU ) . The output at position i is the same as the hidden state in that position . 32 In spite of its simplicity , the Simple RNN provides strong results for sequence tagging ( Xu et al . , 2015 ) as well as language modeling . For comprehensive discussion on using Simple RNNs for language modeling , see the PhD thesis by Mikolov ( 2012 ) . 11.2 LSTM The S-RNN is hard to train effectively because of the vanishing gradients problem . Error signals ( gradients ) in later steps in the sequence diminish quickly in the back-propagation process , and do not reach earlier input signals , making it hard for the S-RNN to capture long-range dependencies . The Long Short-Term Memory ( LSTM ) architecture ( Hochreiter & Schmidhuber , 1997 ) was designed to solve the vanishing gradients problem . The main idea behind the LSTM is to introduce as part of the state representation also “ memory cells ” ( a vector ) that can preserve gradients across time . Access to the memory cells is controlled by gating components – smooth mathematical functions that simulate logical gates . At each input state , a gate is used to decide how much of the new input should be written to the memory cell , and how much of the current content of the memory cell should be forgotten . Concretely , a gate g ∈ [ 0 , 1 ] n is a vector of values in the range [ 0 , 1 ] that is multiplied component-wise with another vector v ∈ Rn , and the result is then added to another vector . The values of g are designed to be close to either 0 or 1 , i.e . by using a sigmoid function . Indices in v corresponding to near-one values in g are allowed to pass , while those corresponding to near-zero values are blocked . 32 . Some authors treat the output at position i as a more complicated function of the state . In our presen - tation , such further transformation of the output are not considered part of the RNN , but as separate computations that are applied to the RNNs output . The distinction between the state and the output are needed for the LSTM architecture , in which not all of the state is observed outside of the RNN . 56 Mathematically , the LSTM architecture is defined as : 33 sj = RLSTM ( sj − 1 , xj ) =[ cj ; hj ] cj = cj − 1 � f + g � i hj = tanh ( cj ) � o i = σ ( xjW xi + hj − 1W hi ) f = σ ( xjW xf + hj − 1W hf ) o = σ ( xjW xo + hj − 1W ho ) g = tanh ( xjW xg + hj − 1W hg ) yj = OLSTM ( sj ) = hj sj ∈ R2 · dh , xi ∈ Rdx , cj , hj , i , f , o , g ∈ Rdh , Wx ◦ ∈ Rdx × dh , Wh ◦ ∈ Rdh × dh , The symbol � is used to denote component-wise product . The state at time j is com - posed of two vectors , cj and hj , where cj is the memory component and hj is the output , or state , component . There are three gates , i , f and o , controlling for input , forget and output . The gate values are computed based on linear combinations of the current input xj and the previous state hj − 1 , passed through a sigmoid activation function . An update candidate g is computed as a linear combination of xj and hj − 1 , passed through a tanh activation function . The memory cj is then updated : the forget gate controls how much of the previous memory to keep ( cj − 1 � f ) , and the input gate controls how much of the proposed update to keep ( g � i ) . Finally , the value of hj ( which is also the output yj ) is determined based on the content of the memory cj , passed through a tanh non-linearity and controlled by the output gate . The gating mechanisms allow for gradients related to the memory part cj to stay high across very long time ranges . For further discussion on the LSTM architecture see the PhD thesis by Alex Graves ( 2008 ) , as well as Chris Olah’s description . 34 For an analysis of the behavior of an LSTM when used as a character-level language model , see ( Karpathy et al . , 2015 ) . LSTMs are currently the most successful type of RNN architecture , and they are re - sponsible for many state-of-the-art sequence modeling results . The main competitor of the LSTM-RNN is the GRU , to be discussed next . Practical Considerations When training LSTM networks , Jozefowicz et al ( 2015 ) strongly recommend to always initialize the bias term of the forget gate to be close to one . When applying dropout to an RNN with an LSTM , Zaremba et al ( 2014 ) found out that it is 33 . There are many variants on the LSTM architecture presented here . For example , forget gates were not part of the original proposal in ( Hochreiter & Schmidhuber , 1997 ) , but are shown to be an important part of the architecture . Other variants include peephole connections and gate-tying . For an overview and comprehensive empirical comparison of various LSTM architectures see ( Greff , Srivastava , Koutńık , Steunebrink , & Schmidhuber , 2015 ) . 34 . http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 57 crucial to apply dropout only on the non-recurrent connection , i.e . only to apply it between layers and not between sequence positions . 11.3 GRU The LSTM architecture is very effective , but also quite complicated . The complexity of the system makes it hard to analyze , and also computationally expensive to work with . The gated recurrent unit ( GRU ) was recently introduced by Cho et al ( 2014b ) as an alternative to the LSTM . It was subsequently shown by Chung et al ( 2014 ) to perform comparably to the LSTM on several ( non textual ) datasets . Like the LSTM , the GRU is also based on a gating mechanism , but with substantially fewer gates and without a separate memory component . sj = RGRU ( sj − 1 , xj ) =( 1 − z ) � sj − 1 + z � h z = σ ( xjW xz + hj − 1W hz ) r = σ ( xjW xr + hj − 1W hr ) h = tanh ( xjW xh + ( hj − 1 � r ) Whg ) yj = OLSTM ( sj ) = sj sj ∈ Rdh , xi ∈ Rdx , z , r , h ∈ Rdh , Wx ◦ ∈ Rdx × dh , Wh ◦ ∈ Rdh × dh , One gate ( r ) is used to control access to the previous state sj − 1 and compute a proposed update h . The updated state sj ( which also serves as the output yj ) is then determined based on an interpolation of the previous state sj − 1 and the proposal h , where the proportions of the interpolation are controlled using the gate z . The GRU was shown to be effective in language modeling and machine translation . However , the jury between the GRU , the LSTM and possible alternative RNN architectures is still out , and the subject is actively researched . For an empirical exploration of the GRU and the LSTM architectures , see ( Jozefowicz et al . , 2015 ) . 11.4 Other Variants The gated architectures of the LSTM and the GRU help in alleviating the vanishing gradi - ents problem of the Simple RNN , and allow these RNNs to capture dependencies that span long time ranges . Some researchers explore simpler architectures than the LSTM and the GRU for achieving similar benefits . Mikolov et al ( 2014 ) observed that the matrix multiplication si − 1Ws coupled with the nonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo large changes at each time step , prohibiting it from remembering information over long time periods . They propose to split the state vector si into a slow changing component ci ( “ context units ” ) and a fast changing component hi . 35 The slow changing component ci is 35 . We depart from the notation in ( Mikolov et al . , 2014 ) and reuse the symbols used in the LSTM descrip - tion . 58 updated according to a linear interpolation of the input and the previous component : ci = ( 1 − α ) xiWx1 + αci − 1 , where α ∈ ( 0 , 1 ) . This update allows ci to accumulate the previous inputs . The fast changing component hi is updated similarly to the Simple RNN update rule , but changed to take ci into account as well : 36 hi = σ ( xiW x2 + hi − 1Wh + ciWc ) . Finally , the output yi is the concatenation of the slow and the fast changing parts of the state : yi = [ ci ; hi ] . Mikolov et al demonstrate that this architecture provides competitive perplexities to the much more complex LSTM on language modeling tasks . The approach of Mikolov et al can be interpreted as constraining the block of the matrix Ws in the S-RNN corresponding to ci to be a multiply of the identity matrix ( see Mikolov et al ( 2014 ) for the details ) . Le et al ( Le , Jaitly , & Hinton , 2015 ) propose an even simpler approach : set the activation function of the S-RNN to a ReLU , and initialize the biases b as zeroes and the matrix Ws as the identify matrix . This causes an untrained RNN to copy the previous state to the current state , add the effect of the current input xi and set the negative values to zero . After setting this initial bias towards state copying , the training procedure allows Ws to change freely . Le et al demonstrate that this simple modification makes the S-RNN comparable to an LSTM with the same number of parameters on several tasks , including language modeling . 36 . The update rule diverges from the S-RNN update rule also by fixing the non-linearity to be a sigmoid function , and by not using a bias term . However , these changes are not discussed as central to the proposal . 59