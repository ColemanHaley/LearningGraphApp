Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 19 Word Senses WordNet get single right meaning better ship-load pearls , resolve single doubt like bottom falling off bucket . Yuen Mei 袁枚 ( 1785 ) ( translation Arthur Waley ) Words ambiguous : same word mean different things . Forambiguous example Chapter 6 saw word “ mouse ” ( least ) two meanings : ( 1 ) small rodent , ( 2 ) hand-operated device control cursor . word “ bank ” mean : ( 1 ) financial institution ( 2 ) sloping mound . say words ‘ mouse ’ ‘ bank ’ polysemous ( Greek ‘ many senses ’ , poly - ‘ many ’ + sema , ‘ sign , mark ’ ) . 1 sense ( word sense ) isword sense discrete representation aspect meaning word . chapter discuss word senses detail introduce WordNet , large online the-WordNet saurus — database represents word senses — versions many languages . WordNet represents relations senses . example , IS-A relation dog mammal ( dog kind mammal ) part-whole relation engine car ( engine part car ) . Knowing relation two senses play important role language understanding . Consider antonymy relation . Two words antonyms opposite meanings , like long short , up down . Distinguishing quite important language understanding ( user asks dialogue agent turn up music , unfortunate turn down ) . fact embedding models like word2vec , antonyms easily confused , often closest words embedding space word ( e.g . , up ) antonym ( e.g . , down ) . Thesauruses represent relationship help ! introduce word sense disambiguation ( WSD ) , task determiningword sensedisambiguation sense word particular context . give supervised unsupervised algorithms deciding sense intended particular context . task long history computational linguistics many ap - plications . question answering , helpful user asks “ bat care ” know sense bat relevant . ( user vampire ? just wants play baseball . ) different senses word often different translations ; Spanish animal bat murciélago baseball bat bate , indeed word sense algorithms help improve MT ( Pu et al . , 2018 ) . Finally , WSD long tool evaluating natural language - standing models , understanding models represent different word senses important analytic direction . 1 word polysemy different way , refer cases word’s senses sort semantic relation , word homonymy cases relation senses . 2 CHAPTER 19 • WORD SENSES WORDNET 19.1 Word Senses sense ( word sense ) discrete representation aspect meaning ofword sense word . Loosely following lexicographic tradition , represent sense superscript : bank1 bank2 , mouse1 mouse2 . context , easy different meanings : mouse1 : . . . . mouse controlling computer system 1968 . mouse2 : . . . . quiet animal like mouse bank1 : . . . bank hold investments custodial account . . . bank2 : . . . agriculture burgeons east bank , river . . . 19.1.1 Defining Word Senses define meaning word sense ? introduced Chapter 6 standard computational approach representing word embedding , point semantic space . intuition embedding models like word2vec GloVe meaning word defined co-occurrences , counts words often occur nearby . tell define meaning word sense . Contextual embeddings like ELMo BERT go further offering embedding represents meaning word textual context , contextual embeddings lie heart modern algorithms word sense disambiguation . first , need consider alternative ways dictionaries - sauruses offer defining senses . based fact dictionaries - sauruses give textual definitions sense called glosses . glossesgloss two senses bank : 1 . financial institution accepts deposits channels money lending activities 2 . sloping land ( especially slope beside body water ) Glosses formal meaning representation ; just written people . Consider following fragments definitions right , left , red , blood American Heritage Dictionary ( Morris , 1985 ) . right adj . located nearer right hand esp . right facing same direction observer . left adj . located nearer side body right . red n . color blood ruby . blood n . red liquid circulates heart , arteries veins animals . Note circularity definitions . definition right makes two direct references itself , entry left contains implicit self-reference phrase side body , presumably means left side . entries red blood reference definitions . humans , entries useful user dictionary sufficient grasp terms . Yet despite circularity lack formal representation , glosses still useful computational modeling senses . gloss just sen - tence , sentences compute sentence embeddings tell - thing meaning sense . Dictionaries often give example sentences glosses , again help build sense representation . 19.1 • WORD SENSES 3 second way thesauruses offer defining sense — like dictionary definitions — defining sense relationship senses . exam - ple , definitions make clear right left similar kinds lemmas stand kind alternation , opposition , another . Similarly , glean red color blood liquid . Sense relations sort ( IS-A , antonymy ) explicitly listed on-line databases like WordNet . sufficiently large database relations , many applications quite capable performing sophisticated semantic tasks word senses ( even really know right left ) . 19.1.2 many senses words ? Dictionaries thesauruses give discrete lists senses . contrast , embeddings ( static contextual ) offer continuous high-dimensional model meaning divide up discrete senses . creating thesaurus depends criteria deciding differ - ing word represented discrete senses . might consider two senses discrete independent truth conditions , different syntactic - havior , independent sense relations , exhibit antagonistic meanings . Consider following verb serve WSJ corpus : ( 19.1 ) rarely serve red meat , preferring prepare seafood . ( 19.2 ) served U.S . ambassador Norway 1976 1977 . ( 19.3 ) might served time , come led upstanding life . serve serving red meat serving time clearly different truth conditions presuppositions ; serve serve ambassador distinct subcategorization structure serve NP . heuristics suggest prob - ably three distinct senses serve . practical technique determining two senses distinct conjoin two word single sentence ; kind conjunction antagonistic readings called zeugma . Consider followingzeugma examples : ( 19.4 ) flights serve breakfast ? ( 19.5 ) Air France serve Philadelphia ? ( 19.6 ) ? Air France serve breakfast Philadelphia ? ( ? ) mark examples semantically ill-formed . oddness invented third example ( case zeugma ) indicates sensible way make single sense serve work breakfast Philadelphia . evidence serve two different senses case . Dictionaries tend many fine-grained senses capture subtle meaning differences , reasonable approach traditional role dictionaries aiding word learners . computational purposes , often need fine distinctions , often group cluster senses ; already examples chapter . Indeed , clustering examples senses , senses broader-grained categories , important computational task discuss Section 19.7 . 4 CHAPTER 19 • WORD SENSES WORDNET 19.2 Relations Senses section explores relations word senses , especially received significant computational investigation like synonymy , antonymy , hy - pernymy . Synonymy introduced Chapter 6 idea two senses two different words ( lemmas ) identical , nearly identical , say two senses synonyms . synonym Synonyms include pairs couch / sofa vomit / throw up filbert / hazelnut car / automobile mentioned practice , word synonym commonly describe relationship approximate rough synonymy . furthermore , syn - onymy actually relationship senses rather words . Considering words big large . seem synonyms following sentences , swap big large sentence retain same meaning : ( 19.7 ) big plane ? ( 19.8 ) flying large small plane ? note following sentence substitute large big : ( 19.9 ) Miss Nelson , instance , became kind big sister Benjamin . ( 19.10 ) ? Miss Nelson , instance , became kind large sister Benjamin . word big sense means older grown up , large lacks sense . Thus , say senses big large ( nearly ) synonymous ones . Antonymy Whereas synonyms words identical similar meanings , antonyms areantonym words opposite meaning , like : long / short big / little fast / slow cold / hot dark / light rise / fall up / down / Two senses antonyms define binary opposition opposite ends scale . case long / short , fast / slow , big / little , opposite ends length size scale . Another group antonyms , reversives , reversives describe change movement opposite directions , rise / fall up / down . Antonyms thus differ completely respect aspect meaning — position scale direction — otherwise similar , sharing almost aspects meaning . Thus , automatically distinguishing synonyms antonyms difficult . Taxonomic Relations Another way word senses related taxonomically . word ( sense ) hyponym another word sense first specific , denoting subclasshyponym . example , car hyponym vehicle , dog hyponym animal , mango hyponym fruit . Conversely , say vehicle hypernym ofhypernym car , animal hypernym dog . unfortunate two words ( hypernym 19.2 • RELATIONS SENSES 5 hyponym ) similar hence easily confused ; reason , word superordinate often hypernym . superordinate Superordinate vehicle fruit furniture mammal Subordinate car mango chair dog define hypernymy formally saying class denoted superordinate extensionally includes class denoted hyponym . Thus , class animals includes members dogs , class moving actions includes walking actions . Hypernymy defined terms entail - ment . definition , sense hyponym sense B everything B , hence entails B , ∀ x ( x ) ⇒ B ( x ) . Hy - ponymy / hypernymy usually transitive relation ; hyponym B B hyponym C , hyponym C . Another name hypernym / hyponym structure IS-A hierarchy , say IS-A B , B subsumes . IS-A Hypernymy useful tasks like textual entailment question answering ; knowing leukemia type cancer , example , certainly useful answering questions leukemia . Meronymy Another common relation meronymy , part-whole relation . leg part apart-whole chair ; wheel part car . say wheel meronym car , car holonym wheel . Structured Polysemy senses word related semantically , case call relationship structured polysemy . Consider sense bank : structuredpolysemy ( 19.11 ) bank corner Nassau Witherspoon . sense , perhaps bank4 , means something like “ building belonging financial institution ” . two kinds senses ( organization build - ing associated organization ) occur together many words well ( school , university , hospital , etc . ) . Thus , systematic relationship senses might represent BUILDING ↔ ORGANIZATION particular subtype polysemy relation called metonymy . Metonymy ismetonymy aspect concept entity refer aspects entity entity itself . performing metonymy phrase White House refer administration whose office White House . common examples metonymy include relation following pairings senses : AUTHOR ↔ WORKS AUTHOR ( Jane Austen wrote Emma ) ( really love Jane Austen ) FRUITTREE ↔ FRUIT ( Plums beautiful blossoms ) ( ate preserved plum yesterday ) 6 CHAPTER 19 • WORD SENSES WORDNET 19.3 WordNet : Database Lexical Relations commonly resource sense relations English many languages WordNet lexical database ( Fellbaum , 1998 ) . English WordNetWordNet consists three separate databases , nouns verbs third adjectives adverbs ; closed class words included . database contains set lemmas , annotated set senses . WordNet 3.0 release 117,798 nouns , 11,529 verbs , 22,479 adjectives , 4,481 adverbs . aver - age noun 1.23 senses , average verb 2.16 senses . WordNet accessed Web downloaded locally . Figure 19.1 shows lemma entry noun adjective bass . noun “ bass ” 8 senses WordNet . 1 . bass1 - ( lowest part musical range ) 2 . bass2 , bass part1 - ( lowest part polyphonic music ) 3 . bass3 , basso1 - ( adult male singer lowest voice ) 4 . sea bass1 , bass4 - ( lean flesh saltwater fish family Serranidae ) 5 . freshwater bass1 , bass5 - ( various North American freshwater fish lean flesh ( especially genus Micropterus ) ) 6 . bass6 , bass voice1 , basso2 - ( lowest adult male singing voice ) 7 . bass7 - ( member lowest range family musical instruments ) 8 . bass8 - ( nontechnical name numerous edible marine freshwater spiny-finned fishes ) Figure 19.1 portion WordNet 3.0 entry noun bass . Note eight senses noun adjective , gloss ( dictionary-style definition ) , list synonyms sense , andgloss sometimes usage examples ( shown adjective sense ) . WordNet represent pronunciation , distinguish pronunciation [ b ae s ] bass4 , bass5 , bass8 senses pronounced [ b ey s ] . set near-synonyms WordNet sense called synset ( synonymsynset set ) ; synsets important primitive WordNet . entry bass includes synsets like { bass1 , deep6 } , { bass6 , bass voice1 , basso2 } . think synset representing concept type discussed Chapter 16 . Thus , representing concepts logical terms , WordNet represents lists word senses express concept . Here’s another synset example : { chump1 , fool2 , gull1 , mark9 , patsy1 , fall guy1 , sucker1 , soft touch1 , mug2 } gloss synset describes : Gloss : person gullible easy take advantage . Glosses properties synset , sense included synset same gloss express concept . share glosses , synsets like fundamental unit associated WordNet entries , hence synsets , wordforms , lemmas , individual senses , participate lexical sense relations WordNet . WordNet labels synset lexicographic category drawn semantic field example 26 categories nouns shown Fig . 19.2 , well 19.3 • WORDNET : DATABASE LEXICAL RELATIONS 7 15 verbs ( plus 2 adjectives 1 adverbs ) . categories often called supersenses , act coarse semantic categories groupings ofsupersense senses useful word senses fine-grained ( Ciaramita Johnson 2003 , Ciaramita Altun 2006 ) . Supersenses defined adjectives ( Tsvetkov et al . , 2014 ) prepositions ( Schneider et al . , 2018 ) . Category Example Category Example Category Example ACT service GROUP place PLANT tree ANIMAL dog LOCATION area POSSESSION price ARTIFACT car MOTIVE reason PROCESS process ATTRIBUTE quality NATURAL EVENT experience QUANTITY amount BODY hair NATURAL OBJECT flower RELATION portion COGNITION way stuff SHAPE square COMMUNICATION review PERSON people STATE pain FEELING discomfort PHENOMENON result SUBSTANCE oil FOOD food TIME day Figure 19.2 Supersenses : 26 lexicographic categories nouns WordNet . 19.3.1 Sense Relations WordNet WordNet represents kinds sense relations discussed previous section , illustrated Fig . 19.3 Fig . 19.4 . Relation Called Definition Example Hypernym Superordinate concepts superordinates breakfast1 → meal1 Hyponym Subordinate concepts subtypes meal1 → lunch1 Instance Hypernym Instance instances concepts Austen1 → author1 Instance Hyponym Has-Instance concepts instances composer1 → Bach1 Part Meronym Has-Part wholes parts table2 → leg3 Part Holonym Part-Of parts wholes course7 → meal1 Antonym Semantic opposition lemmas leader1 ⇐ ⇒ follower1 Derivation Lemmas w / same morphological root destruction1 ⇐ ⇒ destroy1 Figure 19.3 noun relations WordNet . Relation Definition Example Hypernym events superordinate events fly9 → travel5 Troponym events subordinate event walk1 → stroll1 Entails verbs ( events ) verbs ( events ) entail snore1 → sleep1 Antonym Semantic opposition lemmas increase1 ⇐ ⇒ decrease1 Figure 19.4 verb relations WordNet . example WordNet represents hyponymy ( page 4 ) relating synset immediately general specific synsets direct hypernym hyponym relations . relations followed produce longer chains general specific synsets . Figure 19.5 shows hypernym chains bass3 bass7 ; general synsets shown successively indented lines . WordNet two kinds taxonomic entities : classes instances . instance individual , proper noun unique entity . San Francisco instance city , example . city class , hyponym municipality eventually 8 CHAPTER 19 • WORD SENSES WORDNET bass3 , basso ( adult male singer lowest voice ) = > singer , vocalist , vocalizer , vocaliser = > musician , instrumentalist , player = > performer , performing artist = > entertainer = > person , individual , someone . . . = > organism , = > living thing , animate thing , = > whole , unit = > object , physical object = > physical entity = > entity bass7 ( member lowest range family instruments ) = > musical instrument , instrument = > device = > instrumentality , instrumentation = > artifact , artefact = > whole , unit = > object , physical object = > physical entity = > entity Figure 19.5 Hyponymy chains two separate senses lemma bass . Note chains completely distinct , converging abstract level whole , unit . location . Fig . 19.6 shows subgraph WordNet demonstrating many relations . Figure 19.6 WordNet viewed graph . Figure Navigli ( 2016 ) . 19.4 • WORD SENSE DISAMBIGUATION 9 19.4 Word Sense Disambiguation task selecting correct sense word called word sense disambigua - tion , WSD . WSD algorithms take input word context fixed inventoryword sensedisambiguation WSD potential word senses outputs correct word sense context . 19.4.1 WSD : Task Datasets section introduce task setup WSD , turn algorithms . inventory sense tags depends task . sense tagging context translation English Spanish , sense tag inventory English word might set different Spanish translations . automatic indexing med - ical articles , sense-tag inventory might set MeSH ( Medical Subject Headings ) thesaurus entries . set senses resource like WordNet , supersenses coarser-grain set . Figure 19.4.1 shows examples word bass . WordNet Spanish WordNet Sense Translation Supersense Target Word Context bass4 lubina FOOD . . . fish Pacific salmon striped bass . . . bass7 bajo ARTIFACT . . . play bass solo . . . Figure 19.7 possibile sense tag inventories bass . situations , just need disambiguate small number words . lexical sample tasks , small pre-selected set target words anlexical sample inventory senses word lexicon . set words set senses small , simple supervised classification approaches work well . commonly , , harder problem dis - ambiguate words text . all-words task , system anall-words entire texts lexicon inventory senses entry disambiguate every word text ( sometimes just every content word ) . all-words task similar part-of-speech tagging , except larger set tags lemma own set . consequence larger set tags data sparseness . Supervised all-word disambiguation tasks generally trained semantic concordance , corpus open-class word sentence labeledsemanticconcordance word sense specific dictionary thesaurus , often WordNet . SemCor corpus subset Brown Corpus consisting 226,036 words manually tagged WordNet senses ( Miller et al . 1993 , Landes et al . 1998 ) . sense-tagged corpora built SENSEVAL Se - mEval WSD tasks , SENSEVAL-3 Task 1 English all-words test data 2282 annotations ( Snyder Palmer , 2004 ) SemEval-13 Task 12 datasets . Large semantic concordances available languages including Dutch ( Vossen et al . , 2011 ) German ( Henrich et al . , 2012 ) . Here’s example SemCor corpus showing WordNet sense num - bers tagged words ; standard WSD notation subscript marks part speech ( Navigli , 2009 ) : ( 19.12 ) find9v avocado 1 n 1 v unlike 1 j 1 j fruit 1 n ever 1 r tasted 2 v noun , verb , adjective , adverb word hand-labeled test set ( say fruit ) , SemCor-based WSD task choose correct sense possible 10 CHAPTER 19 • WORD SENSES WORDNET senses WordNet . fruit mean choosing correct answer fruit1n ( ripened reproductive body seed plant ) , two senses fruit 2 n ( yield ; amount product ) fruit3n ( consequence effort action ) . Fig . 19.8 sketches task . electric guitar bass player stand off side electric1 : electricity electric2 : tense electric3 : thrilling guitar1 bass1 : low range … bass4 : sea fish … bass7 : instrument … player1 : game player2 : musician player3 : actor … stand1 : upright … stand5 : bear … stand10 : put upright … side1 : relative region … side3 : body … side11 : slope … x1 y1 x2 y2 x3 y3 y4 y5 y6 x4 x5 x6 Figure 19.8 all-words WSD task , mapping input words ( x ) WordNet senses ( y ) . nouns , verbs , adjectives , adverbs mapped , note words ( like guitar example ) sense WordNet . Figure inspired Chaplot Salakhutdinov ( 2018 ) . WSD systems typically evaluated intrinsically , computing F1 against hand-labeled sense tags held-out set , SemCor corpus SemEval corpora discussed . surprisingly strong baseline simply choose frequent sense formost frequentsense word senses labeled corpus ( Gale et al . , 1992a ) . WordNet , corresponds first sense , senses WordNet generally ordered frequent least frequent based counts SemCor sense-tagged corpus . frequent sense baseline quite accurate , often default , supply word sense supervised algorithm insufficient training data . second heuristic , called sense per discourse based work ofone sense perdiscourse Gale et al . ( 1992b ) , noticed word appearing multiple times text discourse often appears same sense . heuristic seems hold better coarse-grained senses particularly cases homonymy rather polysemy , generally baseline . Nonetheless various kinds disambiguation tasks often include bias toward resolving ambiguity same way inside discourse segment . 19.4.2 WSD Algorithm : Contextual Embeddings best-performing WSD algorithm simple 1-nearest-neighbor algorithm contextual word embeddings , due Melamud et al . ( 2016 ) Peters et al . ( 2018 ) . training time pass sentence SemCore labeled dataset contextual embedding ( ELMo BERT ) resulting contextual embedding labeled token SemCore . token ci sense c word , average contextual representations produce contextual sense embedding vs 19.4 • WORD SENSE DISAMBIGUATION 11 c : vs = 1 n ∑ ci ( 19.13 ) test time similarly compute contextual embedding t target word , choose nearest neighbor sense ( sense highest cosine t ) training set . Fig . 19.9 illustrates model . found jar empty Neural Language Model cI cfound find1v cthe cjar cempty find9v find5vfind4v Figure 19.9 nearest-neighbor algorithm WSD . green contextual embed - dings precomputed sense word ; just show few senses find . contextual embedding computed target word found , nearest neighbor sense ( case find9n ) chosen . Figure inspired Loureiro Jorge ( 2019 ) . words haven’t seen sense-labeled training data ? , number senses appear SemCor small fraction words WordNet . simplest algorithm fall back Frequent Sense baseline , i.e . taking first sense WordNet . that’s satisfactory . powerful approach , due Loureiro Jorge ( 2019 ) , impute missing sense embeddings , bottom-up , WordNet taxonomy super - senses . get sense embedding higher-level node WordNet taxon - omy averaging embeddings children , thus computing embedding synset average sense embeddings , embedding hypernym average synset embeddings , lexicographic category ( supersense ) embedding average large set synset embeddings category . formally , missing sense WordNet ŝ ∈ W , let sense embeddings members synset Sŝ , hypernym-specific synset embeddings Hŝ , lexicographic ( supersense-specific ) synset embeddings Lŝ . compute sense embedding ŝ follows : | Sŝ | > 0 , vŝ = 1 | Sŝ | ∑ vs , ∀ vs ∈ Sŝ ( 19.14 ) else | Hŝ | > 0 , vŝ = 1 | Hŝ | ∑ vsyn , ∀ vsyn ∈ Hŝ ( 19.15 ) else | Lŝ | > 0 , vŝ = 1 | Lŝ | ∑ vsyn , ∀ vsyn ∈ Lŝ ( 19.16 ) supersenses labeled data SemCor , algorithm guaranteed representation possible senses time al - 12 CHAPTER 19 • WORD SENSES WORDNET gorithm backs off general ( supersense ) information , although course coarse model . 19.5 Alternate WSD algorithms Tasks 19.5.1 Feature-Based WSD Feature-based algorithms WSD extremely simple function almost well contextual language model algorithms . best-performing IMS algorithm ( Zhong Ng , 2010 ) , augmented embeddings ( Iacobacci et al . 2016 , Raganato et al . 2017b ) , SVM classifier choose sense input word following simple features surrounding words : • part-of-speech tags ( window 3 words side , stopping sen - tence boundaries ) • collocation features words n-grams lengths 1 , 2 , 3 ) particularcollocation location window 3 word side ( i.e . , exactly word right , two words starting 3 words left , ) . • weighted average embeddings ( words window 10 words side , weighted exponentially distance ) Consider ambiguous word bass following WSJ sentence : ( 19.17 ) electric guitar bass player stand off side , small 2-word window , standard feature vector might include parts-of - speech , unigram bigram collocation features , weighted sum g embed - dings , : [ wi − 2 , POSi − 2 , wi − 1 , POSi − 1 , wi + 1 , POSi + 1 , wi + 2 , POSi + 2 , wi − 1i − 2 , wi + 2i + 1 , g ( E ( wi − 2 ) , E ( wi − 1 ) , E ( wi + 1 ) , E ( wi + 2 ) ] ( 19.18 ) yield following vector : [ guitar , NN , , CC , player , NN , stand , VB , guitar , player stand , g ( E ( guitar ) , E ( ) , E ( player ) , E ( stand ) ) ] 19.5.2 Lesk Algorithm WSD Baseline Generating sense labeled corpora like SemCor quite difficult expensive . alternative class WSD algorithms , knowledge-based algorithms , rely solely onknowledge-based WordNet resources require labeled data . supervised algorithms generally work better , knowledge-based methods lan - guages domains thesauruses dictionaries sense labeled corpora available . Lesk algorithm oldest powerful knowledge-based WSDLesk algorithm method , useful baseline . Lesk really family algorithms choose sense whose dictionary gloss definition shares words target word’s neighborhood . Figure 19.10 shows simplest version algorithm , often called Simplified Lesk algorithm ( Kilgarriff Rosenzweig , 2000 ) . Simplified Lesk example Lesk algorithm work , consider disambiguating word bank following context : 19.5 • ALTERNATE WSD ALGORITHMS TASKS 13 function SIMPLIFIED LESK ( word , sentence ) returns best sense word best-sense ← frequent sense word max-overlap ← 0 context ← set words sentence sense senses word signature ← set words gloss examples sense overlap ← COMPUTEOVERLAP ( signature , context ) overlap > max-overlap max-overlap ← overlap best-sense ← sense end return ( best-sense ) Figure 19.10 Simplified Lesk algorithm . COMPUTEOVERLAP function returns number words common two sets , ignoring function words words stop list . original Lesk algorithm defines context complex way . ( 19.19 ) bank guarantee deposits eventually cover future tuition costs invests adjustable-rate mortgage securities . following two WordNet senses : bank1 Gloss : financial institution accepts deposits channels money lending activities Examples : “ cashed check bank ” , “ bank holds mortgage home ” bank2 Gloss : sloping land ( especially slope beside body water ) Examples : “ pulled canoe up bank ” , “ sat bank river watched currents ” Sense bank1 two non-stopwords overlapping context ( 19.19 ) : deposits mortgage , sense bank2 zero words , sense bank1 chosen . many obvious extensions Simplified Lesk , weighing overlapping words IDF ( inverse document frequency ) Chapter 6 downweight frequent words like function words ; best performing word embedding co - sine word overlap compute similarity definition context ( Basile et al . , 2014 ) . Modern neural extensions Lesk definitions compute sense embeddings directly SemCor-training embeddings ( Kumar et al . 2019 , Luo et al . 2018a , Luo et al . 2018b ) . 19.5.3 Word-in-Context Evaluation Word Sense Disambiguation fine-grained evaluation word mean - ing context-free word similarity tasks described Chapter 6 . Recall tasks like LexSim-999 require systems match human judgments context - free similarity two words ( similar cup mug ? ) . think WSD kind contextualized similarity task , goal able dis - tinguish meaning word lke bass context ( playing music ) another context ( fishing ) . Somewhere lies word-in-context task . system givenword-in-context two sentences , same target word different sentential context . system decide target words same sense 14 CHAPTER 19 • WORD SENSES WORDNET two sentences different sense . Fig . 19.11 shows sample pairs WiCWiC dataset Pilehvar Camacho-Collados ( 2019 ) . F There’s lot trash bed river — keep glass water next bed sleep F Justify margins — end justifies means T Air pollution — Open window let air T expanded window give time catch thieves — two-hour window clear weather finish working lawn Figure 19.11 Positive ( T ) negative ( F ) pairs WiC dataset ( Pilehvar Camacho-Collados , 2019 ) . WiC sentences mainly taken example usages senses WordNet . WordNet senses fine-grained . reason tasks like word-in-context first cluster word senses coarser clusters , two sentential contexts target word marked T two senses same cluster . WiC clusters pairs senses first degree connections WordNet semantic graph , including sister senses , belong same supersense ; point sense clustering algorithms end chapter . baseline algorithm solve WIC task contextual embeddings like BERT simple thesholded cosine . first compute contextual embed - dings target word two sentences , compute cosine . threshold tuned devset respond true ( two senses same ) else respond false . 19.5.4 Wikipedia source training data Datasets SemCor all-words WSD . important di - rection Wikipedia source sense-labeled data . concept mentioned Wikipedia article , article text contain explicit link concept’s Wikipedia page , named unique identifier . link sense annotation . example , ambiguous word bar linked different Wikipedia article depending meaning context , including page BAR ( LAW ) , page BAR ( MUSIC ) , , following Wikipedia examples ( Mihalcea , 2007 ) . 1834 , Sumner admitted [ [ bar ( law ) | bar ] ] age twenty-three , entered private practice Boston . danced 3/4 time ( like waltzes ) , couple turning approx . 180 degrees every [ [ bar ( music ) | bar ] ] . Jenga popular beer [ [ bar ( establishment ) | bar ] ] s Thailand . sentences added training data supervised system . order Wikipedia way , , necessary map Wikipedia concepts whatever inventory senses relevant WSD application . Auto - matic algorithms map Wikipedia WordNet , example , involve finding WordNet sense greatest lexical overlap Wikipedia sense , comparing vector words WordNet synset , gloss , related senses vector words Wikipedia page title , outgoing links , page category ( Ponzetto Navigli , 2010 ) . resulting mapping create Babel - Net , large sense-annotated resource ( Navigli Ponzetto , 2012 ) . 19.6 • THESAURUSES IMPROVE EMBEDDINGS 15 19.6 Thesauruses Improve Embeddings Thesauruses improve static contextual word em - beddings . example , static word embeddings problem antonyms . word like expensive often similar embedding cosine antonym like cheap . Antonymy information thesauruses help solve problem ; Fig . 19.12 shows nearest neighbors target words GloVe , improve - ment method . counterfitting counterfitting east west north south eastward eastern easterly expensive pricey cheaper costly costly pricy overpriced British American Australian Britain Brits London BBC Figure 19.12 nearest neighbors GloVe east , expensive , British include antonyms like west . right side showing improvement GloVe nearest neighbors counterfitting method ( Mrkšić et al . , 2016 ) . two families solutions . first requires retraining : modify embedding training incorporate thesaurus relations like synonymy , antonym , supersenses . modifying static embedding loss function word2vec ( Yu Dredze 2014 , Nguyen et al . 2016 ) modifying contextual embedding training ( Levine et al . 2019 , Lauscher et al . 2019 ) . second , static embeddings , light-weight ; embeddings trained learn second mapping based thesaurus shifts embeddings words way synonyms ( according thesaurus ) pushed closer antonyms further apart . methods called retrofittingretrofitting ( Faruqui et al . 2015 , Lengerich et al . 2018 ) counterfitting ( Mrkšić et al . , 2016 ) . 19.7 Word Sense Induction expensive difficult build large corpora word labeled word sense . reason , unsupervised approach sense disambiguation , often called word sense induction WSI , important direction . unsu-word senseinduction pervised approaches , human-defined word senses . , set “ senses ” word created automatically instances word training set . algorithms word sense induction follow early work Schütze ( Schütze 1992 , Schütze 1998 ) sort clustering word embed - dings . training , three steps : 1 . token wi word w corpus , compute context vector c . 2 . clustering algorithm cluster word-token context vectors c predefined number groups clusters . cluster defines sense w . 3 . Compute vector centroid cluster . vector centroid sj sense vector representing sense w . unsupervised algorithm , names “ senses ” w ; just refer jth sense w . disambiguate particular token t w again three steps : 16 CHAPTER 19 • WORD SENSES WORDNET 1 . Compute context vector c t . 2 . Retrieve sense vectors s j w . 3 . Assign t sense represented sense vector s j closest t . need clustering algorithm distance metric vectors . Clustering well-studied problem wide number standard algorithms applied inputs structured vectors numerical values ( Duda Hart , 1973 ) . frequently technique language applications known agglom - erative clustering . technique , N training instances initiallyagglomerativeclustering assigned own cluster . New clusters formed bottom-up fashion successive merging two clusters similar . process con - tinues specified number clusters reached , global goodness measure among clusters achieved . cases number training instances makes method expensive , random sampling original training set achieve similar results . evaluate unsupervised sense disambiguation approaches ? usual , best way extrinsic evaluation embedded end-to-end system ; example SemEval bakeoff improve search result clustering di - versification ( Navigli Vannella , 2013 ) . Intrinsic evaluation requires way map automatically derived sense classes hand-labeled gold-standard set compare hand-labeled test set set labeled unsupervised classifier . Various metrics tested , example SemEval tasks ( Manandhar et al . 2010 , Navigli Vannella 2013 , Jurgens Klapaftis 2013 ) , including cluster overlap metrics , methods map sense cluster pre - defined sense choosing sense ( training set ) overlap cluster . fair say evaluation metric task yet become standard . 19.8 Summary chapter covered wide range issues concerning meanings associated lexical items . following among highlights : • word sense locus word meaning ; definitions meaning relations defined level word sense rather wordforms . • Many words polysemous , many senses . • Relations senses include synonymy , antonymy , meronymy , taxonomic relations hyponymy hypernymy . • WordNet large database lexical relations English , WordNets exist variety languages . • Word-sense disambiguation ( WSD ) task determining correct sense word context . Supervised approaches make corpus sentences individual words ( lexical sample task ) words ( all-words task ) hand-labeled senses resource like WordNet . SemCor largest corpus WordNet-labeled senses . • standard supervised algorithm WSD nearest neighbors contex - tual embeddings . • Feature-based algorithms parts speech embeddings words context target word work well . BIBLIOGRAPHICAL HISTORICAL NOTES 17 • important baseline WSD frequent sense , equivalent , WordNet , take first sense . • Another baseline knowledge-based WSD algorithm called Lesk al - gorithm chooses sense whose dictionary definition shares words target word’s neighborhood . • Word sense induction task learning word senses unsupervised . Bibliographical Historical Notes Word sense disambiguation traces roots earliest applications digital computers . insight underlies modern algorithms word sense disambiguation first articulated Weaver ( 1955 ) context machine translation : examines words book , time opaque mask hole word wide , obviously impossible determine , time , meaning words . [ . . . ] lengthens slit opaque mask , central word question say N words side , N large enough unambiguously decide meaning central word . [ . . . ] practical question : “ minimum value N , least tolerable fraction cases , lead correct choice meaning central word ? ” notions first proposed early period include thesaurus dis - ambiguation ( Masterman , 1957 ) , supervised training Bayesian models disam - biguation ( Madhu Lytel , 1965 ) , clustering word sense analysis ( Sparck Jones , 1986 ) . enormous amount work disambiguation conducted con - text early AI-oriented natural language processing systems . Quillian ( 1968 ) Quillian ( 1969 ) proposed graph-based approach language understanding , dictionary definition words represented network word nodes connected syntactic semantic relations . proposed sense disam - biguation finding shortest path senses conceptual graph . Sim - mons ( 1973 ) another influential early semantic network approach . Wilks proposed earliest non-discrete models Preference Semantics ( Wilks 1975c , Wilks 1975b , Wilks 1975a ) , Small Rieger ( 1982 ) Riesbeck ( 1975 ) pro - posed understanding systems based modeling rich procedural information word . Hirst’s ABSITY system ( Hirst Charniak 1982 , Hirst 1987 , Hirst 1988 ) , technique called marker passing based semantic networks , repre - sents advanced system type . largely symbolic ap - proaches , early neural network ( time called ‘ connectionist ’ ) approaches word sense disambiguation relied small lexicons hand-coded representa - tions ( Cottrell 1985 , Kawamoto 1988 ) . earliest implementation robust empirical approach sense disambigua - tion due Kelly Stone ( 1975 ) , directed team hand-crafted set disambiguation rules 1790 ambiguous English words . Lesk ( 1986 ) first machine-readable dictionary word sense disambiguation . collection work concerning WordNet found Fellbaum ( 1998 ) . Early work 18 CHAPTER 19 • WORD SENSES WORDNET dictionaries lexical resources include Amsler’s ( 1981 ) Merriam Web - ster dictionary Longman’s Dictionary Contemporary English ( Boguraev Briscoe , 1989 ) . Supervised approaches disambiguation began decision trees Black ( 1988 ) . addition IMS contextual-embedding based methods supervised WSD , recent supervised algorithms includes encoder-decoder models ( Raganato et al . , 2017a ) . need large amounts annotated text supervised methods led early investigations bootstrapping methods ( Hearst 1991 , Yarow - sky 1995 ) . example Diab Resnik ( 2002 ) give semi-supervised algorithm sense disambiguation based aligned parallel corpora two languages . example , fact French word catastrophe might translated English disaster instance tragedy another instance disambiguate senses two English words ( i.e . , choose senses disaster tragedy similar ) . earliest clustering study word senses Sparck Jones ( 1986 ) ; Pedersen Bruce ( 1997 ) , Schütze ( 1997 ) , Schütze ( 1998 ) applied dis - tributional methods . Clustering word senses coarse senses usedcoarse senses address problem dictionary senses fine-grained ( Section 19.5.3 ) ( Dolan 1994 , Chen Chang 1998 , Mihalcea Moldovan 2001 , Agirre de Lacalle 2003 , Palmer et al . 2004 , Navigli 2006 , Snow et al . 2007 , Pilehvar et al . 2013 ) . Corpora clustered word senses training supervised clustering algorithms include Palmer et al . ( 2006 ) OntoNotes ( Hovy et al . , 2006 ) . OntoNotes Historical overviews WSD include Agirre Edmonds ( 2006 ) Navigli ( 2009 ) . Pustejovsky ( 1995 ) , Pustejovsky Boguraev ( 1996 ) , Martin ( 1986 ) , Copestake Briscoe ( 1995 ) , inter alia , computational approaches rep - resentation polysemy . Pustejovsky’s theory generative lexicon , ingenerativelexicon particular theory qualia structure words , way accounting thequaliastructure dynamic systematic polysemy words context . Exercises 19.1 Collect small corpus example sentences varying lengths newspaper magazine . WordNet standard dictionary , deter - mine many senses open-class words sen - tence . many distinct combinations senses sentence ? number seem vary sentence length ? 19.2 WordNet standard reference dictionary , tag open-class word corpus correct tag . choosing correct sense always straightforward task ? Report difficulties encountered . 19.3 favorite dictionary , simulate original Lesk word overlap dis - ambiguation algorithm described page 13 phrase Time flies like arrow . Assume words disambiguated time , left right , results earlier decisions later process . 19.4 Build implementation solution previous exercise . EXERCISES 19 WordNet , implement original Lesk word overlap disambiguation algo - rithm described page ? ? phrase Time flies like arrow . 20 Chapter 19 • Word Senses WordNet Agirre , E . de Lacalle , O . L . ( 2003 ) . Clustering WordNet word senses . RANLP 2003 . Agirre , E . Edmonds , P . ( Eds . ) . ( 2006 ) . Word Sense Dis - ambiguation : Algorithms Applications . Kluwer . Amsler , R . . ( 1981 ) . taxonomy English nouns verbs . ACL-81 , 133 – 138 . Basile , P . , Caputo , . , Semeraro , G . ( 2014 ) . en - hanced Lesk word sense disambiguation algorithm distributional semantic model . COLING-14 , 1591 – 1600 . Black , E . ( 1988 ) . experiment computational discrim - ination English word senses . IBM Journal Research Development , 32 ( 2 ) , 185 – 194 . Boguraev , B . K . Briscoe , T . ( Eds . ) . ( 1989 ) . Compu - tational Lexicography Natural Language Processing . Longman . Chaplot , D . S . Salakhutdinov , R . ( 2018 ) . Knowledge - based word sense disambiguation topic models . AAAI-18 . Chen , J . N . Chang , J . S . ( 1998 ) . Topical clustering MRD senses based information retrieval techniques . Computational Linguistics , 24 ( 1 ) , 61 – 96 . Ciaramita , M . Altun , Y . ( 2006 ) . Broad-coverage sense disambiguation information extraction super - sense sequence tagger . EMNLP 2006 , 594 – 602 . Ciaramita , M . Johnson , M . ( 2003 ) . Supersense tagging unknown nouns WordNet . EMNLP-2003 , 168 – 175 . Copestake , . Briscoe , T . ( 1995 ) . Semi-productive pol - ysemy sense extension . Journal Semantics , 12 ( 1 ) , 15 – 68 . Cottrell , G . W . ( 1985 ) . Connectionist Approach Word Sense Disambiguation . Ph . D . thesis , University Rochester , Rochester , NY . Revised version published Pitman , 1989 . Diab , M . Resnik , P . ( 2002 ) . unsupervised method word sense tagging parallel corpora . ACL-02 , 255 – 262 . Dolan , B . ( 1994 ) . Word sense ambiguation : Clustering re - lated senses . COLING-94 , 712 – 716 . Duda , R . O . Hart , P . E . ( 1973 ) . Pattern Classification Scene Analysis . John Wiley Sons . Faruqui , M . , Dodge , J . , Jauhar , S . K . , Dyer , C . , Hovy , E . , Smith , N . . ( 2015 ) . Retrofitting word vectors semantic lexicons . NAACL HLT 2015 , 1606 – 1615 . Fellbaum , C . ( Ed . ) . ( 1998 ) . WordNet : Electronic Lexical Database . MIT Press . Gale , W . . , Church , K . W . , Yarowsky , D . ( 1992a ) . Es - timating upper lower bounds performance word-sense disambiguation programs . ACL-92 , 249 – 256 . Gale , W . . , Church , K . W . , Yarowsky , D . ( 1992b ) . sense per discourse . Proceedings DARPA Speech Natural Language Workshop , 233 – 237 . Hearst , M . . ( 1991 ) . Noun homograph disambiguation . Proceedings 7th Conference University Wa - terloo Centre New OED Text Research , 1 – 19 . Henrich , V . , Hinrichs , E . , Vodolazova , T . ( 2012 ) . - bCAGe – web-harvested corpus annotated Ger - maNet senses . EACL-12 , 387 – 396 . Hirst , G . ( 1987 ) . Semantic Interpretation Resolution Ambiguity . Cambridge University Press . Hirst , G . ( 1988 ) . Resolving lexical ambiguity computa - tionally spreading activation polaroid words . Small , S . L . , Cottrell , G . W . , Tanenhaus , M . K . ( Eds . ) , Lexical Ambiguity Resolution , 73 – 108 . Morgan Kauf - mann . Hirst , G . Charniak , E . ( 1982 ) . Word sense case slot disambiguation . AAAI-82 , 95 – 98 . Hovy , E . H . , Marcus , M . P . , Palmer , M . , Ramshaw , L . . , Weischedel , R . ( 2006 ) . Ontonotes : 90 % solution . HLT-NAACL-06 . Iacobacci , . , Pilehvar , M . T . , Navigli , R . ( 2016 ) . Embed - dings word sense disambiguation : evaluation study . ACL 2016 , 897 – 907 . Jurgens , D . Klapaftis , . P . ( 2013 ) . Semeval-2013 task 13 : Word sense induction graded non-graded senses . * SEM , 290 – 299 . Kawamoto , . H . ( 1988 ) . Distributed representations - biguous words resolution connectionist net - works . Small , S . L . , Cottrell , G . W . , Tanenhaus , M . ( Eds . ) , Lexical Ambiguity Resolution , 195 – 228 . Mor - gan Kaufman . Kelly , E . F . Stone , P . J . ( 1975 ) . Computer Recognition English Word Senses . North-Holland . Kilgarriff , . Rosenzweig , J . ( 2000 ) . Framework results English SENSEVAL . Computers Hu - manities , 34 , 15 – 48 . Kumar , S . , Jat , S . , Saxena , K . , Talukdar , P . ( 2019 ) . Zero - shot word sense disambiguation sense definition em - beddings . ACL 2019 , 5670 – 5681 . Landes , S . , Leacock , C . , Tengi , R . . ( 1998 ) . Building semantic concordances . Fellbaum , C . ( Ed . ) , WordNet : Electronic Lexical Database , 199 – 216 . MIT Press . Lauscher , . , Vulić , . , Ponti , E . M . , Korhonen , . , Glavaš , G . ( 2019 ) . Informing unsupervised pretrain - ing external linguistic knowledge . arXiv preprint arXiv : 1909.02339 . Lengerich , B . , Maas , . , Potts , C . ( 2018 ) . Retrofitting distributional embeddings knowledge graphs func - tional relations . Proceedings 27th International Conference Computational Linguistics , 2423 – 2436 . Lesk , M . E . ( 1986 ) . Automatic sense disambiguation - ing machine readable dictionaries : tell pine cone ice cream cone . Proceedings 5th Interna - tional Conference Systems Documentation , 24 – 26 . Levine , Y . , Lenz , B . , Dagan , O . , Padnos , D . , Sharir , O . , Shalev-Shwartz , S . , Shashua , . , Shoham , Y . ( 2019 ) . Sensebert : Driving sense bert . arXiv preprint arXiv : 1908.05646 . Loureiro , D . Jorge , . ( 2019 ) . Language modelling makes sense : Propagating representations Word - Net full-coverage word sense disambiguation . ACL 2019 , 5682 – 5691 . Luo , F . , Liu , T . , , Z . , Xia , Q . , Sui , Z . , Chang , B . ( 2018a ) . Leveraging gloss knowledge neural word sense disambiguation hierarchical co-attention . EMNLP 2018 , 1402 – 1411 . Exercises 21 Luo , F . , Liu , T . , Xia , Q . , Chang , B . , Sui , Z . ( 2018b ) . - corporating glosses neural word sense disambiguation . ACL 2018 , 2473 – 2482 . Madhu , S . Lytel , D . ( 1965 ) . figure merit technique resolution non-grammatical ambiguity . Mechan - ical Translation , 8 ( 2 ) , 9 – 13 . Manandhar , S . , Klapaftis , . P . , Dligach , D . , Pradhan , S . ( 2010 ) . Semeval-2010 task 14 : Word sense induction & disambiguation . SemEval-2010 , 63 – 68 . Martin , J . H . ( 1986 ) . acquisition polysemy . ICML 1986 , 198 – 204 . Masterman , M . ( 1957 ) . thesaurus syntax seman - tics . Mechanical Translation , 4 ( 1 ) , 1 – 2 . Melamud , O . , Goldberger , J . , Dagan , . ( 2016 ) . con - text2vec : Learning generic context embedding bidi - rectional LSTM . CoNLL-16 , 51 – 61 . Mihalcea , R . ( 2007 ) . wikipedia automatic word sense disambiguation . NAACL-HLT 07 , 196 – 203 . Mihalcea , R . Moldovan , D . ( 2001 ) . Automatic genera - tion coarse grained WordNet . NAACL Workshop WordNet Lexical Resources . Miller , G . . , Leacock , C . , Tengi , R . . , Bunker , R . T . ( 1993 ) . semantic concordance . Proceedings ARPA Workshop Human Language Technology , 303 – 308 . Morris , W . ( Ed . ) . ( 1985 ) . American Heritage Dictionary ( 2nd College Edition Ed . ) . Houghton Mifflin . Mrkšić , N . , Séaghdha , D . Ó . , Thomson , B . , Gašić , M . , Rojas-Barahona , L . M . , Su , P . - H . , Vandyke , D . , Wen , T . - H . , Young , S . ( 2016 ) . Counter-fitting word vectors linguistic constraints . NAACL HLT 2016 , 142 – 148 . Navigli , R . ( 2006 ) . Meaningful clustering senses helps boost word sense disambiguation performance . COL - ING / ACL 2006 , 105 – 112 . Navigli , R . ( 2009 ) . Word sense disambiguation : survey . ACM Computing Surveys , 41 ( 2 ) . Navigli , R . ( 2016 ) . Chapter 20 . ontologies . Mitkov , R . ( Ed . ) , Oxford handbook computational linguistics . Oxford University Press . Navigli , R . Ponzetto , S . P . ( 2012 ) . BabelNet : au - tomatic construction , evaluation application wide - coverage multilingual semantic network . Artificial Intelli - gence , 193 , 217 – 250 . Navigli , R . Vannella , D . ( 2013 ) . Semeval-2013 task 11 : Word sense induction & disambiguation end-user application . * SEM , 193 – 201 . Nguyen , K . . , Schulte Walde , S . , Vu , N . T . ( 2016 ) . Integrating distributional lexical contrast word embed - dings antonym-synonym distinction . ACL 2016 , 454 – 459 . Palmer , M . , Babko-Malaya , O . , Dang , H . T . ( 2004 ) . Dif - ferent sense granularities different applications . HLT - NAACL Workshop Scalable Natural Language - standing , 49 – 56 . Palmer , M . , Dang , H . T . , Fellbaum , C . ( 2006 ) . Mak - ing fine-grained coarse-grained sense distinctions , manually automatically . Natural Language Engineer - ing , 13 ( 2 ) , 137 – 163 . Pedersen , T . Bruce , R . ( 1997 ) . Distinguishing word senses untagged text . EMNLP 1997 . Peters , M . , Neumann , M . , Iyyer , M . , Gardner , M . , Clark , C . , Lee , K . , Zettlemoyer , L . ( 2018 ) . Deep contextualized word representations . NAACL HLT 2018 , 2227 – 2237 . Pilehvar , M . T . Camacho-Collados , J . ( 2019 ) . WiC : word-in-context dataset evaluating context-sensitive meaning representations . NAACL HLT 2019 , 1267 – 1273 . Pilehvar , M . T . , Jurgens , D . , Navigli , R . ( 2013 ) . Align , disambiguate walk : unified approach measuring semantic similarity . ACL 2013 , 1341 – 1351 . Ponzetto , S . P . Navigli , R . ( 2010 ) . Knowledge-rich word sense disambiguation rivaling supervised systems . ACL 2010 , 1522 – 1531 . Pu , X . , Pappas , N . , Henderson , J . , Popescu-Belis , . ( 2018 ) . Integrating weakly supervised word sense disam - biguation neural machine translation . TACL , 6 , 635 – 649 . Pustejovsky , J . ( 1995 ) . Generative Lexicon . MIT Press . Pustejovsky , J . Boguraev , B . K . ( Eds . ) . ( 1996 ) . Lexical Semantics : Problem Polysemy . Oxford University Press . Quillian , M . R . ( 1968 ) . Semantic memory . Minsky , M . ( Ed . ) , Semantic Information Processing , 227 – 270 . MIT Press . Quillian , M . R . ( 1969 ) . teachable language comprehen - der : simulation program theory language . CACM , 12 ( 8 ) , 459 – 476 . Raganato , . , Bovi , C . D . , Navigli , R . ( 2017a ) . Neural sequence learning models word sense disambiguation . EMNLP 2017 , 1156 – 1167 . Raganato , . , Camacho-Collados , J . , Navigli , R . ( 2017b ) . Word sense disambiguation : unified evalua - tion framework empirical comparison . EACL-17 , 99 – 110 . Riesbeck , C . K . ( 1975 ) . Conceptual analysis . Schank , R . C . ( Ed . ) , Conceptual Information Processing , 83 – 156 . American Elsevier , New York . Schneider , N . , Hwang , J . D . , Srikumar , V . , Prange , J . , Blod - gett , . , Moeller , S . R . , Stern , . , Bitan , . , Abend , O . ( 2018 ) . Comprehensive supersense disambiguation English prepositions possessives . ACL 2018 , 185 – 196 . Schütze , H . ( 1992 ) . Dimensions meaning . Proceedings Supercomputing ’ 92 , 787 – 796 . IEEE Press . Schütze , H . ( 1997 ) . Ambiguity Resolution Language Learning : Computational Cognitive Models . CSLI Publications , Stanford , CA . Schütze , H . ( 1998 ) . Automatic word sense discrimination . Computational Linguistics , 24 ( 1 ) , 97 – 124 . Simmons , R . F . ( 1973 ) . Semantic networks : com - putation understanding English sentences . Schank , R . C . Colby , K . M . ( Eds . ) , Computer Models Thought Language , 61 – 113 . W.H . Freeman Co . Small , S . L . Rieger , C . ( 1982 ) . Parsing compre - hending Word Experts . Lehnert , W . G . Ringle , M . H . ( Eds . ) , Strategies Natural Language Processing , 89 – 147 . Lawrence Erlbaum . Snow , R . , Prakash , S . , Jurafsky , D . , Ng , . Y . ( 2007 ) . Learning merge word senses . EMNLP / CoNLL 2007 , 1005 – 1014 . 22 Chapter 19 • Word Senses WordNet Snyder , B . Palmer , M . ( 2004 ) . English all-words task . SENSEVAL-3 , 41 – 43 . Sparck Jones , K . ( 1986 ) . Synonymy Semantic Classifi - cation . Edinburgh University Press , Edinburgh . Republi - cation 1964 PhD Thesis . Tsvetkov , Y . , Schneider , N . , Hovy , D . , Bhatia , . , Faruqui , M . , Dyer , C . ( 2014 ) . Augmenting English adjective senses supersenses . LREC-14 . Vossen , P . , Görög , . , Laan , F . , Van Gompel , M . , Izquierdo , R . , Van Den Bosch , . ( 2011 ) . Dutch-semcor : building semantically annotated corpus dutch . Proceedings eLex , 286 – 296 . Weaver , W . ( 1949/1955 ) . Translation . Locke , W . N . Boothe , . D . ( Eds . ) , Machine Translation Languages , 15 – 23 . MIT Press . Reprinted memorandum written Weaver 1949 . Wilks , Y . ( 1975a ) . intelligent analyzer understander English . CACM , 18 ( 5 ) , 264 – 274 . Wilks , Y . ( 1975b ) . Preference semantics . Keenan , E . L . ( Ed . ) , Formal Semantics Natural Language , 329 – 350 . Cambridge Univ . Press . Wilks , Y . ( 1975c ) . preferential , pattern-seeking , semantics natural language inference . Artificial Intelligence , 6 ( 1 ) , 53 – 74 . Yarowsky , D . ( 1995 ) . Unsupervised word sense disambigua - tion rivaling supervised methods . ACL-95 , 189 – 196 . Yu , M . Dredze , M . ( 2014 ) . Improving lexical embed - dings semantic knowledge . ACL 2014 , 545 – 550 . Zhong , Z . Ng , H . T . ( 2010 ) . makes sense : wide - coverage word sense disambiguation system free text . ACL 2010 , 78 – 83 .