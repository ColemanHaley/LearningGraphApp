Appendix A Probability Probability theory provides a way to reason about random events . The sorts of random events that are typically used to explain probability theory include coin flips , card draws , and the weather . It may seem odd to think about the choice of a word as akin to the flip of a coin , particularly if you are the type of person to choose words carefully . But random or not , language has proven to be extremely difficult to model deterministically . Probability offers a powerful tool for modeling and manipulating linguistic data . Probability can be thought of in terms of random outcomes : for example , a single coin flip has two possible outcomes , heads or tails . The set of possible outcomes is the sample space , and a subset of the sample space is an event . For a sequence of two coin flips , there are four possible outcomes , { HH , HT , TH , TT } , representing the ordered sequences heads-head , heads-tails , tails-heads , and tails-tails . The event of getting exactly one head includes two outcomes : { HT , TH } . Formally , a probability is a function from events to the interval between zero and one : Pr : F → [ 0 , 1 ] , where F is the set of possible events . An event that is certain has proba - bility one ; an event that is impossible has probability zero . For example , the probability of getting fewer than three heads on two coin flips is one . Each outcome is also an event ( a set with exactly one element ) , and for two flips of a fair coin , the probability of each outcome is , Pr ( { HH } ) = Pr ( { HT } ) = Pr ( { TH } ) = Pr ( { TT } ) = 1 4 . [ A . 1 ] A . 1 Probabilities of event combinations Because events are sets of outcomes , we can use set-theoretic operations such as comple - ment , intersection , and union to reason about the probabilities of events and their combi - nations . 475 476 APPENDIX A . PROBABILITY For any event A , there is a complement ¬ A , such that : • The probability of the union A ∪ ¬ A is Pr ( A ∪ ¬ A ) = 1 ; • The intersection A ∩ ¬ A = ∅ is the empty set , and Pr ( A ∩ ¬ A ) = 0 . In the coin flip example , the event of obtaining a single head on two flips corresponds to the set of outcomes { HT , TH } ; the complement event includes the other two outcomes , { TT , HH } . A . 1.1 Probabilities of disjoint events When two events have an empty intersection , A ∩ B = ∅ , they are disjoint . The probabil - ity of the union of two disjoint events is equal to the sum of their probabilities , A ∩ B = ∅ ⇒ Pr ( A ∪ B ) = Pr ( A ) + Pr ( B ) . [ A . 2 ] This is the third axiom of probability , and it can be generalized to any countable sequence of disjoint events . In the coin flip example , this axiom can derive the probability of the event of getting a single head on two flips . This event is the set of outcomes { HT , TH } , which is the union of two simpler events , { HT , TH } = { HT } ∪ { TH } . The events { HT } and { TH } are disjoint . Therefore , Pr ( { HT , TH } ) = Pr ( { HT } ∪ { TH } ) = Pr ( { HT } ) + Pr ( { TH } ) [ A . 3 ] = 1 4 + 1 4 = 1 2 . [ A . 4 ] In the general , the probability of the union of two events is , Pr ( A ∪ B ) = Pr ( A ) + Pr ( B ) − Pr ( A ∩ B ) . [ A . 5 ] This can be seen visually in Figure A . 1 , and it can be derived from the third axiom of probability . Consider an event that includes all outcomes in B that are not in A , denoted as B − ( A ∩ B ) . By construction , this event is disjoint from A . We can therefore apply the additive rule , Pr ( A ∪ B ) = Pr ( A ) + Pr ( B − ( A ∩ B ) ) . [ A . 6 ] Furthermore , the event B is the union of two disjoint events : A ∩ B and B − ( A ∩ B ) . Pr ( B ) = Pr ( B − ( A ∩ B ) ) + Pr ( A ∩ B ) . [ A . 7 ] Reorganizing and subtituting into Equation A . 6 gives the desired result : Pr ( B − ( A ∩ B ) ) = Pr ( B ) − Pr ( A ∩ B ) [ A . 8 ] Pr ( A ∪ B ) = Pr ( A ) + Pr ( B ) − Pr ( A ∩ B ) . [ A . 9 ] Jacob Eisenstein . Draft of October 15 , 2018 . A . 2 . CONDITIONAL PROBABILITY AND BAYES ’ RULE 477 A BA ∩ B Figure A . 1 : A visualization of the probability of non-disjoint events A and B . A . 1.2 Law of total probability A set of events B = { B1 , B2 , . . . , BN } is a partition of the sample space iff each pair of events is disjoint ( Bi ∩ Bj = ∅ ) , and the union of the events is the entire sample space . The law of total probability states that we can marginalize over these events as follows , Pr ( A ) = ∑ Bn ∈ B Pr ( A ∩ Bn ) . [ A . 10 ] For any event B , the union B ∪ ¬ B is a partition of the sample space . Therefore , a special case of the law of total probability is , Pr ( A ) = Pr ( A ∩ B ) + Pr ( A ∩ ¬ B ) . [ A . 11 ] A . 2 Conditional probability and Bayes ’ rule A conditional probability is an expression like Pr ( A | B ) , which is the probability of the event A , assuming that event B happens too . For example , we may be interested in the probability of a randomly selected person answering the phone by saying hello , conditioned on that person being a speaker of English . Conditional probability is defined as the ratio , Pr ( A | B ) = Pr ( A ∩ B ) Pr ( B ) . [ A . 12 ] The chain rule of probability states that Pr ( A ∩ B ) = Pr ( A | B ) × Pr ( B ) , which is just Under contract with MIT Press , shared under CC-BY-NC-ND license . 478 APPENDIX A . PROBABILITY a rearrangement of terms from Equation A . 12 . The chain rule can be applied repeatedly : Pr ( A ∩ B ∩ C ) = Pr ( A | B ∩ C ) × Pr ( B ∩ C ) = Pr ( A | B ∩ C ) × Pr ( B | C ) × Pr ( C ) . Bayes ’ rule ( sometimes called Bayes ’ law or Bayes ’ theorem ) gives us a way to convert between Pr ( A | B ) and Pr ( B | A ) . It follows from the definition of conditional probability and the chain rule : Pr ( A | B ) = Pr ( A ∩ B ) Pr ( B ) = Pr ( B | A ) × Pr ( A ) Pr ( B ) [ A . 13 ] Each term in Bayes rule has a name , which we will occasionally use : • Pr ( A ) is the prior , since it is the probability of event A without knowledge about whether B happens or not . • Pr ( B | A ) is the likelihood , the probability of event B given that event A has oc - curred . • Pr ( A | B ) is the posterior , the probability of event A with knowledge that B has occurred . Example The classic examples for Bayes ’ rule involve tests for rare diseases , but Man - ning and Schütze ( 1999 ) reframe this example in a linguistic setting . Suppose that you are is interested in a rare syntactic construction , such as parasitic gaps , which occur on average once in 100,000 sentences . Here is an example of a parasitic gap : ( A . 1 ) Which class did you attend without registering for ? Lana Linguist has developed a complicated pattern matcher that attempts to identify sentences with parasitic gaps . It’s pretty good , but it’s not perfect : • If a sentence has a parasitic gap , the pattern matcher will find it with probability 0.95 . ( This is the recall , which is one minus the false negative rate . ) • If the sentence doesn’t have a parasitic gap , the pattern matcher will wrongly say it does with probability 0.005 . ( This is the false positive rate , which is one minus the precision . ) Suppose that Lana’s pattern matcher says that a sentence contains a parasitic gap . What is the probability that this is true ? Jacob Eisenstein . Draft of October 15 , 2018 . A . 3 . INDEPENDENCE 479 Let G be the event of a sentence having a parasitic gap , and T be the event of the test being positive . We are interested in the probability of a sentence having a parasitic gap given that the test is positive . This is the conditional probability Pr ( G | T ) , and it can be computed by Bayes ’ rule : Pr ( G | T ) = Pr ( T | G ) × Pr ( G ) Pr ( T ) . [ A . 14 ] We already know both terms in the numerator : Pr ( T | G ) is the recall , which is 0.95 ; Pr ( G ) is the prior , which is 10 − 5 . We are not given the denominator , but it can be computed using tools developed ear - lier in this section . First apply the law of total probability , using the partition { G , ¬ G } : Pr ( T ) = Pr ( T ∩ G ) + Pr ( T ∩ ¬ G ) . [ A . 15 ] This says that the probability of the test being positive is the sum of the probability of a true positive ( T ∩ G ) and the probability of a false positive ( T ∩ ¬ G ) . The probability of each of these events can be computed using the chain rule : Pr ( T ∩ G ) = Pr ( T | G ) × Pr ( G ) = 0.95 × 10 − 5 [ A . 16 ] Pr ( T ∩ ¬ G ) = Pr ( T | ¬ G ) × Pr ( ¬ G ) = 0.005 × ( 1 − 10 − 5 ) ≈ 0.005 [ A . 17 ] Pr ( T ) = Pr ( T ∩ G ) + Pr ( T ∩ ¬ G ) [ A . 18 ] = 0.95 × 10 − 5 + 0.005 . [ A . 19 ] Plugging these terms into Bayes ’ rule gives the desired posterior probability , Pr ( G | T ) = Pr ( T | G ) Pr ( G ) Pr ( T ) [ A . 20 ] = 0.95 × 10 − 5 0.95 × 10 − 5 + 0.005 × ( 1 − 10 − 5 ) [ A . 21 ] ≈ 0.002 . [ A . 22 ] Lana’s pattern matcher seems accurate , with false positive and false negative rates below 5 % . Yet the extreme rarity of the phenomenon means that a positive result from the detector is most likely to be wrong . A . 3 Independence Two events are independent if the probability of their intersection is equal to the product of their probabilities : Pr ( A ∩ B ) = Pr ( A ) × Pr ( B ) . For example , for two flips of a fair Under contract with MIT Press , shared under CC-BY-NC-ND license . 480 APPENDIX A . PROBABILITY coin , the probability of getting heads on the first flip is independent of the probability of getting heads on the second flip : Pr ( { HT , HH } ) = Pr ( HT ) + Pr ( HH ) = 1 4 + 1 4 = 1 2 [ A . 23 ] Pr ( { HH , TH } ) = Pr ( HH ) + Pr ( TH ) = 1 4 + 1 4 = 1 2 [ A . 24 ] Pr ( { HT , HH } ) × Pr ( { HH , TH } ) = 1 2 × 1 2 = 1 4 [ A . 25 ] Pr ( { HT , HH } ∩ { HH , TH } ) = Pr ( HH ) = 1 4 [ A . 26 ] = Pr ( { HT , HH } ) × Pr ( { HH , TH } ) . [ A . 27 ] If Pr ( A ∩ B | C ) = Pr ( A | C ) × Pr ( B | C ) , then the events A and B are conditionally independent , written A ⊥ B | C . Conditional independence plays a important role in probabilistic models such as Naı̈ve Bayes chapter 2 . A . 4 Random variables Random variables are functions from events to Rn , where R is the set of real numbers . This subsumes several useful special cases : • An indicator random variable is a function from events to the set { 0 , 1 } . In the coin flip example , we can define Y as an indicator random variable , taking the value 1 when the coin has come up heads on at least one flip . This would include the outcomes { HH , HT , TH } . The probability Pr ( Y = 1 ) is the sum of the probabilities of these outcomes , Pr ( Y = 1 ) = 14 + 1 4 + 1 4 = 3 4 . • A discrete random variable is a function from events to a discrete subset of R . Con - sider the coin flip example : the number of heads on two flips , X , can be viewed as a discrete random variable , X ∈ 0 , 1 , 2 . The event probability Pr ( X = 1 ) can again be computed as the sum of the probabilities of the events in which there is one head , { HT , TH } , giving Pr ( X = 1 ) = 14 + 14 = 12 . Each possible value of a random variable is associated with a subset of the sample space . In the coin flip example , X = 0 is associated with the event { TT } , X = 1 is associated with the event { HT , TH } , and X = 2 is associated with the event { HH } . Assuming a fair coin , the probabilities of these events are , respectively , 1/4 , 1/2 , and 1/4 . This list of numbers represents the probability distribution over X , written pX , which maps from the possible values of X to the non-negative reals . For a specific value x , we write pX ( x ) , which is equal to the event probability Pr ( X = x ) . 1 The function pX is called 1In general , capital letters ( e.g . , X ) refer to random variables , and lower-case letters ( e.g . , x ) refer to specific values . When the distribution is clear from context , I will simply write p ( x ) . Jacob Eisenstein . Draft of October 15 , 2018 . A . 5 . EXPECTATIONS 481 a probability mass function ( pmf ) ifX is discrete ; it is called a probability density function ( pdf ) if X is continuous . In either case , the function must sum to one , and all values must be non-negative : ∫ x pX ( x ) dx = 1 [ A . 28 ] ∀ x , pX ( x ) ≥ 0 . [ A . 29 ] Probabilities over multiple random variables can written as joint probabilities , e.g . , pA , B ( a , b ) = Pr ( A = a ∩ B = b ) . Several properties of event probabilities carry over to probability distributions over random variables : • The marginal probability distribution is pA ( a ) = ∑ b pA , B ( a , b ) . • The conditional probability distribution is pA | B ( a | b ) = pA , B ( a , b ) pB ( b ) . • Random variables A and B are independent iff pA , B ( a , b ) = pA ( a ) × pB ( b ) . A . 5 Expectations Sometimes we want the expectation of a function , such as E [ g ( x ) ] = ∑ x ∈ X g ( x ) p ( x ) . Expectations are easiest to think about in terms of probability distributions over discrete events : • If it is sunny , Lucia will eat three ice creams . • If it is rainy , she will eat only one ice cream . • There’s a 80 % chance it will be sunny . • The expected number of ice creams she will eat is 0.8 × 3 + 0.2 × 1 = 2.6 . If the random variable X is continuous , the expectation is an integral : E [ g ( x ) ] = ∫ X g ( x ) p ( x ) dx [ A . 30 ] For example , a fast food restaurant in Quebec has a special offer for cold days : they give a 1 % discount on poutine for every degree below zero . Assuming a thermometer with infinite precision , the expected price would be an integral over all possible temperatures , E [ price ( x ) ] = ∫ X min ( 1 , 1 + x ) × original-price × p ( x ) dx . [ A . 31 ] Under contract with MIT Press , shared under CC-BY-NC-ND license . 482 APPENDIX A . PROBABILITY A . 6 Modeling and estimation Probabilistic models provide a principled way to reason about random events and ran - dom variables . Let’s consider the coin toss example . Each toss can be modeled as a ran - dom event , with probability θ of the event H , and probability 1 − θ of the complementary event T . If we write a random variable X as the total number of heads on three coin flips , then the distribution of X depends on θ . In this case , X is distributed as a binomial random variable , meaning that it is drawn from a binomial distribution , with parameters ( θ , N = 3 ) . This is written , X ∼ Binomial ( θ , N = 3 ) . [ A . 32 ] The properties of the binomial distribution enable us to make statements about the X , such as its expected value and the likelihood that its value will fall within some interval . Now suppose that θ is unknown , but we have run an experiment , in which we exe - cuted N trials , and obtained x heads . We can estimate θ by the principle of maximum likelihood : θ̂ = argmax θ pX ( x ; θ , N ) . [ A . 33 ] This says that the estimate θ̂ should be the value that maximizes the likelihood of the data . The semicolon indicates that θ and N are parameters of the probability function . The likelihood pX ( x ; θ , N ) can be computed from the binomial distribution , pX ( x ; θ , N ) = N ! x ! ( N − x ) ! θ x ( 1 − θ ) N − x . [ A . 34 ] This likelihood is proportional to the product of the probability of individual out - comes : for example , the sequence T , H , H , T , H would have probability θ3 ( 1 − θ ) 2 . The term N ! x ! ( N − x ) ! arises from the many possible orderings by which we could obtain x heads on N trials . This term does not depend on θ , so it can be ignored during estimation . In practice , we maximize the log-likelihood , which is a monotonic function of the like - lihood . Under the binomial distribution , the log-likelihood is a convex function of θ ( see Jacob Eisenstein . Draft of October 15 , 2018 . A . 6 . MODELING AND ESTIMATION 483 section 2.4 ) , so it can be maximized by taking the derivative and setting it equal to zero . ` ( θ ) = x log θ + ( N − x ) log ( 1 − θ ) [ A . 35 ] ∂ ` ( θ ) ∂ θ = x θ − N − x 1 − θ [ A . 36 ] N − x 1 − θ = x θ [ A . 37 ] N − x x = 1 − θ θ [ A . 38 ] N x − 1 = 1 θ − 1 [ A . 39 ] θ̂ = x N . [ A . 40 ] In this case , the maximum likelihood estimate is equal to xN , the fraction of trials that came up heads . This intuitive solution is also known as the relative frequency estimate , since it is equal to the relative frequency of the outcome . Is maximum likelihood estimation always the right choice ? Suppose you conduct one trial , and get heads . Would you conclude that θ = 1 , meaning that the coin is guaran - teed to come up heads ? If not , then you must have some prior expectation about θ . To incorporate this prior information , we can treat θ as a random variable , and use Bayes ’ rule : p ( θ | x ; N ) =p ( x | θ ) × p ( θ ) p ( x ) [ A . 41 ] ∝ p ( x | θ ) × p ( θ ) [ A . 42 ] θ̂ = argmax θ p ( x | θ ) × p ( θ ) . [ A . 43 ] This it the maximum a posteriori ( MAP ) estimate . Given a form for p ( θ ) , you can de - rive the MAP estimate using the same approach that was used to derive the maximum likelihood estimate . Additional resources A good introduction to probability theory is offered by Manning and Schütze ( 1999 ) , which helped to motivate this section . For more detail , Sharon Goldwater provides an - other useful reference , http://homepages.inf.ed.ac.uk/sgwater/teaching/general/ probability . pdf . A historical and philosophical perspective on probability is offered by Diaconis and Skyrms ( 2017 ) . Under contract with MIT Press , shared under CC-BY-NC-ND license .