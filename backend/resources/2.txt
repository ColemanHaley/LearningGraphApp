Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 2 Regular Expressions , TextNormalization , Edit Distance User : unhappy . ELIZA : THINK COMING HELP UNHAPPY User : need help , seems certain . ELIZA : MEAN GOT HELP User : Perhaps learn get mother . ELIZA : TELL FAMILY User : mother takes care . ELIZA : ELSE FAMILY TAKES CARE User : father . ELIZA : FATHER User : like father ways . Weizenbaum ( 1966 ) dialogue ELIZA , early natural language processing systemELIZA carry limited conversation user imitating responses Rogerian psychotherapist ( Weizenbaum , 1966 ) . ELIZA surprisingly simple program pattern matching recognize phrases like “ need X ” translate suitable outputs like “ mean got X ? ” . simple technique succeeds domain ELIZA actually need know anything mimic Rogerian psychotherapist . Weizenbaum notes , few dialogue genres listeners act know nothing world . Eliza’s mimicry human conversation remarkably successful : many people interacted ELIZA came believe really understood problems , many continued believe ELIZA’s abilities even program’s operation explained ( Weizenbaum , 1976 ) , even today chatbots fun diversion . chatbots course modern conversational agents diversion ; answer questions , book flights , find restaurants , functions rely sophisticated understanding user’s intent , Chapter 26 . Nonetheless , simple pattern-based methods powered ELIZA chatbots play crucial role natural language processing . begin important tool describing text patterns : regular expression . Regular expressions specify strings might extract document , transforming “ need X ” Eliza , defining strings like $ 199 $ 24.99 extracting tables prices document . turn set tasks collectively called text normalization , whichtextnormalization regular expressions play important part . Normalizing text means converting convenient , standard form . example , going language relies first separating tokenizing words running text , task tokenization . English words often separated othertokenization whitespace , whitespace always sufficient . New York rock ’ n ’ roll sometimes treated large words despite fact contain spaces , sometimes need separate two words . processing tweets texts need tokenize emoticons like :) hashtags like #nlproc . 2 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE languages , like Japanese , spaces words , word tokeniza - tion becomes difficult . Another part text normalization lemmatization , task determininglemmatization two words same root , despite surface differences . example , words sang , sung , sings forms verb sing . word sing common lemma words , lemmatizer maps sing . Lemmatization essential processing morphologically complex languages like Arabic . Stemming refers simpler version lemmatization mainlystemming just strip suffixes end word . Text normalization includes sen - tence segmentation : breaking up text individual sentences , cues likesentencesegmentation periods exclamation points . Finally , need compare words strings . introduce metric called edit distance measures similar two strings based number edits ( insertions , deletions , substitutions ) takes change string . Edit distance algorithm applications throughout language process - ing , spelling correction speech recognition coreference resolution . 2.1 Regular Expressions unsung successes standardization computer science regular expression ( RE ) , language specifying text search strings . prac-regularexpression tical language every computer language , word processor , text pro - cessing tools like Unix tools grep Emacs . Formally , regular expression algebraic notation characterizing set strings . particularly - ful searching texts , pattern search corpus ofcorpus texts search . regular expression search function search corpus , returning texts match pattern . corpus single docu - ment collection . example , Unix command-line tool grep takes regular expression returns every line input document matches expression . search designed return every match line , , just first match . following examples generally underline exact part pattern matches regular expression show first match . show regular expressions delimited slashes note slashes part regular expressions . Regular expressions come many variants . describing extended regu - lar expressions ; different regular expression parsers recognize subsets , treat expressions slightly differently . online regular expres - sion tester handy way test expressions explore variations . 2.1.1 Basic Regular Expression Patterns simplest kind regular expression sequence simple characters . search woodchuck , type / woodchuck / . expression / Buttercup / matches string containing substring Buttercup ; grepwith expression return line called little Buttercup . search string consist single character ( like / ! / ) sequence characters ( like / urgl / ) . Regular expressions case sensitive ; lower case / s / distinct upper case / S / ( / s / matches lower case s upper case S ) . means pattern / woodchucks / match string Woodchucks . solve 2.1 • REGULAR EXPRESSIONS 3 RE Example Patterns Matched / woodchucks / “ interesting links woodchucks lemurs ” / / “ Mary Ann stopped Mona’s ” / ! / “ left burglar behind again ! ” Nori Figure 2.1 simple regex searches . problem square braces [ ] . string characters inside braces specifies disjunction characters match . example , Fig . 2.2 shows pattern / [ wW ] / matches patterns containing w W . RE Match Example Patterns / [ wW ] oodchuck / Woodchuck woodchuck “ Woodchuck ” / [ abc ] / ‘ ’ , ‘ b ’ , ‘ c ’ “ uomini , soldati ” / [ 1234567890 ] / digit “ plenty 7 5 ” Figure 2.2 brackets [ ] specify disjunction characters . regular expression / [ 1234567890 ] / specified single digit . classes characters digits letters important building blocks expressions , get awkward ( e.g . , inconvenient specify / [ ABCDEFGHIJKLMNOPQRSTUVWXYZ ] / mean “ capital letter ” ) . cases well-defined sequence asso - ciated set characters , brackets dash ( - ) specify character range . pattern / [ 2-5 ] / specifies charac-range ters 2 , 3 , 4 , 5 . pattern / [ b-g ] / specifies characters b , c , d , e , f , g . examples shown Fig . 2.3 . RE Match Example Patterns Matched / [ A-Z ] / upper case letter “ call ‘ Drenched Blossoms ’ ” / [ a-z ] / lower case letter “ beans impatient hoed ! ” / [ 0-9 ] / single digit “ Chapter 1 : Down Rabbit Hole ” Figure 2.3 brackets [ ] plus dash - specify range . square braces specify single character , caret ˆ . caret ˆ first symbol open square brace [ , resulting pattern negated . example , pattern / [ ˆa ] / matches single character ( including special characters ) except . true caret first symbol open square brace . occurs anywhere else , usually stands caret ; Fig . 2.4 shows examples . RE Match ( single characters ) Example Patterns Matched / [ ˆA-Z ] / upper case letter “ Oyfn pripetchik ” / [ ˆSs ] / neither ‘ S ’ nor ‘ s ’ “ exquisite reason for’t ” / [ ˆ . ] / period “ resident Djinn ” / [ eˆ ] / ‘ e ’ ‘ ˆ ’ “ look up ˆ ” / aˆb / pattern ‘ aˆb ’ “ look up aˆ b ” Figure 2.4 caret ˆ negation just mean ˆ . below re : backslash escaping period . talk optional elements , like optional s woodchuck woodchucks ? square brackets , allow say “ s S ” , allow say “ s nothing ” . question mark / ? / , means “ preceding character nothing ” , shown Fig . 2.5 . 4 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE RE Match Example Patterns Matched / woodchucks ? / woodchuck woodchucks “ woodchuck ” / colou ? r / color colour “ color ” Figure 2.5 question mark ? marks optionality previous expression . think question mark meaning “ zero instances previous character ” . , way specifying many something , something important regular expressions . example , consider language certain sheep , consists strings look like following : baa ! baaa ! baaaa ! baaaaa ! . . . language consists strings b , followed least two a’s , followed exclamation point . set operators allows say things like “ number ” based asterisk * , commonly called Kleene * ( gen-Kleene * erally pronounced “ cleany star ” ) . Kleene star means “ zero occurrences immediately previous character regular expression ” . / * / means “ string zero ” . match aaaaaa , match Off Minor string Off Minor zero a’s . regular expression matching / aa * / , meaning followed zero . complex patterns repeated . / [ ab ] * / means “ zero a’s b’s ” ( “ zero right square braces ” ) . match strings like aaaa ababab bbbb . specifying multiple digits ( useful finding prices ) extend / [ 0-9 ] / , regular expression single digit . integer ( string digits ) thus / [ 0-9 ] [ 0-9 ] * / . ( Why just / [ 0-9 ] * / ? ) Sometimes annoying write regular expression digits twice , shorter way specify “ least ” character . Kleene + , means “ occurrences immediately precedingKleene + character regular expression ” . Thus , expression / [ 0-9 ] + / normal way specify “ sequence digits ” . thus two ways specify sheep language : / baaa * ! / / baa + ! / . important special character period ( / . / ) , wildcard expression matches single character ( except carriage return ) , shown Fig . 2.6 . RE Match Example Matches / beg.n/ character beg n begin , beg’n , begun Figure 2.6 period . specify character . wildcard often together Kleene star mean “ string characters ” . example , suppose find line particular word , example , aardvark , appears twice . specify regular expression / aardvark . * aardvark / . Anchors special characters anchor regular expressions particular placesAnchors string . common anchors caret ˆ dollar sign $ . caret ˆ matches start line . pattern / ˆThe / matches word 2.1 • REGULAR EXPRESSIONS 5 start line . Thus , caret ˆ three : match start line , - dicate negation inside square brackets , just mean caret . ( contexts allow grep Python know function caret supposed ? ) dollar sign $ matches end line . pattern $ useful pattern matching space end line , / ˆThe dog \ . $ / matches line contains phrase dog . ( backslash . mean “ period ” wildcard . ) two anchors : \ b matches word boundary , \ B matches non-boundary . Thus , / \ bthe \ b / matches word word . technically , “ word ” purposes regular expression defined sequence digits , underscores , letters ; based definition “ words ” programming languages . example , / \ b99 \ b / match string 99 99 bottles beer wall ( 99 follows space ) 99 299 bottles beer wall ( 99 follows number ) . match 99 $ 99 ( 99 follows dollar sign ( $ ) , digit , underscore , letter ) . 2.1.2 Disjunction , Grouping , Precedence Suppose need search texts pets ; perhaps particularly interested cats dogs . case , might search string cat string dog . square brackets search “ cat dog ” ( why say / [ catdog ] / ? ) , need new operator , disjunction operator , alsodisjunction called pipe symbol | . pattern / cat | dog / matches string cat string dog . Sometimes need disjunction operator midst larger se - quence . example , suppose search information pet fish cousin David . specify guppy guppies ? simply say / guppy | ies / , match strings guppy ies . sequences like guppy take precedence disjunction operator | . Precedence make disjunction operator apply specific pattern , need parenthesis operators ( ) . Enclosing pattern parentheses makes act like single character purposes neighboring operators like pipe | Kleene * . pattern / gupp ( y | ies ) / specify meant disjunc - tion apply suffixes y ies . parenthesis operator ( useful counters like Kleene * . Unlike | operator , Kleene * operator applies default single character , whole sequence . Suppose match repeated instances string . Perhaps line column labels form Column 1 Column 2 Column 3 . expression / Column [ 0-9 ] + * / match number columns ; , match single column followed number spaces ! star applies space precedes , whole sequence . parentheses , write expression / ( Column [ 0-9 ] + * ) * / match word Column , followed number optional spaces , whole pattern repeated zero times . idea operator take precedence another , requiring sometimes parentheses specify mean , formalized operator precedence hierarchy regular expressions . following table gives orderoperatorprecedence RE operator precedence , highest precedence lowest precedence . 6 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE Parenthesis ( ) Counters * + ? { } Sequences anchors ˆmy end $ Disjunction | Thus , counters higher precedence sequences , / * / matches theeeee thethe . sequences higher prece - dence disjunction , / | / matches thany theny . Patterns ambiguous another way . Consider expression / [ a-z ] * / matching against text once upon time . / [ a-z ] * / matches zero letters , expression match nothing , just first letter o , , onc , once . cases regular expressions always match largest string ; say patterns greedy , expanding cover string . greedy , , ways enforce non-greedy matching , another mean-non-greedy ing ? qualifier . operator * ? Kleene star matches little text * ? possible . operator + ? Kleene plus matches little text possible . + ? 2.1.3 Simple Example Suppose wanted write RE find cases English article . simple ( incorrect ) pattern might : / / problem pattern miss word begins sentence hence capitalized ( i.e . , ) . might lead following pattern : / [ tT ] / still incorrectly return texts embedded words ( e.g . , theology ) . need specify instances word bound - ary sides : / \ b [ tT ] \ b / Suppose wanted / \ b / . might / \ b / treat underscores numbers word boundaries ; might find context might underlines numbers nearby ( the25 ) . need specify instances alphabetic letters side : / [ ˆa-zA-Z ] [ tT ] [ ˆa-zA-Z ] / still problem pattern : find word begins line . regular expression [ ˆa-zA-Z ] , avoid embedded instances , implies single ( although non-alphabetic ) character . avoid specify - ing require beginning-of-line non-alphabetic character , same end line : / ( ˆ | [ ˆa-zA-Z ] ) [ tT ] ( [ ˆa-zA-Z ] | $ ) / process just went based fixing two kinds errors : false positives , strings incorrectly matched like , false nega-false positives tives , strings incorrectly missed , like . Addressing two kinds offalse negatives 2.1 • REGULAR EXPRESSIONS 7 errors comes up again again implementing speech language processing systems . Reducing overall error rate application thus involves two antag - onistic efforts : • Increasing precision ( minimizing false positives ) • Increasing recall ( minimizing false negatives ) 2.1.4 Complex Example try significant example power REs . Suppose build application help user buy computer Web . user might “ machine least 6 GHz 500 GB disk space less $ 1000 ” . kind retrieval , first need able look expressions like 6 GHz 500 GB Mac $ 999.99 . rest section work simple regular expressions task . First , complete regular expression prices . Here’s regular expres - sion dollar sign followed string digits : / $ [ 0-9 ] + / Note $ character different function end-of-line function discussed earlier . regular expression parsers smart enough realize $ mean end-of-line . ( thought experiment , think regex parsers might figure function $ context . ) just need deal fractions dollars . add decimal point two digits afterwards : / $ [ 0-9 ] + \ . [ 0-9 ] [ 0-9 ] / pattern allows $ 199.99 $ 199 . need make cents optional make sure word boundary : / ( ˆ | \ W ) $ [ 0-9 ] + ( \ . [ 0-9 ] [ 0-9 ] ) ? \ b / last catch ! pattern allows prices like $ 199999.99 far expensive ! need limit dollar / ( ˆ | \ W ) $ [ 0-9 ] { 0,3 } ( \ . [ 0-9 ] [ 0-9 ] ) ? \ b / disk space ? need allow optional fractions again ( 5.5 GB ) ; note ? making final s optional , / * / mean “ zero spaces ” might always extra spaces lying around : / \ b [ 0-9 ] + ( \ . [ 0-9 ] + ) ? * ( GB | [ Gg ] igabytes ? ) \ b / Modifying regular expression matches 500 GB left exercise reader . 2.1.5 Operators Figure 2.7 shows aliases common ranges , mainly save typing . Besides Kleene * Kleene + explicit numbers counters , enclosing curly brackets . regular expression / { 3 } / means “ exactly 3 occurrences previous character expression ” . / \ . { 24 } z / match followed 24 dots followed z ( followed 23 25 dots followed z ) . 8 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE RE Expansion Match First Matches \ d [ 0-9 ] digit Party 5 \ D [ ˆ0-9 ] non-digit Blue moon \ w [ a-zA-Z0-9_ ] alphanumeric / underscore Daiyu \ W [ ˆ \ w ] non-alphanumeric ! ! ! ! \ s [ \ r \ t \ n \ f ] whitespace ( space , tab ) \ S [ ˆ \ s ] Non-whitespace Concord Figure 2.7 Aliases common sets characters . range numbers specified . / { n , m } / specifies n m occurrences previous char expression , / { n , } / means least n occur - rences previous expression . REs counting summarized Fig . 2.8 . RE Match * zero occurrences previous char expression + occurrences previous char expression ? exactly zero occurrence previous char expression { n } n occurrences previous char expression { n , m } n m occurrences previous char expression { n , } least n occurrences previous char expression { , m } up m occurrences previous char expression Figure 2.8 Regular expression operators counting . Finally , certain special characters referred special notation based backslash ( \ ) ( Fig . 2.9 ) . common newline characterNewline \ n tab character \ t . refer characters special themselves ( like . , * , [ , \ ) , precede backslash , ( i.e . , / \ . / , / \ * / , / \ [ / , / \ \ / ) . RE Match First Patterns Matched \ * asterisk “ * ” “ K * * P * L * * N ” \ . period “ . ” “ Dr . Livingston , presume ” \ ? question mark “ Why come lend hand ? ” \ n newline \ t tab Figure 2.9 characters need backslashed . 2.1.6 Substitution , Capture Groups , ELIZA important regular expressions substitutions . example , substi-substitution tution operator s / regexp1 / pattern / Python Unix commands like vim sed allows string characterized regular expression replaced another string : s / colour / color / often useful able refer particular subpart string matching first pattern . example , suppose wanted put angle brackets around integers text , example , changing 35 boxes < 35 > boxes . like way refer integer found easily add brackets . , put parentheses ( ) around first pattern number operator \ 1 second pattern refer back . Here’s looks : 2.1 • REGULAR EXPRESSIONS 9 s / ( [ 0-9 ] + ) / < \ 1 > / parenthesis number operators specify certain string expression occur twice text . example , suppose looking pattern “ Xer , Xer ” , constrain two X’s same string . surrounding first X parenthesis operator , replacing second X number operator \ 1 , follows : / ( . * ) er , \ 1er / \ 1 replaced whatever string matched first item paren - theses . match bigger , bigger bigger , faster . parentheses store pattern memory called capture group . capture group Every time capture group ( i.e . , parentheses surround pattern ) , re - sulting match stored numbered register . match two different sets ofregister parentheses , \ 2 means whatever matched second capture group . Thus / ( . * ) er ( . * ) , \ 1er \ 2 / match faster ran , faster ran faster ran , faster ate . Similarly , third capture group stored \ 3 , fourth \ 4 , . Parentheses thus double function regular expressions ; group terms specifying order operators apply , capture something register . Occasionally might parenthe - ses grouping , capture resulting pattern register . case non-capturing group , specified putting commandsnon-capturinggroup ? : open paren , form ( ? : pattern ) . / ( ? : | few ) ( people | cats ) like \ 1 / match cats like cats cats like few . Substitutions capture groups useful implementing simple chat - bots like ELIZA ( Weizenbaum , 1966 ) . Recall ELIZA simulates Rogerian psychologist carrying conversations like following : User1 : Men alike . ELIZA1 : WAY User2 : They’re always bugging something . ELIZA2 : THINK SPECIFIC EXAMPLE User3 : Well , boyfriend made come . ELIZA3 : BOYFRIEND MADE COME User4 : depressed time . ELIZA4 : SORRY HEAR DEPRESSED ELIZA works series cascade regular expression substitutions matches changes part input lines . Input lines first uppercased . first substitutions change instances , , . next set substitutions matches replaces patterns input . examples : s / . * ( depressed | sad ) . * / SORRY HEAR \ 1 / s / . * ( depressed | sad ) . * / WHY THINK \ 1 / s / . * . * / WAY / s / . * always . * / THINK SPECIFIC EXAMPLE / 10 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE multiple substitutions apply input , substitutions assigned rank applied order . Creating patterns topic Exercise 2.3 , return details ELIZA architecture Chapter 26 . 2.1.7 Lookahead Assertions Finally , times need predict future : look ahead text pattern matches , advance match cursor , deal pattern occurs . lookahead assertions make ( ? syntax saw previ-lookahead ous section non-capture groups . operator ( ? = pattern ) true pattern occurs , zero-width , i.e . match pointer advance . operatorzero-width ( ? ! pattern ) returns true pattern match , again zero-width advance cursor . Negative lookahead commonly parsing complex pattern rule special case . example suppose match , beginning line , single word start “ Volcano ” . negative lookahead : / ˆ ( ? ! Volcano ) [ A-Za-z ] + / 2.2 Words talk processing words , need decide counts word . start looking particular corpus ( plural corpora ) , computer-readablecorpus corpora collection text speech . example Brown corpus million-word col - lection samples 500 written English texts different genres ( newspa - per , fiction , non-fiction , academic , etc . ) , assembled Brown University 1963 – 64 ( Kučera Francis , 1967 ) . many words following Brown sentence ? stepped hall , delighted encounter water brother . sentence 13 words count punctuation marks words , 15 count punctuation . treat period ( “ . ” ) , comma ( “ , ” ) , words depends task . Punctuation critical finding boundaries things ( commas , periods , colons ) identifying aspects meaning ( question marks , exclamation marks , quotation marks ) . tasks , like part-of-speech tagging parsing speech synthesis , sometimes treat punctuation marks separate words . Switchboard corpus American English telephone conversations strangers collected early 1990s ; contains 2430 conversations averaging 6 minutes , totaling 240 hours speech 3 million words ( Godfrey et al . , 1992 ) . corpora spoken language punctuation intro - duce complications regard defining words . look utterance Switchboard ; utterance spoken correlate sentence : utterance uh main - mainly business data processing utterance two kinds disfluencies . broken-off word main - isdisfluency called fragment . Words like uh um called fillers filled pauses . Shouldfragment filled pause consider words ? Again , depends application . building speech transcription system , might eventually strip disfluencies . 2.2 • WORDS 11 sometimes keep disfluencies around . Disfluencies like uh um actually helpful speech recognition predicting upcoming word , signal speaker restarting clause idea , speech recognition treated regular words . people different disflu - encies cue speaker identification . fact Clark Fox Tree ( 2002 ) showed uh um different meanings . think ? capitalized tokens like uncapitalized tokens like same word ? lumped together tasks ( speech recognition ) , part - of-speech named-entity tagging , capitalization useful feature retained . inflected forms like cats versus cat ? two words same lemma cat different wordforms . lemma set lexical forms havinglemma same stem , same major part-of-speech , same word sense . word - form full inflected derived form word . morphologically complexwordform languages like Arabic , often need deal lemmatization . many tasks English , , wordforms sufficient . many words English ? answer question need distinguish two ways talking words . Types number distinct wordsword type corpus ; set words vocabulary V , number types vocabulary size | V | . Tokens total number N running words . ignoreword token punctuation , following Brown sentence 16 tokens 14 types : picnicked pool , lay back grass looked stars . speak number words language , generally referring word types . Corpus Tokens = N Types = | V | Shakespeare 884 thousand 31 thousand Brown corpus 1 million 38 thousand Switchboard telephone conversations 2.4 million 20 thousand COCA 440 million 2 million Google N-grams 1 trillion 13 million Figure 2.10 Rough numbers types tokens English language corpora . largest , Google N-grams corpus , contains 13 million types , count includes types appearing 40 times , true number larger . Fig . 2.10 shows rough numbers types tokens computed popular English corpora . larger corpora look , word types find , fact relationship number types | V | number tokens N called Herdan’s Law ( Herdan , 1960 ) Heaps ’ Law ( Heaps , 1978 ) Herdan’s Law Heaps ’ Law discoverers ( linguistics information retrieval respectively ) . shown Eq . 2.1 , k β positive constants , 0 < β < 1 . | V | = kNβ ( 2.1 ) value β depends corpus size genre , least large corpora Fig . 2.10 , β ranges . 67 . 75 . Roughly say vocabulary size text goes up significantly faster square root length words . Another measure number words language number lem - mas wordform types . Dictionaries help giving lemma counts ; dic - tionary entries boldface forms rough upper bound number 12 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE lemmas ( lemmas multiple boldface forms ) . 1989 edition Oxford English Dictionary 615,000 entries . 2.3 Corpora Words appear nowhere . particular piece text study produced specific speakers writers , specific dialect specific language , specific time , specific place , specific function . Perhaps important dimension variation language . NLP algo - rithms useful apply many languages . world 7097 languages time writing , according online Ethnologue catalog ( Simons Fennig , 2018 ) . NLP tools tend developed official languages large industrialized nations ( Chinese , English , Spanish , Arabic , etc . ) , limit tools just few languages . Furthermore , lan - guages multiple varieties , dialects spoken different regions different social groups . Thus , example , processing text African American Vernacular English ( AAVE ) , dialect spoken millions people theAAVE United States , important make NLP tools function dialect . Twitter posts written AAVE make constructions like iont ( Stan - dard American English ( SAE ) ) , talmbout corresponding SAE talking , SAE examples influence word segmentation ( Blodgett et al . 2016 , Jones 2015 ) . quite common speakers writers multiple languages single communicative act , phenomenon called code switching . Code switch-code switching ing enormously common world ; examples showing Spanish ( transliterated ) Hindi code switching English ( Solorio et al . 2014 , Jurgens et al . 2017 ) : ( 2.2 ) Por primera vez veo @username actually hateful ! beautiful :) [ first time get @username actually hateful ! beautiful :) ] ( 2.3 ) dost tha ra - hega . . . wory . . . dherya rakhe [ “ remain friend . . . worry . . . faith ” ] Another dimension variation genre . text algorithms process might come newswire , fiction non-fiction books , scientific articles , Wikipedia , religious texts . might come spoken genres like telephone conversations , business meetings , police body-worn cameras , medical interviews , transcripts television shows movies . might come work situations like doctors ’ notes , legal text , parliamentary congressional proceedings . Text reflects demographic characteristics writer ( speaker ) : age , gender , race , socioeconomic class influence linguistic properties text processing . finally , time matters . Language changes time , lan - guages good corpora texts different historical periods . language situated , developing computational models lan - guage processing , important consider produced language , context , purpose , make sure models fit data . 2.4 • TEXT NORMALIZATION 13 2.4 Text Normalization almost natural language processing text , text normal - ized . least three tasks commonly applied part normalization process : 1 . Tokenizing ( segmenting ) words 2 . Normalizing word formats 3 . Segmenting sentences next sections walk tasks . 2.4.1 Unix Tools Crude Tokenization Normalization begin easy , somewhat naive version word tokenization nor - malization ( frequency computation ) accomplished English solely single UNIX command-line , inspired Church ( 1994 ) . make Unix commands : tr , systematically change particular characters - put ; sort , sorts input lines alphabetical order ; uniq , collapses counts adjacent identical lines . example begin ‘ complete words ’ Shakespeare textfile , sh . txt . tr tokenize words changing every sequence non - alphabetic characters newline ( ’ A-Za-z ’ means alphabetic , - c option com - plements non-alphabet , - s option squeezes sequences single character ) : tr - sc ’ A-Za-z ’ ’ \ n ’ < sh . txt output command : SONNETS William Shakespeare fairest creatures . . . word per line , sort lines , pass uniq - c collapse count : tr - sc ’ A-Za-z ’ ’ \ n ’ < sh . txt | sort | uniq - c following output : 1945 72 AARON 19 ABBESS 25 Aaron 6 Abate 1 Abates 5 Abbess 6 Abbey 14 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE 3 Abbot . . . Alternatively , collapse upper case lower case : tr - sc ’ A-Za-z ’ ’ \ n ’ < sh . txt | tr A-Z a-z | sort | uniq - c whose output 14725 97 aaron 1 abaissiez 10 abandon 2 abandoned 2 abase 1 abash 14 abate 3 abated 3 abatement . . . sort again find frequent words . - n option sort means sort numerically rather alphabetically , - r option means sort reverse order ( highest-to-lowest ) : tr - sc ’ A-Za-z ’ ’ \ n ’ < sh . txt | tr A-Z a-z | sort | uniq - c | sort - n - r results show frequent words Shakespeare , corpus , short function words like articles , pronouns , prepositions : 27378 26084 22538 19771 17481 14725 13826 12489 11318 11112 . . . Unix tools sort handy building quick word count statistics corpus . 2.4.2 Word Tokenization simple UNIX tools fine getting rough word statistics sophisticated algorithms generally necessary tokenization , task seg-tokenization menting running text words . Unix command sequence just removed numbers punctu - ation , NLP applications need keep tokenization . often break off punctuation separate token ; commas useful piece information parsers , periods help indicate sentence boundaries . often keep punctuation occurs word internally , examples like m.p.h , , Ph . D . , & T , cap’n . Special characters numbers need kept prices 2.4 • TEXT NORMALIZATION 15 ( $ 45.55 ) dates ( 01/02/06 ) ; segment price separate - kens “ 45 ” “ 55 ” . URLs ( http://www.stanford.edu ) , Twitter hashtags ( #nlproc ) , email addresses ( someone@cs.colorado.edu ) . Number expressions introduce complications well ; commas nor - mally appear word boundaries , commas inside numbers English , every three digits : 555,500.50 . Languages , hence tokenization requirements , differ ; many continental European languages like Spanish , French , German , contrast , comma mark decimal point , spaces ( sometimes periods ) English puts commas , example , 555 500,50 . tokenizer expand clitic contractions marked byclitic apostrophes , example , converting what’re two tokens , . clitic part word stand own , occur attached another word . contractions occur alphabetic languages , including articles pronouns French ( j’ai , l’homme ) . Depending application , tokenization algorithms tokenize mul - tiword expressions like New York rock ’ n ’ roll single token , re - quires multiword expression dictionary sort . Tokenization thus inti - mately tied up named entity detection , task detecting names , dates , organizations ( Chapter 18 ) . commonly tokenization standard known Penn Treebank - kenization standard , parsed corpora ( treebanks ) released Lin-Penn Treebanktokenization guistic Data Consortium ( LDC ) , source many useful datasets . standard separates clitics ( becomes plus n’t ) , keeps hyphenated words - gether , separates punctuation ( save space showing visible spaces ‘ ’ tokens , although newlines common output ) : Input : " San Francisco-based restaurant , " , " charge $ 10 " . Output : " San Francisco-based restaurant , " , " n’t charge $ 10 " . practice , tokenization needs run language pro - cessing , needs fast . standard method tokenization deterministic algorithms based regular expressions compiled ef - ficient finite state automata . example , Fig . 2.11 shows example basic regular expression tokenize nltk . regexp tokenize function Python-based Natural Language Toolkit ( NLTK ) ( Bird et al . 2009 ; http://www.nltk.org ) . Carefully designed deterministic algorithms deal ambiguities arise , fact apostrophe needs tokenized differently genitive marker ( book’s cover ) , quotative ‘ class ’ , , clitics like they’re . Word tokenization complex languages like written Chinese , Japanese , Thai , spaces mark potential word-boundaries . Chinese , example , words composed characters ( called hanzi inhanzi Chinese ) . character generally represents single unit meaning ( called morpheme ) pronounceable single syllable . Words 2.4 charac - ters long average . deciding counts word Chinese complex . example , consider following sentence : ( 2.4 ) 姚 明 进入 总 决赛 “ Yao Ming reaches finals ” 16 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE > > > text = ’ U.S.A . poster-print costs $ 12.40 . . . ’ > > > pattern = r ’ ’ ’ ( ? x ) # set flag allow verbose regexps . . . ( [ A-Z ] \ . ) + # abbreviations , e.g . U.S.A . . . . | \ w + ( - \ w + ) * # words optional internal hyphens . . . | \ $ ? \ d + ( \ . \ d + ) ? % ? # currency percentages , e.g . $ 12.40 , 82 % . . . | \ . \ . \ . # ellipsis . . . | [ ] [ . , ; " ’ ? ( ) : - _ ‘ ] # separate tokens ; includes ] , [ . . . ’ ’ ’ > > > nltk . regexp_tokenize ( text , pattern ) [ ’ ’ , ’ U.S.A . ’ , ’ poster-print ’ , ’ costs ’ , ’ $ 12.40 ’ , ’ . . . ’ ] Figure 2.11 python trace regular expression tokenization NLTK ( Bird et al . , 2009 ) Python-based natural language processing toolkit , commented readability ; ( ? x ) verbose flag tells Python strip comments whitespace . Figure Chapter 3 Bird et al . ( 2009 ) . Chen et al . ( 2017 ) point , treated 3 words ( ‘ Chinese Treebank ’ segmentation ) : ( 2.5 ) 姚 明 YaoMing 进入 reaches 总 决赛 finals 5 words ( ‘ Peking University ’ segmentation ) : ( 2.6 ) 姚 Yao 明 Ming 进入 reaches 总 overall 决赛 finals Finally , possible Chinese simply ignore words altogether characters basic elements , treating sentence series 7 characters : ( 2.7 ) 姚 Yao 明 Ming 进 enter 入 enter 总 overall 决 decision 赛 game fact , Chinese NLP tasks turns work better take characters rather words input , characters reasonable semantic level applications , word standards result huge vocabulary large numbers rare words ( Li et al . , 2019 ) . , Japanese Thai character small unit , algo - rithms word segmentation required . useful Chinesewordsegmentation rare situations word rather character boundaries required . standard segmentation algorithms languages neural sequence mod - els trained via supervised machine learning hand-segmented training sets ; introduce sequence models Chapter 8 . 2.4.3 Byte-Pair Encoding Tokenization third option tokenizing text input . defining tokens words ( defined spaces orthographies spaces , complex algorithms ) , characters ( Chinese ) , data automatically tell size tokens . Perhaps sometimes might tokens space-delimited words ( like spinach ) times useful tokens larger words ( like New York Times ) , sometimes smaller words ( like morphemes - est - er . morpheme smallest meaning-bearing unit language ; example word unlikeliest morphemes un - , likely , - est ; return page 20 . reason helpful subword tokens deal unknown words . subword 2.4 • TEXT NORMALIZATION 17 Unknown words particularly relevant machine learning systems . next chapter , machine learning systems often learn facts words corpus ( training corpus ) facts make decisions separate test corpus words . Thus training corpus contains , say words low , lowest , lower , word lower appears test corpus , system know . solution problem kind tokenization tokens words , tokens frequent morphemes subwords like - er , unseen word represented combining parts . simplest algorithm byte-pair encoding , BPE ( Sennrich et al . , BPE 2016 ) . Byte-pair encoding based method text compression ( Gage , 1994 ) , tokenization . intuition algorithm iteratively merge frequent pairs characters , algorithm begins set symbols equal set characters . word represented sequence characters plus special end-of-word symbol . step algorithm , count number symbol pairs , find frequent pair ( ‘ ’ , ‘ B ’ ) , replace new merged symbol ( ‘ AB ’ ) . continue count merge , creating new longer longer character strings , k merges ; k parameter algorithm . resulting symbol set consist original set characters plus k new symbols . algorithm run inside words ( merge word boundaries ) . reason , algorithm take input dictionary words together counts . Consider following tiny input dictionary counts word , starting vocabulary 11 letters : dictionary vocabulary 5 l o w , d , e , , l , n , o , r , s , t , w 2 l o w e s t 6 n e w e r 3 w d e r 2 n e w first count pairs symbols : frequent pair r occurs newer ( frequency 6 ) wider ( frequency 3 ) total 9 oc - currences . merge symbols , treating r symbol , count again : dictionary vocabulary 5 l o w , d , e , , l , n , o , r , s , t , w , r 2 l o w e s t 6 n e w e r 3 w d e r 2 n e w frequent pair e r , merge ; system learned token word-final er , represented er : dictionary vocabulary 5 l o w , d , e , , l , n , o , r , s , t , w , r , er 2 l o w e s t 6 n e w er 3 w d er 2 n e w Next e w ( total count 8 ) get merged ew : 18 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE dictionary vocabulary 5 l o w , d , e , , l , n , o , r , s , t , w , r , er , ew 2 l o w e s t 6 n ew er 3 w d er 2 n ew continue , next merges : Merge Current Vocabulary ( n , ew ) , d , e , , l , n , o , r , s , t , w , r , er , ew , new ( l , o ’ , d , e , , l , n , o , r , s , t , w , r , er , ew , new , lo ( lo , w ) , d , e , , l , n , o , r , s , t , w , r , er , ew , new , lo , low ( new , er ) , d , e , , l , n , o , r , s , t , w , r , er , ew , new , lo , low , newer ( low , ) , d , e , , l , n , o , r , s , t , w , r , er , ew , new , lo , low , newer , low need tokenize test sentence , just run merges learned , greedily , order learned , test data . ( Thus fre - quencies test data play role , just frequencies training data ) . first segment test sentence word characters . apply first rule : replace every instance r test corpus r , second rule : replace every instance e r test corpus er , . end , test corpus contained word n e w e r , tokenized full word . new ( unknown ) word like l o w e r merged two tokens low er . course real algorithms BPE run many thousands merges large input dictionary . result words represented full symbols , rare words ( unknown words ) represented parts . full BPE learning algorithm Fig . 2.12 . Wordpiece Greedy Tokenization alternatives byte pair encoding inducing tokens . Like BPE algorithm , wordpiece algorithm starts simple tokenization ( aswordpiece whitespace ) rough words , breaks rough word tokens subword tokens . wordpiece model differs BPE specialwordpiece word-boundary token appears beginning words rather end , way merges pairs . Rather merging pairs frequent , wordpiece merges pairs minimizes language model likelihood training data . introduce concepts next chapter , simplify , wordpiece model chooses two tokens combine give training corpus highest probability ( Wu et al . , 2016 ) . wordpiece segmenter BERT ( Devlin et al . , 2019 ) , like word - piece variants , input sentence string first split simple basic tokenizer ( like whitespace ) series rough word tokens . word boundary token , word-initial subwords distinguished start words marking internal subwords special symbols # # , might split unaffable [ " un " , " \ # \ #aff " , " \ # \ #able " ] . word token string tokenized greedy longest-match-first algorithm . dif - ferent decoding algorithm introduced BPE , runs merges test sentence same order learned training set . Greedy longest-match-first decoding sometimes called maximum matchingmaximummatching MaxMatch . maximum matching algorithm ( Fig . 2.13 ) vocabu - lary ( learned list wordpiece tokens ) string starts pointing 2.4 • TEXT NORMALIZATION 19 m p o r t re , c o l l e c t o n s d e f g e t s t t s ( vocab ) : p r s = c o l l e c t o n s . d e f u l t d c t ( n t ) f o r word , f r e q n vocab . t e m s ( ) : symbols = word . s p l t ( ) f o r n r n g e ( l e n ( symbols ) − 1 ) : p r s [ symbols [ ] , symbols [ + 1 ] ] + = f r e q r e t u r n p r s d e f merge vocab ( p r , v n ) : v o u t = { } bigram = r e . e s c p e ( ’ ’ . j o n ( p r ) ) p = r e . compi l e ( r ’ ( ? < ! \ S ) ’ + bigram + r ’ ( ? ! \ S ) ’ ) f o r word n v n : w = p . sub ( ’ ’ . j o n ( p r ) , word ) v o u t [ w ] = v n [ word ] r e t u r n v o u t vocab = { ’ l o w < / w > ’ : 5 , ’ l o w e s t < / w > ’ : 2 , ’ n e w e r < / w > ’ : 6 , ’ w d e r < / w > ’ : 3 , ’ n e w < / w > ’ : 2 } num merges = 8 f o r n r n g e ( num merges ) : p r s = g e t s t t s ( vocab ) b e s t = max ( p r s , key = p r s . g e t ) vocab = merge vocab ( b e s t , vocab ) p r n t ( b e s t ) Figure 2.12 Python code BPE learning algorithm Sennrich et al . ( 2016 ) . beginning string . chooses longest token wordpiece vocabulary matches input current position , moves pointer past word string . algorithm applied again starting new pointer position . function MAXMATCH ( string , dictionary ) returns list tokens T string empty return empty list ← length ( sentence ) downto 1 firstword = first chars sentence remainder = rest sentence InDictionary ( firstword , dictionary ) return list ( firstword , MaxMatch ( remainder , dictionary ) ) Figure 2.13 MaxMatch ( ‘ greedy longest-first ’ ) algorithm word tokenization - ing wordpiece vocabularies . Assumes strings successfully tokenized dictionary . Thus token intention dictionary : [ " " , " tent " , " intent " , " # #tent " , " # #tention " , " # #tion " , " #ion " ] BERT tokenizer choose intent ( longer , # #ion complete string , resulting tokenization [ " intent " " # #ion " ] . BERT tokenizer applied string unwanted running produce : ( 2.8 ) [ " un " , " # #want " , " # #ed " , " runn " , " # #ing " ] Another tokenization algorithm called SentencePiece ( Kudo Richardson , SentencePiece 20 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE 2018 ) . BPE wordpiece assume already initial tokeniza - tion words ( spaces , initial dictionary ) never tried induce word parts spaces . contrast , SentencePiece model works raw text ; even whitespace handled normal symbol . Thus need initial tokenization word-list , languages like Chinese Japanese spaces . 2.4.4 Word Normalization , Lemmatization Stemming Word normalization task putting words / tokens standard format , choos-normalization ing single normal form words multiple forms like USA uh-huh uhhuh . standardization valuable , despite spelling information lost normalization process . information retrieval information extraction , might information documents mention USA . Case folding another kind normalization . Mapping everything lowercase folding case means Woodchuck woodchuck represented identically , helpful generalization many tasks , information retrieval speech recognition . sentiment analysis text classification tasks , information extraction , machine translation , contrast , case quite helpful case folding generally . maintaining difference , example , country pronoun outweigh advantage generalization case folding provided words . many natural language processing situations two morpholog - ically different forms word behave similarly . example web search , someone type string woodchucks useful system might return pages mention woodchuck s . especially common mor - phologically complex languages like Russian , example word Moscow different endings phrases Moscow , Moscow , Moscow , . Lemmatization task determining two words same root , despite surface differences . words , , shared lemma ; words dinner dinners lemma dinner . Lemmatizing forms same lemma let find mentions words Russian like Moscow . lemmatized form sentence like reading detective stories thus read detective story . lemmatization ? sophisticated methods lemmatization involve complete morphological parsing word . Morphology study way words built up smaller meaning-bearing units called morphemes . morpheme Two broad classes morphemes distinguished : stems — central mor-stem pheme word , supplying main meaning — affixes — adding “ additional ” affix meanings various kinds . , example , word fox consists morpheme ( morpheme fox ) word cats consists two : morpheme cat morpheme - s . morphological parser takes word like cats parses two morphemes cat s , Spanish word like amaren ( ‘ future love ’ ) morphemes amar ‘ love ’ , 3PL , future subjunctive . Porter Stemmer Lemmatization algorithms complex . reason sometimes make simpler cruder method , mainly consists chopping off word-final affixes . naive version morphological analysis called stemming . ofstemming widely stemming algorithms Porter ( 1980 ) . Porter stemmerPorter stemmer 2.4 • TEXT NORMALIZATION 21 applied following paragraph : map found Billy Bones’s chest , accurate copy , complete things-names heights soundings-with single exception red crosses written notes . produces following stemmed output : Thi wa map found Billi Bone s chest accur copi complet thing name height sound singl except red cross written note algorithm based series rewrite rules run series , cascade , incascade output pass fed input next pass ; sampling rules : ATIONAL → ATE ( e.g . , relational → relate ) ING → ε stem contains vowel ( e.g . , motoring → motor ) SSES → SS ( e.g . , grasses → grass ) Detailed rule lists Porter stemmer , well code ( Java , Python , etc . ) found Martin Porter’s homepage ; original paper ( Porter , 1980 ) . Simple stemmers useful cases need collapse differ - ent variants same lemma . Nonetheless , tend commit errors - under-generalizing , shown table below ( Krovetz , 1993 ) : Errors Commission Errors Omission organization organ European Europe doe analysis analyzes numerical numerous noise noisy policy police sparse sparsity 2.4.5 Sentence Segmentation Sentence segmentation another important step text processing . use-Sentencesegmentation ful cues segmenting text sentences punctuation , like periods , question marks , exclamation points . Question marks exclamation points rela - tively unambiguous markers sentence boundaries . Periods , hand , ambiguous . period character “ . ” ambiguous sentence bound - ary marker marker abbreviations like Mr . Inc . previous sentence just read showed even complex case ambiguity , final period Inc . marked abbreviation sentence boundary marker . reason , sentence tokenization word tokenization addressed jointly . general , sentence tokenization methods work first deciding ( based rules machine learning ) period part word sentence-boundary marker . abbreviation dictionary help determine period part commonly abbreviation ; dictionaries hand-built machine - learned ( Kiss Strunk , 2006 ) , final sentence splitter . Stan - ford CoreNLP toolkit ( Manning et al . , 2014 ) , example sentence splitting rule-based , deterministic consequence tokenization ; sentence ends sentence-ending punctuation ( . , ! , ? ) already grouped charac - ters token ( abbreviation number ) , optionally followed additional final quotes brackets . 22 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE 2.5 Minimum Edit Distance natural language processing concerned measuring similar two strings . example spelling correction , user typed erroneous string — say graffe – know user meant . user prob - ably intended word similar graffe . Among candidate similar words , word giraffe , differs letter graffe , seems intuitively similar , say grail graf , differ letters . Another example comes coreference , task deciding two strings following refer same entity : Stanford President John Hennessy Stanford University President John Hennessy Again , fact two strings similar ( differing word ) seems like useful evidence deciding might coreferent . Edit distance gives way quantify intuitions string sim - ilarity . formally , minimum edit distance two strings definedminimum editdistance minimum number editing operations ( operations like insertion , deletion , substitution ) needed transform string another . gap intention execution , example , 5 ( delete , substi - tute e n , substitute x t , insert c , substitute u n ) . easier looking important visualization string distances , alignmentalignment two strings , shown Fig . 2.14 . two sequences , alignment correspondence substrings two sequences . Thus , say aligns empty string , N E , . Beneath aligned strings another representation ; series symbols expressing operation list converting top string bottom string : d deletion , s substitution , insertion . N T E * N T O N | | | | | | | | | | * E X E C U T O N d s s s Figure 2.14 Representing minimum edit distance two strings alignment . final row gives operation list converting top string bottom string : d deletion , s substitution , insertion . assign particular cost weight operations . Levenshtein distance two sequences simplest weighting factor three operations cost 1 ( Levenshtein , 1966 ) — assume substitution letter itself , example , t t , zero cost . Lev - enshtein distance intention execution 5 . Levenshtein proposed alternative version metric insertion deletion cost 1 substitutions allowed . ( equivalent allowing substitution , giving substitution cost 2 substitution represented insertion deletion ) . version , Levenshtein distance intention execution 8 . 2.5 • MINIMUM EDIT DISTANCE 23 2.5.1 Minimum Edit Distance Algorithm find minimum edit distance ? think search task , searching shortest path — sequence edits — string another . n t e n t o n n t e c n t o n n x e n t o n del ins subst n t e n t o n Figure 2.15 Finding edit distance viewed search problem space possible edits enormous , search naively . , lots distinct edit paths end up same state ( string ) , rather recom - puting paths , just remember shortest path state time saw . dynamic programming . Dynamic programmingdynamicprogramming name class algorithms , first introduced Bellman ( 1957 ) , apply table-driven method solve problems combining solutions sub-problems . commonly algorithms natural language processing make dynamic programming , Viterbi algorithm ( Chapter 8 ) CKY algorithm parsing ( Chapter 13 ) . intuition dynamic programming problem large problem solved properly combining solutions various sub-problems . Consider shortest path transformed words represents minimum edit distance strings intention execution shown Fig . 2.16 . n t e n t o n n t e n t o n e t e n t o n e x e n t o n e x e n u t o n e x e c u t o n delete substitute n e substitute t x insert u substitute n c Figure 2.16 Path intention execution . Imagine string ( perhaps exention ) optimal path ( whatever ) . intuition dynamic programming exention optimal operation list , optimal sequence include optimal path intention exention . Why ? shorter path intention exention , , resulting shorter overall path , optimal sequence optimal , thus leading contradiction . minimum edit distance algorithm named Wagner Fischer ( 1974 ) minimum editdistance independently discovered many people ( Historical Notes section Chapter 8 ) . first define minimum edit distance two strings . two strings , source string X length n , target string Y length m , define D [ , j ] edit distance X [ 1 . . ] Y [ 1 . . j ] , i.e . , first characters X first j characters Y . edit distance X Y thus D [ n , m ] . 24 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE dynamic programming compute D [ n , m ] bottom up , combining - lutions subproblems . base case , source substring length empty target string , going characters 0 requires deletes . target substring length j empty source going 0 characters j characters requires j inserts . computed D [ , j ] small , j compute larger D [ , j ] based previously computed smaller values . value D [ , j ] com - puted taking minimum three possible paths matrix arrive : D [ , j ] = min    D [ − 1 , j ] + del-cost ( source [ ] ) D [ , j − 1 ] + ins-cost ( target [ j ] ) D [ − 1 , j − 1 ] + sub-cost ( source [ ] , target [ j ] ) assume version Levenshtein distance insertions dele - tions cost 1 ( ins-cost ( · ) = del-cost ( · ) = 1 ) , substitutions cost 2 ( except substitution identical letters zero cost ) , computation D [ , j ] becomes : D [ , j ] = min        D [ − 1 , j ] + 1 D [ , j − 1 ] + 1 D [ − 1 , j − 1 ] + { 2 ; source [ ] 6 = target [ j ] 0 ; source [ ] = target [ j ] ( 2.9 ) algorithm summarized Fig . 2.17 ; Fig . 2.18 shows results applying algorithm distance intention execution version Levenshtein Eq . 2.9 . Knowing minimum edit distance useful algorithms like finding poten - tial spelling error corrections . edit distance algorithm important another way ; small change , provide minimum cost alignment two strings . Aligning two strings useful throughout speech language process - ing . speech recognition , minimum edit distance alignment compute word error rate ( Chapter 28 ) . Alignment plays role machine translation , sentences parallel corpus ( corpus text two languages ) need matched . extend edit distance algorithm produce alignment , start visualizing alignment path edit distance matrix . Figure 2.19 shows path boldfaced cell . boldfaced cell represents alignment pair letters two strings . two boldfaced cells occur same row , insertion going source target ; two boldfaced cells same column indicate deletion . Figure 2.19 shows intuition compute alignment path . computation proceeds two steps . first step , augment minimum edit distance algorithm store backpointers cell . backpointer cell points previous cell ( cells ) came entering current cell . shown schematic backpointers Fig . 2.19 . cells mul - tiple backpointers minimum extension come multiple previous cells . second step , perform backtrace . backtrace , startbacktrace last cell ( final row column ) , follow pointers back dynamic programming matrix . complete path final cell initial cell minimum distance alignment . Exercise 2.7 asks modify minimum edit distance algorithm store pointers compute backtrace output alignment . 2.5 • MINIMUM EDIT DISTANCE 25 function MIN-EDIT-DISTANCE ( source , target ) returns min-distance n ← LENGTH ( source ) m ← LENGTH ( target ) Create distance matrix distance [ n + 1 , m + 1 ] # Initialization : zeroth row column distance empty string D [ 0,0 ] = 0 row 1 n D [ , 0 ] ← D [ i-1,0 ] + del-cost ( source [ ] ) column j 1 m D [ 0 , j ] ← D [ 0 , j-1 ] + ins-cost ( target [ j ] ) # Recurrence relation : row 1 n column j 1 m D [ , j ] ← MIN ( D [ − 1 , j ] + del-cost ( source [ ] ) , D [ − 1 , j − 1 ] + sub-cost ( source [ ] , target [ j ] ) , D [ , j − 1 ] + ins-cost ( target [ j ] ) ) # Termination return D [ n , m ] Figure 2.17 minimum edit distance algorithm , example class dynamic programming algorithms . various costs fixed ( e.g . , ∀ x , ins-cost ( x ) = 1 ) specific letter ( model fact letters likely - serted others ) . assume cost substituting letter itself ( i.e . , sub-cost ( x , x ) = 0 ) . Src \ Tar # e x e c u t o n # 0 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 6 7 8 n 2 3 4 5 6 7 8 7 8 7 t 3 4 5 6 7 8 7 8 9 8 e 4 3 4 5 6 7 8 9 10 9 n 5 4 5 6 7 8 9 10 11 10 t 6 5 6 7 8 9 8 9 10 11 7 6 7 8 9 10 9 8 9 10 o 8 7 8 9 10 11 10 9 8 9 n 9 8 9 10 11 12 11 10 9 8 Figure 2.18 Computation minimum edit distance intention execution algorithm Fig . 2.17 , Levenshtein distance cost 1 insertions dele - tions , 2 substitutions . worked example simple Levenshtein distance , algorithm Fig . 2.17 allows arbitrary weights operations . spelling correction , example , substitutions likely happen letters next keyboard . Viterbi algorithm probabilistic extension minimum edit distance . computing “ minimum edit distance ” two strings , Viterbi computes “ maximum probability alignment ” string another . discuss Chapter 8 . 26 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE # e x e c u t o n # 0 ← 1 ← 2 ← 3 ← 4 ← 5 ← 6 ← 7 ← 8 ← 9 ↑ 1 ↖ ← ↑ 2 ↖ ← ↑ 3 ↖ ← ↑ 4 ↖ ← ↑ 5 ↖ ← ↑ 6 ↖ ← ↑ 7 ↖ 6 ← 7 ← 8 n ↑ 2 ↖ ← ↑ 3 ↖ ← ↑ 4 ↖ ← ↑ 5 ↖ ← ↑ 6 ↖ ← ↑ 7 ↖ ← ↑ 8 ↑ 7 ↖ ← ↑ 8 ↖ 7 t ↑ 3 ↖ ← ↑ 4 ↖ ← ↑ 5 ↖ ← ↑ 6 ↖ ← ↑ 7 ↖ ← ↑ 8 ↖ 7 ← ↑ 8 ↖ ← ↑ 9 ↑ 8 e ↑ 4 ↖ 3 ← 4 ↖ ← 5 ← 6 ← 7 ← ↑ 8 ↖ ← ↑ 9 ↖ ← ↑ 10 ↑ 9 n ↑ 5 ↑ 4 ↖ ← ↑ 5 ↖ ← ↑ 6 ↖ ← ↑ 7 ↖ ← ↑ 8 ↖ ← ↑ 9 ↖ ← ↑ 10 ↖ ← ↑ 11 ↖ ↑ 10 t ↑ 6 ↑ 5 ↖ ← ↑ 6 ↖ ← ↑ 7 ↖ ← ↑ 8 ↖ ← ↑ 9 ↖ 8 ← 9 ← 10 ← ↑ 11 ↑ 7 ↑ 6 ↖ ← ↑ 7 ↖ ← ↑ 8 ↖ ← ↑ 9 ↖ ← ↑ 10 ↑ 9 ↖ 8 ← 9 ← 10 o ↑ 8 ↑ 7 ↖ ← ↑ 8 ↖ ← ↑ 9 ↖ ← ↑ 10 ↖ ← ↑ 11 ↑ 10 ↑ 9 ↖ 8 ← 9 n ↑ 9 ↑ 8 ↖ ← ↑ 9 ↖ ← ↑ 10 ↖ ← ↑ 11 ↖ ← ↑ 12 ↑ 11 ↑ 10 ↑ 9 ↖ 8 Figure 2.19 entering value cell , mark three neighboring cells came up three arrows . table full compute alignment ( minimum edit path ) backtrace , starting 8 lower-right corner following arrows back . sequence bold cells represents possible minimum cost alignment two strings . Diagram design Gusfield ( 1997 ) . 2.6 Summary chapter introduced fundamental tool language processing , regular ex - pression , showed perform basic text normalization tasks including word segmentation normalization , sentence segmentation , stemming . introduce important minimum edit distance algorithm comparing strings . Here’s summary main points covered ideas : • regular expression language powerful tool pattern-matching . • Basic operations regular expressions include concatenation symbols , disjunction symbols ( [ ] , | , . ) , counters ( * , + , { n , m } ) , anchors ( ˆ , $ ) precedence operators ( ( , ) ) . • Word tokenization normalization generally cascades simple regular expressions substitutions finite automata . • Porter algorithm simple efficient way stemming , stripping off affixes . high accuracy useful tasks . • minimum edit distance two strings minimum number operations takes edit . Minimum edit distance computed dynamic programming , results alignment two strings . Bibliographical Historical Notes Kleene ( 1951 ) ( 1956 ) first defined regular expressions finite automaton , based McCulloch-Pitts neuron . Ken Thompson first build regular expressions compilers editors text searching ( Thompson , 1968 ) . editor ed included command “ g / regular expression / p ” , Global Regular Expres - sion Print , later became Unix grep utility . Text normalization algorithms applied beginning field . earliest widely-used stemmers Lovins ( 1968 ) . Stemming applied early digital humanities , Packard ( 1973 ) , built affix-stripping morphological parser Ancient Greek . Currently wide vari - EXERCISES 27 ety code tokenization normalization available , Stanford Tokenizer ( http://nlp.stanford.edu/software/tokenizer.shtml ) spe - cialized tokenizers Twitter ( O’Connor et al . , 2010 ) , sentiment ( http : / / sentiment.christopherpotts.net/tokenizing.html ) . Palmer ( 2012 ) survey text preprocessing . NLTK essential tool offers useful Python libraries ( http://www.nltk.org ) textbook descriptions ( Bird et al . , 2009 ) many algorithms including text normalization corpus interfaces . Herdan’s law Heaps ’ Law , Herdan ( 1960 , p . 28 ) , Heaps ( 1978 ) , Egghe ( 2007 ) Baayen ( 2001 ) ; Yasseri et al . ( 2012 ) discuss relation - ship measures linguistic complexity . edit distance , excellent Gusfield ( 1997 ) . example measuring edit distance ‘ intention ’ ‘ execution ’ adapted Kruskal ( 1983 ) . various publicly avail - able packages compute edit distance , including Unix diff NIST sclite program ( NIST , 2005 ) . autobiography Bellman ( 1984 ) explains originally came up term dynamic programming : “ . . . 1950s good years mathematical research . [ ] Secretary Defense . . . pathological fear hatred word , research . . . decided word , “ programming ” . wanted get idea dynamic , multi - stage . . . thought , . . . take word absolutely precise meaning , namely dynamic . . . impossible word , dynamic , pejorative sense . Try thinking combination pos - sibly give pejorative meaning . impossible . Thus , thought dynamic programming good name . something even Congressman object . ” Exercises 2.1 Write regular expressions following languages . 1 . set alphabetic strings ; 2 . set lower case alphabetic strings ending b ; 3 . set strings alphabet , b immedi - ately preceded immediately followed b ; 2.2 Write regular expressions following languages . “ word ” , mean alphabetic string separated words whitespace , relevant punctuation , line breaks , forth . 1 . set strings two consecutive repeated words ( e.g . , “ Hum - bert Humbert ” “ ” “ bug ” “ big bug ” ) ; 2 . strings start beginning line integer end end line word ; 3 . strings word grotto word raven ( , e.g . , words like grottos merely contain word grotto ) ; 4 . write pattern places first word English sentence register . Deal punctuation . 28 CHAPTER 2 • REGULAR EXPRESSIONS , TEXT NORMALIZATION , EDIT DISTANCE 2.3 Implement ELIZA-like program , substitutions described page 9 . might choose different domain Rogerian psy - chologist , although keep mind need domain program legitimately engage lot simple repetition . 2.4 Compute edit distance ( insertion cost 1 , deletion cost 1 , substitution cost 1 ) “ leda ” “ deal ” . Show work ( edit distance grid ) . 2.5 Figure drive closer brief divers edit dis - tance . version distance like . 2.6 implement minimum edit distance algorithm hand-computed results check code . 2.7 Augment minimum edit distance algorithm output alignment ; need store pointers add stage compute backtrace . Exercises 29 Baayen , R . H . ( 2001 ) . Word frequency distributions . Springer . Bellman , R . ( 1957 ) . Dynamic Programming . Princeton Uni - versity Press . Bellman , R . ( 1984 ) . Eye Hurricane : autobiogra - phy . World Scientific Singapore . Bird , S . , Klein , E . , Loper , E . ( 2009 ) . Natural Language Processing Python . O’Reilly . Blodgett , S . L . , Green , L . , O’Connor , B . ( 2016 ) . Demo - graphic dialectal variation social media : case study African-American English . EMNLP 2016 . Chen , X . , Shi , Z . , Qiu , X . , Huang , X . ( 2017 ) . Adversar - ial multi-criteria learning Chinese word segmentation . ACL 2017 , 1193 – 1203 . Church , K . W . ( 1994 ) . Unix Poets . Slides 2nd EL - SNET Summer School unpublished paper ms . Clark , H . H . Fox Tree , J . E . ( 2002 ) . uh um spontaneous speaking . Cognition , 84 , 73 – 111 . Devlin , J . , Chang , M . - W . , Lee , K . , Toutanova , K . ( 2019 ) . BERT : Pre-training deep bidirectional transformers language understanding . NAACL HLT 2019 , 4171 – 4186 . Egghe , L . ( 2007 ) . Untangling Herdan’s law Heaps ’ law : Mathematical informetric arguments . JASIST , 58 ( 5 ) , 702 – 709 . Gage , P . ( 1994 ) . new algorithm data compression . C Users Journal , 12 ( 2 ) , 23 – 38 . Godfrey , J . , Holliman , E . , McDaniel , J . ( 1992 ) . SWITCHBOARD : Telephone speech corpus research development . ICASSP-92 , 517 – 520 . Gusfield , D . ( 1997 ) . Algorithms Strings , Trees , Se - quences : Computer Science Computational Biology . Cambridge University Press . Heaps , H . S . ( 1978 ) . Information retrieval . Computational theoretical aspects . Academic Press . Herdan , G . ( 1960 ) . Type-token mathematics . Hague , Mouton . Jones , T . ( 2015 ) . Toward description African American Vernacular English dialect regions “ Black Twitter ” . American Speech , 90 ( 4 ) , 403 – 440 . Jurgens , D . , Tsvetkov , Y . , Jurafsky , D . ( 2017 ) . Incorpo - rating dialectal variability socially equitable language identification . ACL 2017 , 51 – 57 . Kiss , T . Strunk , J . ( 2006 ) . Unsupervised multilingual sentence boundary detection . Computational Linguistics , 32 ( 4 ) , 485 – 525 . Kleene , S . C . ( 1951 ) . Representation events nerve nets finite automata . Tech . rep . RM-704 , RAND Corpora - tion . RAND Research Memorandum . Kleene , S . C . ( 1956 ) . Representation events nerve nets finite automata . Shannon , C . McCarthy , J . ( Eds . ) , Automata Studies , 3 – 41 . Princeton University Press . Krovetz , R . ( 1993 ) . Viewing morphology inference process . SIGIR-93 , 191 – 202 . Kruskal , J . B . ( 1983 ) . overview sequence compari - son . Sankoff , D . Kruskal , J . B . ( Eds . ) , Time Warps , String Edits , Macromolecules : Theory Prac - tice Sequence Comparison , 1 – 44 . Addison-Wesley . Kudo , T . Richardson , J . ( 2018 ) . SentencePiece : sim - ple language independent subword tokenizer deto - kenizer neural text processing . EMNLP 2018 , 66 – 71 . Kučera , H . Francis , W . N . ( 1967 ) . Computational Anal - ysis Present-Day American English . Brown University Press , Providence , RI . Levenshtein , V . . ( 1966 ) . Binary codes capable correcting deletions , insertions , reversals . Cybernetics Con - trol Theory , 10 ( 8 ) , 707 – 710 . Original Doklady Akademii Nauk SSSR 163 ( 4 ) : 845 – 848 ( 1965 ) . Li , X . , Meng , Y . , Sun , X . , Han , Q . , Yuan , . , Li , J . ( 2019 ) . word segmentation necessary deep learning Chinese representations ? . ACL 2019 , 3242 – 3252 . Lovins , J . B . ( 1968 ) . Development stemming algo - rithm . Mechanical Translation Computational Lin - guistics , 11 ( 1 – 2 ) , 9 – 13 . Manning , C . D . , Surdeanu , M . , Bauer , J . , Finkel , J . , Bethard , S . , McClosky , D . ( 2014 ) . stanford CoreNLP natu - ral language processing toolkit . ACL 2014 , 55 – 60 . NIST ( 2005 ) . Speech recognition scoring toolkit ( sctk ) ver - sion 2.1 . http://www.nist.gov/speech/tools/ . O’Connor , B . , Krieger , M . , Ahn , D . ( 2010 ) . Tweetmotif : Exploratory search topic summarization twitter . ICWSM . Packard , D . W . ( 1973 ) . Computer-assisted morphological analysis ancient Greek . Zampolli , . Calzolari , N . ( Eds . ) , Computational Mathematical Linguistics : Proceedings International Conference Computa - tional Linguistics , 343 – 355 . Leo S . Olschki . Palmer , D . ( 2012 ) . Text preprocessing . Indurkhya , N . Damerau , F . J . ( Eds . ) , Handbook Natural Language Processing , 9 – 30 . CRC Press . Porter , M . F . ( 1980 ) . algorithm suffix stripping . Pro - gram , 14 ( 3 ) , 130 – 137 . Sennrich , R . , Haddow , B . , Birch , . ( 2016 ) . Neural ma - chine translation rare words subword units . ACL 2016 . Simons , G . F . Fennig , C . D . ( 2018 ) . Ethnologue : Lan - guages world , twenty-first edition . . SIL Interna - tional . Solorio , T . , Blair , E . , Maharjan , S . , Bethard , S . , Diab , M . , Ghoneim , M . , Hawwari , . , AlGhamdi , F . , Hirschberg , J . , Chang , . , Fung , P . ( 2014 ) . Overview first shared task language identification code-switched data . Proceedings First Workshop Computa - tional Approaches Code Switching , 62 – 72 . Thompson , K . ( 1968 ) . Regular expression search algorithm . CACM , 11 ( 6 ) , 419 – 422 . Wagner , R . . Fischer , M . J . ( 1974 ) . string-to-string correction problem . Journal Association Comput - ing Machinery , 21 , 168 – 173 . Weizenbaum , J . ( 1966 ) . ELIZA – computer program study natural language communication man machine . CACM , 9 ( 1 ) , 36 – 45 . Weizenbaum , J . ( 1976 ) . Computer Power Human Rea - son : Judgement Calculation . W.H . Freeman Company . 30 Chapter 2 • Regular Expressions , Text Normalization , Edit Distance Wu , Y . , Schuster , M . , Chen , Z . , Le , Q . V . , Norouzi , M . , Macherey , W . , Krikun , M . , Cao , Y . , Gao , Q . , Macherey , K . , Klingner , J . , Shah , . , Johnson , M . , Liu , X . , Kaiser , Ł . , Gouws , S . , Kato , Y . , Kudo , T . , Kazawa , H . , Stevens , K . , Kurian , G . , Patil , N . , Wang , W . , Young , C . , Smith , J . , Riesa , J . , Rudnick , . , Vinyals , O . , Corrado , G . S . , Hughes , M . , Dean , J . ( 2016 ) . Google’s neural machine transla - tion system : Bridging gap human machine translation . arXiv preprint arXiv : 1609.08144 . Yasseri , T . , Kornai , . , Kertész , J . ( 2012 ) . practical approach language complexity : Wikipedia case study . PloS , 7 ( 11 ) .