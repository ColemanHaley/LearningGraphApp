Speech Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . rights reserved . Draft October 2 , 2019 . CHAPTER 3 N-gram Language Models “ uniformly charming ! ” cried , smile associating bowed perceived chaise four wish . Random sentence generated Jane Austen trigram model Predicting difficult — especially future , old quip goes . predicting something seems easier , like next few words someone going say ? word , example , likely follow Please turn homework . . . Hopefully , concluded likely word , possibly , probably refrigerator . following sections formalize intuition introducing models assign probability possible next word . same models serve assign probability entire sentence . model , example , predict following sequence higher probability appearing text : sudden notice three guys standing sidewalk same set words different order : guys notice sidewalk three sudden standing Why predict upcoming words , assign probabilities sen - tences ? Probabilities essential task identify words noisy , ambiguous input , like speech recognition . speech recognizer realize back soonish bassoon dish , helps know back soonish probable sequence bassoon dish . writing tools like spelling correction grammatical error correction , need find correct errors writing like two midterms , mistyped , Everything improve , improve improved . phrase probable , improved improve , allowing help users detecting correcting errors . Assigning probabilities sequences words essential machine trans - lation . Suppose translating Chinese source sentence : 他 向 记者 介绍 了 主要 内容 reporters introduced main content part process might built following set potential rough English translations : introduced reporters main contents statement briefed reporters main contents statement briefed reporters main contents statement 2 CHAPTER 3 • N-GRAM LANGUAGE MODELS probabilistic model word sequences suggest briefed reporters probable English phrase briefed reporters ( awkward briefed ) introduced reporters ( verb less fluent English context ) , allowing correctly select boldfaced sentence . Probabilities important augmentative alternative communi - cation systems ( Trnka et al . 2007 , Kane et al . 2017 ) . People often AACAAC devices physically unable sign eye gaze specific movements select words menu spoken system . Word prediction suggest likely words menu . Models assign probabilities sequences words called language mod - els LMs . chapter introduce simplest model assigns probabilitieslanguage model LM sentences sequences words , n-gram . n-gram sequence N n-gram words : 2-gram ( bigram ) two-word sequence words like “ please turn ” , “ turn ” , ” homework ” , 3-gram ( trigram ) three-word se - quence words like “ please turn ” , “ turn homework ” . n-gram models estimate probability last word n-gram previous words , assign probabilities entire sequences . bit terminological ambiguity , usually drop word “ model ” , thus term n - gram mean word sequence itself predictive model assigns probability . later chapters introduce sophisticated language models like RNN LMs Chapter 9 . 3.1 N-Grams begin task computing P ( w | h ) , probability word w history h . Suppose history h “ water transparent ” know probability next word : P ( | water transparent ) . ( 3.1 ) way estimate probability relative frequency counts : take large corpus , count number times water transparent , count number times followed . answering question “ times saw history h , many times followed word w ” , follows : P ( | water transparent ) = C ( water transparent ) C ( water transparent ) ( 3.2 ) large enough corpus , web , compute counts estimate probability Eq . 3.2 . pause , go web , compute estimate yourself . method estimating probabilities directly counts works fine many cases , turns even web big enough give good estimates cases . language creative ; new sentences created time , always able count entire sentences . Even simple extensions example sentence counts zero web ( “ Walden Pond’s water transparent ” ; well , counts zero ) . 3.1 • N-GRAMS 3 Similarly , wanted know joint probability entire sequence words like water transparent , asking “ possible sequences five words , many water transparent ? ” get count water transparent divide sum counts possible five word sequences . seems rather lot estimate ! reason , need introduce cleverer ways estimating proba - bility word w history h , probability entire word sequence W . start little formalizing notation . represent probability par - ticular random variable Xi taking value “ ” , P ( Xi = “ ” ) , simplification P ( ) . represent sequence N words w1 . . . wn wn1 ( expression w n − 1 1 means string w1 , w2 , . . . , wn − 1 ) . joint prob - ability word sequence particular value P ( X = w1 , Y = w2 , Z = w3 , . . . , W = wn ) P ( w1 , w2 , . . . , wn ) . compute probabilities entire sequences like P ( w1 , w2 , . . . , wn ) ? thing decompose probability chain rule proba - bility : P ( X1 . . . Xn ) = P ( X1 ) P ( X2 | X1 ) P ( X3 | X21 ) . . . P ( Xn | Xn − 11 ) = n ∏ k = 1 P ( Xk | Xk − 11 ) ( 3.3 ) Applying chain rule words , get P ( wn1 ) = P ( w1 ) P ( w2 | w1 ) P ( w3 | w21 ) . . . P ( wn | wn − 11 ) = n ∏ k = 1 P ( wk | wk − 11 ) ( 3.4 ) chain rule shows link computing joint probability se - quence computing conditional probability word previous words . Equation 3.4 suggests estimate joint probability entire se - quence words multiplying together number conditional probabilities . chain rule really seem help ! know way compute exact probability word long sequence preceding words , P ( wn | wn − 11 ) . , just estimate counting number times every word occurs following every long string , language creative particular context might never occurred ! intuition n-gram model computing probability word entire history , approximate history just last few words . bigram model , example , approximates probability word givenbigram previous words P ( wn | wn − 11 ) conditional probability preceding word P ( wn | wn − 1 ) . words , computing probability P ( | Walden Pond’s water transparent ) ( 3.5 ) approximate probability P ( | ) ( 3.6 ) 4 CHAPTER 3 • N-GRAM LANGUAGE MODELS bigram model predict conditional probability next word , thus making following approximation : P ( wn | wn − 11 ) ≈ P ( wn | wn − 1 ) ( 3.7 ) assumption probability word depends previous word called Markov assumption . Markov models class probabilistic modelsMarkov assume predict probability future unit looking far past . generalize bigram ( looks word past ) trigram ( looks two words past ) thus n-gram ( whichn-gram looks n − 1 words past ) . Thus , general equation n-gram approximation conditional probability next word sequence P ( wn | wn − 11 ) ≈ P ( wn | w n − 1 n − N + 1 ) ( 3.8 ) bigram assumption probability individual word , compute probability complete word sequence substituting Eq . 3.7 Eq . 3.4 : P ( wn1 ) ≈ n ∏ k = 1 P ( wk | wk − 1 ) ( 3.9 ) estimate bigram n-gram probabilities ? intuitive way estimate probabilities called maximum likelihood estimation MLE . get maximum likelihood estimation MLE estimate parameters n-gram model getting counts corpus , normalizing counts lie 0 1.1normalize example , compute particular bigram probability word y previous word x , compute count bigram C ( xy ) normalize sum bigrams share same first word x : P ( wn | wn − 1 ) = C ( wn − 1wn ) ∑ w C ( wn − 1w ) ( 3.10 ) simplify equation , sum bigram counts start word wn − 1 equal unigram count word wn − 1 ( reader take moment convinced ) : P ( wn | wn − 1 ) = C ( wn − 1wn ) C ( wn − 1 ) ( 3.11 ) work example mini-corpus three sentences . first need augment sentence special symbol < s > beginning sentence , give bigram context first word . need special end-symbol . < / s > 2 < s > Sam < / s > < s > Sam < / s > < s > like green eggs ham < / s > 1 probabilistic models , normalizing means dividing total count resulting prob - abilities fall legally 0 1 . 2 need end-symbol make bigram grammar true probability distribution . end-symbol , sentence probabilities sentences length sum . model define infinite set probability distributions , distribution per sentence length . Exercise 3.5 . 3.1 • N-GRAMS 5 calculations bigram probabilities corpus P ( | < s > ) = 23 = . 67 P ( Sam | < s > ) = 1 3 = . 33 P ( | ) = 2 3 = . 67 P ( < / s > | Sam ) = 12 = 0.5 P ( Sam | ) = 1 2 = . 5 P ( | ) = 1 3 = . 33 general case MLE n-gram parameter estimation : P ( wn | wn − 1n − N + 1 ) = C ( wn − 1n − N + 1wn ) C ( wn − 1n − N + 1 ) ( 3.12 ) Equation 3.12 ( like Eq . 3.11 ) estimates n-gram probability dividing observed frequency particular sequence observed frequency prefix . ratio called relative frequency . relativerelativefrequency frequencies way estimate probabilities example maximum likelihood estimation MLE . MLE , resulting parameter set maximizes likelihood training set T model M ( i.e . , P ( T | M ) ) . example , suppose word Chinese occurs 400 times corpus million words like Brown corpus . probability random word selected text , say , million words word Chinese ? MLE probability 4001000000 . 0004 . . 0004 best possible estimate probability Chinese occurring situations ; might turn corpus context Chinese unlikely word . probability makes likely Chinese occur 400 times million-word corpus . present ways modify MLE estimates slightly get better probability estimates Section 3.4 . move examples slightly larger corpus 14-word example . data now-defunct Berkeley Restaurant Project , dialogue system last century answered questions database restaurants Berkeley , California ( Jurafsky et al . , 1994 ) . text - normalized sample user queries ( sample 9332 sentences website ) : tell good cantonese restaurants close mid priced thai food looking tell chez panisse give listing kinds food available looking good place eat breakfast caffe venezia open day Figure 3.1 shows bigram counts piece bigram grammar Berkeley Restaurant Project . Note majority values zero . fact , chosen sample words cohere ; matrix selected random set seven words even sparse . Figure 3.2 shows bigram probabilities normalization ( dividing cell Fig . 3.1 appropriate unigram row , taken following set unigram probabilities ) : eat chinese food lunch spend 2533 927 2417 746 158 1093 341 278 few useful probabilities : P ( | < s > ) = 0.25 P ( english | ) = 0.0011 P ( food | english ) = 0.5 P ( < / s > | food ) = 0.68 compute probability sentences like English food Chinese food simply multiplying appropriate bigram probabilities - gether , follows : 6 CHAPTER 3 • N-GRAM LANGUAGE MODELS eat chinese food lunch spend 5 827 0 9 0 0 0 2 2 0 608 1 6 6 5 1 2 0 4 686 2 0 6 211 eat 0 0 2 0 16 2 42 0 chinese 1 0 0 0 0 82 1 0 food 15 0 15 0 1 4 0 0 lunch 2 0 0 0 0 1 0 0 spend 1 0 1 0 0 0 0 0 Figure 3.1 Bigram counts eight words ( V = 1446 ) Berkeley Restau - rant Project corpus 9332 sentences . Zero counts gray . eat chinese food lunch spend 0.002 0.33 0 0.0036 0 0 0 0.00079 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087 eat 0 0 0.0027 0 0.021 0.0027 0.056 0 chinese 0.0063 0 0 0 0 0.52 0.0063 0 food 0.014 0 0.014 0 0.00092 0.0037 0 0 lunch 0.0059 0 0 0 0 0.0029 0 0 spend 0.0036 0 0.0036 0 0 0 0 0 Figure 3.2 Bigram probabilities eight words Berkeley Restaurant Project corpus 9332 sentences . Zero probabilities gray . P ( < s > english food < / s > ) = P ( | < s > ) P ( | ) P ( english | ) P ( food | english ) P ( < / s > | food ) = . 25 × . 33 × . 0011 × 0.5 × 0.68 = . 000031 leave Exercise 3.2 compute probability chinese food . kinds linguistic phenomena captured bigram statistics ? bigram probabilities encode facts think strictly syntactic nature , like fact comes eat usually noun adjective , comes usually verb . Others might fact personal assistant task , like high probability sentences beginning words . might even cultural rather linguistic , like higher probability people looking Chinese versus English food . practical issues : Although pedagogical purposes described bigram models , practice common trigram models , con-trigram dition previous two words rather previous word , 4-gram even4-gram 5-gram models , sufficient training data . Note larger n-5-gram grams , need assume extra context contexts left right sentence end . example , compute trigram probabilities beginning sentence , two pseudo-words first trigram ( i.e . , P ( | < s >< s > ) . always represent compute language model probabilities log format log probabilities . probabilities ( definition ) less equal tologprobabilities 1 , probabilities multiply together , smaller product becomes . Multiplying enough n-grams together result numerical underflow . log probabilities raw probabilities , get numbers small . 3.2 • EVALUATING LANGUAGE MODELS 7 Adding log space equivalent multiplying linear space , combine log probabilities adding . result computation storage log space need convert back probabilities need report end ; just take exp logprob : p1 × p2 × p3 × p4 = exp ( log p1 + log p2 + log p3 + log p4 ) ( 3.13 ) 3.2 Evaluating Language Models best way evaluate performance language model embed application measure application improves . end-to-end evaluation called extrinsic evaluation . Extrinsic evaluation way toextrinsicevaluation know particular improvement component really going help task hand . Thus , speech recognition , compare performance two language models running speech recognizer twice , once language model , seeing gives accurate transcription . Unfortunately , running big NLP systems end-to-end often expensive . - stead , nice metric quickly evaluate potential improvements language model . intrinsic evaluation metric mea-intrinsicevaluation sures quality model independent application . intrinsic evaluation language model need test set . many statistical models field , probabilities n-gram model come corpus trained , training set training corpus . measuretraining set quality n-gram model performance unseen data called test set test corpus . sometimes call test sets datasets thattest set training sets held corpora hold theheld training data . corpus text compare two different n-gram models , divide data training test sets , train parameters models training set , compare well two trained models fit test set . mean “ fit test set ” ? answer simple : whichever model assigns higher probability test set — meaning accurately predicts test set — better model . two probabilistic models , better model tighter fit test data better predicts details test data , hence assign higher probability test data . evaluation metric based test set probability , important let test sentences training set . Suppose trying compute probability particular “ test ” sentence . test sentence part training corpus , mistakenly assign artificially high probability occurs test set . call situation training test set . Training test set introduces bias makes probabilities look high , causes huge inaccuracies perplexity , probability-based metric introduce below . Sometimes particular test set often implicitly tune characteristics . need fresh test set truly unseen . cases , call initial test set development test set , devset . divide ourdevelopmenttest data training , development , test sets ? test set large possible , small test set accidentally unrepresentative , training data possible . minimum , pick 8 CHAPTER 3 • N-GRAM LANGUAGE MODELS smallest test set gives enough statistical power measure statistically significant difference two potential models . practice , often just divide data 80 % training , 10 % development , 10 % test . large corpus divide training test , test data taken continuous sequence text inside corpus , remove smaller “ stripes ” text randomly selected parts corpus combine test set . 3.2.1 Perplexity practice raw probability metric evaluating language mod - els , variant called perplexity . perplexity ( sometimes called PP short ) perplexity language model test set inverse probability test set , normalized number words . test set W = w1w2 . . . wN , : PP ( W ) = P ( w1w2 . . . wN ) − 1 N ( 3.14 ) = N √ 1 P ( w1w2 . . . wN ) chain rule expand probability W : PP ( W ) = N √ √ √ √ N ∏ = 1 1 P ( wi | w1 . . . wi − 1 ) ( 3.15 ) Thus , computing perplexity W bigram language model , get : PP ( W ) = N √ √ √ √ N ∏ = 1 1 P ( wi | wi − 1 ) ( 3.16 ) Note inverse Eq . 3.15 , higher conditional probabil - ity word sequence , lower perplexity . Thus , minimizing perplexity equivalent maximizing test set probability according language model . generally word sequence Eq . 3.15 Eq . 3.16 entire se - quence words test set . sequence cross many sentence boundaries , need include begin - end-sentence markers < s > < / s > probability computation . need include end-of-sentence marker < / s > ( beginning-of-sentence marker < s > ) total count word - kens N . another way think perplexity : weighted average branch - ing factor language . branching factor language number possi - ble next words follow word . Consider task recognizing digits English ( zero , , two , . . . , nine ) , ( training set test set ) 10 digits occurs equal probability P = 110 . perplexity mini-language fact 10 . , imagine test string digits length N , assume training set digits occurred equal probability . Eq . 3.15 , perplexity 3.3 • GENERALIZATION ZEROS 9 PP ( W ) = P ( w1w2 . . . wN ) − 1 N = ( 1 10 N ) − 1 N = 1 10 − 1 = 10 ( 3.17 ) suppose number zero really frequent occurs far often numbers . say 0 occur 91 times training set , digits occurred 1 time . following test set : 0 0 0 0 0 3 0 0 0 0 . expect perplexity test set lower time next number zero , predictable , i.e . high probability . Thus , although branching factor still 10 , perplexity weighted branching factor smaller . leave exact calculation exercise 12 . Section 3.7 perplexity closely related information - theoretic notion entropy . Finally , look example perplexity compare dif - ferent n-gram models . trained unigram , bigram , trigram grammars 38 million words ( including start-of-sentence tokens ) Wall Street Journal , - ing 19,979 word vocabulary . computed perplexity models test set 1.5 million words Eq . 3.16 . table below shows perplexity 1.5 million word WSJ test set according grammars . Unigram Bigram Trigram Perplexity 962 170 109 , information n-gram gives word sequence , lower perplexity ( Eq . 3.15 showed , perplexity related inversely likelihood test sequence according model ) . Note computing perplexities , n-gram model P constructed knowledge test set prior knowledge vocabulary test set . kind knowledge test set cause perplexity artificially low . perplexity two language models comparable identical vocabularies . ( intrinsic ) improvement perplexity guarantee ( extrinsic ) - provement performance language processing task like speech recognition machine translation . Nonetheless , perplexity often correlates improvements , commonly quick check algorithm . model’s improvement perplexity always confirmed end-to-end evaluation real task concluding evaluation model . 3.3 Generalization Zeros n-gram model , like many statistical models , dependent training corpus . implication probabilities often encode specific facts training corpus . Another implication n-grams better better job modeling training corpus increase value N . 10 CHAPTER 3 • N-GRAM LANGUAGE MODELS visualize facts borrowing technique Shannon ( 1951 ) Miller Selfridge ( 1950 ) generating random sentences dif - ferent n-gram models . simplest visualize works unigram case . Imagine words English language covering probability space 0 1 , word covering interval proportional frequency . choose random value 0 1 print word whose interval includes chosen value . continue choosing random numbers generating words randomly generate sentence-final token < / s > . same technique generate bigrams first generating random bigram starts < s > ( according bigram probability ) . say second word bigram w . next chose random bigram starting w ( again , drawn according bigram probability ) , . give intuition increasing power higher-order n-grams , Fig . 3.3 shows random sentences generated unigram , bigram , trigram , 4-gram models trained Shakespeare’s works . 1 – swallowed confess hear . . save trail ay device rote life gram – Hill late speaks ; ! leg less first enter 2 – Why dost stand forth thy canopy , forsooth ; palpable hit King Henry . Live king . Follow . gram – means , sir . confess ? sorts , trim , captain . 3 – Fly , rid news price . sadness parting , say , ’ tis . gram – shall forbid branded , renown made empty . 4 – King Henry . ! go seek traitor Gloucester . Exeunt watch . great banquet serv’d ; gram – . Figure 3.3 Eight sentences randomly generated four n-grams computed Shakespeare’s works . characters mapped lower-case punctuation marks treated words . Output hand-corrected capitalization improve readability . longer context train model , coherent sen - tences . unigram sentences , coherent relation words sentence-final punctuation . bigram sentences local word-to-word coherence ( especially consider punctuation counts word ) . tri - gram 4-gram sentences beginning look lot like Shakespeare . Indeed , careful investigation 4-gram sentences shows look little like Shakespeare . words directly King John . , put knock Shakespeare , oeuvre large corpora go ( N = 884,647 , V = 29,066 ) , n-gram probability matrices ridiculously sparse . V 2 = 844,000,000 possible bigrams alone , number pos - sible 4-grams V 4 = 7 × 1017 . Thus , once generator chosen first 4-gram ( ) , five possible continuations ( , , , thou , ) ; indeed , many 4-grams , continuation . get idea dependence grammar training set , look n-gram grammar trained completely different corpus : Wall Street Journal ( WSJ ) newspaper . Shakespeare Wall Street Journal English , might expect overlap n-grams two genres . Fig . 3.4 3.3 • GENERALIZATION ZEROS 11 shows sentences generated unigram , bigram , trigram grammars trained 40 million words WSJ . 1 Months issue year foreign new exchange’s septemberwere recession exchange new endorsed acquire six executives gram 2 Last December way preserve Hudson corporation N . B . E . C . Taylor seem complete major central planners gram point five percent U . S . E . already old M . X . corporation living information frequently fishing keep 3 point ninety nine point six billion dollars two hundred four oh six three percent rates interest stores Mexico gram Brazil market conditions Figure 3.4 Three sentences randomly generated three n-gram models computed 40 million words Wall Street Journal , lower-casing characters treating punctua - tion words . Output hand-corrected capitalization improve readability . Compare examples pseudo-Shakespeare Fig . 3.3 . model “ English-like sentences ” , clearly overlap generated sentences , little overlap even small phrases . Statistical models likely pretty - less predictors training sets test sets different Shakespeare WSJ . deal problem build n-gram models ? step sure training corpus similar genre whatever task trying accomplish . build language model translating legal documents , need training corpus legal documents . build language model question-answering system , need training corpus questions . equally important get training data appropriate dialect , especially processing social media posts spoken transcripts . Thus tweets AAVE ( African American Vernacular English ) often words like finna — auxiliary verb marks immediate future tense — occur dialects , spellings like den , tweets like ( Blodgett O’Connor , 2017 ) : ( 3.18 ) Bored af den phone finna die ! ! ! tweets varieties like Nigerian English markedly different vocabu - lary n-gram patterns American English ( Jurgens et al . , 2017 ) : ( 3.19 ) @username R u wizard wat gan sef : d mornin - u tweet , afternoon - u tweet , nyt gan u dey tweet . beta get ur placement wiv twitter Matching genres dialects still sufficient . models still subject problem sparsity . n-gram occurred sufficient number times , might good estimate probability . corpus limited , perfectly acceptable English word sequences bound missing . , many cases putative “ zero probability n-grams ” really non-zero probability . Consider words follow bigram denied WSJ Treebank3 corpus , together counts : denied allegations : 5 denied speculation : 2 denied rumors : 1 denied report : 1 suppose test set phrases like : 12 CHAPTER 3 • N-GRAM LANGUAGE MODELS denied offer denied loan model incorrectly estimate P ( offer | denied ) 0 ! zeros — things ever occur training set occur inzeros test set — problem two reasons . First , presence means underestimating probability sorts words might occur , hurt performance application run data . Second , probability word test set 0 , entire probability test set 0 . definition , perplexity based inverse probability test set . Thus words zero probability , compute perplexity , divide 0 ! 3.3.1 Unknown Words previous section discussed problem words whose bigram probability zero . words simply never seen ? Sometimes language task happen know words occur . closed vocabulary system test set canclosedvocabulary contain words lexicon , unknown words . reasonable assumption domains , speech recognition machine translation , pronunciation dictionary phrase table fixed advance , language model words dictionary phrase table . cases deal words haven’t seen , call unknown words , vocabulary ( OOV ) words . percentage OOVOOV words appear test set called OOV rate . open vocabulary systemopenvocabulary model potential unknown words test set adding pseudo-word called < UNK > . two common ways train probabilities unknown word model < UNK > . first turn problem back closed vocabulary choosing fixed vocabulary advance : 1 . Choose vocabulary ( word list ) fixed advance . 2 . Convert training set word set ( OOV word ) unknown word token < UNK > text normalization step . 3 . Estimate probabilities < UNK > counts just like regular word training set . second alternative , situations prior vocabulary ad - vance , create vocabulary implicitly , replacing words training data < UNK > based frequency . example replace < UNK > words occur fewer n times training set , n small number , equivalently select vocabulary size V advance ( say 50,000 ) choose top V words frequency replace rest UNK . case proceed train language model , treating < UNK > like regular word . exact choice < UNK > model effect metrics like perplexity . language model achieve low perplexity choosing small vocabulary assigning unknown word high probability . reason , perplexities compared language models same vocabularies ( Buck et al . , 2014 ) . 3.4 • SMOOTHING 13 3.4 Smoothing words vocabulary ( unknown words ) appear test set unseen context ( example appear word never appeared training ) ? keep language model assigning zero probability unseen events , shave off bit probability mass frequent events give events never seen . modification called smoothing discounting . section fol-smoothing discounting lowing ones introduce variety ways smoothing : add-1 smoothing , add-k smoothing , stupid backoff , Kneser-Ney smoothing . 3.4.1 Laplace Smoothing simplest way smoothing add bigram counts , normalize probabilities . counts zero count 1 , counts 1 2 , . algorithm called Laplace smoothing . Laplace smoothing perform well enough usedLaplacesmoothing modern n-gram models , usefully introduces many concepts smoothing algorithms , gives useful baseline , practical smoothing algorithm tasks like text classification ( Chapter 4 ) . start application Laplace smoothing unigram probabilities . Recall unsmoothed maximum likelihood estimate unigram probability word wi count ci normalized total number word tokens N : P ( wi ) = ci N Laplace smoothing merely adds count ( hence alternate name add - smoothing ) . V words vocabulary incre-add-one mented , need adjust denominator take account extra V observations . ( happens P values increase denominator ? ) PLaplace ( wi ) = ci + 1 N + V ( 3.20 ) changing numerator denominator , convenient describe smoothing algorithm affects numerator , defining adjusted count c ∗ . adjusted count easier compare directly MLE counts turned probability like MLE count normalizing N . define count , changing numerator addition adding 1 need multiply normalization factor NN + V : c ∗ = ( ci + 1 ) N N + V ( 3.21 ) turn c ∗ probability P ∗ normalizing N . related way view smoothing discounting ( lowering ) non-zerodiscounting counts order get probability mass assigned zero counts . Thus , referring discounted counts c ∗ , might describe smooth - ing algorithm terms relative discount dc , ratio discounted countsdiscount original counts : 14 CHAPTER 3 • N-GRAM LANGUAGE MODELS dc = c ∗ c intuition unigram case , smooth Berkeley Restaurant Project bigrams . Figure 3.5 shows add-one smoothed counts bigrams Fig . 3.1 . eat chinese food lunch spend 6 828 1 10 1 1 1 3 3 1 609 2 7 7 6 2 3 1 5 687 3 1 7 212 eat 1 1 3 1 17 3 43 1 chinese 2 1 1 1 1 83 2 1 food 16 1 16 1 2 5 1 1 lunch 3 1 1 1 1 2 1 1 spend 2 1 2 1 1 1 1 1 Figure 3.5 Add-one smoothed bigram counts eight words ( V = 1446 ) Berkeley Restaurant Project corpus 9332 sentences . Previously-zero counts gray . Figure 3.6 shows add-one smoothed probabilities bigrams Fig . 3.2 . Recall normal bigram probabilities computed normalizing row counts unigram count : P ( wn | wn − 1 ) = C ( wn − 1wn ) C ( wn − 1 ) ( 3.22 ) add-one smoothed bigram counts , need augment unigram count number total word types vocabulary V : P ∗ Laplace ( wn | wn − 1 ) = C ( wn − 1wn ) + 1 ∑ w ( C ( wn − 1w ) + 1 ) = C ( wn − 1wn ) + 1 C ( wn − 1 ) + V ( 3.23 ) Thus , unigram counts previous section need augmented V = 1446 . result smoothed bigram probabilities Fig . 3.6 . eat chinese food lunch spend 0.0015 0.21 0.00025 0.0025 0.00025 0.00025 0.00025 0.00075 0.0013 0.00042 0.26 0.00084 0.0029 0.0029 0.0025 0.00084 0.00078 0.00026 0.0013 0.18 0.00078 0.00026 0.0018 0.055 eat 0.00046 0.00046 0.0014 0.00046 0.0078 0.0014 0.02 0.00046 chinese 0.0012 0.00062 0.00062 0.00062 0.00062 0.052 0.0012 0.00062 food 0.0063 0.00039 0.0063 0.00039 0.00079 0.002 0.00039 0.00039 lunch 0.0017 0.00056 0.00056 0.00056 0.00056 0.0011 0.00056 0.00056 spend 0.0012 0.00058 0.0012 0.00058 0.00058 0.00058 0.00058 0.00058 Figure 3.6 Add-one smoothed bigram probabilities eight words ( V = 1446 ) BeRP corpus 9332 sentences . Previously-zero probabilities gray . often convenient reconstruct count matrix smoothing algorithm changed original counts . adjusted counts computed Eq . 3.24 . Figure 3.7 shows reconstructed counts . c ∗ ( wn − 1wn ) = [ C ( wn − 1wn ) + 1 ] × C ( wn − 1 ) C ( wn − 1 ) + V ( 3.24 ) 3.4 • SMOOTHING 15 eat chinese food lunch spend 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78 1.9 0.63 3.1 430 1.9 0.63 4.4 133 eat 0.34 0.34 1 0.34 5.8 1 15 0.34 chinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098 food 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43 lunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19 spend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16 Figure 3.7 Add-one reconstituted counts eight words ( V = 1446 ) BeRP corpus 9332 sentences . Previously-zero counts gray . Note add-one smoothing made big change counts . C ( ) changed 609 238 ! probability space well : P ( | ) decreases . 66 unsmoothed case . 26 smoothed case . Looking discount d ( ratio new old counts ) shows strikingly counts prefix word reduced ; discount bigram . 39 , discount Chinese food . 10 , factor 10 ! sharp change counts probabilities occurs probabil - ity mass moved zeros . 3.4.2 Add-k smoothing alternative add-one smoothing move bit less probability mass seen unseen events . adding 1 count , add frac - tional count k ( . 5 ? . 05 ? . 01 ? ) . algorithm called add-k smoothing . add-k P ∗ Add-k ( wn | wn − 1 ) = C ( wn − 1wn ) + k C ( wn − 1 ) + kV ( 3.25 ) Add-k smoothing requires method choosing k ; , example , optimizing devset . Although add-k useful tasks ( including text classification ) , turns still work well language modeling , generating counts poor variances often inappropriate discounts ( Gale Church , 1994 ) . 3.4.3 Backoff Interpolation discounting discussing far help solve problem zero frequency n-grams . additional source knowledge draw . trying compute P ( wn | wn − 2wn − 1 ) examples particular trigram wn − 2wn − 1wn , estimate probability bigram probability P ( wn | wn − 1 ) . Similarly , counts compute P ( wn | wn − 1 ) , look unigram P ( wn ) . words , sometimes less context good thing , helping general - ize contexts model hasn’t learned . two ways n-gram “ hierarchy ” . backoff , trigram evidence isbackoff sufficient , otherwise bigram , otherwise unigram . words , “ back off ” lower-order n-gram zero evidence higher-order n-gram . contrast , interpolation , always mix probability estimates frominterpolation n-gram estimators , weighing combining trigram , bigram , unigram counts . 16 CHAPTER 3 • N-GRAM LANGUAGE MODELS simple linear interpolation , combine different order n-grams linearly - terpolating models . Thus , estimate trigram probability P ( wn | wn − 2wn − 1 ) mixing together unigram , bigram , trigram probabilities , weighted λ : P̂ ( wn | wn − 2wn − 1 ) = λ1P ( wn | wn − 2wn − 1 ) + λ2P ( wn | wn − 1 ) + λ3P ( wn ) ( 3.26 ) λ s sum 1 : ∑ λi = 1 ( 3.27 ) slightly sophisticated version linear interpolation , λ weight computed conditioning context . way , particularly accurate counts particular bigram , assume counts trigrams based bigram trustworthy , make λ s trigrams higher thus give trigram weight interpolation . Equation 3.28 shows equation interpolation context-conditioned weights : P̂ ( wn | wn − 2wn − 1 ) = λ1 ( wn − 1n − 2 ) P ( wn | wn − 2wn − 1 ) + λ2 ( wn − 1n − 2 ) P ( wn | wn − 1 ) + λ3 ( wn − 1n − 2 ) P ( wn ) ( 3.28 ) λ values set ? simple interpolation conditional inter - polation λ s learned held-out corpus . held-out corpus additionalheld-out training corpus set hyperparameters like λ values , choosing λ values maximize likelihood held-out corpus . , fix n-gram probabilities search λ values — plugged Eq . 3.26 — give highest probability held-out set . various ways find optimal set λ s . way EM algorithm , iterative learning algorithm converges locally optimal λ s ( Jelinek Mercer , 1980 ) . backoff n-gram model , n-gram need zero counts , approxi - mate backing off ( N-1 ) - gram . continue backing off reach history counts . order backoff model give correct probability distribution , discount higher-order n-grams save probability mass lowerdiscount order n-grams . Just add-one smoothing , higher-order n-grams discounted just undiscounted MLE probability , soon replaced n-gram zero probability lower-order n-gram , adding probability mass , total probability assigned possible strings language model greater 1 ! addition explicit discount factor , need function α distribute probability mass lower order n-grams . kind backoff discounting called Katz backoff . Katz back-Katz backoff off rely discounted probability P ∗ seen n-gram ( i.e . , non-zero counts ) . Otherwise , recursively back off Katz probabil - ity shorter-history ( N-1 ) - gram . probability backoff n-gram PBO 3.5 • KNESER-NEY SMOOTHING 17 thus computed follows : PBO ( wn | w n − 1 n − N + 1 ) =    P ∗ ( wn | wn − 1n − N + 1 ) , C ( wnn − N + 1 ) > 0 α ( wn − 1n − N + 1 ) PBO ( wn | w n − 1 n − N + 2 ) , otherwise . ( 3.29 ) Katz backoff often combined smoothing method called Good-Turing . Good-Turing combined Good-Turing backoff algorithm involves quite detailed computation estimating Good-Turing smoothing P ∗ α values . 3.5 Kneser-Ney Smoothing commonly best performing n-gram smoothing methods interpolated Kneser-Ney algorithm ( Kneser Ney 1995 , Chen Good-Kneser-Ney man 1998 ) . Kneser-Ney roots method called absolute discounting . Recall discounting counts frequent n-grams necessary save probability mass smoothing algorithm distribute unseen n-grams . , clever idea Church Gale ( 1991 ) . Consider n-gram count 4 . need discount count amount . discount ? Church Gale’s clever idea look held-out corpus just count bigrams count 4 training set . computed bigram grammar 22 million words AP newswire checked counts bigrams another 22 million words . average , bigram occurred 4 times first 22 million words occurred 3.23 times next 22 million words . following table Church Gale ( 1991 ) shows counts bigrams c 0 9 : Bigram count Bigram count training set heldout set 0 0.0000270 1 0.448 2 1.25 3 2.24 4 3.23 5 4.21 6 5.23 7 6.21 8 7.21 9 8.26 Figure 3.8 bigrams 22 million words AP newswire count 0 , 1 , 2 , . . . , 9 , counts bigrams held-out corpus 22 million words . astute reader noticed except held-out counts 0 1 , bigram counts held-out set estimated pretty well just subtracting 0.75 count training set ! Absolute discountingAbsolutediscounting formalizes intuition subtracting fixed ( absolute ) discount d count . intuition good estimates already high counts , small discount d affect . mainly modify smaller counts , 18 CHAPTER 3 • N-GRAM LANGUAGE MODELS necessarily trust estimate anyway , Fig . 3.8 suggests practice discount actually good bigrams counts 2 9 . equation interpolated absolute discounting applied bigrams : PAbsoluteDiscounting ( wi | wi − 1 ) = C ( wi − 1wi ) − d ∑ v C ( wi − 1 v ) + λ ( wi − 1 ) P ( wi ) ( 3.30 ) first term discounted bigram , second term unigram interpolation weight λ . just set d values . 75 , keep separate discount value 0.5 bigrams counts 1 . Kneser-Ney discounting ( Kneser Ney , 1995 ) augments absolute discount - ing sophisticated way handle lower-order unigram distribution . Consider job predicting next word sentence , assuming inter - polating bigram unigram model . reading . word glasses seems likely follow , say , word Kong , like unigram model prefer glasses . fact Kong common , Hong Kong frequent word . standard unigram model assign Kong higher probability glasses . like capture intuition although Kong frequent , mainly frequent phrase Hong Kong , , word Hong . word glasses wider distribution . words , P ( w ) , answers question “ likely w ? ” , like create unigram model might call PCONTINUATION , answers question “ likely w appear novel continuation ? ” . estimate probability seeing word w novel continuation , new unseen context ? Kneser-Ney intuition base estimate PCONTINUATION number different contexts word w appeared , , number bigram types completes . Every bigram type novel continuation first time seen . hypothesize words appeared contexts past likely appear new context well . number times word w appears novel continuation expressed : PCONTINUATION ( w ) ∝ | { v : C ( vw ) > 0 } | ( 3.31 ) turn count probability , normalize total number word bigram types . summary : PCONTINUATION ( w ) = | { v : C ( vw ) > 0 } | | { ( u ′ , w ′ ) : C ( u ′ w ′ ) > 0 } | ( 3.32 ) equivalent formulation based different metaphor number word types seen precede w ( Eq . 3.31 repeated ) : PCONTINUATION ( w ) ∝ | { v : C ( vw ) > 0 } | ( 3.33 ) normalized number words preceding words , follows : PCONTINUATION ( w ) = | { v : C ( vw ) > 0 } | ∑ w ′ | { v : C ( vw ′ ) > 0 } | ( 3.34 ) frequent word ( Kong ) occurring context ( Hong ) low continuation probability . 3.6 • WEB STUPID BACKOFF 19 final equation Interpolated Kneser-Ney smoothing bigrams : InterpolatedKneser-Ney PKN ( wi | wi − 1 ) = max ( C ( wi − 1wi ) − d , 0 ) C ( wi − 1 ) + λ ( wi − 1 ) PCONTINUATION ( wi ) ( 3.35 ) λ normalizing constant distribute probability mass discounted . : λ ( wi − 1 ) = d ∑ v C ( wi − 1v ) | { w : C ( wi − 1w ) > 0 } | ( 3.36 ) first term , d ∑ v C ( wi − 1v ) , normalized discount . second term , | { w : C ( wi − 1w ) > 0 } | , number word types follow wi − 1 , equiva - lently , number word types discounted ; words , number times applied normalized discount . general recursive formulation follows : PKN ( wi | wi − 1i − n + 1 ) = max ( cKN ( w ii − n + 1 ) − d , 0 ) ∑ v cKN ( w − 1 − n + 1v ) + λ ( wi − 1i − n + 1 ) PKN ( wi | w − 1 − n + 2 ) ( 3.37 ) definition count cKN depends counting highest-order n-gram interpolated ( example trigram interpolating trigram , bigram , unigram ) lower-order n-grams ( bigram unigram interpolating trigram , bigram , unigram ) : cKN ( · ) = { count ( · ) highest order continuationcount ( · ) lower orders ( 3.38 ) continuation count number unique single word contexts · . termination recursion , unigrams interpolated uniform distribution , parameter ε empty string : PKN ( w ) = max ( cKN ( w ) − d , 0 ) ∑ w ′ cKN ( w ′ ) + λ ( ε ) 1 V ( 3.39 ) include unknown word < UNK > , just included regular vo - cabulary entry count zero , hence probability lambda-weighted uniform distribution λ ( ε ) V . best-performing version Kneser-Ney smoothing called modified Kneser - Ney smoothing , due Chen Goodman ( 1998 ) . Rather singlemodifiedKneser-Ney fixed discount d , modified Kneser-Ney three different discounts d1 , d2 , d3 + n-grams counts 1 , 2 three , respectively . Chen Goodman ( 1998 , p . 19 ) Heafield et al . ( 2013 ) details . 3.6 Web Stupid Backoff text web , possible build extremely large language mod - els . 2006 Google released large set n-gram counts , including n-grams ( 1-grams 5-grams ) five-word sequences appear least 20 CHAPTER 3 • N-GRAM LANGUAGE MODELS 40 times 1,024,908,267,229 words running text web ; includes 1,176,470,663 five-word sequences 13 million unique words types ( Franz Brants , 2006 ) . examples : 4-gram Count serve incoming 92 serve incubator 99 serve independent 794 serve index 223 serve indication 72 serve indicator 120 serve indicators 45 serve indispensable 111 serve indispensible 40 serve individual 234 Efficiency considerations important building language models large sets n-grams . Rather store word string , generally represented memory 64-bit hash number , words themselves stored disk . Probabilities generally quantized 4-8 bits ( 8-byte floats ) , n-grams stored reverse tries . N-grams shrunk pruning , example storing n-grams counts greater threshold ( count threshold 40 Google n-gram release ) entropy prune less-important n-grams ( Stolcke , 1998 ) . Another option build approximate language models techniques like Bloom filters ( Talbot Osborne 2007 , Church et al . 2007 ) . Finally , effi-Bloom filters cient language model toolkits like KenLM ( Heafield 2011 , Heafield et al . 2013 ) sorted arrays , efficiently combine probabilities backoffs single value , merge sorts efficiently build probability tables minimal number passes large corpus . Although toolkits possible build web-scale language models full Kneser-Ney smoothing , Brants et al . ( 2007 ) show large lan - guage models simpler algorithm sufficient . algorithm called stupid backoff . Stupid backoff gives up idea trying make languagestupid backoff model true probability distribution . discounting higher-order probabilities . higher-order n-gram zero count , simply backoff lower order n-gram , weighed fixed ( context-independent ) weight . algo - rithm produce probability distribution , follow Brants et al . ( 2007 ) referring S : S ( wi | wi − 1i − k + 1 ) =    count ( wii − k + 1 ) count ( wi − 1i − k + 1 ) count ( wii − k + 1 ) > 0 λS ( wi | wi − 1i − k + 2 ) otherwise ( 3.40 ) backoff terminates unigram , probability S ( w ) = count ( w ) N . Brants et al . ( 2007 ) find value 0.4 worked well λ . 3.7 Advanced : Perplexity’s Relation Entropy introduced perplexity Section 3.2.1 way evaluate n-gram models test set . better n-gram model assigns higher probability 3.7 • ADVANCED : PERPLEXITY’S RELATION ENTROPY 21 test data , perplexity normalized version probability test set . perplexity measure actually arises information-theoretic concept cross-entropy , explains otherwise mysterious properties perplexity ( why inverse probability , example ? ) relationship entropy . Entropy aEntropy measure information . random variable X ranging whatever predicting ( words , letters , parts speech , set call χ ) particular probability function , call p ( x ) , entropy random variable X : H ( X ) = − ∑ x ∈ χ p ( x ) log2 p ( x ) ( 3.41 ) log , principle , computed base . log base 2 , resulting value entropy measured bits . intuitive way think entropy lower bound number bits take encode certain decision piece information optimal coding scheme . Consider example standard information theory textbook Cover Thomas ( 1991 ) . Imagine place bet horse race far go way Yonkers Racetrack , like send short message bookie tell eight horses bet . way encode message just binary representation horse’s number code ; thus , horse 1 001 , horse 2 010 , horse 3 011 , , horse 8 coded 000 . spend whole day betting horse coded 3 bits , average sending 3 bits per race . better ? Suppose spread actual distribution bets placed represent prior probability horse follows : Horse 1 12 Horse 5 1 64 Horse 2 14 Horse 6 1 64 Horse 3 18 Horse 7 1 64 Horse 4 116 Horse 8 1 64 entropy random variable X ranges horses gives lower bound number bits H ( X ) = − = 8 ∑ = 1 p ( ) log p ( ) = − 12 log 1 2 − 1 4 log 1 4 − 1 8 log 1 8 − 1 16 log 1 16 − 4 ( 1 64 log 1 64 ) = 2 bits ( 3.42 ) code averages 2 bits per race built short encodings probable horses , longer encodings less probable horses . example , encode likely horse code 0 , remaining horses 10 , 110 , 1110 , 111100 , 111101 , 111110 , 111111 . horses equally likely ? saw equal - length binary code horse numbers , horse took 3 bits code , average 3 . entropy same ? case horse probability 18 . entropy choice horses H ( X ) = − = 8 ∑ = 1 1 8 log 1 8 = − log 1 8 = 3 bits ( 3.43 ) 22 CHAPTER 3 • N-GRAM LANGUAGE MODELS computing entropy single variable . entropy involves sequences . grammar , example , computing entropy sequence words W = { w0 , w1 , w2 , . . . , wn } . way variable ranges sequences words . example compute entropy random variable ranges finite sequences words length n language L follows : H ( w1 , w2 , . . . , wn ) = − ∑ W n1 ∈ L p ( W n1 ) log p ( W n 1 ) ( 3.44 ) define entropy rate ( think per-wordentropy rate entropy ) entropy sequence divided number words : 1 n H ( W n1 ) = − 1 n ∑ W n1 ∈ L p ( W n1 ) log p ( W n 1 ) ( 3.45 ) measure true entropy language , need consider sequences infinite length . think language stochastic process L produces sequence words , allow W represent sequence words w1 , . . . , wn , L’s entropy rate H ( L ) defined H ( L ) = lim n → ∞ 1 n H ( w1 , w2 , . . . , wn ) = − lim n → ∞ 1 n ∑ W ∈ L p ( w1 , . . . , wn ) log p ( w1 , . . . , wn ) ( 3.46 ) Shannon-McMillan-Breiman theorem ( Algoet Cover 1988 , Cover Thomas 1991 ) states language regular certain ways ( exact , stationary ergodic ) , H ( L ) = lim n → ∞ − 1 n log p ( w1w2 . . . wn ) ( 3.47 ) , take single sequence long enough summing possible sequences . intuition Shannon-McMillan-Breiman - orem long-enough sequence words contain many shorter sequences shorter sequences reoccur longer se - quence according probabilities . stochastic process stationary probabilities assigns aStationary sequence invariant respect shifts time index . words , probability distribution words time t same probability distribution time t + 1 . Markov models , hence n-grams , stationary . example , bigram , Pi dependent Pi − 1 . shift time index x , Pi + x still dependent Pi + x − 1 . natural language stationary , show Chapter 12 , probability upcoming words dependent events arbitrarily distant time dependent . Thus , statistical models give approximation correct distributions entropies natural language . summarize , making incorrect convenient simplifying assump - tions , compute entropy stochastic process taking long sample output computing average log probability . ready introduce cross-entropy . cross-entropy useful whencross-entropy know actual probability distribution p generated data . 3.7 • ADVANCED : PERPLEXITY’S RELATION ENTROPY 23 allows m , model p ( i.e . , approximation p ) . cross-entropy m p defined H ( p , m ) = lim n → ∞ − 1 n ∑ W ∈ L p ( w1 , . . . , wn ) logm ( w1 , . . . , wn ) ( 3.48 ) , draw sequences according probability distribution p , sum log probabilities according m . Again , following Shannon-McMillan-Breiman theorem , stationary er - godic process : H ( p , m ) = lim n → ∞ − 1 n logm ( w1w2 . . . wn ) ( 3.49 ) means , entropy , estimate cross-entropy model m distribution p taking single sequence long enough summing possible sequences . makes cross-entropy useful cross-entropy H ( p , m ) up - per bound entropy H ( p ) . model m : H ( p ) ≤ H ( p , m ) ( 3.50 ) means simplified model m help estimate true en - tropy sequence symbols drawn according probability p . accurate m , closer cross-entropy H ( p , m ) true entropy H ( p ) . Thus , difference H ( p , m ) H ( p ) measure accurate model . two models m1 m2 , accurate model lower cross-entropy . ( cross-entropy never lower true entropy , model err underestimating true entropy . ) finally ready relation perplexity cross-entropy saw Eq . 3.49 . Cross-entropy defined limit , length observed word sequence goes infinity . need approximation cross - entropy , relying ( sufficiently long ) sequence fixed length . approxima - tion cross-entropy model M = P ( wi | wi − N + 1 . . . wi − 1 ) sequence words W H ( W ) = − 1 N logP ( w1w2 . . . wN ) ( 3.51 ) perplexity model P sequence words W formally defined asperplexity exp cross-entropy : Perplexity ( W ) = 2H ( W ) = P ( w1w2 . . . wN ) − 1 N = N √ 1 P ( w1w2 . . . wN ) = N √ √ √ √ N ∏ = 1 1 P ( wi | w1 . . . wi − 1 ) ( 3.52 ) 24 CHAPTER 3 • N-GRAM LANGUAGE MODELS 3.8 Summary chapter introduced language modeling n-gram , widely tools language processing . • Language models offer way assign probability sentence sequence words , predict word preceding words . • n-grams Markov models estimate words fixed window pre - vious words . n-gram probabilities estimated counting corpus normalizing ( maximum likelihood estimate ) . • n-gram language models evaluated extrinsically task , intrinsi - cally perplexity . • perplexity test set according language model geometric mean inverse test set probability computed model . • Smoothing algorithms provide sophisticated way estimate prob - ability n-grams . Commonly smoothing algorithms n-grams rely lower-order n-gram counts backoff interpolation . • backoff interpolation require discounting create probability dis - tribution . • Kneser-Ney smoothing makes probability word novel continuation . interpolated Kneser-Ney smoothing algorithm mixes discounted probability lower-order continuation probability . Bibliographical Historical Notes underlying mathematics n-gram first proposed Markov ( 1913 ) , called Markov chains ( bigrams trigrams ) predict upcoming letter Pushkin’s Eugene Onegin vowel con - sonant . Markov classified 20,000 letters V C computed bigram trigram probability letter vowel previous two letters . Shannon ( 1948 ) applied n-grams compute approximations English word sequences . Based Shannon’s work , Markov models commonly engineering , linguistic , psychological work modeling word sequences 1950s . series extremely influential papers starting Chomsky ( 1956 ) including Chomsky ( 1957 ) Miller Chomsky ( 1963 ) , Noam Chomsky argued “ finite-state Markov processes ” , possibly useful engineering heuristic , incapable complete cognitive model human grammatical knowl - edge . arguments led many linguists computational linguists ignore work statistical modeling decades . resurgence n-gram models came Jelinek colleagues IBM Thomas J . Watson Research Center , influenced Shannon , Baker CMU , influenced work Baum colleagues . Independently two labs successfully n-grams speech recognition systems ( Baker 1975b , Jelinek 1976 , Baker 1975a , Bahl et al . 1983 , Jelinek 1990 ) . trigram model IBM TANGORA speech recognition system 1970s , idea written up later . Add-one smoothing derives Laplace’s 1812 law succession first applied engineering solution zero-frequency problem Jeffreys ( 1948 ) EXERCISES 25 based earlier Add-K suggestion Johnson ( 1932 ) . Problems add - algorithm summarized Gale Church ( 1994 ) . wide variety different language modeling smoothing techniques proposed 80s 90s , including Good-Turing discounting — first applied n-gram smoothing IBM Katz ( Nádas 1984 , Church Gale 1991 ) — Witten-Bell discounting ( Witten Bell , 1991 ) , varieties class-based n - gram models information word classes . class-basedn-gram Starting late 1990s , Chen Goodman produced highly influential series papers comparison different language models ( Chen Good - man 1996 , Chen Goodman 1998 , Chen Goodman 1999 , Goodman 2006 ) . performed number carefully controlled experiments comparing differ - ent discounting algorithms , cache models , class-based models , language model parameters . showed advantages Modified Interpolated Kneser - Ney , become standard baseline language modeling , espe - cially showed caches class-based models provided minor additional improvement . papers recommended reader further interest language modeling . Two commonly toolkits building language models SRILM ( Stolcke , 2002 ) KenLM ( Heafield 2011 , Heafield et al . 2013 ) . publicly available . SRILM offers wider range options types discounting , KenLM optimized speed memory size , making possible build web-scale lan - guage models . highest accuracy language models neural network language models . solve major problem n-gram language models : number parame - ters increases exponentially n-gram order increases , n-grams way generalize training test set . Neural language models project words continuous space words similar contexts similar represen - tations . introduce feedforward language models ( Bengio et al . 2006 , Schwenk 2007 ) Chapter 7 , recurrent language models ( Mikolov , 2012 ) Chapter 9 . Exercises 3.1 Write equation trigram probability estimation ( modifying Eq . 3.11 ) . write non-zero trigram probabilities Sam corpus page 4 . 3.2 Calculate probability sentence chinese food . Give two probabilities , Fig . 3.2 ‘ useful probabilities ’ just below page 6 , another add-1 smoothed table Fig . 3.6 . Assume additional add-1 smoothed probabilities P ( | < s > ) = 0.19 P ( < / s > | food ) = 0.40 . 3.3 two probabilities computed previous exercise higher , unsmoothed smoothed ? Explain why . 3.4 following corpus , modified chapter : < s > Sam < / s > < s > Sam < / s > < s > Sam < / s > < s > like green eggs Sam < / s > 26 CHAPTER 3 • N-GRAM LANGUAGE MODELS bigram language model add-one smoothing , P ( Sam | ) ? Include < s > < / s > counts just like token . 3.5 Suppose end-symbol < / s > . Train unsmoothed bigram grammar following training corpus end-symbol < / s > : < s > b < s > b b < s > b < s > Demonstrate bigram model assign single probability dis - tribution sentence lengths showing sum probability four possible 2 word sentences alphabet { , b } 1.0 , sum probability possible 3 word sentences alphabet { , b } 1.0 . 3.6 Suppose train trigram language model add-one smoothing corpus . corpus contains V word types . Express formula esti - mating P ( w3 | w1 , w2 ) , w3 word follows bigram ( w1 , w2 ) , terms various N-gram counts V . notation c ( w1 , w2 , w3 ) denote number times trigram ( w1 , w2 , w3 ) occurs corpus , bigrams unigrams . 3.7 following corpus , modified chapter : < s > Sam < / s > < s > Sam < / s > < s > Sam < / s > < s > like green eggs Sam < / s > linear interpolation smoothing maximum-likelihood bi - gram model maximum-likelihood unigram model λ1 = 12 λ2 = 1 2 , P ( Sam | ) ? Include < s > < / s > counts just like token . 3.8 Write program compute unsmoothed unigrams bigrams . 3.9 Run n-gram program two different small corpora choice ( might email text newsgroups ) . compare statistics two corpora . differences common unigrams two ? interesting differences bigrams ? 3.10 Add option program generate random sentences . 3.11 Add option program compute perplexity test set . 3.12 training set 100 numbers consists 91 zeros 1 digits 1-9 . following test set : 0 0 0 0 0 3 0 0 0 0 . unigram perplexity ? Exercises 27 Algoet , P . H . Cover , T . M . ( 1988 ) . sandwich proof Shannon-McMillan-Breiman theorem . Annals Probability , 16 ( 2 ) , 899 – 909 . Bahl , L . R . , Jelinek , F . , Mercer , R . L . ( 1983 ) . max - imum likelihood approach continuous speech recogni - tion . IEEE Transactions Pattern Analysis Machine Intelligence , 5 ( 2 ) , 179 – 190 . Baker , J . K . ( 1975a ) . DRAGON system – overview . IEEE Transactions Acoustics , Speech , Signal Pro - cessing , ASSP-23 ( 1 ) , 24 – 29 . Baker , J . K . ( 1975b ) . Stochastic modeling automatic speech understanding . Reddy , D . R . ( Ed . ) , Speech Recognition . Academic Press . Bengio , Y . , Schwenk , H . , Senécal , J . - S . , Morin , F . , Gau - vain , J . - L . ( 2006 ) . Neural probabilistic language models . Innovations Machine Learning , 137 – 186 . Springer . Blodgett , S . L . O’Connor , B . ( 2017 ) . Racial disparity natural language processing : case study social media african-american english . Fairness , Accountability , Transparency Machine Learning ( FAT / ML ) Workshop , KDD . Brants , T . , Popat , . C . , Xu , P . , Och , F . J . , Dean , J . ( 2007 ) . Large language models machine translation . EMNLP / CoNLL 2007 . Buck , C . , Heafield , K . , Van Ooyen , B . ( 2014 ) . N-gram counts language models common crawl . Proceedings LREC . Chen , S . F . Goodman , J . ( 1996 ) . empirical study smoothing techniques language modeling . ACL-96 , 310 – 318 . Chen , S . F . Goodman , J . ( 1998 ) . empirical study smoothing techniques language modeling . Tech . rep . TR-10-98 , Computer Science Group , Harvard University . Chen , S . F . Goodman , J . ( 1999 ) . empirical study smoothing techniques language modeling . Computer Speech Language , 13 , 359 – 394 . Chomsky , N . ( 1956 ) . Three models description language . IRE Transactions Information Theory , 2 ( 3 ) , 113 – 124 . Chomsky , N . ( 1957 ) . Syntactic Structures . Mouton , Hague . Church , K . W . Gale , W . . ( 1991 ) . comparison enhanced Good-Turing deleted estimation methods estimating probabilities English bigrams . Computer Speech Language , 5 , 19 – 54 . Church , K . W . , Hart , T . , Gao , J . ( 2007 ) . Compress - ing trigram language models Golomb coding . EMNLP / CoNLL 2007 , 199 – 207 . Cover , T . M . Thomas , J . . ( 1991 ) . Elements Infor - mation Theory . Wiley . Franz , . Brants , T . ( 2006 ) . n-gram belong . http://googleresearch.blogspot.com/2006/ 08 / all-our-n-gram-are-belong-to-you . html . Gale , W . . Church , K . W . ( 1994 ) . wrong adding ? . Oostdijk , N . de Haan , P . ( Eds . ) , Corpus-Based Research Language , 189 – 198 . Rodopi . Goodman , J . ( 2006 ) . bit progress language mod - eling : Extended version . Tech . rep . MSR-TR-2001-72 , Machine Learning Applied Statistics Group , Microsoft Research , Redmond , WA . Heafield , K . ( 2011 ) . KenLM : Faster smaller language model queries . Workshop Statistical Machine Trans - lation , 187 – 197 . Heafield , K . , Pouzyrevsky , . , Clark , J . H . , Koehn , P . ( 2013 ) . Scalable modified Kneser-Ney language model es - timation . . ACL 2013 , 690 – 696 . Jeffreys , H . ( 1948 ) . Theory Probability ( 2nd Ed . ) . Claren - don Press . Section 3.23 . Jelinek , F . ( 1976 ) . Continuous speech recognition statis - tical methods . Proceedings IEEE , 64 ( 4 ) , 532 – 557 . Jelinek , F . ( 1990 ) . Self-organized language modeling speech recognition . Waibel , . Lee , K . - F . ( Eds . ) , Readings Speech Recognition , 450 – 506 . Morgan Kauf - mann . Originally distributed IBM technical report 1985 . Jelinek , F . Mercer , R . L . ( 1980 ) . Interpolated estimation Markov source parameters sparse data . Gelsema , E . S . Kanal , L . N . ( Eds . ) , Proceedings , Workshop Pattern Recognition Practice , 381 – 397 . North Holland . Johnson , W . E . ( 1932 ) . Probability : deductive inductive problems ( appendix ) . Mind , 41 ( 164 ) , 421 – 423 . Jurafsky , D . , Wooters , C . , Tajchman , G . , Segal , J . , Stolcke , . , Fosler , E . , Morgan , N . ( 1994 ) . Berkeley restau - rant project . ICSLP-94 , 2139 – 2142 . Jurgens , D . , Tsvetkov , Y . , Jurafsky , D . ( 2017 ) . Incorpo - rating dialectal variability socially equitable language identification . ACL 2017 , 51 – 57 . Kane , S . K . , Morris , M . R . , Paradiso , . , Campbell , J . ( 2017 ) . “ times avuncular cantankerous , reflexes mongoose ” : Understanding self-expression augmentative alternative communication de - vices . CSCW 2017 , 1166 – 1179 . Kneser , R . Ney , H . ( 1995 ) . Improved backing-off M - gram language modeling . ICASSP-95 , Vol . 1 , 181 – 184 . Markov , . . ( 1913 ) . Essai d’une recherche statistique sur le texte du roman “ Eugene Onegin ” illustrant la liaison des epreuve en chain ( ‘ Example statistical investigation text “ Eugene Onegin ” illustrating dependence - tween samples chain ’ ) . Izvistia Imperatorskoi Akademii Nauk ( Bulletin de l’Académie Impériale des Sciences de St . - Pétersbourg ) , 7 , 153 – 162 . Mikolov , T . ( 2012 ) . Statistical language models based neural networks . Ph . D . thesis , Ph . D . thesis , Brno Univer - sity Technology . Miller , G . . Chomsky , N . ( 1963 ) . Finitary models language users . Luce , R . D . , Bush , R . R . , Galanter , E . ( Eds . ) , Handbook Mathematical Psychology , Vol . II , 419 – 491 . John Wiley . Miller , G . . Selfridge , J . . ( 1950 ) . Verbal context recall meaningful material . American Journal Psychology , 63 , 176 – 185 . Nádas , . ( 1984 ) . Estimation probabilities language model IBM speech recognition system . IEEE Trans - actions Acoustics , Speech , Signal Processing , 32 ( 4 ) , 859 – 861 . Schwenk , H . ( 2007 ) . Continuous space language models . Computer Speech & Language , 21 ( 3 ) , 492 – 518 . Shannon , C . E . ( 1948 ) . mathematical theory commu - nication . Bell System Technical Journal , 27 ( 3 ) , 379 – 423 . Continued following volume . 28 Chapter 3 • N-gram Language Models Shannon , C . E . ( 1951 ) . Prediction entropy printed English . Bell System Technical Journal , 30 , 50 – 64 . Stolcke , . ( 1998 ) . Entropy-based pruning backoff lan - guage models . Proc . DARPA Broadcast News Transcrip - tion Understanding Workshop , 270 – 274 . Stolcke , . ( 2002 ) . SRILM – extensible language model - ing toolkit . ICSLP-02 . Talbot , D . Osborne , M . ( 2007 ) . Smoothed Bloom Fil - ter Language Models : Tera-Scale LMs Cheap . EMNLP / CoNLL 2007 , 468 – 476 . Trnka , K . , Yarrington , D . , McCaw , J . , McCoy , K . F . , Pennington , C . ( 2007 ) . effects word prediction communication rate AAC . NAACL-HLT 07 , 173 – 176 . Witten , . H . Bell , T . C . ( 1991 ) . zero-frequency problem : Estimating probabilities novel events adaptive text compression . IEEE Transactions Informa - tion Theory , 37 ( 4 ) , 1085 – 1094 .