Chapter 1 Introduction Natural language processing is the set of methods for making human language accessible to computers . In the past decade , natural language processing has become embedded in our daily lives : automatic machine translation is ubiquitous on the web and in social media ; text classification keeps emails from collapsing under a deluge of spam ; search engines have moved beyond string matching and network analysis to a high degree of linguistic sophistication ; dialog systems provide an increasingly common and effective way to get and share information . These diverse applications are based on a common set of ideas , drawing on algo - rithms , linguistics , logic , statistics , and more . The goal of this text is to provide a survey of these foundations . The technical fun starts in the next chapter ; the rest of this current chapter situates natural language processing with respect to other intellectual disciplines , identifies some high-level themes in contemporary natural language processing , and ad - vises the reader on how best to approach the subject . 1.1 Natural language processing and its neighbors Natural language processing draws on many other intellectual traditions , from formal linguistics to statistical physics . This section briefly situates natural language processing with respect to some of its closest neighbors . Computational Linguistics Most of the meetings and journals that host natural lan - guage processing research bear the name “ computational linguistics ” , and the terms may be thought of as essentially synonymous . But while there is substantial overlap , there is an important difference in focus . In linguistics , language is the object of study . Computa - tional methods may be brought to bear , just as in scientific disciplines like computational biology and computational astronomy , but they play only a supporting role . In contrast , 1 2 CHAPTER 1 . INTRODUCTION natural language processing is focused on the design and analysis of computational al - gorithms and representations for processing natural human language . The goal of natu - ral language processing is to provide new computational capabilities around human lan - guage : for example , extracting information from texts , translating between languages , an - swering questions , holding a conversation , taking instructions , and so on . Fundamental linguistic insights may be crucial for accomplishing these tasks , but success is ultimately measured by whether and how well the job gets done . Machine Learning Contemporary approaches to natural language processing rely heav - ily on machine learning , which makes it possible to build complex computer programs from examples . Machine learning provides an array of general techniques for tasks like converting a sequence of discrete tokens in one vocabulary to a sequence of discrete to - kens in another vocabulary — a generalization of what one might informally call “ transla - tion . ” Much of today’s natural language processing research can be thought of as applied machine learning . However , natural language processing has characteristics that distin - guish it from many of machine learning’s other application domains . • Unlike images or audio , text data is fundamentally discrete , with meaning created by combinatorial arrangements of symbolic units . This is particularly consequential for applications in which text is the output , such as translation and summarization , because it is not possible to gradually approach an optimal solution . • Although the set of words is discrete , new words are always being created . Further - more , the distribution over words ( and other linguistic elements ) resembles that of a power law1 ( Zipf , 1949 ) : there will be a few words that are very frequent , and a long tail of words that are rare . A consequence is that natural language processing algo - rithms must be especially robust to observations that do not occur in the training data . • Language is compositional : units such as words can combine to create phrases , which can combine by the very same principles to create larger phrases . For ex - ample , a noun phrase can be created by combining a smaller noun phrase with a prepositional phrase , as in the whiteness of the whale . The prepositional phrase is created by combining a preposition ( in this case , of ) with another noun phrase ( the whale ) . In this way , it is possible to create arbitrarily long phrases , such as , ( 1.1 ) . . . huge globular pieces of the whale of the bigness of a human head . 2 The meaning of such a phrase must be analyzed in accord with the underlying hier - archical structure . In this case , huge globular pieces of the whale acts as a single noun 1Throughout the text , boldface will be used to indicate keywords that appear in the index . 2Throughout the text , this notation will be used to introduce linguistic examples . Jacob Eisenstein . Draft of October 15 , 2018 . 1.1 . NATURAL LANGUAGE PROCESSING AND ITS NEIGHBORS 3 phrase , which is conjoined with the prepositional phrase of the bigness of a human head . The interpretation would be different if instead , huge globular pieces were con - joined with the prepositional phrase of the whale of the bigness of a human head — implying a disappointingly small whale . Even though text appears as a sequence , machine learning methods must account for its implicit recursive structure . Artificial Intelligence The goal of artificial intelligence is to build software and robots with the same range of abilities as humans ( Russell and Norvig , 2009 ) . Natural language processing is relevant to this goal in several ways . On the most basic level , the capacity for language is one of the central features of human intelligence , and is therefore a prerequi - site for artificial intelligence . 3 Second , much of artificial intelligence research is dedicated to the development of systems that can reason from premises to a conclusion , but such algorithms are only as good as what they know ( Dreyfus , 1992 ) . Natural language pro - cessing is a potential solution to the “ knowledge bottleneck ” , by acquiring knowledge from texts , and perhaps also from conversations . This idea goes all the way back to Tur - ing’s 1949 paper Computing Machinery and Intelligence , which proposed the Turing test for determining whether artificial intelligence had been achieved ( Turing , 2009 ) . Conversely , reasoning is sometimes essential for basic tasks of language processing , such as resolving a pronoun . Winograd schemas are examples in which a single word changes the likely referent of a pronoun , in a way that seems to require knowledge and reasoning to decode ( Levesque et al . , 2011 ) . For example , ( 1.2 ) The trophy doesn’t fit into the brown suitcase because it is too [ small / large ] . When the final word is small , then the pronoun it refers to the suitcase ; when the final word is large , then it refers to the trophy . Solving this example requires spatial reasoning ; other schemas require reasoning about actions and their effects , emotions and intentions , and social conventions . Such examples demonstrate that natural language understanding cannot be achieved in isolation from knowledge and reasoning . Yet the history of artificial intelligence has been one of increasing specialization : with the growing volume of research in subdisci - plines such as natural language processing , machine learning , and computer vision , it is 3This view is shared by some , but not all , prominent researchers in artificial intelligence . Michael Jordan , a specialist in machine learning , has said that if he had a billion dollars to spend on any large research project , he would spend it on natural language processing ( https://www.reddit.com/r/ MachineLearning / comments / 2fxi6v / ama_michael_i_jordan / ) . On the other hand , in a public dis - cussion about the future of artificial intelligence in February 2018 , computer vision researcher Yann Lecun argued that despite its many practical applications , language is perhaps “ number 300 ” in the priority list for artificial intelligence research , and that it would be a great achievement if AI could attain the capa - bilities of an orangutan , which do not include language ( http://www.abigailsee.com/2018/02/21/ deep-learning-structure-and-innate-priors . html ) . Under contract with MIT Press , shared under CC-BY-NC-ND license . 4 CHAPTER 1 . INTRODUCTION difficult for anyone to maintain expertise across the entire field . Still , recent work has demonstrated interesting connections between natural language processing and other ar - eas of AI , including computer vision ( e.g . , Antol et al . , 2015 ) and game playing ( e.g . , Branavan et al . , 2009 ) . The dominance of machine learning throughout artificial intel - ligence has led to a broad consensus on representations such as graphical models and computation graphs , and on algorithms such as backpropagation and combinatorial opti - mization . Many of the algorithms and representations covered in this text are part of this consensus . Computer Science The discrete and recursive nature of natural language invites the ap - plication of theoretical ideas from computer science . Linguists such as Chomsky and Montague have shown how formal language theory can help to explain the syntax and semantics of natural language . Theoretical models such as finite-state and pushdown au - tomata are the basis for many practical natural language processing systems . Algorithms for searching the combinatorial space of analyses of natural language utterances can be analyzed in terms of their computational complexity , and theoretically motivated approx - imations can sometimes be applied . The study of computer systems is also relevant to natural language processing . Large datasets of unlabeled text can be processed more quickly by parallelization techniques like MapReduce ( Dean and Ghemawat , 2008 ; Lin and Dyer , 2010 ) ; high-volume data sources such as social media can be summarized efficiently by approximate streaming and sketching techniques ( Goyal et al . , 2009 ) . When deep neural networks are imple - mented in production systems , it is possible to eke out speed gains using techniques such as reduced-precision arithmetic ( Wu et al . , 2016 ) . Many classical natural language process - ing algorithms are not naturally suited to graphics processing unit ( GPU ) parallelization , suggesting directions for further research at the intersection of natural language process - ing and computing hardware ( Yi et al . , 2011 ) . Speech Processing Natural language is often communicated in spoken form , and speech recognition is the task of converting an audio signal to text . From one perspective , this is a signal processing problem , which might be viewed as a preprocessing step before nat - ural language processing can be applied . However , context plays a critical role in speech recognition by human listeners : knowledge of the surrounding words influences percep - tion and helps to correct for noise ( Miller et al . , 1951 ) . For this reason , speech recognition is often integrated with text analysis , particularly with statistical language models , which quantify the probability of a sequence of text ( see chapter 6 ) . Beyond speech recognition , the broader field of speech processing includes the study of speech-based dialogue sys - tems , which are briefly discussed in chapter 19 . Historically , speech processing has often been pursued in electrical engineering departments , while natural language processing Jacob Eisenstein . Draft of October 15 , 2018 . 1.1 . NATURAL LANGUAGE PROCESSING AND ITS NEIGHBORS 5 has been the purview of computer scientists . For this reason , the extent of interaction between these two disciplines is less than it might otherwise be . Ethics As machine learning and artificial intelligence become increasingly ubiquitous , it is crucial to understand how their benefits , costs , and risks are distributed across differ - ent kinds of people . Natural language processing raises some particularly salient issues around ethics , fairness , and accountability : Access . Who is natural language processing designed to serve ? For example , whose lan - guage is translated from , and whose language is translated to ? Bias . Does language technology learn to replicate social biases from text corpora , and does it reinforce these biases as seemingly objective computational conclusions ? Labor . Whose text and speech comprise the datasets that power natural language pro - cessing , and who performs the annotations ? Are the benefits of this technology shared with all the people whose work makes it possible ? Privacy and internet freedom . What is the impact of large-scale text processing on the right to free and private communication ? What is the potential role of natural lan - guage processing in regimes of censorship or surveillance ? This text lightly touches on issues related to fairness and bias in subsection 14.6.3 and subsection 18.1.1 , but these issues are worthy of a book of their own . For more from within the field of computational linguistics , see the papers from the annual workshop on Ethics in Natural Language Processing ( Hovy et al . , 2017 ; Alfano et al . , 2018 ) . For an outside perspective on ethical issues relating to data science at large , see boyd and Crawford ( 2012 ) . Others Natural language processing plays a significant role in emerging interdisciplinary fields like computational social science and the digital humanities . Text classification ( chapter 4 ) , clustering ( chapter 5 ) , and information extraction ( chapter 17 ) are particularly useful tools ; another is probabilistic topic models ( Blei , 2012 ) , which are not covered in this text . Information retrieval ( Manning et al . , 2008 ) makes use of similar tools , and conversely , techniques such as latent semantic analysis ( section 14.3 ) have roots in infor - mation retrieval . Text mining is sometimes used to refer to the application of data mining techniques , especially classification and clustering , to text . While there is no clear distinc - tion between text mining and natural language processing ( nor between data mining and machine learning ) , text mining is typically less concerned with linguistic structure , and more interested in fast , scalable algorithms . Under contract with MIT Press , shared under CC-BY-NC-ND license . 6 CHAPTER 1 . INTRODUCTION 1.2 Three themes in natural language processing Natural language processing covers a diverse range of tasks , methods , and linguistic phe - nomena . But despite the apparent incommensurability between , say , the summarization of scientific articles ( section 16.3.4 ) and the identification of suffix patterns in Spanish verbs ( section 9.1.4 ) , some general themes emerge . The remainder of the introduction fo - cuses on these themes , which will recur in various forms through the text . Each theme can be expressed as an opposition between two extreme viewpoints on how to process natural language . The methods discussed in the text can usually be placed somewhere on the continuum between these two extremes . 1.2.1 Learning and knowledge A recurring topic of debate is the relative importance of machine learning and linguistic knowledge . On one extreme , advocates of “ natural language processing from scratch ” ( Col - lobert et al . , 2011 ) propose to use machine learning to train end-to-end systems that trans - mute raw text into any desired output structure : e.g . , a summary , database , or transla - tion . On the other extreme , the core work of natural language processing is sometimes taken to be transforming text into a stack of general-purpose linguistic structures : from subword units called morphemes , to word-level parts-of-speech , to tree-structured repre - sentations of grammar , and beyond , to logic-based representations of meaning . In theory , these general-purpose structures should then be able to support any desired application . The end-to-end approach has been buoyed by recent results in computer vision and speech recognition , in which advances in machine learning have swept away expert - engineered representations based on the fundamentals of optics and phonology ( Krizhevsky et al . , 2012 ; Graves and Jaitly , 2014 ) . But while machine learning is an element of nearly every contemporary approach to natural language processing , linguistic representations such as syntax trees have not yet gone the way of the visual edge detector or the auditory triphone . Linguists have argued for the existence of a “ language faculty ” in all human be - ings , which encodes a set of abstractions specially designed to facilitate the understanding and production of language . The argument for the existence of such a language faculty is based on the observation that children learn language faster and from fewer examples than would be possible if language was learned from experience alone . 4 From a practi - cal standpoint , linguistic structure seems to be particularly important in scenarios where training data is limited . There are a number of ways in which knowledge and learning can be combined in natural language processing . Many supervised learning systems make use of carefully engineered features , which transform the data into a representation that can facilitate 4The Language Instinct ( Pinker , 2003 ) articulates these arguments in an engaging and popular style . For arguments against the innateness of language , see Elman et al . ( 1998 ) . Jacob Eisenstein . Draft of October 15 , 2018 . 1.2 . THREE THEMES IN NATURAL LANGUAGE PROCESSING 7 learning . For example , in a task like search , it may be useful to identify each word’s stem , so that a system can more easily generalize across related terms such as whale , whales , whalers , and whaling . ( This issue is relatively benign in English , as compared to the many other languages which include much more elaborate systems of prefixed and suffixes . ) Such features could be obtained from a hand-crafted resource , like a dictionary that maps each word to a single root form . Alternatively , features can be obtained from the output of a general-purpose language processing system , such as a parser or part-of-speech tagger , which may itself be built on supervised machine learning . Another synthesis of learning and knowledge is in model structure : building machine learning models whose architectures are inspired by linguistic theories . For example , the organization of sentences is often described as compositional , with meaning of larger units gradually constructed from the meaning of their smaller constituents . This idea can be built into the architecture of a deep neural network , which is then trained using contemporary deep learning techniques ( Dyer et al . , 2016 ) . The debate about the relative importance of machine learning and linguistic knowl - edge sometimes becomes heated . No machine learning specialist likes to be told that their engineering methodology is unscientific alchemy ; 5 nor does a linguist want to hear that the search for general linguistic principles and structures has been made irrelevant by big data . Yet there is clearly room for both types of research : we need to know how far we can go with end-to-end learning alone , while at the same time , we continue the search for linguistic representations that generalize across applications , scenarios , and languages . For more on the history of this debate , see Church ( 2011 ) ; for an optimistic view of the potential symbiosis between computational linguistics and deep learning , see Manning ( 2015 ) . 1.2.2 Search and learning Many natural language processing problems can be written mathematically in the form of optimization , 6 ŷ = argmax y ∈ Y ( x ) Ψ ( x , y ; θ ) , [ 1.1 ] where , • x is the input , which is an element of a set X ; • y is the output , which is an element of a set Y ( x ) ; 5Ali Rahimi argued that much of deep learning research was similar to “ alchemy ” in a presentation at the 2017 conference on Neural Information Processing Systems . He was advocating for more learning theory , not more linguistics . 6Throughout this text , equations will be numbered by square brackets , and linguistic examples will be numbered by parentheses . Under contract with MIT Press , shared under CC-BY-NC-ND license . 8 CHAPTER 1 . INTRODUCTION • Ψ is a scoring function ( also called the model ) , which maps from the set X × Y to the real numbers ; • θ is a vector of parameters for Ψ ; • ŷ is the predicted output , which is chosen to maximize the scoring function . This basic structure can be applied to a huge range of problems . For example , the input x might be a social media post , and the output y might be a labeling of the emotional sentiment expressed by the author ( chapter 4 ) ; or x could be a sentence in French , and the output y could be a sentence in Tamil ( chapter 18 ) ; or x might be a sentence in English , and y might be a representation of the syntactic structure of the sentence ( chapter 10 ) ; or xmight be a news article and y might be a structured record of the events that the article describes ( chapter 17 ) . This formulation reflects an implicit decision that language processing algorithms will have two distinct modules : Search . The search module is responsible for computing the argmax of the function Ψ . In other words , it finds the output ŷ that gets the best score with respect to the in - put x . This is easy when the search space Y ( x ) is small enough to enumerate , or when the scoring function Ψ has a convenient decomposition into parts . In many cases , we will want to work with scoring functions that do not have these proper - ties , motivating the use of more sophisticated search algorithms , such as bottom-up dynamic programming ( section 10.1 ) and beam search ( section 11.3.1 ) . Because the outputs are usually discrete in language processing problems , search often relies on the machinery of combinatorial optimization . Learning . The learning module is responsible for finding the parameters θ . This is typ - ically ( but not always ) done by processing a large dataset of labeled examples , { ( x ( i ) , y ( i ) ) } Ni = 1 . Like search , learning is also approached through the framework of optimization , as we will see in chapter 2 . Because the parameters are usually continuous , learning algorithms generally rely on numerical optimization to iden - tify vectors of real-valued parameters that optimize some function of the model and the labeled data . Some basic principles of numerical optimization are reviewed in Appendix B . The division of natural language processing into separate modules for search and learning makes it possible to reuse generic algorithms across many tasks and models . Much of the work of natural language processing can be focused on the design of the model Ψ — identifying and formalizing the linguistic phenomena that are relevant to the task at hand — while reaping the benefits of decades of progress in search , optimization , and learning . This textbook will describe several classes of scoring functions , and the corresponding algorithms for search and learning . Jacob Eisenstein . Draft of October 15 , 2018 . 1.2 . THREE THEMES IN NATURAL LANGUAGE PROCESSING 9 When a model is capable of making subtle linguistic distinctions , it is said to be ex - pressive . Expressiveness is often traded off against efficiency of search and learning . For example , a word-to-word translation model makes search and learning easy , but it is not expressive enough to distinguish good translations from bad ones . Many of the most im - portant problems in natural language processing seem to require expressive models , in which the complexity of search grows exponentially with the size of the input . In these models , exact search is usually impossible . Intractability threatens the neat modular de - composition between search and learning : if search requires a set of heuristic approxima - tions , then it may be advantageous to learn a model that performs well under these spe - cific heuristics . This has motivated some researchers to take a more integrated approach to search and learning , as briefly mentioned in chapters 11 and 15 . 1.2.3 Relational , compositional , and distributional perspectives Any element of language — a word , a phrase , a sentence , or even a sound — can be described from at least three perspectives . Consider the word journalist . A journalist is a subcategory of a profession , and an anchorwoman is a subcategory of journalist ; further - more , a journalist performs journalism , which is often , but not always , a subcategory of writing . This relational perspective on meaning is the basis for semantic ontologies such as WORDNET ( Fellbaum , 2010 ) , which enumerate the relations that hold between words and other elementary semantic units . The power of the relational perspective is illustrated by the following example : ( 1.3 ) Umashanthi interviewed Ana . She works for the college newspaper . Who works for the college newspaper ? The word journalist , while not stated in the ex - ample , implicitly links the interview to the newspaper , making Umashanthi the most likely referent for the pronoun . ( A general discussion of how to resolve pronouns is found in chapter 15 . ) Yet despite the inferential power of the relational perspective , it is not easy to formalize computationally . Exactly which elements are to be related ? Are journalists and reporters distinct , or should we group them into a single unit ? Is the kind of interview performed by a journalist the same as the kind that one undergoes when applying for a job ? Ontology designers face many such thorny questions , and the project of ontology design hearkens back to Borges ’ ( 1993 ) Celestial Emporium of Benevolent Knowledge , which divides animals into : ( a ) belonging to the emperor ; ( b ) embalmed ; ( c ) tame ; ( d ) suckling pigs ; ( e ) sirens ; ( f ) fabulous ; ( g ) stray dogs ; ( h ) included in the present classification ; ( i ) frenzied ; ( j ) innumerable ; ( k ) drawn with a very fine camelhair brush ; ( l ) et cetera ; ( m ) having just broken the water pitcher ; ( n ) that from a long way off resemble flies . Under contract with MIT Press , shared under CC-BY-NC-ND license . 10 CHAPTER 1 . INTRODUCTION Difficulties in ontology construction have led some linguists to argue that there is no task - independent way to partition up word meanings ( Kilgarriff , 1997 ) . Some problems are easier . Each member in a group of journalists is a journalist : the - s suffix distinguishes the plural meaning from the singular in most of the nouns in English . Similarly , a journalist can be thought of , perhaps colloquially , as someone who produces or works on a journal . ( Taking this approach even further , the word journal derives from the French jour + nal , or day + ly = daily . ) In this way , the meaning of a word is constructed from the constituent parts — the principle of compositionality . This principle can be applied to larger units : phrases , sentences , and beyond . Indeed , one of the great strengths of the compositional view of meaning is that it provides a roadmap for understanding entire texts and dialogues through a single analytic lens , grounding out in the smallest parts of individual words . But alongside journalists and anti-parliamentarians , there are many words that seem to be linguistic atoms : think , for example , of whale , blubber , and Nantucket . Idiomatic phrases like kick the bucket and shoot the breeze have meanings that are quite different from the sum of their parts ( Sag et al . , 2002 ) . Composition is of little help for such words and expressions , but their meanings can be ascertained — or at least approximated — from the contexts in which they appear . Take , for example , blubber , which appears in such contexts as : ( 1.4 ) a . The blubber served them as fuel . b . . . . extracting it from the blubber of the large fish . . . c . Amongst oily substances , blubber has been employed as a manure . These contexts form the distributional properties of the word blubber , and they link it to words which can appear in similar constructions : fat , pelts , and barnacles . This distribu - tional perspective makes it possible to learn about meaning from unlabeled data alone ; unlike relational and compositional semantics , no manual annotation or expert knowl - edge is required . Distributional semantics is thus capable of covering a huge range of linguistic phenomena . However , it lacks precision : blubber is similar to fat in one sense , to pelts in another sense , and to barnacles in still another . The question of why all these words tend to appear in the same contexts is left unanswered . The relational , compositional , and distributional perspectives all contribute to our un - derstanding of linguistic meaning , and all three appear to be critical to natural language processing . Yet they are uneasy collaborators , requiring seemingly incompatible represen - tations and algorithmic approaches . This text presents some of the best known and most successful methods for working with each of these representations , but future research may reveal new ways to combine them . Jacob Eisenstein . Draft of October 15 , 2018 . Part I Learning 11