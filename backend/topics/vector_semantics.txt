b'Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved.\n\nDraft of October 2, 2019.\n\nCopyright c(cid:13) 2019.\n\nAll\n\nCHAPTER\n\n6 Vector Semantics and Embed-\n\ndings\n\nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the\nmiddle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt\npreserves millions of fossil bones from the last of the Ice Ages of the Pleistocene\nEpoch. One of these fossils is the Smilodon, or sabre-toothed tiger, instantly rec-\nognizable by its long canines. Five million years ago or so, a completely different\nsabre-tooth tiger called Thylacosmilus lived\nin Argentina and other parts of South Amer-\nica. Thylacosmilus was a marsupial whereas\nSmilodon was a placental mammal, but Thy-\nlacosmilus had the same long upper canines\nand, like Smilodon, had a protective bone\n\xef\xac\x82ange on the lower jaw. The similarity of\nthese two mammals is one of many examples\nof parallel or convergent evolution, in which particular contexts or environments\nlead to the evolution of very similar structures in different species (Gould, 1980).\n\nThe role of context is also important in the similarity of a less biological kind\nof organism: the word. Words that occur in similar contexts tend to have similar\nmeanings. This link between similarity in how words are distributed and similarity\nin what they mean is called the distributional hypothesis. The hypothesis was\n\xef\xac\x81rst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth\n(1957), who noticed that words which are synonyms (like oculist and eye-doctor)\ntended to occur in the same environment (e.g., near words like eye or examined)\nwith the amount of meaning difference between two words \xe2\x80\x9ccorresponding roughly\nto the amount of difference in their environments\xe2\x80\x9d (Harris, 1954, 157).\n\nIn this chapter we introduce vector semantics, which instantiates this linguistic\nhypothesis by learning representations of the meaning of words, called embeddings,\ndirectly from their distributions in texts. These representations are used in every\nnatural language processing application that makes use of meaning, and underlie the\nmore powerful contextualized word representations like ELMo and BERT that\nwe will introduce in Chapter 10.\n\nThese word representations are also the \xef\xac\x81rst example in this book of repre-\nsentation learning, automatically learning useful representations of the input text.\nFinding such self-supervised ways to learn representations of the input, instead of\ncreating representations by hand via feature engineering, is an important focus of\nNLP research (Bengio et al., 2013).\n\nWe\xe2\x80\x99ll begin, however, by introducing some basic principles of word meaning,\nwhich will motivate the vector semantic models of this chapter as well as extensions\nthat we\xe2\x80\x99ll return to in Chapter 19, Chapter 20, and Chapter 21.\n\ndistributional\nhypothesis\n\nvector\nsemantics\nembeddings\n\nrepresentation\nlearning\n\n\x0c2 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\n6.1 Lexical Semantics\n\nHow should we represent the meaning of a word? In the N-gram models we saw\nin Chapter 3, and in many traditional NLP applications, our only representation of\na word is as a string of letters, or perhaps as an index in a vocabulary list. This\nrepresentation is not that different from a tradition in philosophy, perhaps you\xe2\x80\x99ve\nseen it in introductory logic classes, in which the meaning of words is represented\nby just spelling the word with small capital letters; representing the meaning of\n\xe2\x80\x9cdog\xe2\x80\x9d as DOG, and \xe2\x80\x9ccat\xe2\x80\x9d as CAT).\n\nRepresenting the meaning of a word by capitalizing it is a pretty unsatisfactory\n\nmodel. You might have seen the old philosophy joke:\n\nQ: What\xe2\x80\x99s the meaning of life?\nA: LIFE\n\nSurely we can do better than this! After all, we\xe2\x80\x99ll want a model of word meaning\nto do all sorts of things for us. It should tell us that some words have similar mean-\nings (cat is similar to dog), other words are antonyms (cold is the opposite of hot). It\nshould know that some words have positive connotations (happy) while others have\nnegative connotations (sad). It should represent the fact that the meanings of buy,\nsell, and pay offer differing perspectives on the same underlying purchasing event\n(If I buy something from you, you\xe2\x80\x99ve probably sold it to me, and I likely paid you).\nMore generally, a model of word meaning should allow us to draw useful infer-\nences that will help us solve meaning-related tasks like question-answering, sum-\nmarization, detecting paraphrases or plagiarism, and dialogue.\n\nIn this section we summarize some of these desiderata, drawing on results in the\nlinguistic study of word meaning, which is called lexical semantics; we\xe2\x80\x99ll return to\nand expand on this list in Chapter 19.\nLemmas and Senses Let\xe2\x80\x99s start by looking at how one word (we\xe2\x80\x99ll choose mouse)\nmight be de\xef\xac\x81ned in a dictionary: 1\nmouse (N)\n1.\n2.\n\nany of numerous small rodents...\na hand-operated device that controls a cursor...\nHere the form mouse is the lemma, also called the citation form. The form\nmouse would also be the lemma for the word mice; dictionaries don\xe2\x80\x99t have separate\nde\xef\xac\x81nitions for in\xef\xac\x82ected forms like mice. Similarly sing is the lemma for sing, sang,\nsung. In many languages the in\xef\xac\x81nitive form is used as the lemma for the verb, so\nSpanish dormir \xe2\x80\x9cto sleep\xe2\x80\x9d is the lemma for duermes \xe2\x80\x9cyou sleep\xe2\x80\x9d. The speci\xef\xac\x81c forms\nsung or carpets or sing or duermes are called wordforms.\n\nAs the example above shows, each lemma can have multiple meanings; the\nlemma mouse can refer to the rodent or the cursor control device. We call each\nof these aspects of the meaning of mouse a word sense. The fact that lemmas can\nbe polysemous (have multiple senses) can make interpretation dif\xef\xac\x81cult (is someone\nwho types \xe2\x80\x9cmouse info\xe2\x80\x9d into a search engine looking for a pet or a tool?). Chapter 19\nwill discuss the problem of polysemy, and introduce word sense disambiguation,\nthe task of determining which sense of a word is being used in a particular context.\nSynonymy One important component of word meaning is the relationship be-\ntween word senses. For example when one word has a sense whose meaning is\n\n1 This example shortened from the online dictionary WordNet, discussed in Chapter 19.\n\nlexical\nsemantics\n\nlemma\ncitation form\n\nwordform\n\n\x0csynonym\n\nidentical to a sense of another word, or nearly identical, we say the two senses of\nthose two words are synonyms. Synonyms include such pairs as\n\n6.1\n\n\xe2\x80\xa2 LEXICAL SEMANTICS\n\n3\n\npropositional\nmeaning\n\nprinciple of\ncontrast\n\nsimilarity\n\nrelatedness\nassociation\n\ncouch/sofa vomit/throw up \xef\xac\x81lbert/hazelnut car/automobile\n\nA more formal de\xef\xac\x81nition of synonymy (between words rather than senses) is that\ntwo words are synonymous if they are substitutable one for the other in any sentence\nwithout changing the truth conditions of the sentence, the situations in which the\nsentence would be true. We often say in this case that the two words have the same\npropositional meaning.\n\nWhile substitutions between some pairs of words like car / automobile or wa-\nter / H2O are truth preserving, the words are still not identical in meaning. Indeed,\nprobably no two words are absolutely identical in meaning. One of the fundamen-\ntal tenets of semantics, called the principle of contrast (Girard 1718, Br\xc2\xb4eal 1897,\nClark 1987), is the assumption that a difference in linguistic form is always associ-\nated with at least some difference in meaning. For example, the word H2O is used\nin scienti\xef\xac\x81c contexts and would be inappropriate in a hiking guide\xe2\x80\x94water would be\nmore appropriate\xe2\x80\x94 and this difference in genre is part of the meaning of the word.\nIn practice, the word synonym is therefore commonly used to describe a relationship\nof approximate or rough synonymy.\n\nWord Similarity While words don\xe2\x80\x99t have many synonyms, most words do have\nlots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly\nsimilar words. In moving from synonymy to similarity, it will be useful to shift from\ntalking about relations between word senses (like synonymy) to relations between\nwords (like similarity). Dealing with words avoids having to commit to a particular\nrepresentation of word senses, which will turn out to simplify our task.\n\nThe notion of word similarity is very useful in larger semantic tasks. Know-\ning how similar two words are can help in computing how similar the meaning of\ntwo phrases or sentences are, a very important component of natural language un-\nderstanding tasks like question answering, paraphrasing, and summarization. One\nway of getting values for word similarity is to ask humans to judge how similar one\nword is to another. A number of datasets have resulted from such experiments. For\nexample the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to\n10, like the examples below, which range from near-synonyms (vanish, disappear)\nto pairs that scarcely seem to have anything in common (hole, agreement):\n\nvanish disappear\nbehave obey\nbelief\nmuscle bone\nmodest \xef\xac\x82exible\nhole\n\n9.8\n7.3\nimpression 5.95\n3.65\n0.98\nagreement 0.3\n\nWord Relatedness The meaning of two words can be related in ways other than\nsimilarity. One such class of connections is called word relatedness (Budanitsky\nand Hirst, 2006), also traditionally called word association in psychology.\n\nConsider the meanings of the words coffee and cup. Coffee is not similar to cup;\nthey share practically no features (coffee is a plant or a beverage, while a cup is a\nmanufactured object with a particular shape).\n\nBut coffee and cup are clearly related; they are associated by co-participating in\nan everyday event (the event of drinking coffee out of a cup). Similarly the nouns\n\n\x0c4 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nsemantic \xef\xac\x81eld\n\ntopic models\n\nsemantic frame\n\nconnotations\n\nsentiment\n\nscalpel and surgeon are not similar but are related eventively (a surgeon tends to\nmake use of a scalpel).\n\nOne common kind of relatedness between words is if they belong to the same\nsemantic \xef\xac\x81eld. A semantic \xef\xac\x81eld is a set of words which cover a particular semantic\ndomain and bear structured relations with each other.\n\nFor example, words might be related by being in the semantic \xef\xac\x81eld of hospitals\n(surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food,\nchef), or houses (door, roof, kitchen, family, bed).\n\nSemantic \xef\xac\x81elds are also related to topic models, like Latent Dirichlet Alloca-\ntion, LDA, which apply unsupervised learning on large sets of texts to induce sets of\nassociated words from text. Semantic \xef\xac\x81elds and topic models are very useful tools\nfor discovering topical structure in documents.\n\nIn Chapter 19 we\xe2\x80\x99ll introduce even more relations between senses, including\nhypernymy or IS-A, antonymy (opposite meaning) and meronymy) (part-whole\nrelations).\nSemantic Frames and Roles Closely related to semantic \xef\xac\x81elds is the idea of a\nsemantic frame. A semantic frame is a set of words that denote perspectives or\nparticipants in a particular type of event. A commercial transaction, for example,\nis a kind of event in which one entity trades money to another entity in return for\nsome good or service, after which the good changes hands or perhaps the service is\nperformed. This event can be encoded lexically by using verbs like buy (the event\nfrom the perspective of the buyer), sell (from the perspective of the seller), pay\n(focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles\n(like buyer, seller, goods, money), and words in a sentence can take on these roles.\nKnowing that buy and sell have this relation makes it possible for a system to\nknow that a sentence like Sam bought the book from Ling could be paraphrased as\nLing sold the book to Sam, and that Sam has the role of the buyer in the frame and\nLing the seller. Being able to recognize such paraphrases is important for question\nanswering, and can help in shifting perspective for machine translation.\nConnotation Finally, words have affective meanings or connotations. The word\nconnotation has different meanings in different \xef\xac\x81elds, but here we use it to mean\nthe aspects of a word\xe2\x80\x99s meaning that are related to a writer or reader\xe2\x80\x99s emotions,\nsentiment, opinions, or evaluations. For example some words have positive conno-\ntations (happy) while others have negative connotations (sad). Some words describe\npositive evaluation (great, love) and others negative evaluation (terrible, hate). Pos-\nitive or negative evaluation expressed through language is called sentiment, as we\nsaw in Chapter 4, and word sentiment plays a role in important tasks like sentiment\nanalysis, stance detection, and many applications of natural language processing to\nthe language of politics and consumer reviews.\n\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\nalong three important dimensions of affective meaning. These are now generally\ncalled valence, arousal, and dominance, de\xef\xac\x81ned as follows:\n\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nThus words like happy or satis\xef\xac\x81ed are high on valence, while unhappy or an-\nnoyed are low on valence. Excited or frenzied are high on arousal, while relaxed\nor calm are low on arousal. Important or controlling are high on dominance, while\nawed or in\xef\xac\x82uenced are low on dominance. Each word is thus represented by three\n\n\x0c6.2\n\n\xe2\x80\xa2 VECTOR SEMANTICS\n\n5\n\nnumbers, corresponding to its value on each of the three dimensions, like the exam-\nples below:\n\nValence Arousal Dominance\n\ncourageous 8.05\nmusic\n7.67\nheartbreak 2.45\n6.71\ncub\nlife\n6.68\n\n5.5\n5.57\n5.65\n3.95\n5.59\n\n7.38\n6.5\n3.58\n4.24\n5.89\n\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word\xe2\x80\x99s\nrating on the three scales. This revolutionary idea that word meaning word could\nbe represented as a point in space (e.g., that part of the meaning of heartbreak can\nbe represented as the point [2.45,5.65,3.58]) was the \xef\xac\x81rst expression of the vector\nsemantics models that we introduce next.\n\n6.2 Vector Semantics\n\nvector\nsemantics\n\nHow can we build a computational model that successfully deals with the different\naspects of word meaning we saw in the previous section (word senses, word simi-\nlarity and relatedness, lexical \xef\xac\x81elds and frames, connotation)?\n\nA perfect model that completely deals with each of these aspects of word mean-\ning turns out to be elusive. But the current best model, called vector semantics,\ndraws its inspiration from linguistic and philosophical work of the 1950\xe2\x80\x99s.\n\nDuring that period, the philosopher Ludwig Wittgenstein, skeptical of the possi-\nbility of building a completely formal theory of meaning de\xef\xac\x81nitions for each word,\nsuggested instead that \xe2\x80\x9cthe meaning of a word is its use in the language\xe2\x80\x9d (Wittgen-\nstein, 1953, PI 43). That is, instead of using some logical language to de\xef\xac\x81ne each\nword, we should de\xef\xac\x81ne words by some representation of how the word was used by\nactual people in speaking and understanding.\n\nLinguists of the period like Joos (1950), Harris (1954), and Firth (1957) (the lin-\nguistic distributionalists), came up with a speci\xef\xac\x81c idea for realizing Wittgenstein\xe2\x80\x99s\nintuition: de\xef\xac\x81ne a word by its environment or distribution in language use. A word\xe2\x80\x99s\ndistribution is the set of contexts in which it occurs, the neighboring words or gram-\nmatical environments. The idea is that two words that occur in very similar dis-\ntributions (that occur together with very similar words) are likely to have the same\nmeaning.\n\nLet\xe2\x80\x99s see an example illustrating this distributionalist approach. Suppose you\ndidn\xe2\x80\x99t know what the Cantonese word ongchoi meant, but you do see it in the fol-\nlowing sentences or contexts:\n(6.1) Ongchoi is delicious sauteed with garlic.\n(6.2) Ongchoi is superb over rice.\n(6.3) ...ongchoi leaves with salty sauces...\n\nAnd furthermore let\xe2\x80\x99s suppose that you had seen many of these context words\n\noccurring in contexts like:\n(6.4) ...spinach sauteed with garlic over rice...\n\n\x0c6 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\n(6.5) ...chard stems and leaves are delicious...\n(6.6) ...collard greens and other salty leafy greens\n\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty, as do words like spinach, chard, and collard greens might suggest to the reader\nthat ongchoi is a leafy green similar to these other leafy greens.2\n\nVector semantics thus combines two intuitions:\n\nWe can do the same thing computationally by just counting words in the context\nof ongchoi; we\xe2\x80\x99ll tend to see words like sauteed and eaten and garlic. The fact that\nthese words and other similar context words also occur around the word spinach or\ncollard greens can help us discover the similarity between these words and ongchoi.\nthe distributionalist intuition\n(de\xef\xac\x81ning a word by counting what other words occur in its environment), and the\nvector intuition of Osgood et al. (1957) we saw in the last section on connotation:\nde\xef\xac\x81ning the meaning of a word w as a vector, a list of numbers, a point in N-\ndimensional space. There are various versions of vector semantics, each de\xef\xac\x81ning\nthe numbers in the vector somewhat differently, but in each case the numbers are\nbased in some way on counts of neighboring words.\n\nembeddings\n\nFigure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and\nphrases, showing that words with similar meanings are nearby in space. The original 60-\ndimensional embeddings were trained for sentiment analysis. Simpli\xef\xac\x81ed from Li et al. (2015).\n\nThe idea of vector semantics is thus to represent a word as a point in some multi-\ndimensional semantic space. Vectors for representing words are generally called\nembeddings, because the word is embedded in a particular vector space. Fig. 6.1\ndisplays a visualization of embeddings that were learned for a sentiment analysis\ntask, showing the location of some selected words projected down from the original\n60-dimensional space into a two dimensional space.\n\nNotice that positive and negative words seem to be located in distinct portions of\nthe space (and different also from the neutral function words). This suggests one of\nthe great advantages of vector semantics: it offers a \xef\xac\x81ne-grained model of meaning\nthat lets us also implement word similarity (and phrase similarity). For example,\nthe sentiment analysis classi\xef\xac\x81er we saw in Chapter 4 only works if enough of the\nimportant sentimental words that appear in the test set also appeared in the training\nset. But if words were represented as embeddings, we could assign sentiment as\nlong as words with similar meanings as the test set words occurred in the training\n\n2\n\nIt\xe2\x80\x99s in fact Ipomoea aquatica, a relative of morning glory sometimes called water spinach in English.\n\nto\nthat\n\na\nthan\n\nby\nnow\ni\nwith\n\nare\n\n\xe2\x80\x99s\n\nyou\nis\n\nnot good\n\nbad\ndislike\nincredibly bad\n\nworst\n\nworse\n\nvery good incredibly good\n\nfantastic\n\nwonderful\n\namazing\n\nterri\xef\xac\x81c\n\nnice\ngood\n\n\x0c6.3\n\n\xe2\x80\xa2 WORDS AND VECTORS\n\n7\n\nset. Vector semantic models are also extremely practical because they can be learned\nautomatically from text without any complex labeling or supervision.\n\nAs a result of these advantages, vector models of meaning are now the standard\nway to represent the meaning of words in NLP. In this chapter we\xe2\x80\x99ll introduce the\ntwo most commonly used models. First is the tf-idf model, often used as a baseline,\nin which the meaning of a word is de\xef\xac\x81ned by a simple function of the counts of\nnearby words. We will see that this method results in very long vectors that are\nsparse, i.e. contain mostly zeros (since most words simply never occur in the context\nof others).\n\nThen we\xe2\x80\x99ll introduce the word2vec model, one of a family of models that are\n\nways of constructing short, dense vectors that have useful semantic properties.\n\nWe\xe2\x80\x99ll also introduce the cosine, the standard way to use embeddings (vectors)\nto compute functions like semantic similarity, the similarity between two words,\ntwo sentences, or two documents, an important tool in practical applications like\nquestion answering, summarization, or automatic essay grading.\n\n6.3 Words and Vectors\n\nVector or distributional models of meaning are generally based on a co-occurrence\nmatrix, a way of representing how often words co-occur. This matrix can be con-\nstructed in various ways; let\xe2\x80\x99s begin by looking at one such co-occurrence matrix, a\nterm-document matrix.\n\n6.3.1 Vectors and documents\nIn a term-document matrix, each row represents a word in the vocabulary and each\ncolumn represents a document from some collection of documents. Fig. 6.2 shows a\nsmall selection from a term-document matrix showing the occurrence of four words\nin four plays by Shakespeare. Each cell in this matrix represents the number of times\na particular word (de\xef\xac\x81ned by the row) occurs in a particular document (de\xef\xac\x81ned by\nthe column). Thus fool appeared 58 times in Twelfth Night.\n\nterm-document\nmatrix\n\nAs You Like It\n\nTwelfth Night\n\nJulius Caesar\n\nHenry V\n\nbattle\ngood\nfool\nwit\nFigure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell\ncontains the number of times the (row) word occurs in the (column) document.\n\n1\n114\n36\n20\n\n0\n80\n58\n15\n\n13\n89\n4\n3\n\n7\n62\n1\n2\n\nvector space\nmodel\n\nvector\n\nvector space\ndimension\n\nThe term-document matrix of Fig. 6.2 was \xef\xac\x81rst de\xef\xac\x81ned as part of the vector\nspace model of information retrieval (Salton, 1971). In this model, a document is\nrepresented as a count vector, a column in Fig. 6.3.\n\nTo review some basic linear algebra, a vector is, at heart, just a list or array\nof numbers. So As You Like It is represented as the list [1,114,36,20] and Julius\nCaesar is represented as the list [7,62,1,2]. A vector space is a collection of vectors,\ncharacterized by their dimension.\nIn the example in Fig. 6.3, the vectors are of\ndimension 4, just so they \xef\xac\x81t on the page; in real term-document matrices, the vectors\nrepresenting each document would have dimensionality |V|, the vocabulary size.\n\n\x0c8 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nThe ordering of the numbers in a vector space is not arbitrary; each position\nindicates a meaningful dimension on which the documents can vary. Thus the \xef\xac\x81rst\ndimension for both these vectors corresponds to the number of times the word battle\noccurs, and we can compare each dimension, noting for example that the vectors for\nAs You Like It and Twelfth Night have similar values (1 and 0, respectively) for the\n\xef\xac\x81rst dimension.\n\nAs You Like It\n\nTwelfth Night\n\nJulius Caesar\n\nHenry V\n\nbattle\ngood\nfool\nwit\nFigure 6.3 The term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each document is represented as a column vector of length four.\n\n1\n114\n36\n20\n\n0\n80\n58\n15\n\n13\n89\n4\n3\n\n7\n62\n1\n2\n\nWe can think of the vector for a document as identifying a point in |V|-dimensional\nspace; thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-\ndimensional spaces are hard to draw in textbooks, Fig. 6.4 shows a visualization in\ntwo dimensions; we\xe2\x80\x99ve arbitrarily chosen the dimensions corresponding to the words\nbattle and fool.\n\nFigure 6.4 A spatial visualization of the document vectors for the four Shakespeare play\ndocuments, showing just two of the dimensions, corresponding to the words battle and fool.\nThe comedies have high values for the fool dimension and low values for the battle dimension.\n\nTerm-document matrices were originally de\xef\xac\x81ned as a means of \xef\xac\x81nding similar\ndocuments for the task of document information retrieval. Two documents that are\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLike It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the \xef\xac\x81rst dimension\n(battle) the comedies have low numbers and the others have high numbers, and we\ncan see it visually in Fig. 6.4; we\xe2\x80\x99ll see very shortly how to quantify this intuition\nmore formally.\nA real term-document matrix, of course, wouldn\xe2\x80\x99t just have 4 rows and columns,\nlet alone 2. More generally, the term-document matrix has |V| rows (one for each\nword type in the vocabulary) and D columns (one for each document in the collec-\ntion); as we\xe2\x80\x99ll see, vocabulary sizes are generally in the tens of thousands, and the\nnumber of documents can be enormous (think about all the pages on the web).\n\n40\n\n15\n\n10\n\n5\n\ne\nl\nt\nt\na\nb\n\nHenry V [4,13]\n\nJulius Caesar [1,7]\n\nAs You Like It [36,1]\n\nTwelfth Night [58,0]\n\n5\n\n10\n\n15\n\n20\n\n25\n\n35\n\n30\n fool\n\n40\n\n45\n\n50\n\n55\n\n60\n\n\x0cinformation\nretrieval\n\nrow vector\n\nword-word\nmatrix\n\n6.3\n\n\xe2\x80\xa2 WORDS AND VECTORS\n\n9\n\nInformation retrieval (IR) is the task of \xef\xac\x81nding the document d from the D\ndocuments in some collection that best matches a query q. For IR we\xe2\x80\x99ll therefore also\nrepresent a query by a vector, also of length |V|, and we\xe2\x80\x99ll need a way to compare\ntwo vectors to \xef\xac\x81nd how similar they are. (Doing IR will also require ef\xef\xac\x81cient ways\nto store and manipulate these vectors by making use of the convenient fact that these\nvectors are sparse, i.e., mostly zeros).\n\nLater in the chapter we\xe2\x80\x99ll introduce some of the components of this vector com-\n\nparison process: the tf-idf term weighting, and the cosine similarity metric.\n\n6.3.2 Words as vectors\nWe\xe2\x80\x99ve seen that documents can be represented as vectors in a vector space. But\nvector semantics can also be used to represent the meaning of words, by associating\neach word with a vector.\n\nThe word vector is now a row vector rather than a column vector, and hence the\ndimensions of the vector are different. The four dimensions of the vector for fool,\n[36,58,1,4], correspond to the four Shakespeare plays. The same four dimensions\nare used to form the vectors for the other 3 words: wit, [20,15,2,3]; battle, [1,0,7,13];\nand good [114,80,62,89]. Each entry in the vector thus represents the counts of the\nword\xe2\x80\x99s occurrence in the document corresponding to that dimension.\n\nFor documents, we saw that similar documents had similar vectors, because sim-\nilar documents tend to have similar words. This same principle applies to words:\nsimilar words have similar vectors because they tend to occur in similar documents.\nThe term-document matrix thus lets us represent the meaning of a word by the doc-\numents it tends to occur in.\n\nHowever, it is most common to use a different kind of context for the dimensions\nof a word\xe2\x80\x99s vector representation. Rather than the term-document matrix we use the\nterm-term matrix, more commonly called the word-word matrix or the term-\ncontext matrix, in which the columns are labeled by words rather than documents.\nThis matrix is thus of dimensionality |V|\xc3\x97|V| and each cell records the number of\ntimes the row (target) word and the column (context) word co-occur in some context\nin some training corpus. The context could be the document, in which case the cell\nrepresents the number of times the two words appear in the same document. It is\nmost common, however, to use smaller contexts, generally a window around the\nword, for example of 4 words to the left and 4 words to the right, in which case\nthe cell represents the number of times (in some training corpus) the column word\noccurs in such a \xc2\xb14 word window around the row word. For example here is one\nexample each of some words in their windows:\n\nis traditionally followed by cherry\n\npie, a traditional dessert\noften mixed, such as strawberry rhubarb pie. Apple pie\n\ncomputer peripherals and personal digital\n\nassistants. These devices usually\n\na computer. This includes information available on the internet\n\nIf we then take every occurrence of each word (say strawberry) and count the con-\ntext words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\nsimpli\xef\xac\x81ed subset of the word-word co-occurrence matrix for these four words com-\nputed from the Wikipedia corpus (Davies, 2015).\n\nNote in Fig. 6.5 that the two words cherry and strawberry are more similar to\neach other (both pie and sugar tend to occur in their window) than they are to other\nwords like digital; conversely, digital and information are more similar to each other\nthan, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n\n\x0c10 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\ncherry\n\nstrawberry\n\ndigital\n\naardvark\n\ncomputer\n\ndata\n\nresult\n\nsugar\n\n...\n\n0\n0\n0\n0\n\n...\n...\n...\n...\n...\n\n2\n0\n\n8\n0\n\n1670\n3325\n\n1683\n3982\n\n9\n1\n85\n378\n\npie\n442\n60\n5\n5\n\n25\n19\n4\n13\n\ninformation\nFigure 6.5 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser.\n\nFigure 6.6 A spatial visualization of word vectors for digital and information, showing just\ntwo of the dimensions, corresponding to the words data and computer.\n\nNote that |V|, the length of the vector, is generally the size of the vocabulary,\nusually between 10,000 and 50,000 words (using the most frequent words in the\ntraining corpus; keeping words after about the most frequent 50,000 or so is gener-\nally not helpful). But of course since most of these numbers are zero these are sparse\nvector representations, and there are ef\xef\xac\x81cient algorithms for storing and computing\nwith sparse matrices.\n\nNow that we have some intuitions, let\xe2\x80\x99s move on to examine the details of com-\nputing word similarity. Afterwards we\xe2\x80\x99ll discuss the tf-idf method of weighting\ncells.\n\n6.4 Cosine for measuring similarity\n\ndot product\ninner product\n\nTo de\xef\xac\x81ne similarity between two target words v and w, we need a measure for taking\ntwo such vectors and giving a measure of vector similarity. By far the most common\nsimilarity metric is the cosine of the angle between the vectors.\n\nThe cosine\xe2\x80\x94like most measures for vector similarity used in NLP\xe2\x80\x94is based on\n\nthe dot product operator from linear algebra, also called the inner product:\n\ndot product(v,w) = v\xc2\xb7 w =\n\nviwi = v1w1 + v2w2 + ... + vNwN\n\n(6.7)\n\ni=1\n\nAs we will see, most metrics for similarity between vectors are based on the dot\nproduct. The dot product acts as a similarity metric because it will tend to be high\njust when the two vectors have large values in the same dimensions. Alternatively,\nvectors that have zeros in different dimensions\xe2\x80\x94orthogonal vectors\xe2\x80\x94will have a\ndot product of 0, representing their strong dissimilarity.\n\nN(cid:88)\n\n4000\n\n3000\n\n2000\n\n1000\n\nr\ne\nt\nu\np\nm\no\nc\n\ndigital\n\n [1683,1670]\n\ninformation\n [3982,3325] \n\n1000 2000 3000 4000\n\n data\n\n\x0cvector length\n\ncosine\n\nunit vector\n\n(cid:118)(cid:117)(cid:117)(cid:116) N(cid:88)\n\n|v| =\n\n6.4\n\n\xe2\x80\xa2 COSINE FOR MEASURING SIMILARITY\n\n11\n\nThis raw dot product, however, has a problem as a similarity metric: it favors\n\nlong vectors. The vector length is de\xef\xac\x81ned as\n\nv2\ni\n\n(6.8)\n\ni=1\n\nThe dot product is higher if a vector is longer, with higher values in each dimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwords and have higher co-occurrence values with each of them. The raw dot product\nthus will be higher for frequent words. But this is a problem; we\xe2\x80\x99d like a similarity\nmetric that tells us how similar two words are regardless of their frequency.\n\nThe simplest way to modify the dot product to normalize for the vector length is\nto divide the dot product by the lengths of each of the two vectors. This normalized\ndot product turns out to be the same as the cosine of the angle between the two\nvectors, following from the de\xef\xac\x81nition of the dot product between two vectors a and\nb:\n\na\xc2\xb7 b = |a||b|cos\xce\xb8\na\xc2\xb7 b\n|a||b| = cos\xce\xb8\n\n(6.9)\n\nThe cosine similarity metric between two vectors v and w thus can be computed as:\n\ncosine(v,w) =\n\nv\xc2\xb7 w\n|v||w| =\n\nN(cid:88)\n(cid:118)(cid:117)(cid:117)(cid:116) N(cid:88)\n\ni=1\n\nv2\ni\n\ni=1\n\nviwi\n\n(cid:118)(cid:117)(cid:117)(cid:116) N(cid:88)\n\ni=1\n\nw2\ni\n\n(6.10)\n\nFor some applications we pre-normalize each vector, by dividing it by its length,\ncreating a unit vector of length 1. Thus we could compute a unit vector from a by\ndividing it by |a|. For unit vectors, the dot product is the same as the cosine.\n\nThe cosine value ranges from 1 for vectors pointing in the same direction, through\n0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.\nBut raw frequency values are non-negative, so the cosine for these vectors ranges\nfrom 0\xe2\x80\x931.\n\nLet\xe2\x80\x99s see how the cosine computes which of the words cherry or digital is closer\nin meaning to information, just using raw counts from the following shortened table:\n\ncherry\ndigital\n\ninformation\n\n8\n\npie data computer\n442\n5\n5\n\n1683\n3982\n\n1670\n3325\n\n2\n\ncos(cherry,information) =\n\ncos(digital,information) =\n\n442\xe2\x88\x97 5 + 8\xe2\x88\x97 3982 + 2\xe2\x88\x97 3325\n\n\xe2\x88\x9a\n52 + 39822 + 33252\n5\xe2\x88\x97 5 + 1683\xe2\x88\x97 3982 + 1670\xe2\x88\x97 3325\n\n\xe2\x88\x9a\n4422 + 82 + 22\n\xe2\x88\x9a\n52 + 16832 + 16702\n\n\xe2\x88\x9a\n52 + 39822 + 33252\n\n= .017\n\n= .996\n\nThe model decides that information is way closer to digital than it is to cherry, a\n\nresult that seems sensible. Fig. 6.7 shows a visualization.\n\n\x0c12 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nFigure 6.7 A (rough) graphical demonstration of cosine similarity, showing vectors for\nthree words (cherry, digital, and information) in the two dimensional space de\xef\xac\x81ned by counts\nof the words computer and pie nearby. Note that the angle between digital and information is\nsmaller than the angle between cherry and information. When two vectors are more similar,\nthe cosine is larger but the angle is smaller; the cosine has its maximum (1) when the angle\nbetween two vectors is smallest (0\xe2\x97\xa6); the cosine of all other angles is less than 1.\n\n6.5 TF-IDF: Weighing terms in the vector\n\nThe co-occurrence matrix in Fig. 6.5 represented each cell by the raw frequency of\nthe co-occurrence of two words.\n\nIt turns out, however, that simple frequency isn\xe2\x80\x99t the best measure of association\nbetween words. One problem is that raw frequency is very skewed and not very\ndiscriminative. If we want to know what kinds of contexts are shared by cherry and\nstrawberry but not by digital and information, we\xe2\x80\x99re not going to get good discrimi-\nnation from words like the, it, or they, which occur frequently with all sorts of words\nand aren\xe2\x80\x99t informative about any particular word. We saw this also in Fig. 6.3 for\nthe Shakespeare corpus; the dimension for the word good is not very discrimina-\ntive between plays; good is simply a frequent word and has roughly equivalent high\nfrequencies in each of the plays.\n\nIt\xe2\x80\x99s a bit of a paradox. Words that occur nearby frequently (maybe pie nearby\ncherry) are more important than words that only appear once or twice. Yet words\nthat are too frequent\xe2\x80\x94ubiquitous, like the or good\xe2\x80\x94 are unimportant. How can we\nbalance these two con\xef\xac\x82icting constraints?\n\nThe tf-idf algorithm (the \xe2\x80\x98-\xe2\x80\x99 here is a hyphen, not a minus sign) is the product\n\nof two terms, each term capturing one of these two intuitions:\n\nterm frequency\n\nThe \xef\xac\x81rst is the term frequency (Luhn, 1957): the frequency of the word t in the\n\ndocument d. We can just use the raw count as the term frequency:\n\ntft,d = count(t,d)\n\n(6.11)\n\nAlternatively we can squash the raw frequency a bit, by using the log10 of the fre-\nquency instead. The intuition is that a word appearing 100 times in a document\ndoesn\xe2\x80\x99t make that word 100 times more likely to be relevant to the meaning of the\ndocument. Because we can\xe2\x80\x99t take the log of 0, we normally add 1 to the count:3\n\ntft,d = log10(count(t,d) + 1)\n\n(6.12)\n\nIf we use log weighting, terms which occur 10 times in a document would have a\ntf=2, 100 times in a document tf=3, 1000 times tf=4, and so on.\n\n(cid:26) 1 + log10 count(t,d)\n\n0\n\nif count(t,d) > 0\notherwise\n\n3 Or we can use this alternative:\n\ntft,d =\n\n\xe2\x80\x99\n\ni\n\ne\np\n\n \n\ni\n\n\xe2\x80\x98\n \n:\n1\nn\no\ns\nn\ne\nm\nD\n\ni\n\n500\n\ncherry\n\ndigital\n\n500\n\n1500\n\n1000\n\n2000\nDimension 2: \xe2\x80\x98computer\xe2\x80\x99\n\ninformation\n\n2500\n\n3000\n\n\x0cdocument\nfrequency\n\nidf\n\n6.5\n\n\xe2\x80\xa2 TF-IDF: WEIGHING TERMS IN THE VECTOR\n\n13\n\nThe second factor is used to give a higher weight to words that occur only in a\nfew documents. Terms that are limited to a few documents are useful for discrimi-\nnating those documents from the rest of the collection; terms that occur frequently\nacross the entire collection aren\xe2\x80\x99t as helpful. The document frequency dft of a\nterm t is the number of documents it occurs in. Document frequency is not the\nsame as the collection frequency of a term, which is the total number of times the\nword appears in the whole collection in any document. Consider in the collection of\nShakespeare\xe2\x80\x99s 37 plays the two words Romeo and action. The words have identical\ncollection frequencies (they both occur 113 times in all the plays) but very different\ndocument frequencies, since Romeo only occurs in a single play. If our goal is \xef\xac\x81nd\ndocuments about the romantic tribulations of Romeo, the word Romeo should be\nhighly weighted, but not action:\n\nCollection Frequency Document Frequency\n\nRomeo 113\n113\naction\n\n1\n31\n\nWe emphasize discriminative words like Romeo via the inverse document fre-\nquency or idf term weight (Sparck Jones, 1972). The idf is de\xef\xac\x81ned using the frac-\ntion N/dft, where N is the total number of documents in the collection, and dft is\nthe number of documents in which term t occurs. The fewer documents in which a\nterm occurs, the higher this weight. The lowest weight of 1 is assigned to terms that\noccur in all the documents. It\xe2\x80\x99s usually clear what counts as a document: in Shake-\nspeare we would use a play; when processing a collection of encyclopedia articles\nlike Wikipedia, the document is a Wikipedia page; in processing newspaper articles,\nthe document is a single article. Occasionally your corpus might not have appropri-\nate document divisions and you might need to break up the corpus into documents\nyourself for the purposes of computing idf.\n\nBecause of the large number of documents in many collections, this measure\ntoo is usually squashed with a log function. The resulting de\xef\xac\x81nition for inverse\ndocument frequency (idf) is thus\n\nidft = log10\n\n(6.13)\n\nHere are some idf values for some words in the Shakespeare corpus, ranging from\nextremely informative words which occur in only one play like Romeo, to those that\noccur in a few like salad or Falstaff, to those which are very common like fool or so\ncommon as to be completely non-discriminative since they occur in all 37 plays like\ngood or sweet.4\n\nWord\nRomeo\nsalad\nFalstaff\nforest\nbattle\nwit\nfool\ngood\nsweet\n\ndf\n1\n2\n4\n12\n21\n34\n36\n37\n37\n\nidf\n1.57\n1.27\n0.967\n0.489\n0.246\n0.037\n0.012\n0\n0\n\n4 Sweet was one of Shakespeare\xe2\x80\x99s favorite adjectives, a fact probably related to the increased use of\nsugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).\n\n(cid:18) N\n\n(cid:19)\n\ndft\n\n\x0c14 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\ntf-idf\n\nThe tf-idf weighted value wt,d for word t in document d thus combines term\n\nfrequency tft,d (de\xef\xac\x81ned either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:\n\nwt,d = tft,d \xc3\x97 idft\n\n(6.14)\n\nFig. 6.8 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2,\nusing the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-\nsponding to the word good have now all become 0; since this word appears in every\ndocument, the tf-idf algorithm leads it to be ignored in any comparison of the plays.\nSimilarly, the word fool, which appears in 36 out of the 37 plays, has a much lower\nweight.\n\nTwelfth Night\n0\n0\n0.021\n0.044\n\nAs You Like It\n0.074\n0\n0.019\n0.049\n\nbattle\ngood\nfool\nwit\nFigure 6.8 A tf-idf weighted term-document matrix for four words in four Shakespeare\nplays, using the counts in Fig. 6.2. For example the 0.049 value for wit in As You Like It is\nthe product of tf = log10(20 + 1) = 1.322 and idf = .037. Note that the idf weighting has\neliminated the importance of the ubiquitous word good and vastly reduced the impact of the\nalmost-ubiquitous word fool.\n\nJulius Caesar\n0.22\n0\n0.0036\n0.018\n\nHenry V\n0.28\n0\n0.0083\n0.022\n\nThe tf-idf weighting is the way for weighting co-occurrence matrices in infor-\nmation retrieval, but also plays a role in many other aspects of natural language\nprocessing. It\xe2\x80\x99s also a great baseline, the simple thing to try \xef\xac\x81rst. We\xe2\x80\x99ll look at other\nweightings like PPMI (Positive Pointwise Mutual Information) in Section 6.7.\n\n6.6 Applications of the tf-idf vector model\n\nIn summary, the vector semantics model we\xe2\x80\x99ve described so far represents a target\nword as a vector with dimensions corresponding to all the words in the vocabulary\n(length |V|, with vocabularies of 20,000 to 50,000), which is also sparse (most values\nare zero). The values in each dimension are the frequency with which the target\nword co-occurs with each neighboring context word, weighted by tf-idf. The model\ncomputes the similarity between two words x and y by taking the cosine of their\ntf-idf vectors; high cosine, high similarity. This entire model is sometimes referred\nto for short as the tf-idf model, after the weighting function.\n\nOne common use for a tf-idf model is to compute word similarity, a useful tool\nfor tasks like \xef\xac\x81nding word paraphrases, tracking changes in word meaning, or au-\ntomatically discovering meanings of words in different corpora. For example, we\ncan \xef\xac\x81nd the 10 most similar words to any target word w by computing the cosines\nbetween w and each of the V \xe2\x88\x92 1 other words, sorting, and looking at the top 10.\n\nThe tf-idf vector model can also be used to decide if two documents are similar.\nWe represent a document by taking the vectors of all the words in the document, and\ncomputing the centroid of all those vectors. The centroid is the multidimensional\nversion of the mean; the centroid of a set of vectors is a single vector that has the\nminimum sum of squared distances to each of the vectors in the set. Given k word\nvectors w1,w2, ...,wk, the centroid document vector d is:\n\nd =\n\nw1 + w2 + ... + wk\n\nk\n\n(6.15)\n\ncentroid\n\ndocument\nvector\n\n\x0c6.7\n\n\xe2\x80\xa2 OPTIONAL: POINTWISE MUTUAL INFORMATION (PMI)\n\n15\n\nGiven two documents, we can then compute their document vectors d1 and d2,\n\nand estimate the similarity between the two documents by cos(d1,d2).\n\nDocument similarity is also useful for all sorts of applications; information re-\ntrieval, plagiarism detection, news recommender systems, and even for digital hu-\nmanities tasks like comparing different versions of a text to see which are similar to\neach other.\n\n6.7 Optional: Pointwise Mutual Information (PMI)\n\nAn alternative weighting function to tf-idf is called PPMI (positive pointwise mutual\ninformation). PPMI draws on the intuition that the best way to weigh the association\nbetween two words is to ask how much more the two words co-occur in our corpus\nthan we would have a priori expected them to appear by chance.\n\nPointwise mutual information (Fano, 1961)5 is one of the most important con-\ncepts in NLP. It is a measure of how often two events x and y occur, compared with\nwhat we would expect if they were independent:\n\npointwise\nmutual\ninformation\n\nI(x,y) = log2\n\nP(x,y)\nP(x)P(y)\n\n(6.17)\n\nThe pointwise mutual information between a target word w and a context word\n\nc (Church and Hanks 1989, Church and Hanks 1990) is then de\xef\xac\x81ned as:\n\nPMI(w,c) = log2\n\nP(w,c)\nP(w)P(c)\n\n(6.18)\n\nThe numerator tells us how often we observed the two words together (assuming\nwe compute probability by using the MLE). The denominator tells us how often\nwe would expect the two words to co-occur assuming they each occurred indepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmate of how much more the two words co-occur than we expect by chance. PMI is\na useful tool whenever we need to \xef\xac\x81nd words that are strongly associated.\n\nPMI values range from negative to positive in\xef\xac\x81nity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwo words whose individual probability is each 10\xe2\x88\x926 occur together less often than\nchance, we would need to be certain that the probability of the two occurring to-\ngether is signi\xef\xac\x81cantly different than 10\xe2\x88\x9212, and this kind of granularity would require\nan enormous corpus. Furthermore it\xe2\x80\x99s not clear whether it\xe2\x80\x99s even possible to evalu-\nate such scores of \xe2\x80\x98unrelatedness\xe2\x80\x99 with human judgments. For this reason it is more\ncommon to use Positive PMI (called PPMI) which replaces all negative PMI values\n\nPPMI\n\n5 Pointwise mutual information is based on the mutual information between two random variables X\nand Y , which is de\xef\xac\x81ned as:\n\nI(X,Y ) =\n\nP(x,y)log2\n\nP(x,y)\nP(x)P(y)\n\n(6.16)\n\n(cid:88)\n\n(cid:88)\n\nx\n\ny\n\nIn a confusion of terminology, Fano used the phrase mutual information to refer to what we now call\npointwise mutual information and the phrase expectation of the mutual information for what we now call\nmutual information\n\n\x0c16 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nwith zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)6:\n\nPPMI(w,c) = max(log2\n\nP(w,c)\nP(w)P(c)\n\n,0)\n\n(6.19)\n\n(6.20)\n\n(6.21)\n\nMore formally, let\xe2\x80\x99s assume we have a co-occurrence matrix F with W rows (words)\nand C columns (contexts), where fi j gives the number of times word wi occurs in\ncontext c j. This can be turned into a PPMI matrix where ppmii j gives the PPMI\nvalue of word wi with context c j as follows:\n\npi j =\n\n(cid:80)W\n\ni=1\n\n(cid:80)C\n\nfi j\n\nj=1 fi j\n\npi\xe2\x88\x97 =\n\n(cid:80)C\n(cid:80)W\n(cid:80)C\n\nj=1 fi j\n\ni=1\n\nj=1 fi j\n\n(cid:80)W\n(cid:80)W\n(cid:80)C\n\ni=1 fi j\n\ni=1\n\nj=1 fi j\n\np\xe2\x88\x97 j =\n\nPPMIi j = max(log2\n\npi j\n\npi\xe2\x88\x97 p\xe2\x88\x97 j\n\n,0)\n\nLet\xe2\x80\x99s see some PPMI calculations. We\xe2\x80\x99ll use Fig. 6.9, which repeats Fig. 6.5 plus all\nthe count marginals, and let\xe2\x80\x99s pretend for ease of calculation that these are the only\nwords/contexts that matter.\n\ncherry\n\nstrawberry\n\ndigital\n\ninformation\n\ncomputer\n\ndata\n\nresult\n\n2\n0\n\n1670\n3325\n\n8\n0\n\n1683\n3982\n\n9\n1\n85\n378\n\npie\n442\n60\n5\n5\n\nsugar\n\ncount(w)\n\n25\n19\n4\n13\n\n486\n80\n3447\n7703\n\n4997\n\ncount(context)\nFigure 6.9 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,\ntogether with the marginals, pretending for the purpose of this calculation that no other\nwords/contexts matter.\n\n11716\n\n5673\n\n512\n\n473\n\n61\n\nThus for example we could compute PPMI(w=information,c=data), assuming\nwe pretended that Fig. 6.5 encompassed all the relevant word contexts/dimensions,\nas follows:\n\nP(w=information,c=data) =\n\nP(w=information) =\n\nP(c=data) =\n\n3982\n11716\n7703\n11716\n5673\n11716\n\n= .3399\n\n= .6575\n\n= .4842\n\nppmi(information,data) = log2(.3399/(.6575\xe2\x88\x97 .4842)) = .0944\n\nFig. 6.10 shows the joint probabilities computed from the counts in Fig. 6.9, and\nFig. 6.11 shows the PPMI values. Not surprisingly, cherry and strawberry are highly\nassociated with both pie and sugar, and data is mildly associated with information.\nPMI has the problem of being biased toward infrequent events; very rare words\ntend to have very high PMI values. One way to reduce this bias toward low frequency\nevents is to slightly change the computation for P(c), using a different function P\xce\xb1 (c)\nthat raises the probability of the context word to the power of \xce\xb1:\n\nPPMI\xce\xb1 (w,c) = max(log2\n\nP(w,c)\n\nP(w)P\xce\xb1 (c)\n\n,0)\n\n(6.22)\n\n6 Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the\n\xe2\x88\x92\xe2\x88\x9e from log(0).\n\n\x0c6.8\n\n\xe2\x80\xa2 WORD2VEC\n\n17\n\ncomputer\n\n0.0002\n0.0000\n0.1425\n0.2838\n\np(w,context)\nresult\n0.0008\n0.0001\n0.0073\n0.0323\n\ndata\n0.0007\n0.0000\n0.1436\n0.3399\n\npie\n\n0.0377\n0.0051\n0.0004\n0.0004\n\nsugar\n0.0021\n0.0016\n0.0003\n0.0011\n\np(w)\np(w)\n0.0415\n0.0068\n0.2942\n0.6575\n\ncherry\n\nstrawberry\n\ndigital\n\ninformation\n\np(context)\nFigure 6.10 Replacing the counts in Fig. 6.5 with joint probabilities, showing the marginals\naround the outside.\n\n0.0052\n\n0.4265\n\n0.4842\n\n0.0404\n\n0.0437\n\ncherry\n\nstrawberry\n\ndigital\n\ncomputer\n\n0\n0\n\ndata\n\n0\n0\n\n0.18\n0.02\n\n0.01\n0.09\n\nresult\n\n0\n0\n0\n\npie\n4.38\n4.10\n\nsugar\n3.30\n5.51\n\n0\n0\n\n0\n0\n\ninformation\nFigure 6.11 The PPMI matrix showing the association between words and context words,\ncomputed from the counts in Fig. 6.10. Note that most of the 0 PPMI values are ones that had\na negative PMI; for example PMI(cherry,computer) = -6.7, meaning that cherry and computer\nco-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace\nnegative values by zero.\n\n0.28\n\n(cid:80)\n\ncount(c)\xce\xb1\nc count(c)\xce\xb1\n\nP\xce\xb1 (c) =\n\n(6.23)\n\nLevy et al. (2015) found that a setting of \xce\xb1 = 0.75 improved performance of\nembeddings on a wide range of tasks (drawing on a similar weighting used for skip-\ngrams described below in Eq. 6.32). This works because raising the count to \xce\xb1 =\n0.75 increases the probability assigned to rare contexts, and hence lowers their PMI\n(P\xce\xb1 (c) > P(c) when c is rare).\n\nAnother possible solution is Laplace smoothing: Before computing PMI, a small\nconstant k (values of 0.1-3 are common) is added to each of the counts, shrinking\n(discounting) all the non-zero values. The larger the k, the more the non-zero counts\nare discounted.\n\n6.8 Word2vec\n\nIn the previous sections we saw how to represent a word as a sparse, long vector with\ndimensions corresponding to the words in the vocabulary, and whose values were tf-\nidf or PPMI functions of the count of the word co-occurring with each neighboring\nword. In this section we turn to an alternative method for representing a word: the\nuse of vectors that are short (of length perhaps 50-1000) and dense (most values are\nnon-zero).\n\nIt turns out that dense vectors work better in every NLP task than sparse vec-\ntors. While we don\xe2\x80\x99t completely understand all the reasons for this, we have some\nintuitions. First, dense vectors may be more successfully included as features in\nmachine learning systems; for example if we use 100-dimensional word embed-\ndings as features, a classi\xef\xac\x81er can just learn 100 weights to represent a function of\nword meaning; if we instead put in a 50,000 dimensional vector, a classi\xef\xac\x81er would\nhave to learn tens of thousands of weights for each of the sparse dimensions. Sec-\nond, because they contain fewer parameters than sparse vectors of explicit counts,\n\n\x0c18 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nskip-gram\nSGNS\nword2vec\n\ndense vectors may generalize better and help avoid over\xef\xac\x81tting. Finally, dense vec-\ntors may do a better job of capturing synonymy than sparse vectors. For example,\ncar and automobile are synonyms; but in a typical sparse vector representation, the\ncar dimension and the automobile dimension are distinct dimensions. Because the\nrelationship between these two dimensions is not modeled, sparse vectors may fail\nto capture the similarity between a word with car as a neighbor and a word with\nautomobile as a neighbor.\n\nIn this section we introduce one method for very dense, short vectors, skip-\ngram with negative sampling, sometimes called SGNS. The skip-gram algorithm\nis one of two algorithms in a software package called word2vec, and so sometimes\nthe algorithm is loosely referred to as word2vec (Mikolov et al. 2013, Mikolov\net al. 2013a). The word2vec methods are fast, ef\xef\xac\x81cient to train, and easily avail-\nable online with code and pretrained embeddings. We point to other embedding\nmethods, like the equally popular GloVe (Pennington et al., 2014), at the end of the\nchapter.\n\nThe intuition of word2vec is that instead of counting how often each word w oc-\ncurs near, say, apricot, we\xe2\x80\x99ll instead train a classi\xef\xac\x81er on a binary prediction task: \xe2\x80\x9cIs\nword w likely to show up near apricot?\xe2\x80\x9d We don\xe2\x80\x99t actually care about this prediction\ntask; instead we\xe2\x80\x99ll take the learned classi\xef\xac\x81er weights as the word embeddings.\n\nThe revolutionary intuition here is that we can just use running text as implicitly\nsupervised training data for such a classi\xef\xac\x81er; a word s that occurs near the target\nword apricot acts as gold \xe2\x80\x98correct answer\xe2\x80\x99 to the question \xe2\x80\x9cIs word w likely to show\nup near apricot?\xe2\x80\x9d This avoids the need for any sort of hand-labeled supervision\nsignal. This idea was \xef\xac\x81rst proposed in the task of neural language modeling, when\nBengio et al. (2003) and Collobert et al. (2011) showed that a neural language model\n(a neural network that learned to predict the next word from prior words) could just\nuse the next word in running text as its supervision signal, and could be used to learn\nan embedding representation for each word as part of doing this prediction task.\n\nWe\xe2\x80\x99ll see how to do neural networks in the next chapter, but word2vec is a\nmuch simpler model than the neural network language model, in two ways. First,\nword2vec simpli\xef\xac\x81es the task (making it binary classi\xef\xac\x81cation instead of word pre-\ndiction). Second, word2vec simpli\xef\xac\x81es the architecture (training a logistic regression\nclassi\xef\xac\x81er instead of a multi-layer neural network with hidden layers that demand\nmore sophisticated training algorithms). The intuition of skip-gram is:\n\n1. Treat the target word and a neighboring context word as positive examples.\n2. Randomly sample other words in the lexicon to get negative samples.\n3. Use logistic regression to train a classi\xef\xac\x81er to distinguish those two cases.\n4. Use the regression weights as the embeddings.\n\n6.8.1 The classi\xef\xac\x81er\nLet\xe2\x80\x99s start by thinking about the classi\xef\xac\x81cation task, and then turn to how to train.\nImagine a sentence like the following, with a target word apricot, and assume we\xe2\x80\x99re\nusing a window of \xc2\xb12 context words:\n\n... lemon,\n\na [tablespoon of apricot jam,\n\nc1\n\nc2\n\nt\n\nc3\n\na] pinch ...\nc4\n\nOur goal is to train a classi\xef\xac\x81er such that, given a tuple (t,c) of a target word\nt paired with a candidate context word c (for example (apricot, jam), or perhaps\n\n\x0c(apricot, aardvark)) it will return the probability that c is a real context word (true\nfor jam, false for aardvark):\n\n6.8\n\n\xe2\x80\xa2 WORD2VEC\n\n19\n\n(6.24)\nThe probability that word c is not a real context word for t is just 1 minus\n\nP(+|t,c)\n\nEq. 6.24:\n\nP(\xe2\x88\x92|t,c) = 1\xe2\x88\x92 P(+|t,c)\n\n(6.25)\nHow does the classi\xef\xac\x81er compute the probability P? The intuition of the skip-\ngram model is to base this probability on similarity: a word is likely to occur near\nthe target if its embedding is similar to the target embedding. How can we compute\nsimilarity between embeddings? Recall that two vectors are similar if they have a\nhigh dot product (cosine, the most popular similarity metric, is just a normalized dot\nproduct). In other words:\n\nSimilarity(t,c) \xe2\x89\x88 t \xc2\xb7 c\n\n(6.26)\nOf course, the dot product t \xc2\xb7 c is not a probability, it\xe2\x80\x99s just a number ranging from\n\xe2\x88\x92\xe2\x88\x9e to \xe2\x88\x9e. (Recall, for that matter, that cosine isn\xe2\x80\x99t a probability either). To turn the\ndot product into a probability, we\xe2\x80\x99ll use the logistic or sigmoid function \xcf\x83 (x), the\nfundamental core of logistic regression:\n\n\xcf\x83 (x) =\n\n1\n\n1 + e\xe2\x88\x92x\n\n(6.27)\n\nThe probability that word c is a real context word for target word t is thus computed\nas:\n\nP(+|t,c) =\n\n1\n\n1 + e\xe2\x88\x92t\xc2\xb7c\n\n(6.28)\n\nThe sigmoid function just returns a number between 0 and 1, so to make it a proba-\nbility we\xe2\x80\x99ll need to make sure that the total probability of the two possible events (c\nbeing a context word, and c not being a context word) sums to 1.\n\nThe probability that word c is not a real context word for t is thus:\n\nP(\xe2\x88\x92|t,c) = 1\xe2\x88\x92 P(+|t,c)\n\ne\xe2\x88\x92t\xc2\xb7c\n1 + e\xe2\x88\x92t\xc2\xb7c\n\n=\n\n(6.29)\n\nEquation 6.28 gives us the probability for one word, but we need to take account\nof the multiple context words in the window. Skip-gram makes the strong but very\nuseful simplifying assumption that all context words are independent, allowing us to\njust multiply their probabilities:\n\nP(+|t,c1:k) =\n\nlogP(+|t,c1:k) =\n\n1\n\n1 + e\xe2\x88\x92t\xc2\xb7ci\n\nlog\n\n1\n\n1 + e\xe2\x88\x92t\xc2\xb7ci\n\n(6.30)\n\n(6.31)\n\nk(cid:89)\nk(cid:88)\n\ni=1\n\ni=1\n\nIn summary, skip-gram trains a probabilistic classi\xef\xac\x81er that, given a test target word\nt and its context window of k words c1:k, assigns a probability based on how similar\n\n\x0c20 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nthis context window is to the target word. The probability is based on applying the\nlogistic (sigmoid) function to the dot product of the embeddings of the target word\nwith each context word. We could thus compute this probability if only we had\nembeddings for each target word and context word in the vocabulary. Let\xe2\x80\x99s now turn\nto learning these embeddings (which is the real goal of training this classi\xef\xac\x81er in the\n\xef\xac\x81rst place).\n\n6.8.2 Learning skip-gram embeddings\nWord2vec learns embeddings by starting with an initial set of embedding vectors\nand then iteratively shifting the embedding of each word w to be more like the em-\nbeddings of words that occur nearby in texts, and less like the embeddings of words\nthat don\xe2\x80\x99t occur nearby. Let\xe2\x80\x99s start by considering a single piece of training data:\n\n... lemon,\n\na [tablespoon of apricot jam,\n\nc1\n\nc2\n\nt\n\nc3\n\na] pinch ...\nc4\n\nThis example has a target word t (apricot), and 4 context words in the L = \xc2\xb12\n\nwindow, resulting in 4 positive training instances (on the left below):\nnegative examples -\nc\n\nc\n\nc\ntablespoon\n\npositive examples +\nt\napricot\napricot of\napricot\napricot a\n\njam\n\nt\n\nt\napricot aardvark apricot seven\napricot my\napricot where\napricot coaxial\n\napricot forever\napricot dear\napricot\n\nif\n\nFor training a binary classi\xef\xac\x81er we also need negative examples. In fact skip-\ngram uses more negative examples than positive examples (with the ratio between\nthem set by a parameter k). So for each of these (t,c) training instances we\xe2\x80\x99ll create\nk negative samples, each consisting of the target t plus a \xe2\x80\x98noise word\xe2\x80\x99. A noise word\nis a random word from the lexicon, constrained not to be the target word t. The\nright above shows the setting where k = 2, so we\xe2\x80\x99ll have 2 negative examples in the\nnegative training set \xe2\x88\x92 for each positive example t,c.\n\nThe noise words are chosen according to their weighted unigram frequency\np\xce\xb1 (w), where \xce\xb1 is a weight. If we were sampling according to unweighted fre-\nquency p(w), it would mean that with unigram probability p(\xe2\x80\x9cthe\xe2\x80\x9d) we would choose\nthe word the as a noise word, with unigram probability p(\xe2\x80\x9caardvark\xe2\x80\x9d) we would\nchoose aardvark, and so on. But in practice it is common to set \xce\xb1 = .75, i.e. use the\nweighting p 3\n\n4 (w):\n\n(cid:80)\n\nP\xce\xb1 (w) =\n\ncount(w)\xce\xb1\nw(cid:48) count(w(cid:48))\xce\xb1\n\n(6.32)\n\nSetting \xce\xb1 = .75 gives better performance because it gives rare noise words slightly\nhigher probability: for rare words, P\xce\xb1 (w) > P(w). To visualize this intuition, it\nmight help to work out the probabilities for an example with two events, P(a) = .99\nand P(b) = .01:\n\nP\xce\xb1 (a) =\n\nP\xce\xb1 (b) =\n\n.99.75\n\n.99.75 + .01.75 = .97\n.99.75 + .01.75 = .03\n\n.01.75\n\n(6.33)\n\n\x0c6.8\n\n\xe2\x80\xa2 WORD2VEC\n\n21\n\nGiven the set of positive and negative training instances, and an initial set of em-\nbeddings, the goal of the learning algorithm is to adjust those embeddings such that\nwe\n\nfrom the positive examples\n\n\xe2\x80\xa2 Maximize the similarity of the target word, context word pairs (t,c) drawn\n\xe2\x80\xa2 Minimize the similarity of the (t,c) pairs drawn from the negative examples.\nWe can express this formally over the whole training set as:\n\nL(\xce\xb8 ) =\n\nlogP(+|t,c) +\n\nlogP(\xe2\x88\x92|t,c)\n\n(6.34)\n\n(cid:88)\n\n(t,c)\xe2\x88\x88+\n\n(cid:88)\n\n(t,c)\xe2\x88\x88\xe2\x88\x92\n\nIf we look at one word/context pair (t,c) with its k noise words n1...nk, the learning\nobjective L is:\n\nL(\xce\xb8 ) = logP(+|t,c) +\n\nlogP(\xe2\x88\x92|t,ni)\n\n= log\xcf\x83 (c\xc2\xb7t) +\n\nlog\xcf\x83 (\xe2\x88\x92ni \xc2\xb7t)\n\ni=1\n\nk(cid:88)\nk(cid:88)\nk(cid:88)\n\ni=1\n\ni=1\n\n1\n\n= log\n\n1 + e\xe2\x88\x92c\xc2\xb7t +\n\nlog\n\n1\n\n1 + eni\xc2\xb7t\n\n(6.35)\n\ntarget\nembedding\ncontext\nembedding\n\nThat is, we want to maximize the dot product of the word with the actual context\nwords, and minimize the dot products of the word with the k negative sampled non-\nneighbor words.\n\nWe can then use stochastic gradient descent to train to this objective, iteratively\nmodifying the parameters (the embeddings for each target word t and each context\nword or noise word c in the vocabulary) to maximize the objective.\n\nNote that the skip-gram model thus actually learns two separate embeddings\nfor each word w: the target embedding t and the context embedding c. These\nembeddings are stored in two matrices, the target matrix T and the context matrix\nC. So each row i of the target matrix T is the 1\xc3\x97 d vector embedding ti for word\ni in the vocabulary V , and each column j of the context matrix C is a d \xc3\x97 1 vector\nembedding c j for word j in V . Fig. 6.12 shows an intuition of the learning task for\nthe embeddings encoded in these two matrices.\n\nJust as in logistic regression, then, the learning algorithm starts with randomly\ninitialized W and C matrices, and then walks through the training corpus using gra-\ndient descent to move W and C so as to maximize the objective in Eq. 6.35. Thus\nthe matrices W and C function as the parameters \xce\xb8 that logistic regression is tuning.\nOnce the embeddings are learned, we\xe2\x80\x99ll have two embeddings for each word wi:\nti and ci. We can choose to throw away the C matrix and just keep W , in which case\neach word i will be represented by the vector ti.\n\nAlternatively we can add the two embeddings together, using the summed em-\nbedding ti + ci as the new d-dimensional embedding, or we can concatenate them\ninto an embedding of dimensionality 2d.\n\nAs with the simple count-based methods like tf-idf, the context window size L\naffects the performance of skip-gram embeddings, and experiments often tune the\nparameter L on a devset. One difference from the count-based methods is that for\nskip-grams, the larger the window size the more computation the algorithm requires\nfor training (more neighboring words must be predicted).\n\n\x0c22 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nFigure 6.12 The skip-gram model tries to shift embeddings so the target embeddings (here\nfor apricot) are closer to (have a higher dot product with) context embeddings for nearby\nwords (here jam) and further from (have a lower dot product with) context embeddings for\nwords that don\xe2\x80\x99t occur nearby (here aardvark).\n\n6.9 Visualizing Embeddings\n\n\xe2\x80\x9cI see well in many dimensions as long as the dimensions are around two.\xe2\x80\x9d\n\nThe late economist Martin Shubek\n\nVisualizing embeddings is an important goal in helping understand, apply, and\nimprove these models of word meaning. But how can we visualize a (for example)\n100-dimensional vector?\n\nThe simplest way to visualize the meaning of a word w embedded in a space is to\nlist the most similar words to w by sorting the vectors for all words in the vocabulary\nby their cosine with the vector for w. For example the 7 closest words to frog using\nthe GloVe embeddings are: frogs, toad, litoria, leptodactylidae, rana, lizard, and\neleutherodactylus (Pennington et al., 2014)\n\nYet another visualization method is to use a clus-\ntering algorithm to show a hierarchical representa-\ntion of which words are similar to others in the em-\nbedding space. The uncaptioned example on the\nright uses hierarchical clustering of some embedding\nvectors for nouns as a visualization method (Rohde\net al., 2006).\n\nProbably the most common visualization method,\nhowever, is to project the 100 dimensions of a word\ndown into 2 dimensions. Fig. 6.1 showed one such\nvisualization, as does Fig. 6.13, using a projection\nmethod called t-SNE (van der Maaten and Hinton, 2008).\n\n6.10 Semantic properties of embeddings\n\nVector semantic models have a number of parameters. One parameter that is relevant\nto both sparse tf-idf vectors and dense word2vec vectors is the size of the context\n\nT\n\n1. ..    \xe2\x80\xa6   d\n\napricot\ntarget word\n\n1\n.\ni\n.\n.\n.\n.\n.\n.\nV\n\nsimilarity( apricot , aardvark)\n\ndecrease\nti . ck\n\nsimilarity( apricot , jam)\n\nincrease\nti . cj\n\n\xe2\x80\x9c\xe2\x80\xa6apricot jam\xe2\x80\xa6\xe2\x80\x9d\n\n1\n.\n.\n.\nd\n\n1.2\xe2\x80\xa6\xe2\x80\xa6.j\xe2\x80\xa6\xe2\x80\xa6.\xe2\x80\xa6\xe2\x80\xa6.\xe2\x80\xa6k\xe2\x80\xa6\xe2\x80\xa6V\n\nC\n\njam\nneighbor word\n\naardvark\nrandom noise word\n\nOYSTER\n\nBULL\n\nFigure 8: Multidimensional scaling for three noun classes.\n\nWRIST\nANKLE\nSHOULDER\nARM\nLEG\nHAND\nHEAD\n\nFOOT\nNOSE\nFINGER\nTOE\nFACE\nEAR\n\nEYE\n\nTOOTH\n\nDOG\nCAT\n\nPUPPY\nKITTEN\nCOW\nTURTLE\n\nMOUSE\n\nOYSTER\n\nLION\nBULL\nCHICAGO\nATLANTA\n\nMONTREAL\nNASHVILLE\nTOKYO\n\nCHINA\nRUSSIA\nAFRICA\nASIA\nEUROPE\n\nAMERICA\n\nBRAZIL\nFRANCE\n\nMOSCOW\n\nHAWAII\n\nFigure 9: Hierarchical clustering for three noun classes using distances based on vector correlations.\n\n\x0c6.10\n\n\xe2\x80\xa2 SEMANTIC PROPERTIES OF EMBEDDINGS\n\n23\n\nwindow used to collect counts. This is generally between 1 and 10 words on each\nside of the target word (for a total context of 3-20 words).\n\nThe choice depends on the goals of the representation. Shorter context windows\ntend to lead to representations that are a bit more syntactic, since the information is\ncoming from immediately nearby words. When the vectors are computed from short\ncontext windows, the most similar words to a target word w tend to be semantically\nsimilar words with the same parts of speech. When vectors are computed from long\ncontext windows, the highest cosine words to a target word w tend to be words that\nare topically related but not similar.\nFor example Levy and Goldberg (2014a) showed that using skip-gram with a\nwindow of \xc2\xb12, the most similar words to the word Hogwarts (from the Harry Potter\nseries) were names of other \xef\xac\x81ctional schools: Sunnydale (from Buffy the Vampire\nSlayer) or Evernight (from a vampire series). With a window of \xc2\xb15, the most similar\nwords to Hogwarts were other words topically related to the Harry Potter series:\nDumbledore, Malfoy, and half-blood.\n\nIt\xe2\x80\x99s also often useful to distinguish two kinds of similarity or association between\nwords (Sch\xc2\xa8utze and Pedersen, 1993). Two words have \xef\xac\x81rst-order co-occurrence\n(sometimes called syntagmatic association) if they are typically nearby each other.\nThus wrote is a \xef\xac\x81rst-order associate of book or poem. Two words have second-order\nco-occurrence (sometimes called paradigmatic association) if they have similar\nneighbors. Thus wrote is a second-order associate of words like said or remarked.\nAnalogy Another semantic property of embeddings is their ability to capture re-\nlational meanings. Mikolov et al. (2013b) and Levy and Goldberg (2014b) show\nthat the offsets between vector embeddings can capture some analogical relations\nbetween words. For example, the result of the expression vector(\xe2\x80\x98king\xe2\x80\x99) - vec-\ntor(\xe2\x80\x98man\xe2\x80\x99) + vector(\xe2\x80\x98woman\xe2\x80\x99) is a vector close to vector(\xe2\x80\x98queen\xe2\x80\x99); the left panel\nin Fig. 6.13 visualizes this, again projected down into 2 dimensions. Similarly, they\nfound that the expression vector(\xe2\x80\x98Paris\xe2\x80\x99) - vector(\xe2\x80\x98France\xe2\x80\x99) + vector(\xe2\x80\x98Italy\xe2\x80\x99) results\nin a vector that is very close to vector(\xe2\x80\x98Rome\xe2\x80\x99).\n\n\xef\xac\x81rst-order\nco-occurrence\n\nsecond-order\nco-occurrence\n\n(a)\n\n(b)\n\nFigure 6.13 Relational properties of the vector space, shown by projecting vectors onto two dimensions. (a)\n\xe2\x80\x99king\xe2\x80\x99 - \xe2\x80\x99man\xe2\x80\x99 + \xe2\x80\x99woman\xe2\x80\x99 is close to \xe2\x80\x99queen\xe2\x80\x99 (b) offsets seem to capture comparative and superlative morphology\n(Pennington et al., 2014).\n\nEmbeddings and Historical Semantics: Embeddings can also be a useful tool\nfor studying how meaning changes over time, by computing multiple embedding\n\n\x0c24 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nspaces, each from texts written in a particular time period. For example Fig. 6.14\nshows a visualization of changes in meaning in English words over the last two\ncenturies, computed by building separate embedding spaces for each decade from\nhistorical corpora like Google N-grams (Lin et al., 2012) and the Corpus of Histori-\ncal American English (Davies, 2012).\n\nFigure 6.14 A t-SNE visualization of the semantic change of 3 words in English using\nword2vec vectors. The modern sense of each word, and the grey context words, are com-\nputed from the most recent (modern) time-point embedding space. Earlier points are com-\nputed from earlier historical embedding spaces. The visualizations show the changes in the\nword gay from meanings related to \xe2\x80\x9ccheerful\xe2\x80\x9d or \xe2\x80\x9cfrolicsome\xe2\x80\x9d to referring to homosexuality,\nthe development of the modern \xe2\x80\x9ctransmission\xe2\x80\x9d sense of broadcast from its original sense of\nsowing seeds, and the pejoration of the word awful as it shifted from meaning \xe2\x80\x9cfull of awe\xe2\x80\x9d\nto meaning \xe2\x80\x9cterrible or appalling\xe2\x80\x9d (Hamilton et al., 2016).\n\n6.11 Bias and Embeddings\n\nIn addition to their ability to learn word meaning from text, embeddings, alas, also\nreproduce the implicit biases and stereotypes that were latent in the text. Recall that\nembeddings model analogical relations; \xe2\x80\x98queen\xe2\x80\x99 as the closest word to \xe2\x80\x98king\xe2\x80\x99 - \xe2\x80\x98man\xe2\x80\x99\n+ \xe2\x80\x98woman\xe2\x80\x99 implies the analogy man:woman::king:queen. But embedding analogies\nalso exhibit gender stereotypes. For example Bolukbasi et al. (2016) \xef\xac\x81nd that the\nclosest occupation to \xe2\x80\x98man\xe2\x80\x99 - \xe2\x80\x98computer programmer\xe2\x80\x99 + \xe2\x80\x98woman\xe2\x80\x99 in word2vec em-\nbeddings trained on news text is \xe2\x80\x98homemaker\xe2\x80\x99, and that the embeddings similarly\nsuggest the analogy \xe2\x80\x98father\xe2\x80\x99 is to \xe2\x80\x98doctor\xe2\x80\x99 as \xe2\x80\x98mother\xe2\x80\x99 is to \xe2\x80\x98nurse\xe2\x80\x99. Algorithms that\nuse embeddings as part of a search for potential programmers or doctors might thus\nincorrectly downweight documents with women\xe2\x80\x99s names.\n\nEmbeddings also encode the implicit associations that are a property of human\nreasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-\nple\xe2\x80\x99s associations between concepts (like \xe2\x80\x98\xef\xac\x82owers\xe2\x80\x99 or \xe2\x80\x98insects\xe2\x80\x99) and attributes (like\n\xe2\x80\x98pleasantness\xe2\x80\x99 and \xe2\x80\x98unpleasantness\xe2\x80\x99) by measuring differences in the latency with\nwhich they label words in the various categories.7 Using such methods, people\nin the United States have been shown to associate African-American names with\nunpleasant words (more than European-American names), male names more with\n\n7 Roughly speaking, if humans associate \xe2\x80\x98\xef\xac\x82owers\xe2\x80\x99 with \xe2\x80\x98pleasantness\xe2\x80\x99 and \xe2\x80\x98insects\xe2\x80\x99 with \xe2\x80\x98unpleasant-\nness\xe2\x80\x99, when they are instructed to push a green button for \xe2\x80\x98\xef\xac\x82owers\xe2\x80\x99 (daisy, iris, lilac) and \xe2\x80\x98pleasant words\xe2\x80\x99\n(love, laughter, pleasure) and a red button for \xe2\x80\x98insects\xe2\x80\x99 (\xef\xac\x82ea, spider, mosquito) and \xe2\x80\x98unpleasant words\xe2\x80\x99\n(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for\n\xe2\x80\x98\xef\xac\x82owers\xe2\x80\x99 and \xe2\x80\x98unpleasant words\xe2\x80\x99 and a green button for \xe2\x80\x98insects\xe2\x80\x99 and \xe2\x80\x98pleasant words\xe2\x80\x99.\n\nCHAPTER 5. DYNAMIC SOCIAL REPRESENTATIONS OF WORD MEANING79\n\n\x0cdebiasing\n\n6.12\n\n\xe2\x80\xa2 EVALUATING VECTOR MODELS\n\n25\n\nmathematics and female names with the arts, and old people\xe2\x80\x99s names with unpleas-\nant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan\net al. (2017) replicated all these \xef\xac\x81ndings of implicit associations using GloVe vectors\nand cosine similarity instead of human latencies. For example African-American\nnames like \xe2\x80\x98Leroy\xe2\x80\x99 and \xe2\x80\x98Shaniqua\xe2\x80\x99 had a higher GloVe cosine with unpleasant words\nwhile European-American names (\xe2\x80\x98Brad\xe2\x80\x99, \xe2\x80\x98Greg\xe2\x80\x99, \xe2\x80\x98Courtney\xe2\x80\x99) had a higher cosine\nwith pleasant words. Any embedding-aware algorithm that made use of word senti-\nment could thus lead to bias against African Americans.\n\nRecent research focuses on ways to try to remove these kinds of biases, for ex-\nample by developing a transformation of the embedding space that removes gender\nstereotypes but preserves de\xef\xac\x81nitional gender (Bolukbasi et al. 2016, Zhao et al. 2017)\nor changing the training procedure (Zhao et al., 2018). However, although these sorts\nof debiasing may reduce bias in embeddings, they do not eliminate it (Gonen and\nGoldberg, 2019), and this remains an open problem.\n\nHistorical embeddings are also being used to measure biases in the past. Garg\net al. (2018) used embeddings from historical texts to measure the association be-\ntween embeddings for occupations and embeddings for names of various ethnici-\nties or genders (for example the relative cosine similarity of women\xe2\x80\x99s names versus\nmen\xe2\x80\x99s to occupation words like \xe2\x80\x98librarian\xe2\x80\x99 or \xe2\x80\x98carpenter\xe2\x80\x99) across the 20th century.\nThey found that the cosines correlate with the empirical historical percentages of\nwomen or ethnic groups in those occupations. Historical embeddings also repli-\ncated old surveys of ethnic stereotypes; the tendency of experimental participants in\n1933 to associate adjectives like \xe2\x80\x98industrious\xe2\x80\x99 or \xe2\x80\x98superstitious\xe2\x80\x99 with, e.g., Chinese\nethnicity, correlates with the cosine between Chinese last names and those adjectives\nusing embeddings trained on 1930s text. They also were able to document historical\ngender biases, such as the fact that embeddings for adjectives related to competence\n(\xe2\x80\x98smart\xe2\x80\x99, \xe2\x80\x98wise\xe2\x80\x99, \xe2\x80\x98thoughtful\xe2\x80\x99, \xe2\x80\x98resourceful\xe2\x80\x99) had a higher cosine with male than fe-\nmale words, and showed that this bias has been slowly decreasing since 1960. We\nreturn in later chapters to this question about the role of bias in natural language\nprocessing.\n\n6.12 Evaluating Vector Models\n\nThe most important evaluation metric for vector models is extrinsic evaluation on\ntasks; adding them as features into any NLP task and seeing whether this improves\nperformance over some other model.\n\nNonetheless it is useful to have intrinsic evaluations. The most common metric\nis to test their performance on similarity, computing the correlation between an\nalgorithm\xe2\x80\x99s word similarity scores and word similarity ratings assigned by humans.\nWordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0\nto 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.\nSimLex-999 (Hill et al., 2015) is a more dif\xef\xac\x81cult dataset that quanti\xef\xac\x81es similarity\n(cup, mug) rather than relatedness (cup, coffee), and including both concrete and\nabstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions,\neach consisting of a target word with 4 additional word choices; the task is to choose\nwhich is the correct synonym, as in the example: Levied is closest in meaning to:\nimposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\ndatasets present words without context.\n\nSlightly more realistic are intrinsic similarity tasks that include context. The\n\n\x0c26 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nStanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the\nWord-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offers richer\nevaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their\nsentential context, while WiC gives target words in two sentential contexts that are\neither in the same or different senses; see Section ??. The semantic textual similarity\ntask (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of sentence-\nlevel similarity algorithms, consisting of a set of pairs of sentences, each pair with\nhuman-labeled similarity scores.\n\nAnother task used for evaluate is an analogy task, where the system has to solve\nproblems of the form a is to b as c is to d, given a, b, and c and having to \xef\xac\x81nd d.\nThus given Athens is to Greece as Oslo is to\n, the system must \xef\xac\x81ll in the word\nNorway. Or more syntactically-oriented examples: given mouse, mice, and dollar\nthe system must return dollars. Large sets of such tuples have been created (Mikolov\net al. 2013, Mikolov et al. 2013b).\n\n6.13 Summary\n\n\xe2\x80\xa2 In vector semantics, a word is modeled as a vector\xe2\x80\x94a point in high-dimensional\n\nspace, also called an embedding.\n\n\xe2\x80\xa2 Vector semantic models fall into two classes: sparse and dense.\n\xe2\x80\xa2 In sparse models like tf-idf each dimension corresponds to a word in the vo-\ncabulary V ; cells in sparse models are functions of co-occurrence counts.\nThe term-document matrix has rows for each word (term) in the vocabulary\nand a column for each document. The word-context matrix has a row for\neach (target) word in the vocabulary and a column for each context term in\nthe vocabulary.\n\n\xe2\x80\xa2 The most widely used sparse weighting is tf-idf, which weights each cell by\nits term frequency and inverse document frequency. PPMI (pointwise pos-\nitive mutual information) is an alternative weighting scheme to tf-idf.\n\n\xe2\x80\xa2 Dense vector models have dimensionality 50\xe2\x80\x931000 and the dimensions are\nharder to interpret. Word2vec algorithms like skip-gram are a popular and\nef\xef\xac\x81cient way to compute dense embeddings. Skip-gram trains a logistic re-\ngression classi\xef\xac\x81er to compute the probability that two words are \xe2\x80\x98likely to\noccur nearby in text\xe2\x80\x99. This probability is computed from the dot product be-\ntween the embeddings for the two words.\n\n\xe2\x80\xa2 Skip-gram uses stochastic gradient descent to train the classi\xef\xac\x81er, by learning\nembeddings that have a high dot product with embeddings of words that occur\nnearby and a low dot product with noise words.\n\n\xe2\x80\xa2 Other important embedding algorithms include GloVe, a method based on ra-\ntios of word co-occurrence probabilities, and fasttext, an open-source library\nfor computing word embeddings by summing embeddings of the bag of char-\nacter n-grams that make up a word.\n\n\xe2\x80\xa2 Whether using sparse or dense vectors, word and document similarities are\ncomputed by some function of the dot product between vectors. The cosine\nof two vectors\xe2\x80\x94a normalized dot product\xe2\x80\x94is the most popular such metric.\n\n\x0cBIBLIOGRAPHICAL AND HISTORICAL NOTES\n\n27\n\nBibliographical and Historical Notes\n\nThe idea of vector semantics arose out of research in the 1950s in three distinct\n\xef\xac\x81elds: linguistics, psychology, and computer science, each of which contributed a\nfundamental aspect of the model.\n\nThe idea that meaning is related to the distribution of words in context was\nwidespread in linguistic theory of the 1950s, among distributionalists like Zellig\nHarris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos\n(1950) put it,\n\nthe linguist\xe2\x80\x99s \xe2\x80\x9cmeaning\xe2\x80\x9d of a morpheme. . . is by de\xef\xac\x81nition the set of conditional\nprobabilities of its occurrence in context with all other morphemes.\n\nThe idea that the meaning of a word might be modeled as a point in a multi-\ndimensional semantic space came from psychologists like Charles E. Osgood, who\nhad been studying how people responded to the meaning of words by assigning val-\nues along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the\nmeaning of a word in general could be modeled as a point in a multidimensional\nEuclidean space, and that the similarity of meaning between two words could be\nmodeled as the distance between these points in the space.\n\nA \xef\xac\x81nal intellectual source in the 1950s and early 1960s was the \xef\xac\x81eld then called\nmechanical indexing, now known as information retrieval. In what became known\nas the vector space model for information retrieval (Salton 1971, Sparck Jones 1986),\nresearchers demonstrated new ways to de\xef\xac\x81ne the meaning of words in terms of vec-\ntors (Switzer, 1965), and re\xef\xac\x81ned methods for word similarity based on measures\nof statistical association between words like mutual information (Giuliano, 1965)\nand idf (Sparck Jones, 1972), and showed that the meaning of documents could be\nrepresented in the same vector spaces used for words.\n\nMore distantly related is the idea of de\xef\xac\x81ning words by a vector of discrete fea-\ntures, which has a venerable history in our \xef\xac\x81eld, with roots at least as far back as\nDescartes and Leibniz (Wierzbicka 1992, Wierzbicka 1996). By the middle of the\n20th century, beginning with the work of Hjelmslev (Hjelmslev, 1969) and \xef\xac\x82eshed\nout in early models of generative grammar (Katz and Fodor, 1963), the idea arose of\nrepresenting meaning with semantic features, symbols that represent some sort of\nprimitive meaning. For example words like hen, rooster, or chick, have something\nin common (they all describe chickens) and something different (their age and sex),\nrepresentable as:\n\nmechanical\nindexing\n\nsemantic\nfeature\n\nhen\n+female, +chicken, +adult\nrooster -female, +chicken, +adult\nchick\n\n+chicken, -adult\n\nThe dimensions used by vector models of meaning to de\xef\xac\x81ne words, however, are\nonly abstractly related to this idea of a small \xef\xac\x81xed number of hand-built dimensions.\nNonetheless, there has been some attempt to show that certain dimensions of em-\nbedding models do contribute some speci\xef\xac\x81c compositional aspect of meaning like\nthese early semantic features.\n\nThe \xef\xac\x81rst use of dense vectors to model word meaning was the latent seman-\ntic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent seman-\ntic analysis) (Deerwester et al., 1990). In LSA singular value decomposition\xe2\x80\x94\nSVD\xe2\x80\x94 is applied to a term-document matrix (each cell weighted by log frequency\nand normalized by entropy), and then the \xef\xac\x81rst 300 dimensions are used as the LSA\n\nSVD\n\n\x0c28 CHAPTER 6\n\n\xe2\x80\xa2 VECTOR SEMANTICS AND EMBEDDINGS\n\nembedding. Singular Value Decomposition (SVD) is a method for \xef\xac\x81nding the most\nimportant dimensions of a data set, those dimensions along which the data varies\nthe most. LSA was then quickly widely applied: as a cognitive model Landauer\nand Dumais (1997), and tasks like spell checking (Jones and Martin, 1997), lan-\nguage modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000)\nmorphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001), and\nessay grading (Rehder et al., 1998). Related models were simultaneously devel-\noped and applied to word sense disambiguation by Sch\xc2\xa8utze (1992). LSA also led to\nthe earliest use of embeddings to represent words in a probabilistic classi\xef\xac\x81er, in the\nlogistic regression document router of Sch\xc2\xa8utze et al. (1995). The idea of SVD on\nthe term-term matrix (rather than the term-document matrix) as a model of mean-\ning for NLP was proposed soon after LSA by Sch\xc2\xa8utze (1992). Sch\xc2\xa8utze applied the\nlow-rank (97-dimensional) embeddings produced by SVD to the task of word sense\ndisambiguation, analyzed the resulting semantic space, and also suggested possible\ntechniques like dropping high-order dimensions. See Sch\xc2\xa8utze (1997).\n\nA number of alternative matrix models followed on from the early SVD work,\nincluding Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent\nDirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-\ntion (NMF) (Lee and Seung, 1999).\n\nBy the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that\nneural language models could also be used to develop embeddings as part of the task\nof word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and\nCollobert et al. (2011) then demonstrated that embeddings could play a role for rep-\nresenting word meanings for a number of NLP tasks. Turian et al. (2010) compared\nthe value of different kinds of embeddings for different NLP tasks. Mikolov et al.\n(2011) showed that recurrent neural nets could be used as language models. The\nidea of simplifying the hidden layer of these neural net language models to create\nthe skip-gram (and also CBOW) algorithms was proposed by Mikolov et al. (2013).\nThe negative sampling training algorithm was proposed in Mikolov et al. (2013a).\n\nStudies of embeddings include results showing an elegant mathematical relation-\nship between sparse and dense embeddings (Levy and Goldberg, 2014c), as well\nas numerous surveys of embeddings and their parameterizations. (Bullinaria and\nLevy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014,\nLevy et al. 2015).\n\nThe most widely-used embedding model besides word2vec is GloVe (Penning-\nton et al., 2014). The name stands for Global Vectors, because the model is based on\ncapturing global corpus statistics. GloVe is based on ratios of probabilities from the\nword-word co-occurrence matrix, combining the intuitions of count-based models\nlike PPMI while also capturing the linear structures used by methods like word2vec.\nAn extension of word2vec, fasttext (Bojanowski et al., 2017), deals with un-\nknown words and sparsity in languages with rich morphology, by using subword\nmodels. Each word in fasttext is represented as itself plus a bag of constituent n-\ngrams, with special boundary symbols < and > added to each word. For example,\nwith n = 3 the word where would be represented by the character n-grams:\n\n<wh, whe, her, ere, re>\n\nplus the sequence\n\n<where>\n\nThen a skipgram embedding is learned for each constituent n-gram, and the word\nwhere is represented by the sum of all of the embeddings of its constituent n-grams.\n\nfasttext\n\n\x0cEXERCISES\n\n29\n\nA fasttext open-source library, including pretrained embeddings for 157 languages,\nis available at https://fasttext.cc.\n\nThere are many other embedding algorithms, using methods like non-negative\nmatrix factorization (Fyshe et al., 2015), or by converting sparse PPMI embeddings\nto dense vectors by using SVD (Levy and Goldberg, 2014c).\n\nIn Chapter 10 we introduce contextual embeddings like ELMo (Peters et al.,\n2018) and BERT (Devlin et al., 2019) in which the representation for a word is\ncontextual, a function of the entire input sentence.\n\nSee Manning et al. (2008) for a deeper understanding of the role of vectors in in-\nformation retrieval, including how to compare queries with documents, more details\non tf-idf, and issues of scaling to very large datasets.\n\nCruse (2004) is a useful introductory linguistic text on lexical semantics.\n\nExercises\n\n\x0c30 Chapter 6 \xe2\x80\xa2 Vector Semantics and Embeddings\n\nAgirre, E., Banea, C., Cardie, C., Cer, D., Diab, M.,\nGonzalez-Agirre, A., Guo, W., Lopez-Gazpio, I., Maritx-\nalar, M., Mihalcea, R., Rigau, G., Uria, L., and Wiebe,\nJ. (2015). 2015 SemEval-2015 Task 2: Semantic Textual\nSimilarity, English, Spanish and Pilot on Interpretability.\nIn SemEval-15, 252\xe2\x80\x93263.\n\nAgirre, E., Diab, M., Cer, D., and Gonzalez-Agirre, A.\n(2012). Semeval-2012 task 6: A pilot on semantic textual\nsimilarity. In SemEval-12, 385\xe2\x80\x93393.\n\nBellegarda, J. R. (1997). A latent semantic analysis frame-\nwork for large-span language modeling. In Eurospeech-97.\nBellegarda, J. R. (2000). Exploiting latent semantic infor-\nmation in statistical language modeling. Proceedings of\nthe IEEE, 89(8), 1279\xe2\x80\x931296.\n\nBengio, Y., Courville, A., and Vincent, P. (2013). Repre-\nsentation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence, 35(8), 1798\xe2\x80\x931828.\n\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003).\nA neural probabilistic language model. Journal of machine\nlearning research, 3(Feb), 1137\xe2\x80\x931155.\n\nBengio, Y., Schwenk, H., Sen\xc2\xb4ecal, J.-S., Morin, F., and Gau-\nvain, J.-L. (2006). Neural probabilistic language models.\nIn Innovations in Machine Learning, 137\xe2\x80\x93186. Springer.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent\n\nDirichlet allocation. JMLR, 3(5), 993\xe2\x80\x931022.\n\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T.\n(2017). Enriching word vectors with subword information.\nTACL, 5, 135\xe2\x80\x93146.\n\nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., and\nKalai, A. T. (2016). Man is to computer programmer as\nwoman is to homemaker? Debiasing word embeddings. In\nNIPS 16, 4349\xe2\x80\x934357.\n\nBr\xc2\xb4eal, M. (1897). Essai de S\xc2\xb4emantique: Science des signi\xef\xac\x81-\n\ncations. Hachette.\n\nBudanitsky, A. and Hirst, G. (2006). Evaluating WordNet-\nbased measures of lexical semantic relatedness. Computa-\ntional Linguistics, 32(1), 13\xe2\x80\x9347.\n\nBullinaria, J. A. and Levy, J. P. (2007). Extracting seman-\ntic representations from word co-occurrence statistics: A\ncomputational study. Behavior research methods, 39(3),\n510\xe2\x80\x93526.\n\nBullinaria, J. A. and Levy, J. P. (2012). Extracting semantic\nrepresentations from word co-occurrence statistics: stop-\nlists, stemming, and SVD. Behavior research methods,\n44(3), 890\xe2\x80\x93907.\n\nCaliskan, A., Bryson, J. J., and Narayanan, A. (2017). Se-\nmantics derived automatically from language corpora con-\ntain human-like biases. Science, 356(6334), 183\xe2\x80\x93186.\n\nChurch, K. W. and Hanks, P. (1989). Word association\nnorms, mutual information, and lexicography. In ACL-89,\n76\xe2\x80\x9383.\n\nCoccaro, N. and Jurafsky, D. (1998). Towards better inte-\ngration of semantic predictors in statistical language mod-\neling. In ICSLP-98, 2403\xe2\x80\x932406.\n\nCollobert, R. and Weston, J. (2007). Fast semantic extrac-\ntion using a novel neural network architecture. In ACL-07,\n560\xe2\x80\x93567.\n\nCollobert, R. and Weston, J. (2008). A uni\xef\xac\x81ed architec-\nture for natural language processing: Deep neural networks\nwith multitask learning. In ICML, 160\xe2\x80\x93167.\n\nCollobert, R., Weston,\n\nJ., Bottou, L., Karlen, M.,\nKavukcuoglu, K., and Kuksa, P. (2011). Natural language\nprocessing (almost) from scratch. JMLR, 12, 2493\xe2\x80\x932537.\nCruse, D. A. (2004). Meaning in Language: an Introduction\nto Semantics and Pragmatics. Oxford University Press.\nSecond edition.\n\nDagan, I., Marcus, S., and Markovitch, S. (1993). Contex-\nIn\n\ntual word similarity and estimation from sparse data.\nACL-93, 164\xe2\x80\x93171.\n\nDavies, M. (2012). Expanding horizons in historical linguis-\ntics with the 400-million word Corpus of Historical Amer-\nican English. Corpora, 7(2), 121\xe2\x80\x93157.\n\nDavies, M. (2015). The Wikipedia Corpus: 4.6 million arti-\ncles, 1.9 billion words. Adapted from Wikipedia. Available\nonline at https://www.english-corpora.org/wiki/.\nDeerwester, S. C., Dumais, S. T., Furnas, G. W., Harshman,\nR. A., Landauer, T. K., Lochbaum, K. E., and Streeter, L.\n(1988). Computer information retrieval using latent seman-\ntic structure: US Patent 4,839,853..\n\nDeerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas,\nIndexing by latent\n\nG. W., and Harshman, R. A. (1990).\nsemantics analysis. JASIS, 41(6), 391\xe2\x80\x93407.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).\nBERT: Pre-training of deep bidirectional transformers for\nIn NAACL HLT 2019, 4171\xe2\x80\x93\nlanguage understanding.\n4186.\n\nFano, R. M. (1961). Transmission of Information: A Statis-\n\ntical Theory of Communications. MIT Press.\n\nFinkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan,\nZ., Wolfman, G., and Ruppin, E. (2002). Placing search in\ncontext: The concept revisited. ACM Transactions on In-\nformation Systems, 20(1), 116\xe2\x80\x93\xe2\x80\x93131.\n\nFirth, J. R. (1957). A synopsis of linguistic theory 1930\xe2\x80\x93\n1955. In Studies in Linguistic Analysis. Philological Soci-\nety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers of\nJ. R. Firth. Longman, Harlow.\n\nFyshe, A., Wehbe, L., Talukdar, P. P., Murphy, B., and\nMitchell, T. M. (2015). A compositional and interpretable\nsemantic space. In NAACL HLT 2015.\n\nGarg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018).\nWord embeddings quantify 100 years of gender and eth-\nnic stereotypes. Proceedings of the National Academy of\nSciences, 115(16), E3635\xe2\x80\x93E3644.\n\nChurch, K. W. and Hanks, P. (1990). Word association\nnorms, mutual information, and lexicography. Computa-\ntional Linguistics, 16(1), 22\xe2\x80\x9329.\n\nGirard, G. (1718). La justesse de la lange franc\xc2\xb8oise: ou\nles differentes signi\xef\xac\x81cations des mots qui passent pour syn-\nonymes.\n\nClark, E. (1987). The principle of contrast: A constraint on\nIn MacWhinney, B. (Ed.), Mecha-\n\nlanguage acquisition.\nnisms of language acquisition, 1\xe2\x80\x9333. LEA.\n\n\x0cGiuliano, V. E. (1965). The interpretation of word as-\nsociations.\nIn Stevens, M. E., Giuliano, V. E., and\nHeilprin, L. B. (Eds.), Statistical Association Methods\nFor Mechanized Documentation. Symposium Proceed-\nings. Washington, D.C., USA, March 17, 1964,\n25\xe2\x80\x93\n32. https://nvlpubs.nist.gov/nistpubs/Legacy/\nMP/nbsmiscellaneouspub269.pdf.\n\nGonen, H. and Goldberg, Y. (2019). Lipstick on a pig: De-\nbiasing methods cover up systematic gender biases in word\nembeddings but do not remove them. In NAACL HLT 2019.\n\nGould, S. J. (1980). The Panda\xe2\x80\x99s Thumb. Penguin Group.\nGreenwald, A. G., McGhee, D. E., and Schwartz, J. L. K.\n(1998). Measuring individual differences in implicit cog-\nnition: the implicit association test.. Journal of personality\nand social psychology, 74(6), 1464\xe2\x80\x931480.\n\nHamilton, W. L., Leskovec, J., and Jurafsky, D. (2016). Di-\nachronic word embeddings reveal statistical laws of seman-\ntic change. In ACL 2016.\n\nHarris, Z. S. (1954). Distributional structure. Word, 10,\n146\xe2\x80\x93162. Reprinted in J. Fodor and J. Katz, The Struc-\nture of Language, Prentice Hall, 1964 and in Z. S. Har-\nris, Papers in Structural and Transformational Linguistics,\nReidel, 1970, 775\xe2\x80\x93794.\n\nHill, F., Reichart, R., and Korhonen, A. (2015). Simlex-999:\nEvaluating semantic models with (genuine) similarity esti-\nmation. Computational Linguistics, 41(4), 665\xe2\x80\x93695.\n\nHjelmslev, L. (1969). Prologomena to a Theory of Lan-\nguage. University of Wisconsin Press. Translated by Fran-\ncis J. Whit\xef\xac\x81eld; original Danish edition 1943.\n\nHofmann, T. (1999). Probabilistic latent semantic indexing.\n\nIn SIGIR-99.\n\nHuang, E. H., Socher, R., Manning, C. D., and Ng, A. Y.\n(2012). Improving word representations via global context\nand multiple word prototypes. In ACL 2012, 873\xe2\x80\x93882.\n\nJones, M. P. and Martin, J. H. (1997). Contextual spelling\ncorrection using latent semantic analysis. In ANLP 1997,\n166\xe2\x80\x93173.\n\nJoos, M. (1950). Description of language design. JASA, 22,\n\n701\xe2\x80\x93708.\n\nJurafsky, D. (2014). The Language of Food. W. W. Norton,\n\nNew York.\n\nKatz, J. J. and Fodor, J. A. (1963). The structure of a seman-\n\ntic theory. Language, 39, 170\xe2\x80\x93210.\n\nKiela, D. and Clark, S. (2014). A systematic study of seman-\ntic vector space model parameters. In Proceedings of the\nEACL 2nd Workshop on Continuous Vector Space Models\nand their Compositionality (CVSC), 21\xe2\x80\x9330.\n\nLandauer, T. K. and Dumais, S. T. (1997). A solution to\nPlato\xe2\x80\x99s problem: The Latent Semantic Analysis theory of\nacquisition, induction, and representation of knowledge.\nPsychological Review, 104, 211\xe2\x80\x93240.\n\nLapesa, G. and Evert, S. (2014). A large scale evaluation\nof distributional semantic models: Parameters, interactions\nand model selection. TACL, 2, 531\xe2\x80\x93545.\n\nLee, D. D. and Seung, H. S. (1999). Learning the parts\nof objects by non-negative matrix factorization. Nature,\n401(6755), 788\xe2\x80\x93791.\n\nLevy, O. and Goldberg, Y. (2014a). Dependency-based word\n\nembeddings. In ACL 2014.\n\nExercises\n\n31\n\nLevy, O. and Goldberg, Y. (2014b). Linguistic regularities in\n\nsparse and explicit word representations. In CoNLL-14.\n\nLevy, O. and Goldberg, Y. (2014c). Neural word embedding\nas implicit matrix factorization. In NIPS 14, 2177\xe2\x80\x932185.\nLevy, O., Goldberg, Y., and Dagan, I. (2015). Improving dis-\ntributional similarity with lessons learned from word em-\nbeddings. TACL, 3, 211\xe2\x80\x93225.\n\nLi, J., Chen, X., Hovy, E. H., and Jurafsky, D. (2015). Visual-\nizing and understanding neural models in NLP. In NAACL\nHLT 2015.\n\nLin, Y., Michel, J.-B., Lieberman Aiden, E., Orwant, J.,\nBrockman, W., and Petrov, S. (2012). Syntactic annota-\nIn ACL 2012,\ntions for the google books ngram corpus.\n169\xe2\x80\x93174.\n\nLuhn, H. P. (1957). A statistical approach to the mechanized\nencoding and searching of literary information. IBM Jour-\nnal of Research and Development, 1(4), 309\xe2\x80\x93317.\n\nManning, C. D., Raghavan, P., and Sch\xc2\xa8utze, H. (2008). In-\n\ntroduction to Information Retrieval. Cambridge.\n\nMikolov, T., Chen, K., Corrado, G. S., and Dean, J.\n(2013). Ef\xef\xac\x81cient estimation of word representations in vec-\ntor space. In ICLR 2013.\nMikolov, T., Kombrink, S., Burget, L., \xcb\x87Cernock`y, J. H., and\nKhudanpur, S. (2011). Extensions of recurrent neural net-\nwork language model. In ICASSP-11, 5528\xe2\x80\x935531.\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. (2013a). Distributed representations of words and\nIn NIPS 13, 3111\xe2\x80\x93\nphrases and their compositionality.\n3119.\n\nMikolov, T., Yih, W.-t., and Zweig, G. (2013b). Linguistic\nIn\n\nregularities in continuous space word representations.\nNAACL HLT 2013, 746\xe2\x80\x93751.\n\nNiwa, Y. and Nitta, Y. (1994). Co-occurrence vectors from\ncorpora vs. distance vectors from dictionaries. In ACL-94,\n304\xe2\x80\x93309.\n\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. (2002a).\nHarvesting implicit group attitudes and beliefs from a\ndemonstration web site. Group Dynamics: Theory, Re-\nsearch, and Practice, 6(1), 101.\n\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. (2002b).\nMath=male, me=female, therefore math(cid:54)= me. Journal of\npersonality and social psychology, 83(1), 44.\n\nOsgood, C. E., Suci, G. J., and Tannenbaum, P. H. (1957).\nThe Measurement of Meaning. University of Illinois Press.\nPennington, J., Socher, R., and Manning, C. D. (2014).\nGlove: Global vectors for word representation. In EMNLP\n2014, 1532\xe2\x80\x931543.\n\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\nLee, K., and Zettlemoyer, L. (2018). Deep contextualized\nword representations. In NAACL HLT 2018, 2227\xe2\x80\x932237.\nPilehvar, M. T. and Camacho-Collados, J. (2019). WiC:\nthe word-in-context dataset for evaluating context-sensitive\nIn NAACL HLT 2019, 1267\xe2\x80\x93\nmeaning representations.\n1273.\n\nRehder, B., Schreiner, M. E., Wolfe, M. B. W., Laham, D.,\nLandauer, T. K., and Kintsch, W. (1998). Using Latent\nSemantic Analysis to assess knowledge: Some technical\nconsiderations. Discourse Processes, 25(2-3), 337\xe2\x80\x93354.\n\n\x0c32 Chapter 6 \xe2\x80\xa2 Vector Semantics and Embeddings\n\nRohde, D. L. T., Gonnerman, L. M., and Plaut, D. C. (2006).\nAn improved model of semantic similarity based on lexical\nco-occurrence. CACM, 8, 627\xe2\x80\x93633.\n\nSalton, G. (1971). The SMART Retrieval System: Experi-\n\nments in Automatic Document Processing. Prentice Hall.\n\nSchone, P. and Jurafsky, D. (2000). Knowlege-free induction\nof morphology using latent semantic analysis. In CoNLL-\n00.\n\nSchone, P. and Jurafsky, D. (2001). Knowledge-free induc-\n\ntion of in\xef\xac\x82ectional morphologies. In NAACL 2001.\n\nSch\xc2\xa8utze, H. (1992). Dimensions of meaning. In Proceedings\n\nof Supercomputing \xe2\x80\x9992, 787\xe2\x80\x93796. IEEE Press.\n\nSch\xc2\xa8utze, H. (1997). Ambiguity Resolution in Language\nLearning \xe2\x80\x93 Computational and Cognitive Models. CSLI,\nStanford, CA.\n\nSch\xc2\xa8utze, H., Hull, D. A., and Pedersen, J. (1995). A com-\nparison of classi\xef\xac\x81ers and document representations for the\nrouting problem. In SIGIR-95, 229\xe2\x80\x93237.\n\nSch\xc2\xa8utze, H. and Pedersen, J. (1993). A vector model for syn-\ntagmatic and paradigmatic relatedness. In Proceedings of\nthe 9th Annual Conference of the UW Centre for the New\nOED and Text Research, 104\xe2\x80\x93113.\n\nSparck Jones, K. (1972). A statistical interpretation of term\nspeci\xef\xac\x81city and its application in retrieval. Journal of Doc-\numentation, 28(1), 11\xe2\x80\x9321.\n\nSparck Jones, K. (1986). Synonymy and Semantic Classi\xef\xac\x81-\ncation. Edinburgh University Press, Edinburgh. Republi-\ncation of 1964 PhD Thesis.\n\nSwitzer, P.\n\nVector\n\nimages in document\n\n(1965).\nre-\nIn Stevens, M. E., Giuliano, V. E., and\ntrieval.\n(Eds.), Statistical Association Meth-\nHeilprin, L. B.\nods For Mechanized Documentation. Symposium Pro-\nceedings. Washington, D.C., USA, March 17, 1964,\n163\xe2\x80\x93171.\nhttps://nvlpubs.nist.gov/nistpubs/\nLegacy/MP/nbsmiscellaneouspub269.pdf.\n\nTurian, J., Ratinov, L., and Bengio, Y. (2010). Word\nrepresentations: a simple and general method for semi-\nsupervised learning. In ACL 2010, 384\xe2\x80\x93394.\n\nvan der Maaten, L. and Hinton, G. E. (2008). Visualizing\n\nhigh-dimensional data using t-sne. JMLR, 9, 2579\xe2\x80\x932605.\n\nWierzbicka, A. (1992). Semantics, Culture, and Cognition:\nUniversity Human Concepts in Culture-Speci\xef\xac\x81c Con\xef\xac\x81gura-\ntions. Oxford University Press.\n\nWierzbicka, A. (1996). Semantics: Primes and Universals.\n\nOxford University Press.\nWittgenstein, L. (1953).\n\nPhilosophical Investigations.\n\n(Translated by Anscombe, G.E.M.). Blackwell.\n\nZhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-\nW. (2017). Men also like shopping: Reducing gender bias\nIn EMNLP\nampli\xef\xac\x81cation using corpus-level constraints.\n2017.\n\nZhao, J., Zhou, Y., Li, Z., Wang, W., and Chang, K.-W.\nIn\n\n(2018). Learning gender-neutral word embeddings.\nEMNLP 2018, 4847\xe2\x80\x934853.\n\n\x0c'