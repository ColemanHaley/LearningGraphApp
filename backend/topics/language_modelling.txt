Speech and Language Processing Daniel Jurafsky James H Martin Copyright c 2019 All rights reserved Draft of October 2 2019 CHAPTER 3 Ngram Language Models You are uniformly charming cried he with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for Random sentence generated from a Jane Austen trigram model Predicting is difficultespecially about the future as the old quip goes But how about predicting something that seems much easier like the next few words someone is going to say What word for example is likely to follow Please turn your homework Hopefully most of you concluded that a very likely word is in or possibly over but probably not refrigerator or the In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word The same models will also serve to assign a probability to an entire sentence Such a model for example could predict that the following sequence has a much higher probability of appearing in a text all of a sudden I notice three guys standing on the sidewalk than does this same set of words in a different order on guys all I of notice sidewalk three a sudden standing the Why would you want to predict upcoming words or assign probabilities to sen tences Probabilities are essential in any task in which we have to identify words in noisy ambiguous input like speech recognition For a speech recognizer to realize that you said I will be back soonish and not I will be bassoon dish it helps to know that back soonish is a much more probable sequence than bassoon dish For writing tools like spelling correction or grammatical error correction we need to find and correct errors in writing like Their are two midterms in which There was mistyped as Their or Everything has improve in which improve should have been improved The phrase There are will be much more probable than Their are and has improved than has improve allowing us to help users by detecting and correcting these errors Assigning probabilities to sequences of words is also essential in machine trans lation Suppose we are translating a Chinese source sentence 他 向 记者 介绍 了 主要 内容 He to reporters introduced main content As part of the process we might have built the following set of potential rough English translations he introduced reporters to the main contents of the statement he briefed to reporters the main contents of the statement he briefed reporters on the main contents of the statement 2 CHAPTER 3 NGRAM LANGUAGE MODELS A probabilistic model of word sequences could suggest that briefed reporters on is a more probable English phrase than briefed to reporters which has an awkward to after briefed or introduced reporters to which uses a verb that is less fluent English in this context allowing us to correctly select the boldfaced sentence above Probabilities are also important for augmentative and alternative communi cation systems Trnka et al 2007 Kane et al 2017 People often use such AACAAC devices if they are physically unable or sign but can instead using eye gaze or other specific movements to select words from a menu to be spoken by the system Word prediction can be used to suggest likely words for the menu Models that assign probabilities to sequences of words are called language mod els or LMs In this chapter we introduce the simplest model that assigns probabilitieslanguage model LM to sentences and sequences of words the ngram An ngram is a sequence of N ngram words a 2gram or bigram is a twoword sequence of words like please turn turn your or your homework and a 3gram or trigram is a threeword se quence of words like please turn your or turn your homework Well see how to use ngram models to estimate the probability of the last word of an ngram given the previous words and also to assign probabilities to entire sequences In a bit of terminological ambiguity we usually drop the word model and thus the term n gram is used to mean either the word sequence itself or the predictive model that assigns it a probability In later chapters well introduce more sophisticated language models like the RNN LMs of Chapter 9 31 NGrams Lets begin with the task of computing Pwh the probability of a word w given some history h Suppose the history h is its water is so transparent that and we want to know the probability that the next word is the Ptheits water is so transparent that 31 One way to estimate this probability is from relative frequency counts take a very large corpus count the number of times we see its water is so transparent that and count the number of times this is followed by the This would be answering the question Out of the times we saw the history h how many times was it followed by the word w as follows Ptheits water is so transparent that Cits water is so transparent that the Cits water is so transparent that 32 With a large enough corpus such as the web we can compute these counts and estimate the probability from Eq 32 You should pause now go to the web and compute this estimate for yourself While this method of estimating probabilities directly from counts works fine in many cases it turns out that even the web isnt big enough to give us good estimates in most cases This is because language is creative new sentences are created all the time and we wont always be able to count entire sentences Even simple extensions of the example sentence may have counts of zero on the web such as Walden Ponds water is so transparent that the well used to have counts of zero 31 NGRAMS 3 Similarly if we wanted to know the joint probability of an entire sequence of words like its water is so transparent we could do it by asking out of all possible sequences of five words how many of them are its water is so transparent We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences That seems rather a lot to estimate For this reason well need to introduce cleverer ways of estimating the proba bility of a word w given a history h or the probability of an entire word sequence W Lets start with a little formalizing of notation To represent the probability of a par ticular random variable Xi taking on the value the or PXi the we will use the simplification Pthe Well represent a sequence of N words either as w1 wn or wn1 so the expression w n1 1 means the string w1w2 wn1 For the joint prob ability of each word in a sequence having a particular value PX w1Y w2Z w3 W wn well use Pw1w2 wn Now how can we compute probabilities of entire sequences like Pw1w2 wn One thing we can do is decompose this probability using the chain rule of proba bility PX1Xn PX1PX2X1PX3X21 PXnXn11 n k1 PXkXk11 33 Applying the chain rule to words we get Pwn1 Pw1Pw2w1Pw3w21 Pwnwn11 n k1 Pwkwk11 34 The chain rule shows the link between computing the joint probability of a se quence and computing the conditional probability of a word given previous words Equation 34 suggests that we could estimate the joint probability of an entire se quence of words by multiplying together a number of conditional probabilities But using the chain rule doesnt really seem to help us We dont know any way to compute the exact probability of a word given a long sequence of preceding words Pwnwn11 As we said above we cant just estimate by counting the number of times every word occurs following every long string because language is creative and any particular context might have never occurred before The intuition of the ngram model is that instead of computing the probability of a word given its entire history we can approximate the history by just the last few words The bigram model for example approximates the probability of a word givenbigram all the previous words Pwnwn11 by using only the conditional probability of the preceding word Pwnwn1 In other words instead of computing the probability PtheWalden Ponds water is so transparent that 35 we approximate it with the probability Pthethat 36 4 CHAPTER 3 NGRAM LANGUAGE MODELS When we use a bigram model to predict the conditional probability of the next word we are thus making the following approximation Pwnwn11 Pwnwn1 37 The assumption that the probability of a word depends only on the previous word is called a Markov assumption Markov models are the class of probabilistic modelsMarkov that assume we can predict the probability of some future unit without looking too far into the past We can generalize the bigram which looks one word into the past to the trigram which looks two words into the past and thus to the ngram whichngram looks n1 words into the past Thus the general equation for this ngram approximation to the conditional probability of the next word in a sequence is Pwnwn11 Pwnw n1 nN1 38 Given the bigram assumption for the probability of an individual word we can compute the probability of a complete word sequence by substituting Eq 37 into Eq 34 Pwn1 n k1 Pwkwk1 39 How do we estimate these bigram or ngram probabilities An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE We get maximum likelihood estimation the MLE estimate for the parameters of an ngram model by getting counts from a corpus and normalizing the counts so that they lie between 0 and 11normalize For example to compute a particular bigram probability of a word y given a previous word x well compute the count of the bigram Cxy and normalize by the sum of all the bigrams that share the same first word x Pwnwn1 Cwn1wn w Cwn1w 310 We can simplify this equation since the sum of all bigram counts that start with a given word wn1 must be equal to the unigram count for that word wn1 the reader should take a moment to be convinced of this Pwnwn1 Cwn1wn Cwn1 311 Lets work through an example using a minicorpus of three sentences Well first need to augment each sentence with a special symbol s at the beginning of the sentence to give us the bigram context of the first word Well also need a special endsymbol s2 s I am Sam s s Sam I am s s I do not like green eggs and ham s 1 For probabilistic models normalizing means dividing by some total count so that the resulting prob abilities fall legally between 0 and 1 2 We need the endsymbol to make the bigram grammar a true probability distribution Without an endsymbol the sentence probabilities for all sentences of a given length would sum to one This model would define an infinite set of probability distributions with one distribution per sentence length See Exercise 35 31 NGRAMS 5 Here are the calculations for some of the bigram probabilities from this corpus PIs 23 67 PSams 1 3 33 PamI 2 3 67 PsSam 12 05 PSamam 1 2 5 PdoI 1 3 33 For the general case of MLE ngram parameter estimation Pwnwn1nN1 Cwn1nN1wn Cwn1nN1 312 Equation 312 like Eq 311 estimates the ngram probability by dividing the observed frequency of a particular sequence by the observed frequency of a prefix This ratio is called a relative frequency We said above that this use of relativerelativefrequency frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE In MLE the resulting parameter set maximizes the likelihood of the training set T given the model M ie PT M For example suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus What is the probability that a random word selected from some other text of say a million words will be the word Chinese The MLE of its probability is 4001000000 or 0004 Now 0004 is not the best possible estimate of the probability of Chinese occurring in all situations it might turn out that in some other corpus or context Chinese is a very unlikely word But it is the probability that makes it most likely that Chinese will occur 400 times in a millionword corpus We present ways to modify the MLE estimates slightly to get better probability estimates in Section 34 Lets move on to some examples from a slightly larger corpus than our 14word example above Well use data from the nowdefunct Berkeley Restaurant Project a dialogue system from the last century that answered questions about a database of restaurants in Berkeley California Jurafsky et al 1994 Here are some text normalized sample user queries a sample of 9332 sentences is on the website can you tell me about any good cantonese restaurants close by mid priced thai food is what im looking for tell me about chez panisse can you give me a listing of the kinds of food that are available im looking for a good place to eat breakfast when is caffe venezia open during the day Figure 31 shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project Note that the majority of the values are zero In fact we have chosen the sample words to cohere with each other a matrix selected from a random set of seven words would be even more sparse Figure 32 shows the bigram probabilities after normalization dividing each cell in Fig 31 by the appropriate unigram for its row taken from the following set of unigram probabilities i want to eat chinese food lunch spend 2533 927 2417 746 158 1093 341 278 Here are a few other useful probabilities Pis 025 Penglishwant 00011 Pfoodenglish 05 Psfood 068 Now we can compute the probability of sentences like I want English food or I want Chinese food by simply multiplying the appropriate bigram probabilities to gether as follows 6 CHAPTER 3 NGRAM LANGUAGE MODELS i want to eat chinese food lunch spend i 5 827 0 9 0 0 0 2 want 2 0 608 1 6 6 5 1 to 2 0 4 686 2 0 6 211 eat 0 0 2 0 16 2 42 0 chinese 1 0 0 0 0 82 1 0 food 15 0 15 0 1 4 0 0 lunch 2 0 0 0 0 1 0 0 spend 1 0 1 0 0 0 0 0 Figure 31 Bigram counts for eight of the words out of V 1446 in the Berkeley Restau rant Project corpus of 9332 sentences Zero counts are in gray i want to eat chinese food lunch spend i 0002 033 0 00036 0 0 0 000079 want 00022 0 066 00011 00065 00065 00054 00011 to 000083 0 00017 028 000083 0 00025 0087 eat 0 0 00027 0 0021 00027 0056 0 chinese 00063 0 0 0 0 052 00063 0 food 0014 0 0014 0 000092 00037 0 0 lunch 00059 0 0 0 0 00029 0 0 spend 00036 0 00036 0 0 0 0 0 Figure 32 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus of 9332 sentences Zero probabilities are in gray Ps i want english food s PisPwantiPenglishwant PfoodenglishPsfood 25 33 001105068 000031 We leave it as Exercise 32 to compute the probability of i want chinese food What kinds of linguistic phenomena are captured in these bigram statistics Some of the bigram probabilities above encode some facts that we think of as strictly syntactic in nature like the fact that what comes after eat is usually a noun or an adjective or that what comes after to is usually a verb Others might be a fact about the personal assistant task like the high probability of sentences beginning with the words I And some might even be cultural rather than linguistic like the higher probability that people are looking for Chinese versus English food Some practical issues Although for pedagogical purposes we have only described bigram models in practice its more common to use trigram models which contrigram dition on the previous two words rather than the previous word or 4gram or even4gram 5gram models when there is sufficient training data Note that for these larger n5gram grams well need to assume extra context for the contexts to the left and right of the sentence end For example to compute trigram probabilities at the very beginning of the sentence we can use two pseudowords for the first trigram ie PIss We always represent and compute language model probabilities in log format as log probabilities Since probabilities are by definition less than or equal tologprobabilities 1 the more probabilities we multiply together the smaller the product becomes Multiplying enough ngrams together would result in numerical underflow By using log probabilities instead of raw probabilities we get numbers that are not as small 32 EVALUATING LANGUAGE MODELS 7 Adding in log space is equivalent to multiplying in linear space so we combine log probabilities by adding them The result of doing all computation and storage in log space is that we only need to convert back into probabilities if we need to report them at the end then we can just take the exp of the logprob p1 p2 p3 p4 explog p1 log p2 log p3 log p4 313 32 Evaluating Language Models The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves Such endtoend evaluation is called extrinsic evaluation Extrinsic evaluation is the only way toextrinsicevaluation know if a particular improvement in a component is really going to help the task at hand Thus for speech recognition we can compare the performance of two language models by running the speech recognizer twice once with each language model and seeing which gives the more accurate transcription Unfortunately running big NLP systems endtoend is often very expensive In stead it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model An intrinsic evaluation metric is one that meaintrinsicevaluation sures the quality of a model independent of any application For an intrinsic evaluation of a language model we need a test set As with many of the statistical models in our field the probabilities of an ngram model come from the corpus it is trained on the training set or training corpus We can then measuretraining set the quality of an ngram model by its performance on some unseen data called the test set or test corpus We will also sometimes call test sets and other datasets thattest set are not in our training sets held out corpora because we hold them out from theheld out training data So if we are given a corpus of text and want to compare two different ngram models we divide the data into training and test sets train the parameters of both models on the training set and then compare how well the two trained models fit the test set But what does it mean to fit the test set The answer is simple whichever model assigns a higher probability to the test setmeaning it more accurately predicts the test setis a better model Given two probabilistic models the better model is the one that has a tighter fit to the test data or that better predicts the details of the test data and hence will assign a higher probability to the test data Since our evaluation metric is based on test set probability its important not to let the test sentences into the training set Suppose we are trying to compute the probability of a particular test sentence If our test sentence is part of the training corpus we will mistakenly assign it an artificially high probability when it occurs in the test set We call this situation training on the test set Training on the test set introduces a bias that makes the probabilities all look too high and causes huge inaccuracies in perplexity the probabilitybased metric we introduce below Sometimes we use a particular test set so often that we implicitly tune to its characteristics We then need a fresh test set that is truly unseen In such cases we call the initial test set the development test set or devset How do we divide ourdevelopmenttest data into training development and test sets We want our test set to be as large as possible since a small test set may be accidentally unrepresentative but we also want as much training data as possible At the minimum we would want to pick 8 CHAPTER 3 NGRAM LANGUAGE MODELS the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models In practice we often just divide our data into 80 training 10 development and 10 test Given a large corpus that we want to divide into training and test test data can either be taken from some continuous sequence of text inside the corpus or we can remove smaller stripes of text from randomly selected parts of our corpus and combine them into a test set 321 Perplexity In practice we dont use raw probability as our metric for evaluating language mod els but a variant called perplexity The perplexity sometimes called PP for shortperplexity of a language model on a test set is the inverse probability of the test set normalized by the number of words For a test set W w1w2 wN PPW Pw1w2 wN 1 N 314 N 1 Pw1w2 wN We can use the chain rule to expand the probability of W PPW N N i1 1 Pwiw1 wi1 315 Thus if we are computing the perplexity of W with a bigram language model we get PPW N N i1 1 Pwiwi1 316 Note that because of the inverse in Eq 315 the higher the conditional probabil ity of the word sequence the lower the perplexity Thus minimizing perplexity is equivalent to maximizing the test set probability according to the language model What we generally use for word sequence in Eq 315 or Eq 316 is the entire se quence of words in some test set Since this sequence will cross many sentence boundaries we need to include the begin and endsentence markers s and s in the probability computation We also need to include the endofsentence marker s but not the beginningofsentence marker s in the total count of word to kens N There is another way to think about perplexity as the weighted average branch ing factor of a language The branching factor of a language is the number of possi ble next words that can follow any word Consider the task of recognizing the digits in English zero one two nine given that both in some training set and in some test set each of the 10 digits occurs with equal probability P 110 The perplexity of this minilanguage is in fact 10 To see that imagine a test string of digits of length N and assume that in the training set all the digits occurred with equal probability By Eq 315 the perplexity will be 33 GENERALIZATION AND ZEROS 9 PPW Pw1w2 wN 1 N 1 10 N 1 N 1 10 1 10 317 But suppose that the number zero is really frequent and occurs far more often than other numbers Lets say that 0 occur 91 times in the training set and each of the other digits occurred 1 time each Now we see the following test set 0 0 0 0 0 3 0 0 0 0 We should expect the perplexity of this test set to be lower since most of the time the next number will be zero which is very predictable ie has a high probability Thus although the branching factor is still 10 the perplexity or weighted branching factor is smaller We leave this exact calculation as exercise 12 We see in Section 37 that perplexity is also closely related to the information theoretic notion of entropy Finally lets look at an example of how perplexity can be used to compare dif ferent ngram models We trained unigram bigram and trigram grammars on 38 million words including startofsentence tokens from the Wall Street Journal us ing a 19979 word vocabulary We then computed the perplexity of each of these models on a test set of 15 million words with Eq 316 The table below shows the perplexity of a 15 million word WSJ test set according to each of these grammars Unigram Bigram Trigram Perplexity 962 170 109 As we see above the more information the ngram gives us about the word sequence the lower the perplexity since as Eq 315 showed perplexity is related inversely to the likelihood of the test sequence according to the model Note that in computing perplexities the ngram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set Any kind of knowledge of the test set can cause the perplexity to be artificially low The perplexity of two language models is only comparable if they use identical vocabularies An intrinsic improvement in perplexity does not guarantee an extrinsic im provement in the performance of a language processing task like speech recognition or machine translation Nonetheless because perplexity often correlates with such improvements it is commonly used as a quick check on an algorithm But a models improvement in perplexity should always be confirmed by an endtoend evaluation of a real task before concluding the evaluation of the model 33 Generalization and Zeros The ngram model like many statistical models is dependent on the training corpus One implication of this is that the probabilities often encode specific facts about a given training corpus Another implication is that ngrams do a better and better job of modeling the training corpus as we increase the value of N 10 CHAPTER 3 NGRAM LANGUAGE MODELS We can visualize both of these facts by borrowing the technique of Shannon 1951 and Miller and Selfridge 1950 of generating random sentences from dif ferent ngram models Its simplest to visualize how this works for the unigram case Imagine all the words of the English language covering the probability space between 0 and 1 each word covering an interval proportional to its frequency We choose a random value between 0 and 1 and print the word whose interval includes this chosen value We continue choosing random numbers and generating words until we randomly generate the sentencefinal token s We can use the same technique to generate bigrams by first generating a random bigram that starts with s according to its bigram probability Lets say the second word of that bigram is w We next chose a random bigram starting with w again drawn according to its bigram probability and so on To give an intuition for the increasing power of higherorder ngrams Fig 33 shows random sentences generated from unigram bigram trigram and 4gram models trained on Shakespeares works 1 To him swallowed confess hear both Which Of save on trail for are ay device and rote life have gram Hill he late speaks or a more to leg less first you enter 2 Why dost stand forth thy canopy forsooth he is this palpable hit the King Henry Live king Follow gram What means sir I confess she then all sorts he is trim captain 3 Fly and will rid me these news of price Therefore the sadness of parting as they say tis done gram This shall forbid it should be branded if renown made it empty 4 King Henry What I will go seek the traitor Gloucester Exeunt some of the watch A great banquet servd in gram It cannot be but so Figure 33 Eight sentences randomly generated from four ngrams computed from Shakespeares works All characters were mapped to lowercase and punctuation marks were treated as words Output is handcorrected for capitalization to improve readability The longer the context on which we train the model the more coherent the sen tences In the unigram sentences there is no coherent relation between words or any sentencefinal punctuation The bigram sentences have some local wordtoword coherence especially if we consider that punctuation counts as a word The tri gram and 4gram sentences are beginning to look a lot like Shakespeare Indeed a careful investigation of the 4gram sentences shows that they look a little too much like Shakespeare The words It cannot be but so are directly from King John This is because not to put the knock on Shakespeare his oeuvre is not very large as corpora go N 884647V 29066 and our ngram probability matrices are ridiculously sparse There are V 2 844000000 possible bigrams alone and the number of pos sible 4grams is V 4 71017 Thus once the generator has chosen the first 4gram It cannot be but there are only five possible continuations that I he thou and so indeed for many 4grams there is only one continuation To get an idea of the dependence of a grammar on its training set lets look at an ngram grammar trained on a completely different corpus the Wall Street Journal WSJ newspaper Shakespeare and the Wall Street Journal are both English so we might expect some overlap between our ngrams for the two genres Fig 34 33 GENERALIZATION AND ZEROS 11 shows sentences generated by unigram bigram and trigram grammars trained on 40 million words from WSJ 1 Months the my and issue of year foreign new exchanges septemberwere recession exchange new endorsed a acquire to six executives gram 2 Last December through the way to preserve the Hudson corporation N B E C Taylor would seem to complete the major central planners one gram point five percent of U S E has already old M X corporation of living on information such as more frequently fishing to keep her 3 They also point to ninety nine point six billion dollars from two hundred four oh six three percent of the rates of interest stores as Mexico and gram Brazil on market conditions Figure 34 Three sentences randomly generated from three ngram models computed from 40 million words of the Wall Street Journal lowercasing all characters and treating punctua tion as words Output was then handcorrected for capitalization to improve readability Compare these examples to the pseudoShakespeare in Fig 33 While they both model Englishlike sentences there is clearly no overlap in generated sentences and little overlap even in small phrases Statistical models are likely to be pretty use less as predictors if the training sets and the test sets are as different as Shakespeare and WSJ How should we deal with this problem when we build ngram models One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish To build a language model for translating legal documents we need a training corpus of legal documents To build a language model for a questionanswering system we need a training corpus of questions It is equally important to get training data in the appropriate dialect especially when processing social media posts or spoken transcripts Thus tweets in AAVE African American Vernacular English often use words like finnaan auxiliary verb that marks immediate future tense that dont occur in other dialects or spellings like den for then in tweets like this one Blodgett and OConnor 2017 318 Bored af den my phone finna die while tweets from varieties like Nigerian English have markedly different vocabu lary and ngram patterns from American English Jurgens et al 2017 319 username R u a wizard or wat gan sef in d mornin u tweet afternoon u tweet nyt gan u dey tweet beta get ur IT placement wiv twitter Matching genres and dialects is still not sufficient Our models may still be subject to the problem of sparsity For any ngram that occurred a sufficient number of times we might have a good estimate of its probability But because any corpus is limited some perfectly acceptable English word sequences are bound to be missing from it That is well have many cases of putative zero probability ngrams that should really have some nonzero probability Consider the words that follow the bigram denied the in the WSJ Treebank3 corpus together with their counts denied the allegations 5 denied the speculation 2 denied the rumors 1 denied the report 1 But suppose our test set has phrases like 12 CHAPTER 3 NGRAM LANGUAGE MODELS denied the offer denied the loan Our model will incorrectly estimate that the Pofferdenied the is 0 These zeros things that dont ever occur in the training set but do occur inzeros the test setare a problem for two reasons First their presence means we are underestimating the probability of all sorts of words that might occur which will hurt the performance of any application we want to run on this data Second if the probability of any word in the test set is 0 the entire probability of the test set is 0 By definition perplexity is based on the inverse probability of the test set Thus if some words have zero probability we cant compute perplexity at all since we cant divide by 0 331 Unknown Words The previous section discussed the problem of words whose bigram probability is zero But what about words we simply have never seen before Sometimes we have a language task in which this cant happen because we know all the words that can occur In such a closed vocabulary system the test set canclosedvocabulary only contain words from this lexicon and there will be no unknown words This is a reasonable assumption in some domains such as speech recognition or machine translation where we have a pronunciation dictionary or a phrase table that are fixed in advance and so the language model can only use the words in that dictionary or phrase table In other cases we have to deal with words we havent seen before which well call unknown words or out of vocabulary OOV words The percentage of OOVOOV words that appear in the test set is called the OOV rate An open vocabulary systemopenvocabulary is one in which we model these potential unknown words in the test set by adding a pseudoword called UNK There are two common ways to train the probabilities of the unknown word model UNK The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance 1 Choose a vocabulary word list that is fixed in advance 2 Convert in the training set any word that is not in this set any OOV word to the unknown word token UNK in a text normalization step 3 Estimate the probabilities for UNK from its counts just like any other regular word in the training set The second alternative in situations where we dont have a prior vocabulary in ad vance is to create such a vocabulary implicitly replacing words in the training data by UNK based on their frequency For example we can replace by UNK all words that occur fewer than n times in the training set where n is some small number or equivalently select a vocabulary size V in advance say 50000 and choose the top V words by frequency and replace the rest by UNK In either case we then proceed to train the language model as before treating UNK like a regular word The exact choice of UNK model does have an effect on metrics like perplexity A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability For this reason perplexities should only be compared across language models with the same vocabularies Buck et al 2014 34 SMOOTHING 13 34 Smoothing What do we do with words that are in our vocabulary they are not unknown words but appear in a test set in an unseen context for example they appear after a word they never appeared after in training To keep a language model from assigning zero probability to these unseen events well have to shave off a bit of probability mass from some more frequent events and give it to the events weve never seen This modification is called smoothing or discounting In this section and the folsmoothing discounting lowing ones well introduce a variety of ways to do smoothing add1 smoothing addk smoothing stupid backoff and KneserNey smoothing 341 Laplace Smoothing The simplest way to do smoothing is to add one to all the bigram counts before we normalize them into probabilities All the counts that used to be zero will now have a count of 1 the counts of 1 will be 2 and so on This algorithm is called Laplace smoothing Laplace smoothing does not perform well enough to be usedLaplacesmoothing in modern ngram models but it usefully introduces many of the concepts that we see in other smoothing algorithms gives a useful baseline and is also a practical smoothing algorithm for other tasks like text classification Chapter 4 Lets start with the application of Laplace smoothing to unigram probabilities Recall that the unsmoothed maximum likelihood estimate of the unigram probability of the word wi is its count ci normalized by the total number of word tokens N Pwi ci N Laplace smoothing merely adds one to each count hence its alternate name add one smoothing Since there are V words in the vocabulary and each one was increaddone mented we also need to adjust the denominator to take into account the extra V observations What happens to our P values if we dont increase the denominator PLaplacewi ci 1 N V 320 Instead of changing both the numerator and denominator it is convenient to describe how a smoothing algorithm affects the numerator by defining an adjusted count c This adjusted count is easier to compare directly with the MLE counts and can be turned into a probability like an MLE count by normalizing by N To define this count since we are only changing the numerator in addition to adding 1 well also need to multiply by a normalization factor NNV ci ci 1 N N V 321 We can now turn ci into a probability P i by normalizing by N A related way to view smoothing is as discounting lowering some nonzerodiscounting counts in order to get the probability mass that will be assigned to the zero counts Thus instead of referring to the discounted counts c we might describe a smooth ing algorithm in terms of a relative discount dc the ratio of the discounted countsdiscount to the original counts 14 CHAPTER 3 NGRAM LANGUAGE MODELS dc c c Now that we have the intuition for the unigram case lets smooth our Berkeley Restaurant Project bigrams Figure 35 shows the addone smoothed counts for the bigrams in Fig 31 i want to eat chinese food lunch spend i 6 828 1 10 1 1 1 3 want 3 1 609 2 7 7 6 2 to 3 1 5 687 3 1 7 212 eat 1 1 3 1 17 3 43 1 chinese 2 1 1 1 1 83 2 1 food 16 1 16 1 2 5 1 1 lunch 3 1 1 1 1 2 1 1 spend 2 1 2 1 1 1 1 1 Figure 35 Addone smoothed bigram counts for eight of the words out of V 1446 in the Berkeley Restaurant Project corpus of 9332 sentences Previouslyzero counts are in gray Figure 36 shows the addone smoothed probabilities for the bigrams in Fig 32 Recall that normal bigram probabilities are computed by normalizing each row of counts by the unigram count Pwnwn1 Cwn1wn Cwn1 322 For addone smoothed bigram counts we need to augment the unigram count by the number of total word types in the vocabulary V PLaplacewnwn1 Cwn1wn1 w Cwn1w1 Cwn1wn1 Cwn1V 323 Thus each of the unigram counts given in the previous section will need to be augmented by V 1446 The result is the smoothed bigram probabilities in Fig 36 i want to eat chinese food lunch spend i 00015 021 000025 00025 000025 000025 000025 000075 want 00013 000042 026 000084 00029 00029 00025 000084 to 000078 000026 00013 018 000078 000026 00018 0055 eat 000046 000046 00014 000046 00078 00014 002 000046 chinese 00012 000062 000062 000062 000062 0052 00012 000062 food 00063 000039 00063 000039 000079 0002 000039 000039 lunch 00017 000056 000056 000056 000056 00011 000056 000056 spend 00012 000058 00012 000058 000058 000058 000058 000058 Figure 36 Addone smoothed bigram probabilities for eight of the words out of V 1446 in the BeRP corpus of 9332 sentences Previouslyzero probabilities are in gray It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts These adjusted counts can be computed by Eq 324 Figure 37 shows the reconstructed counts cwn1wn Cwn1wn1Cwn1 Cwn1V 324 34 SMOOTHING 15 i want to eat chinese food lunch spend i 38 527 064 64 064 064 064 19 want 12 039 238 078 27 27 23 078 to 19 063 31 430 19 063 44 133 eat 034 034 1 034 58 1 15 034 chinese 02 0098 0098 0098 0098 82 02 0098 food 69 043 69 043 086 22 043 043 lunch 057 019 019 019 019 038 019 019 spend 032 016 032 016 016 016 016 016 Figure 37 Addone reconstituted counts for eight words of V 1446 in the BeRP corpus of 9332 sentences Previouslyzero counts are in gray Note that addone smoothing has made a very big change to the counts Cwant to changed from 609 to 238 We can see this in probability space as well Ptowant decreases from 66 in the unsmoothed case to 26 in the smoothed case Looking at the discount d the ratio between new and old counts shows us how strikingly the counts for each prefix word have been reduced the discount for the bigram want to is 39 while the discount for Chinese food is 10 a factor of 10 The sharp change in counts and probabilities occurs because too much probabil ity mass is moved to all the zeros 342 Addk smoothing One alternative to addone smoothing is to move a bit less of the probability mass from the seen to the unseen events Instead of adding 1 to each count we add a frac tional count k 5 05 01 This algorithm is therefore called addk smoothingaddk PAddkwnwn1 Cwn1wn k Cwn1 kV 325 Addk smoothing requires that we have a method for choosing k this can be done for example by optimizing on a devset Although addk is useful for some tasks including text classification it turns out that it still doesnt work well for language modeling generating counts with poor variances and often inappropriate discounts Gale and Church 1994 343 Backoff and Interpolation The discounting we have been discussing so far can help solve the problem of zero frequency ngrams But there is an additional source of knowledge we can draw on If we are trying to compute Pwnwn2wn1 but we have no examples of a particular trigram wn2wn1wn we can instead estimate its probability by using the bigram probability Pwnwn1 Similarly if we dont have counts to compute Pwnwn1 we can look to the unigram Pwn In other words sometimes using less context is a good thing helping to general ize more for contexts that the model hasnt learned much about There are two ways to use this ngram hierarchy In backoff we use the trigram if the evidence isbackoff sufficient otherwise we use the bigram otherwise the unigram In other words we only back off to a lowerorder ngram if we have zero evidence for a higherorder ngram By contrast in interpolation we always mix the probability estimates frominterpolation all the ngram estimators weighing and combining the trigram bigram and unigram counts 16 CHAPTER 3 NGRAM LANGUAGE MODELS In simple linear interpolation we combine different order ngrams by linearly in terpolating all the models Thus we estimate the trigram probability Pwnwn2wn1 by mixing together the unigram bigram and trigram probabilities each weighted by a λ P̂wnwn2wn1 λ1Pwnwn2wn1 λ2Pwnwn1 λ3Pwn 326 such that the λ s sum to 1 i λi 1 327 In a slightly more sophisticated version of linear interpolation each λ weight is computed by conditioning on the context This way if we have particularly accurate counts for a particular bigram we assume that the counts of the trigrams based on this bigram will be more trustworthy so we can make the λ s for those trigrams higher and thus give that trigram more weight in the interpolation Equation 328 shows the equation for interpolation with contextconditioned weights P̂wnwn2wn1 λ1wn1n2Pwnwn2wn1 λ2wn1n2Pwnwn1 λ3wn1n2Pwn 328 How are these λ values set Both the simple interpolation and conditional inter polation λ s are learned from a heldout corpus A heldout corpus is an additionalheldout training corpus that we use to set hyperparameters like these λ values by choosing the λ values that maximize the likelihood of the heldout corpus That is we fix the ngram probabilities and then search for the λ values thatwhen plugged into Eq 326give us the highest probability of the heldout set There are various ways to find this optimal set of λ s One way is to use the EM algorithm an iterative learning algorithm that converges on locally optimal λ s Jelinek and Mercer 1980 In a backoff ngram model if the ngram we need has zero counts we approxi mate it by backing off to the N1gram We continue backing off until we reach a history that has some counts In order for a backoff model to give a correct probability distribution we have to discount the higherorder ngrams to save some probability mass for the lowerdiscount order ngrams Just as with addone smoothing if the higherorder ngrams arent discounted and we just used the undiscounted MLE probability then as soon as we replaced an ngram which has zero probability with a lowerorder ngram we would be adding probability mass and the total probability assigned to all possible strings by the language model would be greater than 1 In addition to this explicit discount factor well need a function α to distribute this probability mass to the lower order ngrams This kind of backoff with discounting is also called Katz backoff In Katz backKatz backoff off we rely on a discounted probability P if weve seen this ngram before ie if we have nonzero counts Otherwise we recursively back off to the Katz probabil ity for the shorterhistory N1gram The probability for a backoff ngram PBO is 35 KNESERNEY SMOOTHING 17 thus computed as follows PBOwnw n1 nN1    P wnwn1nN1 if CwnnN1 0 αwn1nN1PBOwnw n1 nN2 otherwise 329 Katz backoff is often combined with a smoothing method called GoodTuringGoodTuring The combined GoodTuring backoff algorithm involves quite detailed computation for estimating the GoodTuring smoothing and the P and α values 35 KneserNey Smoothing One of the most commonly used and best performing ngram smoothing methods is the interpolated KneserNey algorithm Kneser and Ney 1995 Chen and GoodKneserNey man 1998 KneserNey has its roots in a method called absolute discounting Recall that discounting of the counts for frequent ngrams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen ngrams To see this we can use a clever idea from Church and Gale 1991 Consider an ngram that has count 4 We need to discount this count by some amount But how much should we discount it Church and Gales clever idea was to look at a heldout corpus and just see what the count is for all those bigrams that had count 4 in the training set They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words On average a bigram that occurred 4 times in the first 22 million words occurred 323 times in the next 22 million words The following table from Church and Gale 1991 shows these counts for bigrams with c from 0 to 9 Bigram count in Bigram count in training set heldout set 0 00000270 1 0448 2 125 3 224 4 323 5 421 6 523 7 621 8 721 9 826 Figure 38 For all bigrams in 22 million words of AP newswire of count 0 1 29 the counts of these bigrams in a heldout corpus also of 22 million words The astute reader may have noticed that except for the heldout counts for 0 and 1 all the other bigram counts in the heldout set could be estimated pretty well by just subtracting 075 from the count in the training set Absolute discountingAbsolutediscounting formalizes this intuition by subtracting a fixed absolute discount d from each count The intuition is that since we have good estimates already for the very high counts a small discount d wont affect them much It will mainly modify the smaller counts 18 CHAPTER 3 NGRAM LANGUAGE MODELS for which we dont necessarily trust the estimate anyway and Fig 38 suggests that in practice this discount is actually a good one for bigrams with counts 2 through 9 The equation for interpolated absolute discounting applied to bigrams PAbsoluteDiscountingwiwi1 Cwi1wid v Cwi1 v λ wi1Pwi 330 The first term is the discounted bigram and the second term is the unigram with an interpolation weight λ We could just set all the d values to 75 or we could keep a separate discount value of 05 for the bigrams with counts of 1 KneserNey discounting Kneser and Ney 1995 augments absolute discount ing with a more sophisticated way to handle the lowerorder unigram distribution Consider the job of predicting the next word in this sentence assuming we are inter polating a bigram and a unigram model I cant see without my reading The word glasses seems much more likely to follow here than say the word Kong so wed like our unigram model to prefer glasses But in fact its Kong that is more common since Hong Kong is a very frequent word A standard unigram model will assign Kong a higher probability than glasses We would like to capture the intuition that although Kong is frequent it is mainly only frequent in the phrase Hong Kong that is after the word Hong The word glasses has a much wider distribution In other words instead of Pw which answers the question How likely is w wed like to create a unigram model that we might call PCONTINUATION which answers the question How likely is w to appear as a novel continuation How can we estimate this probability of seeing the word w as a novel continuation in a new unseen context The KneserNey intuition is to base our estimate of PCONTINUATION on the number of different contexts word w has appeared in that is the number of bigram types it completes Every bigram type was a novel continuation the first time it was seen We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well The number of times a word w appears as a novel continuation can be expressed as PCONTINUATIONw v Cvw 0 331 To turn this count into a probability we normalize by the total number of word bigram types In summary PCONTINUATIONw v Cvw 0 uw Cuw 0 332 An equivalent formulation based on a different metaphor is to use the number of word types seen to precede w Eq 331 repeated PCONTINUATIONw v Cvw 0 333 normalized by the number of words preceding all words as follows PCONTINUATIONw v Cvw 0 w v Cvw 0 334 A frequent word Kong occurring in only one context Hong will have a low continuation probability 36 THE WEB AND STUPID BACKOFF 19 The final equation for Interpolated KneserNey smoothing for bigrams is thenInterpolatedKneserNey PKNwiwi1 maxCwi1wid0 Cwi1 λ wi1PCONTINUATIONwi 335 The λ is a normalizing constant that is used to distribute the probability mass weve discounted λ wi1 d v Cwi1v w Cwi1w 0 336 The first term d v Cwi1v is the normalized discount The second term w Cwi1w 0 is the number of word types that can follow wi1 or equiva lently the number of word types that we discounted in other words the number of times we applied the normalized discount The general recursive formulation is as follows PKNwiwi1in1 maxcKNw iin1d0 v cKNw i1 in1v λ wi1in1PKNwiw i1 in2 337 where the definition of the count cKN depends on whether we are counting the highestorder ngram being interpolated for example trigram if we are interpolating trigram bigram and unigram or one of the lowerorder ngrams bigram or unigram if we are interpolating trigram bigram and unigram cKN count for the highest order continuationcount for lower orders 338 The continuation count is the number of unique single word contexts for At the termination of the recursion unigrams are interpolated with the uniform distribution where the parameter ε is the empty string PKNw maxcKNwd0 w cKNw λ ε 1 V 339 If we want to include an unknown word UNK its just included as a regular vo cabulary entry with count zero and hence its probability will be a lambdaweighted uniform distribution λ εV The bestperforming version of KneserNey smoothing is called modified Kneser Ney smoothing and is due to Chen and Goodman 1998 Rather than use a singlemodifiedKneserNey fixed discount d modified KneserNey uses three different discounts d1 d2 and d3 for ngrams with counts of 1 2 and three or more respectively See Chen and Goodman 1998 p 19 or Heafield et al 2013 for the details 36 The Web and Stupid Backoff By using text from the web it is possible to build extremely large language mod els In 2006 Google released a very large set of ngram counts including ngrams 1grams through 5grams from all the fiveword sequences that appear at least 20 CHAPTER 3 NGRAM LANGUAGE MODELS 40 times from 1024908267229 words of running text on the web this includes 1176470663 fiveword sequences using over 13 million unique words types Franz and Brants 2006 Some examples 4gram Count serve as the incoming 92 serve as the incubator 99 serve as the independent 794 serve as the index 223 serve as the indication 72 serve as the indicator 120 serve as the indicators 45 serve as the indispensable 111 serve as the indispensible 40 serve as the individual 234 Efficiency considerations are important when building language models that use such large sets of ngrams Rather than store each word as a string it is generally represented in memory as a 64bit hash number with the words themselves stored on disk Probabilities are generally quantized using only 48 bits instead of 8byte floats and ngrams are stored in reverse tries Ngrams can also be shrunk by pruning for example only storing ngrams with counts greater than some threshold such as the count threshold of 40 used for the Google ngram release or using entropy to prune lessimportant ngrams Stolcke 1998 Another option is to build approximate language models using techniques like Bloom filters Talbot and Osborne 2007 Church et al 2007 Finally effiBloom filters cient language model toolkits like KenLM Heafield 2011 Heafield et al 2013 use sorted arrays efficiently combine probabilities and backoffs in a single value and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large corpus Although with these toolkits it is possible to build webscale language models using full KneserNey smoothing Brants et al 2007 show that with very large lan guage models a much simpler algorithm may be sufficient The algorithm is called stupid backoff Stupid backoff gives up the idea of trying to make the languagestupid backoff model a true probability distribution There is no discounting of the higherorder probabilities If a higherorder ngram has a zero count we simply backoff to a lower order ngram weighed by a fixed contextindependent weight This algo rithm does not produce a probability distribution so well follow Brants et al 2007 in referring to it as S Swiwi1ik1    countwiik1 countwi1ik1 if countwiik1 0 λSwiwi1ik2 otherwise 340 The backoff terminates in the unigram which has probability Sw countwN Brants et al 2007 find that a value of 04 worked well for λ 37 Advanced Perplexitys Relation to Entropy We introduced perplexity in Section 321 as a way to evaluate ngram models on a test set A better ngram model is one that assigns a higher probability to the 37 ADVANCED PERPLEXITYS RELATION TO ENTROPY 21 test data and perplexity is a normalized version of the probability of the test set The perplexity measure actually arises from the informationtheoretic concept of crossentropy which explains otherwise mysterious properties of perplexity why the inverse probability for example and its relationship to entropy Entropy is aEntropy measure of information Given a random variable X ranging over whatever we are predicting words letters parts of speech the set of which well call χ and with a particular probability function call it px the entropy of the random variable X is HX xχ px log2 px 341 The log can in principle be computed in any base If we use log base 2 the resulting value of entropy will be measured in bits One intuitive way to think about entropy is as a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme Consider an example from the standard information theory textbook Cover and Thomas 1991 Imagine that we want to place a bet on a horse race but it is too far to go all the way to Yonkers Racetrack so wed like to send a short message to the bookie to tell him which of the eight horses to bet on One way to encode this message is just to use the binary representation of the horses number as the code thus horse 1 would be 001 horse 2 010 horse 3 011 and so on with horse 8 coded as 000 If we spend the whole day betting and each horse is coded with 3 bits on average we would be sending 3 bits per race Can we do better Suppose that the spread is the actual distribution of the bets placed and that we represent it as the prior probability of each horse as follows Horse 1 12 Horse 5 1 64 Horse 2 14 Horse 6 1 64 Horse 3 18 Horse 7 1 64 Horse 4 116 Horse 8 1 64 The entropy of the random variable X that ranges over horses gives us a lower bound on the number of bits and is HX i8 i1 pi log pi 12 log 1 2 1 4 log 1 4 1 8 log 1 8 1 16 log 1 164 1 64 log 1 64 2 bits 342 A code that averages 2 bits per race can be built with short encodings for more probable horses and longer encodings for less probable horses For example we could encode the most likely horse with the code 0 and the remaining horses as 10 then 110 1110 111100 111101 111110 and 111111 What if the horses are equally likely We saw above that if we used an equal length binary code for the horse numbers each horse took 3 bits to code so the average was 3 Is the entropy the same In this case each horse would have a probability of 18 The entropy of the choice of horses is then HX i8 i1 1 8 log 1 8 log 1 8 3 bits 343 22 CHAPTER 3 NGRAM LANGUAGE MODELS Until now we have been computing the entropy of a single variable But most of what we will use entropy for involves sequences For a grammar for example we will be computing the entropy of some sequence of words W w0w1w2 wn One way to do this is to have a variable that ranges over sequences of words For example we can compute the entropy of a random variable that ranges over all finite sequences of words of length n in some language L as follows Hw1w2 wn W n1 L pW n1 log pW n 1 344 We could define the entropy rate we could also think of this as the perwordentropy rate entropy as the entropy of this sequence divided by the number of words 1 n HW n1 1 n W n1 L pW n1 log pW n 1 345 But to measure the true entropy of a language we need to consider sequences of infinite length If we think of a language as a stochastic process L that produces a sequence of words and allow W to represent the sequence of words w1 wn then Ls entropy rate HL is defined as HL lim n 1 n Hw1w2 wn lim n 1 n WL pw1 wn log pw1 wn 346 The ShannonMcMillanBreiman theorem Algoet and Cover 1988 Cover and Thomas 1991 states that if the language is regular in certain ways to be exact if it is both stationary and ergodic HL lim n 1 n log pw1w2 wn 347 That is we can take a single sequence that is long enough instead of summing over all possible sequences The intuition of the ShannonMcMillanBreiman the orem is that a longenough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer se quence according to their probabilities A stochastic process is said to be stationary if the probabilities it assigns to aStationary sequence are invariant with respect to shifts in the time index In other words the probability distribution for words at time t is the same as the probability distribution at time t 1 Markov models and hence ngrams are stationary For example in a bigram Pi is dependent only on Pi1 So if we shift our time index by x Pix is still dependent on Pix1 But natural language is not stationary since as we show in Chapter 12 the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent Thus our statistical models only give an approximation to the correct distributions and entropies of natural language To summarize by making some incorrect but convenient simplifying assump tions we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability Now we are ready to introduce crossentropy The crossentropy is useful whencrossentropy we dont know the actual probability distribution p that generated some data It 37 ADVANCED PERPLEXITYS RELATION TO ENTROPY 23 allows us to use some m which is a model of p ie an approximation to p The crossentropy of m on p is defined by Hpm lim n 1 n WL pw1 wn logmw1 wn 348 That is we draw sequences according to the probability distribution p but sum the log of their probabilities according to m Again following the ShannonMcMillanBreiman theorem for a stationary er godic process Hpm lim n 1 n logmw1w2 wn 349 This means that as for entropy we can estimate the crossentropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences What makes the crossentropy useful is that the crossentropy Hpm is an up per bound on the entropy Hp For any model m Hp Hpm 350 This means that we can use some simplified model m to help estimate the true en tropy of a sequence of symbols drawn according to probability p The more accurate m is the closer the crossentropy Hpm will be to the true entropy Hp Thus the difference between Hpm and Hp is a measure of how accurate a model is Between two models m1 and m2 the more accurate model will be the one with the lower crossentropy The crossentropy can never be lower than the true entropy so a model cannot err by underestimating the true entropy We are finally ready to see the relation between perplexity and crossentropy as we saw it in Eq 349 Crossentropy is defined in the limit as the length of the observed word sequence goes to infinity We will need an approximation to cross entropy relying on a sufficiently long sequence of fixed length This approxima tion to the crossentropy of a model M PwiwiN1wi1 on a sequence of words W is HW 1 N logPw1w2 wN 351 The perplexity of a model P on a sequence of words W is now formally defined asperplexity the exp of this crossentropy PerplexityW 2HW Pw1w2 wN 1 N N 1 Pw1w2 wN N N i1 1 Pwiw1 wi1 352 24 CHAPTER 3 NGRAM LANGUAGE MODELS 38 Summary This chapter introduced language modeling and the ngram one of the most widely used tools in language processing Language models offer a way to assign a probability to a sentence or other sequence of words and to predict a word from preceding words ngrams are Markov models that estimate words from a fixed window of pre vious words ngram probabilities can be estimated by counting in a corpus and normalizing the maximum likelihood estimate ngram language models are evaluated extrinsically in some task or intrinsi cally using perplexity The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model Smoothing algorithms provide a more sophisticated way to estimate the prob ability of ngrams Commonly used smoothing algorithms for ngrams rely on lowerorder ngram counts through backoff or interpolation Both backoff and interpolation require discounting to create a probability dis tribution KneserNey smoothing makes use of the probability of a word being a novel continuation The interpolated KneserNey smoothing algorithm mixes a discounted probability with a lowerorder continuation probability Bibliographical and Historical Notes The underlying mathematics of the ngram was first proposed by Markov 1913 who used what are now called Markov chains bigrams and trigrams to predict whether an upcoming letter in Pushkins Eugene Onegin would be a vowel or a con sonant Markov classified 20000 letters as V or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters Shannon 1948 applied ngrams to compute approximations to English word sequences Based on Shannons work Markov models were commonly used in engineering linguistic and psychological work on modeling word sequences by the 1950s In a series of extremely influential papers starting with Chomsky 1956 and including Chomsky 1957 and Miller and Chomsky 1963 Noam Chomsky argued that finitestate Markov processes while a possibly useful engineering heuristic were incapable of being a complete cognitive model of human grammatical knowl edge These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades The resurgence of ngram models came from Jelinek and colleagues at the IBM Thomas J Watson Research Center who were influenced by Shannon and Baker at CMU who was influenced by the work of Baum and colleagues Independently these two labs successfully used ngrams in their speech recognition systems Baker 1975b Jelinek 1976 Baker 1975a Bahl et al 1983 Jelinek 1990 A trigram model was used in the IBM TANGORA speech recognition system in the 1970s but the idea was not written up until later Addone smoothing derives from Laplaces 1812 law of succession and was first applied as an engineering solution to the zerofrequency problem by Jeffreys 1948 EXERCISES 25 based on an earlier AddK suggestion by Johnson 1932 Problems with the add one algorithm are summarized in Gale and Church 1994 A wide variety of different language modeling and smoothing techniques were proposed in the 80s and 90s including GoodTuring discountingfirst applied to the ngram smoothing at IBM by Katz Nádas 1984 Church and Gale 1991 WittenBell discounting Witten and Bell 1991 and varieties of classbased n gram models that used information about word classesclassbasedngram Starting in the late 1990s Chen and Goodman produced a highly influential series of papers with a comparison of different language models Chen and Good man 1996 Chen and Goodman 1998 Chen and Goodman 1999 Goodman 2006 They performed a number of carefully controlled experiments comparing differ ent discounting algorithms cache models classbased models and other language model parameters They showed the advantages of Modified Interpolated Kneser Ney which has since become the standard baseline for language modeling espe cially because they showed that caches and classbased models provided only minor additional improvement These papers are recommended for any reader with further interest in language modeling Two commonly used toolkits for building language models are SRILM Stolcke 2002 and KenLM Heafield 2011 Heafield et al 2013 Both are publicly available SRILM offers a wider range of options and types of discounting while KenLM is optimized for speed and memory size making it possible to build webscale lan guage models The highest accuracy language models are neural network language models These solve a major problem with ngram language models the number of parame ters increases exponentially as the ngram order increases and ngrams have no way to generalize from training to test set Neural language models instead project words into a continuous space in which words with similar contexts have similar represen tations Well introduce both feedforward language models Bengio et al 2006 Schwenk 2007 in Chapter 7 and recurrent language models Mikolov 2012 in Chapter 9 Exercises 31 Write out the equation for trigram probability estimation modifying Eq 311 Now write out all the nonzero trigram probabilities for the I am Sam corpus on page 4 32 Calculate the probability of the sentence i want chinese food Give two probabilities one using Fig 32 and the useful probabilities just below it on page 6 and another using the add1 smoothed table in Fig 36 Assume the additional add1 smoothed probabilities Pis 019 and Psfood 040 33 Which of the two probabilities you computed in the previous exercise is higher unsmoothed or smoothed Explain why 34 We are given the following corpus modified from the one in the chapter s I am Sam s s Sam I am s s I am Sam s s I do not like green eggs and Sam s 26 CHAPTER 3 NGRAM LANGUAGE MODELS Using a bigram language model with addone smoothing what is PSam am Include s and s in your counts just like any other token 35 Suppose we didnt use the endsymbol s Train an unsmoothed bigram grammar on the following training corpus without using the endsymbol s s a b s b b s b a s a a Demonstrate that your bigram model does not assign a single probability dis tribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet ab is 10 and the sum of the probability of all possible 3 word sentences over the alphabet ab is also 10 36 Suppose we train a trigram language model with addone smoothing on a given corpus The corpus contains V word types Express a formula for esti mating Pw3w1w2 where w3 is a word which follows the bigram w1w2 in terms of various Ngram counts and V Use the notation cw1w2w3 to denote the number of times that trigram w1w2w3 occurs in the corpus and so on for bigrams and unigrams 37 We are given the following corpus modified from the one in the chapter s I am Sam s s Sam I am s s I am Sam s s I do not like green eggs and Sam s If we use linear interpolation smoothing between a maximumlikelihood bi gram model and a maximumlikelihood unigram model with λ1 12 and λ2 1 2 what is PSamam Include s and s in your counts just like any other token 38 Write a program to compute unsmoothed unigrams and bigrams 39 Run your ngram program on two different small corpora of your choice you might use email text or newsgroups Now compare the statistics of the two corpora What are the differences in the most common unigrams between the two How about interesting differences in bigrams 310 Add an option to your program to generate random sentences 311 Add an option to your program to compute the perplexity of a test set 312 Given a training set of 100 numbers consists of 91 zeros and 1 each of the other digits 19 Now we see the following test set 0 0 0 0 0 3 0 0 0 0 What is the unigram perplexity Exercises 27 Algoet P H and Cover T M 1988 A sandwich proof of the ShannonMcMillanBreiman theorem The Annals of Probability 162 899909 Bahl L R Jelinek F and Mercer R L 1983 A max imum likelihood approach to continuous speech recogni tion IEEE Transactions on Pattern Analysis and Machine Intelligence 52 179190 Baker J K 1975a The DRAGON system An overview IEEE Transactions on Acoustics Speech and Signal Pro cessing ASSP231 2429 Baker J K 1975b Stochastic modeling for automatic speech understanding In Reddy D R Ed Speech Recognition Academic Press Bengio Y Schwenk H Senécal JS Morin F and Gau vain JL 2006 Neural probabilistic language models In Innovations in Machine Learning 137186 Springer Blodgett S L and OConnor B 2017 Racial disparity in natural language processing A case study of social media africanamerican english In Fairness Accountability and Transparency in Machine Learning FATML Workshop KDD Brants T Popat A C Xu P Och F J and Dean J 2007 Large language models in machine translation In EMNLPCoNLL 2007 Buck C Heafield K and Van Ooyen B 2014 Ngram counts and language models from the common crawl In Proceedings of LREC Chen S F and Goodman J 1996 An empirical study of smoothing techniques for language modeling In ACL96 310318 Chen S F and Goodman J 1998 An empirical study of smoothing techniques for language modeling Tech rep TR1098 Computer Science Group Harvard University Chen S F and Goodman J 1999 An empirical study of smoothing techniques for language modeling Computer Speech and Language 13 359394 Chomsky N 1956 Three models for the description of language IRE Transactions on Information Theory 23 113124 Chomsky N 1957 Syntactic Structures Mouton The Hague Church K W and Gale W A 1991 A comparison of the enhanced GoodTuring and deleted estimation methods for estimating probabilities of English bigrams Computer Speech and Language 5 1954 Church K W Hart T and Gao J 2007 Compress ing trigram language models with Golomb coding In EMNLPCoNLL 2007 199207 Cover T M and Thomas J A 1991 Elements of Infor mation Theory Wiley Franz A and Brants T 2006 All our ngram are belong to you httpgoogleresearchblogspotcom2006 08allourngramarebelongtoyouhtml Gale W A and Church K W 1994 What is wrong with adding one In Oostdijk N and de Haan P Eds CorpusBased Research into Language 189198 Rodopi Goodman J 2006 A bit of progress in language mod eling Extended version Tech rep MSRTR200172 Machine Learning and Applied Statistics Group Microsoft Research Redmond WA Heafield K 2011 KenLM Faster and smaller language model queries In Workshop on Statistical Machine Trans lation 187197 Heafield K Pouzyrevsky I Clark J H and Koehn P 2013 Scalable modified KneserNey language model es timation In ACL 2013 690696 Jeffreys H 1948 Theory of Probability 2nd Ed Claren don Press Section 323 Jelinek F 1976 Continuous speech recognition by statis tical methods Proceedings of the IEEE 644 532557 Jelinek F 1990 Selforganized language modeling for speech recognition In Waibel A and Lee KF Eds Readings in Speech Recognition 450506 Morgan Kauf mann Originally distributed as IBM technical report in 1985 Jelinek F and Mercer R L 1980 Interpolated estimation of Markov source parameters from sparse data In Gelsema E S and Kanal L N Eds Proceedings Workshop on Pattern Recognition in Practice 381397 North Holland Johnson W E 1932 Probability deductive and inductive problems appendix to Mind 41164 421423 Jurafsky D Wooters C Tajchman G Segal J Stolcke A Fosler E and Morgan N 1994 The Berkeley restau rant project In ICSLP94 21392142 Jurgens D Tsvetkov Y and Jurafsky D 2017 Incorpo rating dialectal variability for socially equitable language identification In ACL 2017 5157 Kane S K Morris M R Paradiso A and Campbell J 2017 at times avuncular and cantankerous with the reflexes of a mongoose Understanding selfexpression through augmentative and alternative communication de vices In CSCW 2017 11661179 Kneser R and Ney H 1995 Improved backingoff for M gram language modeling In ICASSP95 Vol 1 181184 Markov A A 1913 Essai dune recherche statistique sur le texte du roman Eugene Onegin illustrant la liaison des epreuve en chain Example of a statistical investigation of the text of Eugene Onegin illustrating the dependence be tween samples in chain Izvistia Imperatorskoi Akademii Nauk Bulletin de lAcadémie Impériale des Sciences de StPétersbourg 7 153162 Mikolov T 2012 Statistical language models based on neural networks PhD thesis Ph D thesis Brno Univer sity of Technology Miller G A and Chomsky N 1963 Finitary models of language users In Luce R D Bush R R and Galanter E Eds Handbook of Mathematical Psychology Vol II 419491 John Wiley Miller G A and Selfridge J A 1950 Verbal context and the recall of meaningful material American Journal of Psychology 63 176185 Nádas A 1984 Estimation of probabilities in the language model of the IBM speech recognition system IEEE Trans actions on Acoustics Speech Signal Processing 324 859861 Schwenk H 2007 Continuous space language models Computer Speech Language 213 492518 Shannon C E 1948 A mathematical theory of commu nication Bell System Technical Journal 273 379423 Continued in the following volume 28 Chapter 3 Ngram Language Models Shannon C E 1951 Prediction and entropy of printed English Bell System Technical Journal 30 5064 Stolcke A 1998 Entropybased pruning of backoff lan guage models In Proc DARPA Broadcast News Transcrip tion and Understanding Workshop 270274 Stolcke A 2002 SRILM an extensible language model ing toolkit In ICSLP02 Talbot D and Osborne M 2007 Smoothed Bloom Fil ter Language Models TeraScale LMs on the Cheap In EMNLPCoNLL 2007 468476 Trnka K Yarrington D McCaw J McCoy K F and Pennington C 2007 The effects of word prediction on communication rate for AAC In NAACLHLT 07 173 176 Witten I H and Bell T C 1991 The zerofrequency problem Estimating the probabilities of novel events in adaptive text compression IEEE Transactions on Informa tion Theory 374 1085109475 NEURAL LANGUAGE MODELS 15 Well also need the derivatives of each of the other activation functions The derivative of tanh is d tanhz dz 1 tanh2z 724 The derivative of the ReLU is d ReLUz dz 0 f or x 0 1 f or x 0 725 745 More details on learning Optimization in neural networks is a nonconvex optimization problem more com plex than for logistic regression and for that and other reasons there are many best practices for successful learning For logistic regression we can initialize gradient descent with all the weights and biases having the value 0 In neural networks by contrast we need to initialize the weights with small random numbers Its also helpful to normalize the input values to have 0 mean and unit variance Various forms of regularization are used to prevent overfitting One of the most important is dropout randomly dropping some units and their connections fromdropout the network during training Hinton et al 2012 Srivastava et al 2014 Tuning of hyperparameters is also important The parameters of a neural network are thehyperparameter weights W and biases b those are learned by gradient descent The hyperparameters are things that are chosen by the algorithm designer optimal values are tuned on a devset rather than by gradient descent learning on the training set Hyperparameters include the learning rate η the minibatch size the model architecture the number of layers the number of hidden nodes per layer the choice of activation functions how to regularize and so on Gradient descent itself also has many architectural variants such as Adam Kingma and Ba 2015 Finally most modern neural networks are built using computation graph for malisms that make it easy and natural to do gradient computation and parallelization onto vectorbased GPUs Graphic Processing Units Pytorch Paszke et al 2017 and TensorFlow Abadi et al 2015 are two of the most popular The interested reader should consult a neural network textbook for further details some sugges tions are at the end of the chapter 75 Neural Language Models As our first application of neural networks lets consider language modeling pre dicting upcoming words from prior word context Neural netbased language models turn out to have many advantages over the n gram language models of Chapter 3 Among these are that neural language models dont need smoothing they can handle much longer histories and they can general ize over contexts of similar words For a training set of a given size a neural lan guage model has much higher predictive accuracy than an ngram language model Furthermore neural language models underlie many of the models well introduce for tasks like machine translation dialog and language generation 16 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS On the other hand there is a cost for this improved performance neural net language models are strikingly slower to train than traditional language models and so for many tasks an ngram language model is still the right tool In this chapter well describe simple feedforward neural language models first introduced by Bengio et al 2003 Modern neural language models are generally not feedforward but recurrent using the technology that we will introduce in Chap ter 9 A feedforward neural LM is a standard feedforward network that takes as input at time t a representation of some number of previous words wt1wt2 etc and outputs a probability distribution over possible next words Thuslike the ngram LMthe feedforward neural LM approximates the probability of a word given the entire prior context Pwt wt11 by approximating based on the N previous words Pwt wt11 Pwt w t1 tN1 726 In the following examples well use a 4gram example so well show a net to estimate the probability Pwt iwt1wt2wt3 751 Embeddings In neural language models the prior context is represented by embeddings of the previous words Representing the prior context as embeddings rather than by ex act words as used in ngram language models allows neural language models to generalize to unseen data much better than ngram language models For example suppose weve seen this sentence in training I have to make sure when I get home to feed the cat but weve never seen the word dog after the words feed the In our test set we are trying to predict what comes after the prefix I forgot when I got home to feed the An ngram language model will predict cat but not dog But a neural LM which can make use of the fact that cat and dog have similar embeddings will be able to assign a reasonably high probability to dog as well as cat merely because they have similar vectors Lets see how this works in practice Lets assume we have an embedding dic tionary E that gives us for each word in our vocabulary V the embedding for that word perhaps precomputed by an algorithm like word2vec from Chapter 6 Fig 712 shows a sketch of this simplified feedforward neural language model with N3 we have a moving window at time t with an embedding vector represent ing each of the 3 previous words words wt1 wt2 and wt3 These 3 vectors are concatenated together to produce x the input layer of a neural network whose output is a softmax with a probability distribution over words Thus y42 the value of output node 42 is the probability of the next word wt being V42 the vocabulary word with index 42 The model shown in Fig 712 is quite sufficient assuming we learn the embed dings separately by a method like the word2vec methods of Chapter 6 The method of using another algorithm to learn the embedding representations we use for input words is called pretraining If those pretrained embeddings are sufficient for yourpretraining purposes then this is all you need However often wed like to learn the embeddings simultaneously with training the network This is true when whatever task the network is designed for sentiment 75 NEURAL LANGUAGE MODELS 17 h1 h2 y1 h3 hdh U W y42 yV Projection layer 13d concatenated embeddings for context words Hidden layer Output layer Pwu in thehole ground there lived word 42 embedding for word 35 embedding for word 9925 embedding for word 45180 wt1wt2 wtwt3 dh3d 1dh Vdh PwtV42wt3wt2wt3 1V Figure 712 A simplified view of a feedforward neural language model moving through a text At each timestep t the network takes the 3 context words converts each to a ddimensional embedding and concatenates the 3 embeddings together to get the 1Nd unit input layer x for the network These units are multiplied by a weight matrix W and bias vector b and then an activation function to produce a hidden layer h which is then multiplied by another weight matrix U For graphic simplicity we dont show b in this and future pictures Finally a softmax output layer predicts at each node i the probability that the next word wt will be vocabulary word Vi This picture is simplified because it assumes we just look up in an embedding dictionary E the ddimensional embedding vector for each word precomputed by an algorithm like word2vec classification or translation or parsing places strong constraints on what makes a good representation Lets therefore show an architecture that allows the embeddings to be learned To do this well add an extra layer to the network and propagate the error all the way back to the embedding vectors starting with embeddings with random values and slowly moving toward sensible representations For this to work at the input layer instead of pretrained embeddings were going to represent each of the N previous words as a onehot vector of length V ie with one dimension for each word in the vocabulary A onehot vector is a vectoronehot vector that has one element equal to 1in the dimension corresponding to that words index in the vocabulary while all the other elements are set to zero Thus in a onehot representation for the word toothpaste supposing it happens to have index 5 in the vocabulary x5 is one and and xi 0 i 6 5 as shown here 0 0 0 0 1 0 0 0 0 0 0 1 2 3 4 5 6 7 V Fig 713 shows the additional layers needed to learn the embeddings during LM training Here the N3 context words are represented as 3 onehot vectors fully connected to the embedding layer via 3 instantiations of the embedding matrix E Note that we dont want to learn separate weight matrices for mapping each of the 3 previous words to the projection layer we want one single embedding dictionary E thats shared among these three Thats because over time many different words will appear as wt2 or wt1 and wed like to just represent each word with one vector whichever context position it appears in The embedding weight matrix E thus has 18 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS h1 h2 y1 h3 hdh U W y42 yV Projection layer 13d Hidden layer Output layer Pwcontext in thehole ground there lived word 42 wt1wt2 wtwt3 dh3d 1dh Vdh PwtV42wt3wt2wt3 1V Input layer onehot vectors index word 35 0 0 1 00 1 V35 0 0 1 00 1 V45180 0 0 1 00 1 V9925 0 0 index word 9925 index word 45180 E 1V dV E is shared across words Figure 713 Learning all the way back to embeddings Notice that the embedding matrix E is shared among the 3 context words a row for each word each a vector of d dimensions and hence has dimensionality V d Lets walk through the forward pass of Fig 713 1 Select three embeddings from E Given the three previous words we look up their indices create 3 onehot vectors and then multiply each by the em bedding matrix E Consider wt3 The onehot vector for the index 35 is multiplied by the embedding matrix E to give the first part of the first hidden layer called the projection layer Since each row of the input matrix E is justprojection layer an embedding for a word and the input is a onehot column vector xi for word Vi the projection layer for input w will be Exi ei the embedding for word i We now concatenate the three embeddings for the context words 2 Multiply by W We now multiply by W and add b and pass through the rectified linear or other activation function to get the hidden layer h 3 Multiply by U h is now multiplied by U 4 Apply softmax After the softmax each node i in the output layer estimates the probability Pwt iwt1wt2wt3 In summary if we use e to represent the projection layer formed by concate nating the 3 embeddings for the three context vectors the equations for a neural language model become e Ex1Ex2 Ex 727 h σWeb 728 z Uh 729 y softmaxz 730 76 SUMMARY 19 752 Training the neural language model To train the model ie to set all the parameters θ EWUb we do gradient descent Fig using error backpropagation on the computation graph to compute the gradient Training thus not only sets the weights W and U of the network but also as were predicting upcoming words were learning the embeddings E for each words that best predict upcoming words Generally training proceeds by taking as input a very long text concatenating all the sentences starting with random weights and then iteratively moving through the text predicting each word wt At each word wt the crossentropy negative log likelihood loss is L log pwt wt1 wtn1 731 The gradient for this loss is then θt1 θt η log pwt wt1 wtn1 θ 732 This gradient can be computed in any standard neural network framework which will then backpropagate through U W b E Training the parameters to minimize loss will result both in an algorithm for language modeling a word predictor but also a new set of embeddings E that can be used as word representations for other tasks 76 Summary Neural networks are built out of neural units originally inspired by human neurons but now simply an abstract computational device Each neural unit multiplies input values by a weight vector adds a bias and then applies a nonlinear activation function like sigmoid tanh or rectified linear In a fullyconnected feedforward network each unit in layer i is connected to each unit in layer i1 and there are no cycles The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network Neural networks are trained by optimization algorithms like gradient de scent Error backpropagation backward differentiation on a computation graph is used to compute the gradients of the loss function for a network Neural language models use a neural network as a probabilistic classifier to compute the probability of the next word given the previous n words Neural language models can use pretrained embeddings or can learn embed dings from scratch in the process of language modeling Bibliographical and Historical Notes The origins of neural networks lie in the 1940s McCullochPitts neuron McCul loch and Pitts 1943 a simplified model of the human neuron as a kind of com 20 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS puting element that could be described in terms of propositional logic By the late 1950s and early 1960s a number of labs including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford developed research into neural networks this phase saw the development of the perceptron Rosenblatt 1958 and the transformation of the threshold into a bias a notation we still use Widrow and Hoff 1960 The field of neural networks declined after it was shown that a single percep tron unit was unable to model functions as simple as XOR Minsky and Papert 1969 While some small amount of work continued during the next two decades a major revival for the field didnt come until the 1980s when practical tools for building deeper networks like error backpropagation became widespread Rumel hart et al 1986 During the 1980s a wide variety of neural network and related architectures were developed particularly for applications in psychology and cog nitive science Rumelhart and McClelland 1986b McClelland and Elman 1986 Rumelhart and McClelland 1986a Elman 1990 for which the term connection ist or parallel distributed processing was often used Feldman and Ballard 1982connectionist Smolensky 1988 Many of the principles and techniques developed in this period are foundational to modern work including the ideas of distributed representations Hinton 1986 recurrent networks Elman 1990 and the use of tensors for com positionality Smolensky 1990 By the 1990s larger neural networks began to be applied to many practical lan guage processing tasks as well like handwriting recognition LeCun et al 1989 LeCun et al 1990 and speech recognition Morgan and Bourlard 1989 Morgan and Bourlard 1990 By the early 2000s improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks leading to the modern term deep learning Hinton et al 2006 Bengio et al 2007 We cover more related history in Chapter 9 There are a number of excellent books on the subject Goldberg 2017 has a superb and comprehensive coverage of neural networks for natural language pro cessing For neural networks in general see Goodfellow et al 2016 and Nielsen 2015 Bibliographical and Historical Notes 21 Abadi M Agarwal A Barham P Brevdo E Chen Z Citro C Corrado G S Davis A Dean J Devin M Ghemawat S Goodfellow I Harp A Irving G Isard M Jia Y Jozefowicz R Kaiser L Kudlur M Lev enberg J Mané D Monga R Moore S Murray D Olah C Schuster M Shlens J Steiner B Sutskever I Talwar K Tucker P Vanhoucke V Vasudevan V Viégas F Vinyals O Warden P Wattenberg M Wicke M Yu Y and Zheng X 2015 TensorFlow Large scale machine learning on heterogeneous systems Soft ware available from tensorfloworg Bengio Y Ducharme R Vincent P and Jauvin C 2003 A neural probabilistic language model Journal of machine learning research 3Feb 11371155 Bengio Y Lamblin P Popovici D and Larochelle H 2007 Greedy layerwise training of deep networks In NIPS 2007 153160 Elman J L 1990 Finding structure in time Cognitive science 142 179211 Feldman J A and Ballard D H 1982 Connectionist models and their properties Cognitive Science 6 205 254 Goldberg Y 2017 Neural Network Methods for Natural Language Processing Vol 10 of Synthesis Lectures on Hu man Language Technologies Morgan Claypool Goodfellow I Bengio Y and Courville A 2016 Deep Learning MIT Press Hinton G E 1986 Learning distributed representations of concepts In COGSCI86 112 Hinton G E Osindero S and Teh YW 2006 A fast learning algorithm for deep belief nets Neural computa tion 187 15271554 Hinton G E Srivastava N Krizhevsky A Sutskever I and Salakhutdinov R R 2012 Improving neural networks by preventing coadaptation of feature detectors arXiv preprint arXiv12070580 Kingma D and Ba J 2015 Adam A method for stochas tic optimization In ICLR 2015 LeCun Y Boser B Denker J S Henderson D Howard R E Hubbard W and Jackel L D 1989 Backpropa gation applied to handwritten zip code recognition Neural computation 14 541551 LeCun Y Boser B E Denker J S Henderson D Howard R E Hubbard W E and Jackel L D 1990 Handwritten digit recognition with a backpropagation net work In NIPS 1990 396404 McClelland J L and Elman J L 1986 The TRACE model of speech perception Cognitive Psychology 18 1 86 McCulloch W S and Pitts W 1943 A logical calculus of ideas immanent in nervous activity Bulletin of Mathemat ical Biophysics 5 115133 Reprinted in Neurocomput ing Foundations of Research ed by J A Anderson and E Rosenfeld MIT Press 1988 Minsky M and Papert S 1969 Perceptrons MIT Press Morgan N and Bourlard H 1989 Generalization and parameter estimation in feedforward nets Some experi ments In Advances in neural information processing sys tems 630637 Morgan N and Bourlard H 1990 Continuous speech recognition using multilayer perceptrons with hidden markov models In ICASSP90 413416 Nielsen M A 2015 Neural networks and Deep learning Determination Press USA Paszke A Gross S Chintala S Chanan G Yang E De Vito Z Lin Z Desmaison A Antiga L and Lerer A 2017 Automatic differentiation in pytorch In NIPSW Rosenblatt F 1958 The perceptron A probabilistic model for information storage and organization in the brain Psy chological review 656 386408 Rumelhart D E Hinton G E and Williams R J 1986 Learning internal representations by error propagation In Rumelhart D E and McClelland J L Eds Parallel Distributed Processing Vol 2 318362 MIT Press Rumelhart D E and McClelland J L 1986a On learn ing the past tense of English verbs In Rumelhart D E and McClelland J L Eds Parallel Distributed Processing Vol 2 216271 MIT Press Rumelhart D E and McClelland J L Eds 1986b Par allel Distributed Processing MIT Press Russell S and Norvig P 2002 Artificial Intelligence A Modern Approach 2nd Ed Prentice Hall Smolensky P 1988 On the proper treatment of connec tionism Behavioral and brain sciences 111 123 Smolensky P 1990 Tensor product variable binding and the representation of symbolic structures in connectionist systems Artificial intelligence 4612 159216 Srivastava N Hinton G E Krizhevsky A Sutskever I and Salakhutdinov R R 2014 Dropout a simple way to prevent neural networks from overfitting JMLR 151 19291958 Widrow B and Hoff M E 1960 Adaptive switching cir cuits In IRE WESCON Convention Record Vol 4 96 104
