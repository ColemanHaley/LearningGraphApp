b'Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved.\n\nDraft of October 2, 2019.\n\nCopyright c(cid:13) 2019.\n\nAll\n\nCHAPTER\n\n3 N-gram Language Models\n\n\xe2\x80\x9cYou are uniformly charming!\xe2\x80\x9d cried he, with a smile of associating and now\nand then I bowed and they perceived a chaise and four to wish for.\n\nRandom sentence generated from a Jane Austen trigram model\n\nPredicting is dif\xef\xac\x81cult\xe2\x80\x94especially about the future, as the old quip goes. But how\nabout predicting something that seems much easier, like the next few words someone\nis going to say? What word, for example, is likely to follow\n\nPlease turn your homework ...\n\nHopefully, most of you concluded that a very likely word is in, or possibly over,\nbut probably not refrigerator or the. In the following sections we will formalize\nthis intuition by introducing models that assign a probability to each possible next\nword. The same models will also serve to assign a probability to an entire sentence.\nSuch a model, for example, could predict that the following sequence has a much\nhigher probability of appearing in a text:\n\nall of a sudden I notice three guys standing on the sidewalk\n\nthan does this same set of words in a different order:\n\non guys all I of notice sidewalk three a sudden standing the\n\nWhy would you want to predict upcoming words, or assign probabilities to sen-\ntences? Probabilities are essential in any task in which we have to identify words in\nnoisy, ambiguous input, like speech recognition. For a speech recognizer to realize\nthat you said I will be back soonish and not I will be bassoon dish, it helps to know\nthat back soonish is a much more probable sequence than bassoon dish. For writing\ntools like spelling correction or grammatical error correction, we need to \xef\xac\x81nd and\ncorrect errors in writing like Their are two midterms, in which There was mistyped\nas Their, or Everything has improve, in which improve should have been improved.\nThe phrase There are will be much more probable than Their are, and has improved\nthan has improve, allowing us to help users by detecting and correcting these errors.\nAssigning probabilities to sequences of words is also essential in machine trans-\n\nlation. Suppose we are translating a Chinese source sentence:\n\n\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xba\x86 \xe4\xb8\xbb\xe8\xa6\x81 \xe5\x86\x85\xe5\xae\xb9\n\n\xe4\xbb\x96 \xe5\x90\x91 \xe8\xae\xb0\xe8\x80\x85\nHe to reporters introduced main content\nAs part of the process we might have built the following set of potential rough\n\nEnglish translations:\n\nhe introduced reporters to the main contents of the statement\nhe briefed to reporters the main contents of the statement\nhe briefed reporters on the main contents of the statement\n\n\x0c2 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nAAC\n\nlanguage model\nLM\nn-gram\n\nA probabilistic model of word sequences could suggest that briefed reporters on\nis a more probable English phrase than briefed to reporters (which has an awkward\nto after briefed) or introduced reporters to (which uses a verb that is less \xef\xac\x82uent\nEnglish in this context), allowing us to correctly select the boldfaced sentence above.\nProbabilities are also important for augmentative and alternative communi-\ncation systems (Trnka et al. 2007, Kane et al. 2017). People often use such AAC\ndevices if they are physically unable or sign but can instead using eye gaze or other\nspeci\xef\xac\x81c movements to select words from a menu to be spoken by the system. Word\nprediction can be used to suggest likely words for the menu.\n\nModels that assign probabilities to sequences of words are called language mod-\nels or LMs. In this chapter we introduce the simplest model that assigns probabilities\nto sentences and sequences of words, the n-gram. An n-gram is a sequence of N\nwords: a 2-gram (or bigram) is a two-word sequence of words like \xe2\x80\x9cplease turn\xe2\x80\x9d,\n\xe2\x80\x9cturn your\xe2\x80\x9d, or \xe2\x80\x9dyour homework\xe2\x80\x9d, and a 3-gram (or trigram) is a three-word se-\nquence of words like \xe2\x80\x9cplease turn your\xe2\x80\x9d, or \xe2\x80\x9cturn your homework\xe2\x80\x9d. We\xe2\x80\x99ll see how\nto use n-gram models to estimate the probability of the last word of an n-gram given\nthe previous words, and also to assign probabilities to entire sequences. In a bit of\nterminological ambiguity, we usually drop the word \xe2\x80\x9cmodel\xe2\x80\x9d, and thus the term n-\ngram is used to mean either the word sequence itself or the predictive model that\nassigns it a probability. In later chapters we\xe2\x80\x99ll introduce more sophisticated language\nmodels like the RNN LMs of Chapter 9.\n\n3.1 N-Grams\n\nLet\xe2\x80\x99s begin with the task of computing P(w|h), the probability of a word w given\nsome history h. Suppose the history h is \xe2\x80\x9cits water is so transparent that\xe2\x80\x9d and we\nwant to know the probability that the next word is the:\n\nP(the|its water is so transparent that).\n\n(3.1)\n\nOne way to estimate this probability is from relative frequency counts: take a\nvery large corpus, count the number of times we see its water is so transparent that,\nand count the number of times this is followed by the. This would be answering the\nquestion \xe2\x80\x9cOut of the times we saw the history h, how many times was it followed by\nthe word w\xe2\x80\x9d, as follows:\n\nP(the|its water is so transparent that) =\nC(its water is so transparent that the)\n\nC(its water is so transparent that)\n\n(3.2)\n\nWith a large enough corpus, such as the web, we can compute these counts and\nestimate the probability from Eq. 3.2. You should pause now, go to the web, and\ncompute this estimate for yourself.\n\nWhile this method of estimating probabilities directly from counts works \xef\xac\x81ne in\nmany cases, it turns out that even the web isn\xe2\x80\x99t big enough to give us good estimates\nin most cases. This is because language is creative; new sentences are created all the\ntime, and we won\xe2\x80\x99t always be able to count entire sentences. Even simple extensions\nof the example sentence may have counts of zero on the web (such as \xe2\x80\x9cWalden\nPond\xe2\x80\x99s water is so transparent that the\xe2\x80\x9d; well, used to have counts of zero).\n\n\x0c3.1\n\n\xe2\x80\xa2 N-GRAMS\n\n3\n\nSimilarly, if we wanted to know the joint probability of an entire sequence of\nwords like its water is so transparent, we could do it by asking \xe2\x80\x9cout of all possible\nsequences of \xef\xac\x81ve words, how many of them are its water is so transparent?\xe2\x80\x9d We\nwould have to get the count of its water is so transparent and divide by the sum of\nthe counts of all possible \xef\xac\x81ve word sequences. That seems rather a lot to estimate!\nFor this reason, we\xe2\x80\x99ll need to introduce cleverer ways of estimating the proba-\nbility of a word w given a history h, or the probability of an entire word sequence W .\nLet\xe2\x80\x99s start with a little formalizing of notation. To represent the probability of a par-\nticular random variable Xi taking on the value \xe2\x80\x9cthe\xe2\x80\x9d, or P(Xi = \xe2\x80\x9cthe\xe2\x80\x9d), we will use\nthe simpli\xef\xac\x81cation P(the). We\xe2\x80\x99ll represent a sequence of N words either as w1 . . .wn\nor wn\n1 means the string w1,w2, ...,wn\xe2\x88\x921). For the joint prob-\nability of each word in a sequence having a particular value P(X = w1,Y = w2,Z =\nw3, ...,W = wn) we\xe2\x80\x99ll use P(w1,w2, ...,wn).\n\n1 (so the expression wn\xe2\x88\x921\n\nNow how can we compute probabilities of entire sequences like P(w1,w2, ...,wn)?\nOne thing we can do is decompose this probability using the chain rule of proba-\nbility:\n\nP(X1...Xn) = P(X1)P(X2|X1)P(X3|X 2\n\n1 ) . . .P(Xn|X n\xe2\x88\x921\n\n1\n\n)\n\nn(cid:89)\n\n=\n\nP(Xk|X k\xe2\x88\x921\n\n1\n\n)\n\nk=1\n\nApplying the chain rule to words, we get\n\nP(wn\n\n1) = P(w1)P(w2|w1)P(w3|w2\n\n1) . . .P(wn|wn\xe2\x88\x921\n\n1\n\nn(cid:89)\n\n=\n\nP(wk|wk\xe2\x88\x921\n\n1\n\n)\n\n(3.3)\n\n(3.4)\n\n)\n\nk=1\n\nThe chain rule shows the link between computing the joint probability of a se-\nquence and computing the conditional probability of a word given previous words.\nEquation 3.4 suggests that we could estimate the joint probability of an entire se-\nquence of words by multiplying together a number of conditional probabilities. But\nusing the chain rule doesn\xe2\x80\x99t really seem to help us! We don\xe2\x80\x99t know any way to\ncompute the exact probability of a word given a long sequence of preceding words,\nP(wn|wn\xe2\x88\x921\n). As we said above, we can\xe2\x80\x99t just estimate by counting the number of\ntimes every word occurs following every long string, because language is creative\nand any particular context might have never occurred before!\n\n1\n\nThe intuition of the n-gram model is that instead of computing the probability of\na word given its entire history, we can approximate the history by just the last few\nwords.\nThe bigram model, for example, approximates the probability of a word given\nall the previous words P(wn|wn\xe2\x88\x921\n) by using only the conditional probability of the\npreceding word P(wn|wn\xe2\x88\x921). In other words, instead of computing the probability\n\n1\n\nP(the|Walden Pond\xe2\x80\x99s water is so transparent that)\n\nwe approximate it with the probability\n\nP(the|that)\n\n(3.5)\n\n(3.6)\n\nbigram\n\n\x0c4 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nWhen we use a bigram model to predict the conditional probability of the next\n\nword, we are thus making the following approximation:\n) \xe2\x89\x88 P(wn|wn\xe2\x88\x921)\n\nP(wn|wn\xe2\x88\x921\n\n1\n\n(3.7)\n\nMarkov\n\nn-gram\n\nmaximum\nlikelihood\nestimation\n\nnormalize\n\nThe assumption that the probability of a word depends only on the previous word\nis called a Markov assumption. Markov models are the class of probabilistic models\nthat assume we can predict the probability of some future unit without looking too\nfar into the past. We can generalize the bigram (which looks one word into the past)\nto the trigram (which looks two words into the past) and thus to the n-gram (which\nlooks n\xe2\x88\x92 1 words into the past).\n\nThus, the general equation for this n-gram approximation to the conditional\n\nprobability of the next word in a sequence is\n\nP(wn|wn\xe2\x88\x921\n\n) \xe2\x89\x88 P(wn|wn\xe2\x88\x921\n\n(3.8)\nGiven the bigram assumption for the probability of an individual word, we can\ncompute the probability of a complete word sequence by substituting Eq. 3.7 into\nEq. 3.4:\n\nn\xe2\x88\x92N+1)\n\n1\n\nP(wn\n\n1) \xe2\x89\x88\n\nP(wk|wk\xe2\x88\x921)\n\n(3.9)\n\nn(cid:89)\n\nk=1\n\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\nestimate probabilities is called maximum likelihood estimation or MLE. We get\nthe MLE estimate for the parameters of an n-gram model by getting counts from a\ncorpus, and normalizing the counts so that they lie between 0 and 1.1\n\nFor example, to compute a particular bigram probability of a word y given a\nprevious word x, we\xe2\x80\x99ll compute the count of the bigram C(xy) and normalize by the\nsum of all the bigrams that share the same \xef\xac\x81rst word x:\n\nP(wn|wn\xe2\x88\x921) =\n\n(cid:80)\n\nC(wn\xe2\x88\x921wn)\nwC(wn\xe2\x88\x921w)\n\n(3.10)\n\nWe can simplify this equation, since the sum of all bigram counts that start with\na given word wn\xe2\x88\x921 must be equal to the unigram count for that word wn\xe2\x88\x921 (the reader\nshould take a moment to be convinced of this):\n\nP(wn|wn\xe2\x88\x921) =\n\nC(wn\xe2\x88\x921wn)\nC(wn\xe2\x88\x921)\n\n(3.11)\n\nLet\xe2\x80\x99s work through an example using a mini-corpus of three sentences. We\xe2\x80\x99ll\n\xef\xac\x81rst need to augment each sentence with a special symbol <s> at the beginning\nof the sentence, to give us the bigram context of the \xef\xac\x81rst word. We\xe2\x80\x99ll also need a\nspecial end-symbol. </s>2\n\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I do not like green eggs and ham </s>\n\n1 For probabilistic models, normalizing means dividing by some total count so that the resulting prob-\nabilities fall legally between 0 and 1.\n2 We need the end-symbol to make the bigram grammar a true probability distribution. Without an\nend-symbol, the sentence probabilities for all sentences of a given length would sum to one. This model\nwould de\xef\xac\x81ne an in\xef\xac\x81nite set of probability distributions, with one distribution per sentence length. See\nExercise 3.5.\n\n\x0c3.1\n\n\xe2\x80\xa2 N-GRAMS\n\n5\n\nHere are the calculations for some of the bigram probabilities from this corpus\nP(I|<s>) = 2\nP(</s>|Sam) = 1\nFor the general case of MLE n-gram parameter estimation:\n\nP(Sam|<s>) = 1\nP(Sam|am) = 1\n\nP(am|I) = 2\nP(do|I) = 1\n\n3 = .33\n2 = .5\n\n3 = .67\n3 = .33\n\n2 = 0.5\n\n3 = .67\n\nrelative\nfrequency\n\nP(wn|wn\xe2\x88\x921\n\nn\xe2\x88\x92N+1) =\n\nC(wn\xe2\x88\x921\nC(wn\xe2\x88\x921\n\nn\xe2\x88\x92N+1wn)\nn\xe2\x88\x92N+1)\n\n(3.12)\n\nEquation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\nobserved frequency of a particular sequence by the observed frequency of a pre\xef\xac\x81x.\nThis ratio is called a relative frequency. We said above that this use of relative\nfrequencies as a way to estimate probabilities is an example of maximum likelihood\nestimation or MLE. In MLE, the resulting parameter set maximizes the likelihood\nof the training set T given the model M (i.e., P(T|M)). For example, suppose the\nword Chinese occurs 400 times in a corpus of a million words like the Brown corpus.\nWhat is the probability that a random word selected from some other text of, say,\na million words will be the word Chinese? The MLE of its probability is\n1000000\nor .0004. Now .0004 is not the best possible estimate of the probability of Chinese\noccurring in all situations; it might turn out that in some other corpus or context\nChinese is a very unlikely word. But it is the probability that makes it most likely\nthat Chinese will occur 400 times in a million-word corpus. We present ways to\nmodify the MLE estimates slightly to get better probability estimates in Section 3.4.\nLet\xe2\x80\x99s move on to some examples from a slightly larger corpus than our 14-word\nexample above. We\xe2\x80\x99ll use data from the now-defunct Berkeley Restaurant Project,\na dialogue system from the last century that answered questions about a database\nof restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text-\nnormalized sample user queries (a sample of 9332 sentences is on the website):\n\n400\n\ncan you tell me about any good cantonese restaurants close by\nmid priced thai food is what i\xe2\x80\x99m looking for\ntell me about chez panisse\ncan you give me a listing of the kinds of food that are available\ni\xe2\x80\x99m looking for a good place to eat breakfast\nwhen is caffe venezia open during the day\n\nFigure 3.1 shows the bigram counts from a piece of a bigram grammar from the\nBerkeley Restaurant Project. Note that the majority of the values are zero. In fact,\nwe have chosen the sample words to cohere with each other; a matrix selected from\na random set of seven words would be even more sparse.\n\nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell\nin Fig. 3.1 by the appropriate unigram for its row, taken from the following set of\nunigram probabilities):\n\nwant to\n\ni\n2533 927\n\neat chinese food lunch spend\n\n2417 746 158\n\n1093 341\n\n278\n\nHere are a few other useful probabilities:\nP(i|<s>) = 0.25\nP(food|english) = 0.5\n\nP(english|want) = 0.0011\nP(</s>|food) = 0.68\n\nNow we can compute the probability of sentences like I want English food or\nI want Chinese food by simply multiplying the appropriate bigram probabilities to-\ngether, as follows:\n\n\x0c6 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\ni\n5\n2\n2\n0\n1\n15\n2\n1\n\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\nFigure 3.1 Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\nrant Project corpus of 9332 sentences. Zero counts are in gray.\n\nchinese\n0\n6\n2\n16\n0\n1\n0\n0\n\nspend\n2\n1\n211\n0\n0\n0\n0\n0\n\nlunch\n0\n5\n6\n42\n1\n0\n0\n0\n\nwant\n827\n0\n0\n0\n0\n0\n0\n0\n\nfood\n0\n6\n0\n2\n82\n4\n1\n0\n\neat\n9\n1\n686\n0\n0\n0\n0\n0\n\nto\n0\n608\n4\n2\n0\n15\n0\n1\n\nchinese\n\nlunch\n0\n\neat\n0.0036 0\n0.0011 0.0065\n\nspend\nfood\n0\n0.00079\n0.0065 0.0054 0.0011\n0.0025 0.087\n\nwant\ni\n0.33\n0.002\n0\n0.0022\n0.00083 0\n0\n0\n0\n0\n0\n0\n\ni\nwant\nto\neat\nchinese 0.0063\nfood\n0.014\nlunch\n0.0059\nspend\n0.0036\nFigure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus\nof 9332 sentences. Zero probabilities are in gray.\n\n0.00083 0\n0.021\n0\n0.00092 0.0037 0\n0.0029 0\n0\n0\n0\n0\n\n0\n0.0063 0\n0\n0\n0\n\nto\n0\n0.66\n0.0017 0.28\n0.0027 0\n0\n0\n0.014\n0\n0\n0\n0.0036 0\n\n0.0027 0.056\n0.52\n\nP(<s> i want english food </s>)\n\n= P(i|<s>)P(want|i)P(english|want)\n\nP(food|english)P(</s>|food)\n\n= .25\xc3\x97 .33\xc3\x97 .0011\xc3\x97 0.5\xc3\x97 0.68\n= .000031\n\nWe leave it as Exercise 3.2 to compute the probability of i want chinese food.\nWhat kinds of linguistic phenomena are captured in these bigram statistics?\nSome of the bigram probabilities above encode some facts that we think of as strictly\nsyntactic in nature, like the fact that what comes after eat is usually a noun or an\nadjective, or that what comes after to is usually a verb. Others might be a fact about\nthe personal assistant task, like the high probability of sentences beginning with\nthe words I. And some might even be cultural rather than linguistic, like the higher\nprobability that people are looking for Chinese versus English food.\nSome practical issues: Although for pedagogical purposes we have only described\nbigram models, in practice it\xe2\x80\x99s more common to use trigram models, which con-\ndition on the previous two words rather than the previous word, or 4-gram or even\n5-gram models, when there is suf\xef\xac\x81cient training data. Note that for these larger n-\ngrams, we\xe2\x80\x99ll need to assume extra context for the contexts to the left and right of the\nsentence end. For example, to compute trigram probabilities at the very beginning of\nthe sentence, we can use two pseudo-words for the \xef\xac\x81rst trigram (i.e., P(I|<s><s>).\nWe always represent and compute language model probabilities in log format\nas log probabilities. Since probabilities are (by de\xef\xac\x81nition) less than or equal to\n1, the more probabilities we multiply together, the smaller the product becomes.\nMultiplying enough n-grams together would result in numerical under\xef\xac\x82ow. By using\nlog probabilities instead of raw probabilities, we get numbers that are not as small.\n\ntrigram\n4-gram\n5-gram\n\nlog\nprobabilities\n\n\x0c3.2\n\n\xe2\x80\xa2 EVALUATING LANGUAGE MODELS\n\n7\n\nAdding in log space is equivalent to multiplying in linear space, so we combine log\nprobabilities by adding them. The result of doing all computation and storage in log\nspace is that we only need to convert back into probabilities if we need to report\nthem at the end; then we can just take the exp of the logprob:\n\np1 \xc3\x97 p2 \xc3\x97 p3 \xc3\x97 p4 = exp(log p1 + log p2 + log p3 + log p4)\n\n(3.13)\n\n3.2 Evaluating Language Models\n\nextrinsic\nevaluation\n\nintrinsic\nevaluation\n\ntraining set\n\ntest set\nheld out\n\ndevelopment\ntest\n\nThe best way to evaluate the performance of a language model is to embed it in\nan application and measure how much the application improves. Such end-to-end\nevaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to\nknow if a particular improvement in a component is really going to help the task\nat hand. Thus, for speech recognition, we can compare the performance of two\nlanguage models by running the speech recognizer twice, once with each language\nmodel, and seeing which gives the more accurate transcription.\n\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\nstead, it would be nice to have a metric that can be used to quickly evaluate potential\nimprovements in a language model. An intrinsic evaluation metric is one that mea-\nsures the quality of a model independent of any application.\n\nFor an intrinsic evaluation of a language model we need a test set. As with many\nof the statistical models in our \xef\xac\x81eld, the probabilities of an n-gram model come from\nthe corpus it is trained on, the training set or training corpus. We can then measure\nthe quality of an n-gram model by its performance on some unseen data called the\ntest set or test corpus. We will also sometimes call test sets and other datasets that\nare not in our training sets held out corpora because we hold them out from the\ntraining data.\n\nSo if we are given a corpus of text and want to compare two different n-gram\nmodels, we divide the data into training and test sets, train the parameters of both\nmodels on the training set, and then compare how well the two trained models \xef\xac\x81t the\ntest set.\n\nBut what does it mean to \xe2\x80\x9c\xef\xac\x81t the test set\xe2\x80\x9d? The answer is simple: whichever\nmodel assigns a higher probability to the test set\xe2\x80\x94meaning it more accurately\npredicts the test set\xe2\x80\x94is a better model. Given two probabilistic models, the better\nmodel is the one that has a tighter \xef\xac\x81t to the test data or that better predicts the details\nof the test data, and hence will assign a higher probability to the test data.\n\nSince our evaluation metric is based on test set probability, it\xe2\x80\x99s important not to\nlet the test sentences into the training set. Suppose we are trying to compute the\nprobability of a particular \xe2\x80\x9ctest\xe2\x80\x9d sentence. If our test sentence is part of the training\ncorpus, we will mistakenly assign it an arti\xef\xac\x81cially high probability when it occurs\nin the test set. We call this situation training on the test set. Training on the test\nset introduces a bias that makes the probabilities all look too high, and causes huge\ninaccuracies in perplexity, the probability-based metric we introduce below.\n\nSometimes we use a particular test set so often that we implicitly tune to its\ncharacteristics. We then need a fresh test set that is truly unseen. In such cases, we\ncall the initial test set the development test set or, devset. How do we divide our\ndata into training, development, and test sets? We want our test set to be as large\nas possible, since a small test set may be accidentally unrepresentative, but we also\nwant as much training data as possible. At the minimum, we would want to pick\n\n\x0c8 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nthe smallest test set that gives us enough statistical power to measure a statistically\nsigni\xef\xac\x81cant difference between two potential models. In practice, we often just divide\nour data into 80% training, 10% development, and 10% test. Given a large corpus\nthat we want to divide into training and test, test data can either be taken from some\ncontinuous sequence of text inside the corpus, or we can remove smaller \xe2\x80\x9cstripes\xe2\x80\x9d\nof text from randomly selected parts of our corpus and combine them into a test set.\n\nperplexity\n\n3.2.1 Perplexity\nIn practice we don\xe2\x80\x99t use raw probability as our metric for evaluating language mod-\nels, but a variant called perplexity. The perplexity (sometimes called PP for short)\nof a language model on a test set is the inverse probability of the test set, normalized\nby the number of words. For a test set W = w1w2 . . .wN,:\n\nWe can use the chain rule to expand the probability of W :\n\nPP(W ) = P(w1w2 . . .wN)\xe2\x88\x92 1\n\nN\n\n= N\n\n1\n\nP(w1w2 . . .wN)\n\n(cid:115)\n\n(cid:118)(cid:117)(cid:117)(cid:116) N(cid:89)\n\ni=1\n\nPP(W ) = N\n\n1\n\nP(wi|w1 . . .wi\xe2\x88\x921)\n\n(3.14)\n\n(3.15)\n\nThus, if we are computing the perplexity of W with a bigram language model,\n\nwe get:\n\n(cid:118)(cid:117)(cid:117)(cid:116) N(cid:89)\n\nPP(W ) = N\n\ni=1\n\n1\n\nP(wi|wi\xe2\x88\x921)\n\n(3.16)\n\nNote that because of the inverse in Eq. 3.15, the higher the conditional probabil-\nity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is\nequivalent to maximizing the test set probability according to the language model.\nWhat we generally use for word sequence in Eq. 3.15 or Eq. 3.16 is the entire se-\nquence of words in some test set. Since this sequence will cross many sentence\nboundaries, we need to include the begin- and end-sentence markers <s> and </s>\nin the probability computation. We also need to include the end-of-sentence marker\n</s> (but not the beginning-of-sentence marker <s>) in the total count of word to-\nkens N.\n\nThere is another way to think about perplexity: as the weighted average branch-\ning factor of a language. The branching factor of a language is the number of possi-\nble next words that can follow any word. Consider the task of recognizing the digits\nin English (zero, one, two,..., nine), given that (both in some training set and in some\ntest set) each of the 10 digits occurs with equal probability P = 1\n10. The perplexity of\nthis mini-language is in fact 10. To see that, imagine a test string of digits of length\nN, and assume that in the training set all the digits occurred with equal probability.\nBy Eq. 3.15, the perplexity will be\n\n\x0c3.3\n\n\xe2\x80\xa2 GENERALIZATION AND ZEROS\n\n9\n\nPP(W ) = P(w1w2 . . .wN)\xe2\x88\x92 1\n\nN\n\nN\n\n)\xe2\x88\x92 1\n\nN\n\n= (\n\n1\n10\n1\n=\n10\n= 10\n\n\xe2\x88\x921\n\n(3.17)\n\nBut suppose that the number zero is really frequent and occurs far more often\nthan other numbers. Let\xe2\x80\x99s say that 0 occur 91 times in the training set, and each\nof the other digits occurred 1 time each. Now we see the following test set: 0 0\n0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since\nmost of the time the next number will be zero, which is very predictable, i.e. has\na high probability. Thus, although the branching factor is still 10, the perplexity or\nweighted branching factor is smaller. We leave this exact calculation as exercise 12.\nWe see in Section 3.7 that perplexity is also closely related to the information-\n\ntheoretic notion of entropy.\n\nFinally, let\xe2\x80\x99s look at an example of how perplexity can be used to compare dif-\nferent n-gram models. We trained unigram, bigram, and trigram grammars on 38\nmillion words (including start-of-sentence tokens) from the Wall Street Journal, us-\ning a 19,979 word vocabulary. We then computed the perplexity of each of these\nmodels on a test set of 1.5 million words with Eq. 3.16. The table below shows the\nperplexity of a 1.5 million word WSJ test set according to each of these grammars.\n\nUnigram Bigram Trigram\n\nPerplexity 962\n\n170\n\n109\n\nAs we see above, the more information the n-gram gives us about the word\nsequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related\ninversely to the likelihood of the test sequence according to the model).\n\nNote that in computing perplexities, the n-gram model P must be constructed\nwithout any knowledge of the test set or any prior knowledge of the vocabulary of\nthe test set. Any kind of knowledge of the test set can cause the perplexity to be\narti\xef\xac\x81cially low. The perplexity of two language models is only comparable if they\nuse identical vocabularies.\n\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-\nprovement in the performance of a language processing task like speech recognition\nor machine translation. Nonetheless, because perplexity often correlates with such\nimprovements, it is commonly used as a quick check on an algorithm. But a model\xe2\x80\x99s\nimprovement in perplexity should always be con\xef\xac\x81rmed by an end-to-end evaluation\nof a real task before concluding the evaluation of the model.\n\n3.3 Generalization and Zeros\n\nThe n-gram model, like many statistical models, is dependent on the training corpus.\nOne implication of this is that the probabilities often encode speci\xef\xac\x81c facts about a\ngiven training corpus. Another implication is that n-grams do a better and better job\nof modeling the training corpus as we increase the value of N.\n\n\x0c10 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nWe can visualize both of these facts by borrowing the technique of Shannon\n(1951) and Miller and Selfridge (1950) of generating random sentences from dif-\nferent n-gram models.\nIt\xe2\x80\x99s simplest to visualize how this works for the unigram\ncase. Imagine all the words of the English language covering the probability space\nbetween 0 and 1, each word covering an interval proportional to its frequency. We\nchoose a random value between 0 and 1 and print the word whose interval includes\nthis chosen value. We continue choosing random numbers and generating words\nuntil we randomly generate the sentence-\xef\xac\x81nal token </s>. We can use the same\ntechnique to generate bigrams by \xef\xac\x81rst generating a random bigram that starts with\n<s> (according to its bigram probability). Let\xe2\x80\x99s say the second word of that bigram\nis w. We next chose a random bigram starting with w (again, drawn according to its\nbigram probability), and so on.\n\nTo give an intuition for the increasing power of higher-order n-grams, Fig. 3.3\nshows random sentences generated from unigram, bigram, trigram, and 4-gram\nmodels trained on Shakespeare\xe2\x80\x99s works.\n\ngram\n\ngram\n\nrote life have\n\xe2\x80\x93Hill he late speaks; or! a more to leg less \xef\xac\x81rst you enter\n\nking. Follow.\n\xe2\x80\x93What means, sir. I confess she? then all sorts, he is trim, captain.\n\n1 \xe2\x80\x93To him swallowed confess hear both. Which. Of save on trail for are ay device and\n2 \xe2\x80\x93Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n3 \xe2\x80\x93Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n4 \xe2\x80\x93King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n\n\xe2\x80\x99tis done.\n\xe2\x80\x93This shall forbid it should be branded, if renown made it empty.\n\ngreat banquet serv\xe2\x80\x99d in;\n\xe2\x80\x93It cannot be but so.\n\ngram\n\ngram\n\nFigure 3.3 Eight sentences randomly generated from four n-grams computed from Shakespeare\xe2\x80\x99s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\nfor capitalization to improve readability.\n\nThe longer the context on which we train the model, the more coherent the sen-\ntences. In the unigram sentences, there is no coherent relation between words or any\nsentence-\xef\xac\x81nal punctuation. The bigram sentences have some local word-to-word\ncoherence (especially if we consider that punctuation counts as a word). The tri-\ngram and 4-gram sentences are beginning to look a lot like Shakespeare. Indeed, a\ncareful investigation of the 4-gram sentences shows that they look a little too much\nlike Shakespeare. The words It cannot be but so are directly from King John. This is\nbecause, not to put the knock on Shakespeare, his oeuvre is not very large as corpora\ngo (N = 884,647,V = 29,066), and our n-gram probability matrices are ridiculously\nsparse. There are V 2 = 844,000,000 possible bigrams alone, and the number of pos-\nsible 4-grams is V 4 = 7\xc3\x971017. Thus, once the generator has chosen the \xef\xac\x81rst 4-gram\n(It cannot be but), there are only \xef\xac\x81ve possible continuations (that, I, he, thou, and\nso); indeed, for many 4-grams, there is only one continuation.\n\nTo get an idea of the dependence of a grammar on its training set, let\xe2\x80\x99s look at an\nn-gram grammar trained on a completely different corpus: the Wall Street Journal\n(WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so\nwe might expect some overlap between our n-grams for the two genres. Fig. 3.4\n\n\x0c3.3\n\n\xe2\x80\xa2 GENERALIZATION AND ZEROS\n\n11\n\nshows sentences generated by unigram, bigram, and trigram grammars trained on\n40 million words from WSJ.\n\ngram\n\nwere recession exchange new endorsed a acquire to six executives\n\n1 Months the my and issue of year foreign new exchange\xe2\x80\x99s september\n2 Last December through the way to preserve the Hudson corporation N.\n\nB. E. C. Taylor would seem to complete the major central planners one\npoint \xef\xac\x81ve percent of U. S. E. has already old M. X. corporation of living\non information such as more frequently \xef\xac\x81shing to keep her\n\ngram\n\n3 They also point to ninety nine point six billion dollars from two hundred\n\nfour oh six three percent of the rates of interest stores as Mexico and\nBrazil on market conditions\n\ngram\n\nFigure 3.4 Three sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-\ntion as words. Output was then hand-corrected for capitalization to improve readability.\n\nCompare these examples to the pseudo-Shakespeare in Fig. 3.3. While they both\nmodel \xe2\x80\x9cEnglish-like sentences\xe2\x80\x9d, there is clearly no overlap in generated sentences,\nand little overlap even in small phrases. Statistical models are likely to be pretty use-\nless as predictors if the training sets and the test sets are as different as Shakespeare\nand WSJ.\n\nHow should we deal with this problem when we build n-gram models? One step\nis to be sure to use a training corpus that has a similar genre to whatever task we are\ntrying to accomplish. To build a language model for translating legal documents,\nwe need a training corpus of legal documents. To build a language model for a\nquestion-answering system, we need a training corpus of questions.\n\nIt is equally important to get training data in the appropriate dialect, especially\nwhen processing social media posts or spoken transcripts. Thus tweets in AAVE\n(African American Vernacular English) often use words like \xef\xac\x81nna\xe2\x80\x94an auxiliary\nverb that marks immediate future tense \xe2\x80\x94that don\xe2\x80\x99t occur in other dialects, or\nspellings like den for then, in tweets like this one (Blodgett and O\xe2\x80\x99Connor, 2017):\n(3.18) Bored af den my phone \xef\xac\x81nna die!!!\nwhile tweets from varieties like Nigerian English have markedly different vocabu-\nlary and n-gram patterns from American English (Jurgens et al., 2017):\n(3.19) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\n\ntweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\n\nMatching genres and dialects is still not suf\xef\xac\x81cient. Our models may still be\nsubject to the problem of sparsity. For any n-gram that occurred a suf\xef\xac\x81cient number\nof times, we might have a good estimate of its probability. But because any corpus is\nlimited, some perfectly acceptable English word sequences are bound to be missing\nfrom it. That is, we\xe2\x80\x99ll have many cases of putative \xe2\x80\x9czero probability n-grams\xe2\x80\x9d that\nshould really have some non-zero probability. Consider the words that follow the\nbigram denied the in the WSJ Treebank3 corpus, together with their counts:\n\ndenied the allegations: 5\ndenied the speculation: 2\ndenied the rumors:\n1\n1\ndenied the report:\n\nBut suppose our test set has phrases like:\n\n\x0c12 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nzeros\n\nclosed\nvocabulary\n\nOOV\nopen\nvocabulary\n\ndenied the offer\ndenied the loan\n\nOur model will incorrectly estimate that the P(offer|denied the) is 0!\nThese zeros\xe2\x80\x94 things that don\xe2\x80\x99t ever occur in the training set but do occur in\nthe test set\xe2\x80\x94are a problem for two reasons. First, their presence means we are\nunderestimating the probability of all sorts of words that might occur, which will\nhurt the performance of any application we want to run on this data.\n\nSecond, if the probability of any word in the test set is 0, the entire probability\nof the test set is 0. By de\xef\xac\x81nition, perplexity is based on the inverse probability of the\ntest set. Thus if some words have zero probability, we can\xe2\x80\x99t compute perplexity at\nall, since we can\xe2\x80\x99t divide by 0!\n\n3.3.1 Unknown Words\nThe previous section discussed the problem of words whose bigram probability is\nzero. But what about words we simply have never seen before?\n\nSometimes we have a language task in which this can\xe2\x80\x99t happen because we know\nall the words that can occur. In such a closed vocabulary system the test set can\nonly contain words from this lexicon, and there will be no unknown words. This is\na reasonable assumption in some domains, such as speech recognition or machine\ntranslation, where we have a pronunciation dictionary or a phrase table that are \xef\xac\x81xed\nin advance, and so the language model can only use the words in that dictionary or\nphrase table.\n\nIn other cases we have to deal with words we haven\xe2\x80\x99t seen before, which we\xe2\x80\x99ll\ncall unknown words, or out of vocabulary (OOV) words. The percentage of OOV\nwords that appear in the test set is called the OOV rate. An open vocabulary system\nis one in which we model these potential unknown words in the test set by adding a\npseudo-word called <UNK>.\n\nThere are two common ways to train the probabilities of the unknown word\nmodel <UNK>. The \xef\xac\x81rst one is to turn the problem back into a closed vocabulary one\nby choosing a \xef\xac\x81xed vocabulary in advance:\n\n1. Choose a vocabulary (word list) that is \xef\xac\x81xed in advance.\n2. Convert in the training set any word that is not in this set (any OOV word) to\n\nthe unknown word token <UNK> in a text normalization step.\n\n3. Estimate the probabilities for <UNK> from its counts just like any other regular\n\nword in the training set.\n\nThe second alternative, in situations where we don\xe2\x80\x99t have a prior vocabulary in ad-\nvance, is to create such a vocabulary implicitly, replacing words in the training data\nby <UNK> based on their frequency. For example we can replace by <UNK> all words\nthat occur fewer than n times in the training set, where n is some small number, or\nequivalently select a vocabulary size V in advance (say 50,000) and choose the top\nV words by frequency and replace the rest by UNK. In either case we then proceed\nto train the language model as before, treating <UNK> like a regular word.\n\nThe exact choice of <UNK> model does have an effect on metrics like perplexity.\nA language model can achieve low perplexity by choosing a small vocabulary and\nassigning the unknown word a high probability. For this reason, perplexities should\nonly be compared across language models with the same vocabularies (Buck et al.,\n2014).\n\n\x0c3.4 Smoothing\n\n3.4\n\n\xe2\x80\xa2 SMOOTHING\n\n13\n\nsmoothing\ndiscounting\n\nLaplace\nsmoothing\n\nWhat do we do with words that are in our vocabulary (they are not unknown words)\nbut appear in a test set in an unseen context (for example they appear after a word\nthey never appeared after in training)? To keep a language model from assigning\nzero probability to these unseen events, we\xe2\x80\x99ll have to shave off a bit of probability\nmass from some more frequent events and give it to the events we\xe2\x80\x99ve never seen.\nThis modi\xef\xac\x81cation is called smoothing or discounting. In this section and the fol-\nlowing ones we\xe2\x80\x99ll introduce a variety of ways to do smoothing: add-1 smoothing,\nadd-k smoothing, stupid backoff, and Kneser-Ney smoothing.\n\n3.4.1 Laplace Smoothing\nThe simplest way to do smoothing is to add one to all the bigram counts, before\nwe normalize them into probabilities. All the counts that used to be zero will now\nhave a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\nLaplace smoothing. Laplace smoothing does not perform well enough to be used\nin modern n-gram models, but it usefully introduces many of the concepts that we\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\nsmoothing algorithm for other tasks like text classi\xef\xac\x81cation (Chapter 4).\n\nLet\xe2\x80\x99s start with the application of Laplace smoothing to unigram probabilities.\nRecall that the unsmoothed maximum likelihood estimate of the unigram probability\nof the word wi is its count ci normalized by the total number of word tokens N:\n\nP(wi) =\n\nci\nN\n\nadd-one\n\nLaplace smoothing merely adds one to each count (hence its alternate name add-\none smoothing). Since there are V words in the vocabulary and each one was incre-\nmented, we also need to adjust the denominator to take into account the extra V\nobservations. (What happens to our P values if we don\xe2\x80\x99t increase the denominator?)\n\nPLaplace(wi) =\n\nci + 1\nN +V\n\n(3.20)\n\nInstead of changing both the numerator and denominator, it is convenient to\ndescribe how a smoothing algorithm affects the numerator, by de\xef\xac\x81ning an adjusted\ncount c\xe2\x88\x97. This adjusted count is easier to compare directly with the MLE counts and\ncan be turned into a probability like an MLE count by normalizing by N. To de\xef\xac\x81ne\nthis count, since we are only changing the numerator in addition to adding 1 we\xe2\x80\x99ll\nalso need to multiply by a normalization factor N\n\nc\xe2\x88\x97\ni = (ci + 1)\n\nN+V :\nN\n\nN +V\n\n(3.21)\n\ndiscounting\n\ndiscount\n\nWe can now turn c\xe2\x88\x97\n\ni into a probability P\xe2\x88\x97\n\ni by normalizing by N.\n\nA related way to view smoothing is as discounting (lowering) some non-zero\ncounts in order to get the probability mass that will be assigned to the zero counts.\nThus, instead of referring to the discounted counts c\xe2\x88\x97, we might describe a smooth-\ning algorithm in terms of a relative discount dc,\nthe ratio of the discounted counts\nto the original counts:\n\n\x0c14 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\ndc =\n\nc\xe2\x88\x97\nc\n\nNow that we have the intuition for the unigram case, let\xe2\x80\x99s smooth our Berkeley\nRestaurant Project bigrams. Figure 3.5 shows the add-one smoothed counts for the\nbigrams in Fig. 3.1.\n\ni\n6\n3\n3\n1\n2\n16\n3\n2\n\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\nFigure 3.5 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.\n\nchinese\n1\n7\n3\n17\n1\n2\n1\n1\n\nspend\n3\n2\n212\n1\n1\n1\n1\n1\n\nlunch\n1\n6\n7\n43\n2\n1\n1\n1\n\nwant\n828\n1\n1\n1\n1\n1\n1\n1\n\nfood\n1\n7\n1\n3\n83\n5\n2\n1\n\nto\n1\n609\n5\n3\n1\n16\n1\n2\n\neat\n10\n2\n687\n1\n1\n1\n1\n1\n\nFigure 3.6 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2.\nRecall that normal bigram probabilities are computed by normalizing each row of\ncounts by the unigram count:\n\nP(wn|wn\xe2\x88\x921) =\n\nC(wn\xe2\x88\x921wn)\nC(wn\xe2\x88\x921)\n\n(3.22)\n\nFor add-one smoothed bigram counts, we need to augment the unigram count by\n\nthe number of total word types in the vocabulary V :\n\n(cid:80)\n\nP\xe2\x88\x97\nLaplace(wn|wn\xe2\x88\x921) =\n\nC(wn\xe2\x88\x921wn) + 1\nw (C(wn\xe2\x88\x921w) + 1)\n\n=\n\nC(wn\xe2\x88\x921wn) + 1\nC(wn\xe2\x88\x921) +V\n\n(3.23)\n\nThus, each of the unigram counts given in the previous section will need to be\naugmented by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.6.\n\ni\n0.0015\n0.0013\n0.00078\n0.00046\n0.0012\n0.0063\n0.0017\n0.0012\n\nspend\n0.00075\ni\n0.00084\nwant\n0.055\nto\n0.00046\neat\n0.00062\nchinese\n0.00039\nfood\n0.00056\nlunch\nspend\n0.00058\nFigure 3.6 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\ncorpus of 9332 sentences. Previously-zero probabilities are in gray.\n\nwant\n0.21\n0.00042\n0.00026\n0.00046\n0.00062\n0.00039\n0.00056\n0.00058\n\neat\n0.0025\n0.00084\n0.18\n0.00046\n0.00062\n0.00039\n0.00056\n0.00058\n\nchinese\n0.00025\n0.0029\n0.00078\n0.0078\n0.00062\n0.00079\n0.00056\n0.00058\n\nfood\n0.00025\n0.0029\n0.00026\n0.0014\n0.052\n0.002\n0.0011\n0.00058\n\nto\n0.00025\n0.26\n0.0013\n0.0014\n0.00062\n0.0063\n0.00056\n0.0012\n\nlunch\n0.00025\n0.0025\n0.0018\n0.02\n0.0012\n0.00039\n0.00056\n0.00058\n\nIt is often convenient to reconstruct the count matrix so we can see how much a\nsmoothing algorithm has changed the original counts. These adjusted counts can be\ncomputed by Eq. 3.24. Figure 3.7 shows the reconstructed counts.\n\nc\xe2\x88\x97(wn\xe2\x88\x921wn) =\n\n[C(wn\xe2\x88\x921wn) + 1]\xc3\x97C(wn\xe2\x88\x921)\n\nC(wn\xe2\x88\x921) +V\n\n(3.24)\n\n\x0c3.4\n\n\xe2\x80\xa2 SMOOTHING\n\n15\n\ni\n3.8\n1.2\n1.9\n0.34\n0.2\n6.9\n0.57\n0.32\n\ni\nwant\nto\neat\nchinese\nfood\nlunch\nspend\nFigure 3.7 Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus\nof 9332 sentences. Previously-zero counts are in gray.\n\nchinese\n0.64\n2.7\n1.9\n5.8\n0.098\n0.86\n0.19\n0.16\n\nspend\n1.9\n0.78\n133\n0.34\n0.098\n0.43\n0.19\n0.16\n\nlunch\n0.64\n2.3\n4.4\n15\n0.2\n0.43\n0.19\n0.16\n\neat\n6.4\n0.78\n430\n0.34\n0.098\n0.43\n0.19\n0.16\n\nwant\n527\n0.39\n0.63\n0.34\n0.098\n0.43\n0.19\n0.16\n\nto\n0.64\n238\n3.1\n1\n0.098\n6.9\n0.19\n0.32\n\nfood\n0.64\n2.7\n0.63\n1\n8.2\n2.2\n0.38\n0.16\n\nNote that add-one smoothing has made a very big change to the counts. C(want to)\nchanged from 609 to 238! We can see this in probability space as well: P(to|want)\ndecreases from .66 in the unsmoothed case to .26 in the smoothed case. Looking at\nthe discount d (the ratio between new and old counts) shows us how strikingly the\ncounts for each pre\xef\xac\x81x word have been reduced; the discount for the bigram want to\nis .39, while the discount for Chinese food is .10, a factor of 10!\n\nThe sharp change in counts and probabilities occurs because too much probabil-\n\nity mass is moved to all the zeros.\n\n3.4.2 Add-k smoothing\nOne alternative to add-one smoothing is to move a bit less of the probability mass\nfrom the seen to the unseen events. Instead of adding 1 to each count, we add a frac-\ntional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.\n\nadd-k\n\nP\xe2\x88\x97\nAdd-k(wn|wn\xe2\x88\x921) =\n\nC(wn\xe2\x88\x921wn) + k\nC(wn\xe2\x88\x921) + kV\n\n(3.25)\n\nAdd-k smoothing requires that we have a method for choosing k; this can be\ndone, for example, by optimizing on a devset. Although add-k is useful for some\ntasks (including text classi\xef\xac\x81cation), it turns out that it still doesn\xe2\x80\x99t work well for\nlanguage modeling, generating counts with poor variances and often inappropriate\ndiscounts (Gale and Church, 1994).\n\n3.4.3 Backoff and Interpolation\nThe discounting we have been discussing so far can help solve the problem of zero\nfrequency n-grams. But there is an additional source of knowledge we can draw on.\nIf we are trying to compute P(wn|wn\xe2\x88\x922wn\xe2\x88\x921) but we have no examples of a particular\ntrigram wn\xe2\x88\x922wn\xe2\x88\x921wn, we can instead estimate its probability by using the bigram\nprobability P(wn|wn\xe2\x88\x921). Similarly, if we don\xe2\x80\x99t have counts to compute P(wn|wn\xe2\x88\x921),\nwe can look to the unigram P(wn).\n\nIn other words, sometimes using less context is a good thing, helping to general-\nize more for contexts that the model hasn\xe2\x80\x99t learned much about. There are two ways\nto use this n-gram \xe2\x80\x9chierarchy\xe2\x80\x9d. In backoff, we use the trigram if the evidence is\nsuf\xef\xac\x81cient, otherwise we use the bigram, otherwise the unigram. In other words, we\nonly \xe2\x80\x9cback off\xe2\x80\x9d to a lower-order n-gram if we have zero evidence for a higher-order\nn-gram. By contrast, in interpolation, we always mix the probability estimates from\nall the n-gram estimators, weighing and combining the trigram, bigram, and unigram\ncounts.\n\nbackoff\n\ninterpolation\n\n\x0c16 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nIn simple linear interpolation, we combine different order n-grams by linearly in-\nterpolating all the models. Thus, we estimate the trigram probability P(wn|wn\xe2\x88\x922wn\xe2\x88\x921)\nby mixing together the unigram, bigram, and trigram probabilities, each weighted\nby a \xce\xbb :\n\n\xcb\x86P(wn|wn\xe2\x88\x922wn\xe2\x88\x921) = \xce\xbb1P(wn|wn\xe2\x88\x922wn\xe2\x88\x921)\n\n+\xce\xbb2P(wn|wn\xe2\x88\x921)\n+\xce\xbb3P(wn)\n\nsuch that the \xce\xbb s sum to 1:\n\n(cid:88)\n\ni\n\n\xce\xbbi = 1\n\n(3.26)\n\n(3.27)\n\nIn a slightly more sophisticated version of linear interpolation, each \xce\xbb weight is\ncomputed by conditioning on the context. This way, if we have particularly accurate\ncounts for a particular bigram, we assume that the counts of the trigrams based on\nthis bigram will be more trustworthy, so we can make the \xce\xbb s for those trigrams\nhigher and thus give that trigram more weight in the interpolation. Equation 3.28\nshows the equation for interpolation with context-conditioned weights:\n\n\xcb\x86P(wn|wn\xe2\x88\x922wn\xe2\x88\x921) = \xce\xbb1(wn\xe2\x88\x921\n+\xce\xbb2(wn\xe2\x88\x921\n+ \xce\xbb3(wn\xe2\x88\x921\n\nn\xe2\x88\x922)P(wn|wn\xe2\x88\x922wn\xe2\x88\x921)\nn\xe2\x88\x922)P(wn|wn\xe2\x88\x921)\nn\xe2\x88\x922)P(wn)\n\n(3.28)\n\nheld-out\n\ndiscount\n\nKatz backoff\n\nHow are these \xce\xbb values set? Both the simple interpolation and conditional inter-\npolation \xce\xbb s are learned from a held-out corpus. A held-out corpus is an additional\ntraining corpus that we use to set hyperparameters like these \xce\xbb values, by choosing\nthe \xce\xbb values that maximize the likelihood of the held-out corpus. That is, we \xef\xac\x81x\nthe n-gram probabilities and then search for the \xce\xbb values that\xe2\x80\x94when plugged into\nEq. 3.26\xe2\x80\x94give us the highest probability of the held-out set. There are various ways\nto \xef\xac\x81nd this optimal set of \xce\xbb s. One way is to use the EM algorithm, an iterative\nlearning algorithm that converges on locally optimal \xce\xbb s (Jelinek and Mercer, 1980).\nIn a backoff n-gram model, if the n-gram we need has zero counts, we approxi-\nmate it by backing off to the (N-1)-gram. We continue backing off until we reach a\nhistory that has some counts.\n\nIn order for a backoff model to give a correct probability distribution, we have\nto discount the higher-order n-grams to save some probability mass for the lower\norder n-grams. Just as with add-one smoothing, if the higher-order n-grams aren\xe2\x80\x99t\ndiscounted and we just used the undiscounted MLE probability, then as soon as we\nreplaced an n-gram which has zero probability with a lower-order n-gram, we would\nbe adding probability mass, and the total probability assigned to all possible strings\nby the language model would be greater than 1! In addition to this explicit discount\nfactor, we\xe2\x80\x99ll need a function \xce\xb1 to distribute this probability mass to the lower order\nn-grams.\nThis kind of backoff with discounting is also called Katz backoff. In Katz back-\noff we rely on a discounted probability P\xe2\x88\x97 if we\xe2\x80\x99ve seen this n-gram before (i.e., if\nwe have non-zero counts). Otherwise, we recursively back off to the Katz probabil-\nity for the shorter-history (N-1)-gram. The probability for a backoff n-gram PBO is\n\n\x0c3.5\n\n\xe2\x80\xa2 KNESER-NEY SMOOTHING\n\n17\n\nthus computed as follows:\n\nPBO(wn|wn\xe2\x88\x921\n\nn\xe2\x88\x92N+1) =\n\n\xef\xa3\xb1\xef\xa3\xb2\xef\xa3\xb3 P\xe2\x88\x97(wn|wn\xe2\x88\x921\n\n\xce\xb1(wn\xe2\x88\x921\n\nn\xe2\x88\x92N+1),\n\nn\xe2\x88\x92N+1)PBO(wn|wn\xe2\x88\x921\n\nn\xe2\x88\x92N+2),\n\nn\xe2\x88\x92N+1) > 0\n\nif C(wn\notherwise.\n\nGood-Turing\n\n(3.29)\nKatz backoff is often combined with a smoothing method called Good-Turing.\nThe combined Good-Turing backoff algorithm involves quite detailed computation\nfor estimating the Good-Turing smoothing and the P\xe2\x88\x97 and \xce\xb1 values.\n\n3.5 Kneser-Ney Smoothing\n\nKneser-Ney\n\nOne of the most commonly used and best performing n-gram smoothing methods\nis the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Good-\nman 1998).\n\nKneser-Ney has its roots in a method called absolute discounting. Recall that\ndiscounting of the counts for frequent n-grams is necessary to save some probability\nmass for the smoothing algorithm to distribute to the unseen n-grams.\n\nTo see this, we can use a clever idea from Church and Gale (1991). Consider\nan n-gram that has count 4. We need to discount this count by some amount. But\nhow much should we discount it? Church and Gale\xe2\x80\x99s clever idea was to look at a\nheld-out corpus and just see what the count is for all those bigrams that had count\n4 in the training set. They computed a bigram grammar from 22 million words of\nAP newswire and then checked the counts of each of these bigrams in another 22\nmillion words. On average, a bigram that occurred 4 times in the \xef\xac\x81rst 22 million\nwords occurred 3.23 times in the next 22 million words. The following table from\nChurch and Gale (1991) shows these counts for bigrams with c from 0 to 9:\n\nBigram count in Bigram count in\n\ntraining set heldout set\n0 0.0000270\n1 0.448\n2 1.25\n3 2.24\n4 3.23\n5 4.21\n6 5.23\n7 6.21\n8 7.21\n9 8.26\n\nFigure 3.8 For all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the\ncounts of these bigrams in a held-out corpus also of 22 million words.\n\nAbsolute\ndiscounting\n\nThe astute reader may have noticed that except for the held-out counts for 0\nand 1, all the other bigram counts in the held-out set could be estimated pretty well\nby just subtracting 0.75 from the count in the training set! Absolute discounting\nformalizes this intuition by subtracting a \xef\xac\x81xed (absolute) discount d from each count.\nThe intuition is that since we have good estimates already for the very high counts, a\nsmall discount d won\xe2\x80\x99t affect them much. It will mainly modify the smaller counts,\n\n\x0c18 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nfor which we don\xe2\x80\x99t necessarily trust the estimate anyway, and Fig. 3.8 suggests that\nin practice this discount is actually a good one for bigrams with counts 2 through 9.\nThe equation for interpolated absolute discounting applied to bigrams:\n\nPAbsoluteDiscounting(wi|wi\xe2\x88\x921) =\n\n(cid:80)\n\nC(wi\xe2\x88\x921wi)\xe2\x88\x92 d\nvC(wi\xe2\x88\x921 v)\n\n+ \xce\xbb (wi\xe2\x88\x921)P(wi)\n\n(3.30)\n\nThe \xef\xac\x81rst term is the discounted bigram, and the second term is the unigram with\nan interpolation weight \xce\xbb . We could just set all the d values to .75, or we could keep\na separate discount value of 0.5 for the bigrams with counts of 1.\n\nKneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount-\ning with a more sophisticated way to handle the lower-order unigram distribution.\nConsider the job of predicting the next word in this sentence, assuming we are inter-\npolating a bigram and a unigram model.\n\nI can\xe2\x80\x99t see without my reading\n\n.\n\nThe word glasses seems much more likely to follow here than, say, the word\nKong, so we\xe2\x80\x99d like our unigram model to prefer glasses. But in fact it\xe2\x80\x99s Kong that is\nmore common, since Hong Kong is a very frequent word. A standard unigram model\nwill assign Kong a higher probability than glasses. We would like to capture the\nintuition that although Kong is frequent, it is mainly only frequent in the phrase Hong\nKong, that is, after the word Hong. The word glasses has a much wider distribution.\nIn other words, instead of P(w), which answers the question \xe2\x80\x9cHow likely is\nw?\xe2\x80\x9d, we\xe2\x80\x99d like to create a unigram model that we might call PCONTINUATION, which\nanswers the question \xe2\x80\x9cHow likely is w to appear as a novel continuation?\xe2\x80\x9d. How can\nwe estimate this probability of seeing the word w as a novel continuation, in a new\nunseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION\non the number of different contexts word w has appeared in, that is, the number of\nbigram types it completes. Every bigram type was a novel continuation the \xef\xac\x81rst time\nit was seen. We hypothesize that words that have appeared in more contexts in the\npast are more likely to appear in some new context as well. The number of times a\nword w appears as a novel continuation can be expressed as:\nPCONTINUATION(w) \xe2\x88\x9d |{v : C(vw) > 0}|\n\n(3.31)\n\nTo turn this count into a probability, we normalize by the total number of word\nbigram types. In summary:\n\nPCONTINUATION(w) =\n\n|{v : C(vw) > 0}|\n\n|{(u(cid:48),w(cid:48)) : C(u(cid:48)w(cid:48)) > 0}|\n\n(3.32)\n\nAn equivalent formulation based on a different metaphor is to use the number of\n\nword types seen to precede w (Eq. 3.31 repeated):\n\nPCONTINUATION(w) \xe2\x88\x9d |{v : C(vw) > 0}|\n\nnormalized by the number of words preceding all words, as follows:\n\nPCONTINUATION(w) =\n\n(cid:80)\n|{v : C(vw) > 0}|\nw(cid:48) |{v : C(vw(cid:48)) > 0}|\n\n(3.33)\n\n(3.34)\n\nA frequent word (Kong) occurring in only one context (Hong) will have a low\n\ncontinuation probability.\n\n\x0c3.6\n\n\xe2\x80\xa2 THE WEB AND STUPID BACKOFF\n\n19\n\nInterpolated\nKneser-Ney\n\nThe \xef\xac\x81nal equation for Interpolated Kneser-Ney smoothing for bigrams is then:\n\nPKN(wi|wi\xe2\x88\x921) =\n\nmax(C(wi\xe2\x88\x921wi)\xe2\x88\x92 d,0)\n\nC(wi\xe2\x88\x921)\n\n+ \xce\xbb (wi\xe2\x88\x921)PCONTINUATION(wi)\n\n(3.35)\n\n\xce\xbb (wi\xe2\x88\x921) =\n\nd(cid:80)\n\nThe \xce\xbb is a normalizing constant that is used to distribute the probability mass\n\nwe\xe2\x80\x99ve discounted.:\n\nd(cid:80)\n\nvC(wi\xe2\x88\x921v)\n\n|{w : C(wi\xe2\x88\x921w) > 0}|\n\n(3.36)\n\nThe \xef\xac\x81rst term,\n\n, is the normalized discount. The second term,\n|{w : C(wi\xe2\x88\x921w) > 0}|, is the number of word types that can follow wi\xe2\x88\x921 or, equiva-\nlently, the number of word types that we discounted; in other words, the number of\ntimes we applied the normalized discount.\n\nvC(wi\xe2\x88\x921v)\n\nThe general recursive formulation is as follows:\n\nPKN(wi|wi\xe2\x88\x921\n\ni\xe2\x88\x92n+1) =\n\nmax(cKN(wi\n\n(cid:80)\nv cKN(wi\xe2\x88\x921\n\ni\xe2\x88\x92n+1)\xe2\x88\x92 d,0)\ni\xe2\x88\x92n+1v)\n\n+ \xce\xbb (wi\xe2\x88\x921\n\ni\xe2\x88\x92n+1)PKN(wi|wi\xe2\x88\x921\n\ni\xe2\x88\x92n+2) (3.37)\n\nwhere the de\xef\xac\x81nition of the count cKN depends on whether we are counting the\nhighest-order n-gram being interpolated (for example trigram if we are interpolating\ntrigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram\nif we are interpolating trigram, bigram, and unigram):\n\ncKN(\xc2\xb7) =\n\nfor the highest order\n\ncontinuationcount(\xc2\xb7)\n\nfor lower orders\n\n(3.38)\n\nThe continuation count is the number of unique single word contexts for \xc2\xb7.\n\nAt the termination of the recursion, unigrams are interpolated with the uniform\n\ndistribution, where the parameter \xce\xb5 is the empty string:\n\nPKN(w) =\n\n(cid:80)\n\nmax(cKN(w)\xe2\x88\x92 d,0)\n\nw(cid:48) cKN(w(cid:48))\n\n+ \xce\xbb (\xce\xb5)\n\n1\nV\n\n(3.39)\n\n(cid:26) count(\xc2\xb7)\n\nmodi\xef\xac\x81ed\nKneser-Ney\n\nIf we want to include an unknown word <UNK>, it\xe2\x80\x99s just included as a regular vo-\ncabulary entry with count zero, and hence its probability will be a lambda-weighted\nuniform distribution \xce\xbb (\xce\xb5)\nV .\n\nThe best-performing version of Kneser-Ney smoothing is called modi\xef\xac\x81ed Kneser-\n\nNey smoothing, and is due to Chen and Goodman (1998). Rather than use a single\n\xef\xac\x81xed discount d, modi\xef\xac\x81ed Kneser-Ney uses three different discounts d1, d2, and\nd3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and\nGoodman (1998, p. 19) or Hea\xef\xac\x81eld et al. (2013) for the details.\n\n3.6 The Web and Stupid Backoff\n\nBy using text from the web, it is possible to build extremely large language mod-\nels. In 2006 Google released a very large set of n-gram counts, including n-grams\n(1-grams through 5-grams) from all the \xef\xac\x81ve-word sequences that appear at least\n\n\x0c20 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\n40 times from 1,024,908,267,229 words of running text on the web; this includes\n1,176,470,663 \xef\xac\x81ve-word sequences using over 13 million unique words types (Franz\nand Brants, 2006). Some examples:\n\n4-gram\nserve as the incoming\nserve as the incubator\nserve as the independent\nserve as the index\nserve as the indication\nserve as the indicator\nserve as the indicators\nserve as the indispensable\nserve as the indispensible\nserve as the individual\n\nCount\n92\n99\n794\n223\n72\n120\n45\n111\n40\n234\n\nEf\xef\xac\x81ciency considerations are important when building language models that use\nsuch large sets of n-grams. Rather than store each word as a string, it is generally\nrepresented in memory as a 64-bit hash number, with the words themselves stored\non disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte\n\xef\xac\x82oats), and n-grams are stored in reverse tries.\n\nN-grams can also be shrunk by pruning, for example only storing n-grams with\ncounts greater than some threshold (such as the count threshold of 40 used for the\nGoogle n-gram release) or using entropy to prune less-important n-grams (Stolcke,\n1998). Another option is to build approximate language models using techniques\nlike Bloom \xef\xac\x81lters (Talbot and Osborne 2007, Church et al. 2007). Finally, ef\xef\xac\x81-\ncient language model toolkits like KenLM (Hea\xef\xac\x81eld 2011, Hea\xef\xac\x81eld et al. 2013) use\nsorted arrays, ef\xef\xac\x81ciently combine probabilities and backoffs in a single value, and\nuse merge sorts to ef\xef\xac\x81ciently build the probability tables in a minimal number of\npasses through a large corpus.\n\nAlthough with these toolkits it is possible to build web-scale language models\nusing full Kneser-Ney smoothing, Brants et al. (2007) show that with very large lan-\nguage models a much simpler algorithm may be suf\xef\xac\x81cient. The algorithm is called\nstupid backoff. Stupid backoff gives up the idea of trying to make the language\nmodel a true probability distribution. There is no discounting of the higher-order\nprobabilities. If a higher-order n-gram has a zero count, we simply backoff to a\nlower order n-gram, weighed by a \xef\xac\x81xed (context-independent) weight. This algo-\nrithm does not produce a probability distribution, so we\xe2\x80\x99ll follow Brants et al. (2007)\nin referring to it as S:\n\nBloom \xef\xac\x81lters\n\nstupid backoff\n\n\xef\xa3\xb1\xef\xa3\xb2\xef\xa3\xb3 count(wi\n\nS(wi|wi\xe2\x88\x921\n\ni\xe2\x88\x92k+1) =\n\ni\xe2\x88\x92k+1)\nif count(wi\ncount(wi\xe2\x88\x921\ni\xe2\x88\x92k+1)\n\xce\xbb S(wi|wi\xe2\x88\x921\ni\xe2\x88\x92k+2) otherwise\n\ni\xe2\x88\x92k+1) > 0\n\n(3.40)\n\nThe backoff terminates in the unigram, which has probability S(w) = count(w)\net al. (2007) \xef\xac\x81nd that a value of 0.4 worked well for \xce\xbb .\n\nN\n\n. Brants\n\n3.7 Advanced: Perplexity\xe2\x80\x99s Relation to Entropy\n\nWe introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on\na test set. A better n-gram model is one that assigns a higher probability to the\n\n\x0c3.7\n\n\xe2\x80\xa2 ADVANCED: PERPLEXITY\xe2\x80\x99S RELATION TO ENTROPY\n\n21\n\nEntropy\n\ntest data, and perplexity is a normalized version of the probability of the test set.\nThe perplexity measure actually arises from the information-theoretic concept of\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\nthe inverse probability, for example?) and its relationship to entropy. Entropy is a\nmeasure of information. Given a random variable X ranging over whatever we are\npredicting (words, letters, parts of speech, the set of which we\xe2\x80\x99ll call \xcf\x87) and with a\nparticular probability function, call it p(x), the entropy of the random variable X is:\n\nH(X) = \xe2\x88\x92(cid:88)\n\nx\xe2\x88\x88\xcf\x87\n\np(x)log2 p(x)\n\n(3.41)\n\nThe log can, in principle, be computed in any base. If we use log base 2, the\n\nresulting value of entropy will be measured in bits.\n\nOne intuitive way to think about entropy is as a lower bound on the number of\nbits it would take to encode a certain decision or piece of information in the optimal\ncoding scheme.\n\nConsider an example from the standard information theory textbook Cover and\nThomas (1991). Imagine that we want to place a bet on a horse race but it is too\nfar to go all the way to Yonkers Racetrack, so we\xe2\x80\x99d like to send a short message to\nthe bookie to tell him which of the eight horses to bet on. One way to encode this\nmessage is just to use the binary representation of the horse\xe2\x80\x99s number as the code;\nthus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded\nas 000. If we spend the whole day betting and each horse is coded with 3 bits, on\naverage we would be sending 3 bits per race.\n\nCan we do better? Suppose that the spread is the actual distribution of the bets\n\nplaced and that we represent it as the prior probability of each horse as follows:\n\nHorse 1 1\nHorse 5 1\n2\n64\nHorse 2 1\nHorse 6 1\n4\n64\nHorse 3 1\nHorse 7 1\n64\n8\nHorse 4 1\n16 Horse 8 1\n64\n\nThe entropy of the random variable X that ranges over horses gives us a lower\n\nbound on the number of bits and is\n\nH(X) = \xe2\x88\x92\n\np(i)log p(i)\n\ni=8(cid:88)\n\ni=1\n= \xe2\x88\x92 1\n2 log 1\n= 2 bits\n\n2\xe2\x88\x92 1\n\n4 log 1\n\n4\xe2\x88\x92 1\n\n8 log 1\n\n8\xe2\x88\x92 1\n\n16 log 1\n\n16\xe2\x88\x924( 1\n\n64 log 1\n64 )\n\n(3.42)\nA code that averages 2 bits per race can be built with short encodings for more\nprobable horses, and longer encodings for less probable horses. For example, we\ncould encode the most likely horse with the code 0, and the remaining horses as 10,\nthen 110, 1110, 111100, 111101, 111110, and 111111.\n\nWhat if the horses are equally likely? We saw above that if we used an equal-\nlength binary code for the horse numbers, each horse took 3 bits to code, so the\naverage was 3.\nIs the entropy the same? In this case each horse would have a\nprobability of 1\n\n8. The entropy of the choice of horses is then\n\nH(X) = \xe2\x88\x92\n\n1\n8\n\nlog\n\n1\n8\n\n= \xe2\x88\x92log\n\n1\n8\n\n= 3 bits\n\n(3.43)\n\ni=8(cid:88)\n\ni=1\n\n\x0c22 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nUntil now we have been computing the entropy of a single variable. But most of\nwhat we will use entropy for involves sequences. For a grammar, for example, we\nwill be computing the entropy of some sequence of words W = {w0,w1,w2, . . . ,wn}.\nOne way to do this is to have a variable that ranges over sequences of words. For\nexample we can compute the entropy of a random variable that ranges over all \xef\xac\x81nite\nsequences of words of length n in some language L as follows:\n\np(W n\n\n1 )log p(W n\n1 )\n\n(3.44)\n\nWe could de\xef\xac\x81ne the entropy rate (we could also think of this as the per-word\n\nentropy) as the entropy of this sequence divided by the number of words:\n\nH(w1,w2, . . . ,wn) = \xe2\x88\x92(cid:88)\n\n1 \xe2\x88\x88L\nW n\n\n1\nn\n\nH(W n\n\n1 ) = \xe2\x88\x921\nn\n\n(cid:88)\n\n1 \xe2\x88\x88L\nW n\n\np(W n\n\n1 )log p(W n\n1 )\n\n(3.45)\n\nBut to measure the true entropy of a language, we need to consider sequences of\nin\xef\xac\x81nite length. If we think of a language as a stochastic process L that produces a\nsequence of words, and allow W to represent the sequence of words w1, . . . ,wn, then\nL\xe2\x80\x99s entropy rate H(L) is de\xef\xac\x81ned as\n\n1\nH(L) = lim\nn\xe2\x86\x92\xe2\x88\x9e\nn\n= \xe2\x88\x92 lim\nn\xe2\x86\x92\xe2\x88\x9e\n\n(cid:88)\n\nH(w1,w2, . . . ,wn)\n1\nn\n\nW\xe2\x88\x88L\n\np(w1, . . . ,wn)log p(w1, . . . ,wn)\n\n(3.46)\n\nThe Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and\nThomas 1991) states that if the language is regular in certain ways (to be exact, if it\nis both stationary and ergodic),\n\nH(L) = lim\nn\xe2\x86\x92\xe2\x88\x9e\n\n\xe2\x88\x921\nn\n\nlog p(w1w2 . . .wn)\n\n(3.47)\n\nThat is, we can take a single sequence that is long enough instead of summing\nover all possible sequences. The intuition of the Shannon-McMillan-Breiman the-\norem is that a long-enough sequence of words will contain in it many other shorter\nsequences and that each of these shorter sequences will reoccur in the longer se-\nquence according to their probabilities.\n\nA stochastic process is said to be stationary if the probabilities it assigns to a\nsequence are invariant with respect to shifts in the time index. In other words, the\nprobability distribution for words at time t is the same as the probability distribution\nat time t + 1. Markov models, and hence n-grams, are stationary. For example, in\na bigram, Pi is dependent only on Pi\xe2\x88\x921. So if we shift our time index by x, Pi+x is\nstill dependent on Pi+x\xe2\x88\x921. But natural language is not stationary, since as we show\nin Chapter 12, the probability of upcoming words can be dependent on events that\nwere arbitrarily distant and time dependent. Thus, our statistical models only give\nan approximation to the correct distributions and entropies of natural language.\n\nTo summarize, by making some incorrect but convenient simplifying assump-\ntions, we can compute the entropy of some stochastic process by taking a very long\nsample of the output and computing its average log probability.\n\nNow we are ready to introduce cross-entropy. The cross-entropy is useful when\nIt\n\nwe don\xe2\x80\x99t know the actual probability distribution p that generated some data.\n\nentropy rate\n\nStationary\n\ncross-entropy\n\n\x0c3.7\n\n\xe2\x80\xa2 ADVANCED: PERPLEXITY\xe2\x80\x99S RELATION TO ENTROPY\n\n23\n\nallows us to use some m, which is a model of p (i.e., an approximation to p). The\ncross-entropy of m on p is de\xef\xac\x81ned by\n\nH(p,m) = lim\nn\xe2\x86\x92\xe2\x88\x9e\n\n\xe2\x88\x921\nn\n\n(cid:88)\n\nW\xe2\x88\x88L\n\np(w1, . . . ,wn)logm(w1, . . . ,wn)\n\n(3.48)\n\nThat is, we draw sequences according to the probability distribution p, but sum\n\nthe log of their probabilities according to m.\n\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary er-\n\ngodic process:\n\nH(p,m) = lim\nn\xe2\x86\x92\xe2\x88\x9e\n\n\xe2\x88\x921\nn\n\nlogm(w1w2 . . .wn)\n\n(3.49)\n\nThis means that, as for entropy, we can estimate the cross-entropy of a model\nm on some distribution p by taking a single sequence that is long enough instead of\nsumming over all possible sequences.\n\nWhat makes the cross-entropy useful is that the cross-entropy H(p,m) is an up-\n\nper bound on the entropy H(p). For any model m:\nH(p) \xe2\x89\xa4 H(p,m)\n\n(3.50)\n\nThis means that we can use some simpli\xef\xac\x81ed model m to help estimate the true en-\ntropy of a sequence of symbols drawn according to probability p. The more accurate\nm is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus,\nthe difference between H(p,m) and H(p) is a measure of how accurate a model is.\nBetween two models m1 and m2, the more accurate model will be the one with the\nlower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\na model cannot err by underestimating the true entropy.)\n\nWe are \xef\xac\x81nally ready to see the relation between perplexity and cross-entropy as\nwe saw it in Eq. 3.49. Cross-entropy is de\xef\xac\x81ned in the limit, as the length of the\nobserved word sequence goes to in\xef\xac\x81nity. We will need an approximation to cross-\nentropy, relying on a (suf\xef\xac\x81ciently long) sequence of \xef\xac\x81xed length. This approxima-\ntion to the cross-entropy of a model M = P(wi|wi\xe2\x88\x92N+1...wi\xe2\x88\x921) on a sequence of\nwords W is\n\nH(W ) = \xe2\x88\x92 1\nN\n\nlogP(w1w2 . . .wN)\n\n(3.51)\n\nperplexity\n\nThe perplexity of a model P on a sequence of words W is now formally de\xef\xac\x81ned as\nthe exp of this cross-entropy:\n\nPerplexity(W ) = 2H(W )\n\n= P(w1w2 . . .wN)\xe2\x88\x92 1\n\nN\n\n1\n\nP(w1w2 . . .wN)\n\n(cid:115)\n(cid:118)(cid:117)(cid:117)(cid:116) N(cid:89)\n\ni=1\n\n= N\n\n= N\n\n1\n\nP(wi|w1 . . .wi\xe2\x88\x921)\n\n(3.52)\n\n\x0c24 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\n3.8 Summary\n\nThis chapter introduced language modeling and the n-gram, one of the most widely\nused tools in language processing.\n\n\xe2\x80\xa2 Language models offer a way to assign a probability to a sentence or other\n\nsequence of words, and to predict a word from preceding words.\n\n\xe2\x80\xa2 n-grams are Markov models that estimate words from a \xef\xac\x81xed window of pre-\nvious words. n-gram probabilities can be estimated by counting in a corpus\nand normalizing (the maximum likelihood estimate).\n\n\xe2\x80\xa2 n-gram language models are evaluated extrinsically in some task, or intrinsi-\n\ncally using perplexity.\n\n\xe2\x80\xa2 The perplexity of a test set according to a language model is the geometric\n\nmean of the inverse test set probability computed by the model.\n\n\xe2\x80\xa2 Smoothing algorithms provide a more sophisticated way to estimate the prob-\nability of n-grams. Commonly used smoothing algorithms for n-grams rely on\nlower-order n-gram counts through backoff or interpolation.\n\n\xe2\x80\xa2 Both backoff and interpolation require discounting to create a probability dis-\n\ntribution.\n\n\xe2\x80\xa2 Kneser-Ney smoothing makes use of the probability of a word being a novel\ncontinuation. The interpolated Kneser-Ney smoothing algorithm mixes a\ndiscounted probability with a lower-order continuation probability.\n\nBibliographical and Historical Notes\n\nThe underlying mathematics of the n-gram was \xef\xac\x81rst proposed by Markov (1913),\nwho used what are now called Markov chains (bigrams and trigrams) to predict\nwhether an upcoming letter in Pushkin\xe2\x80\x99s Eugene Onegin would be a vowel or a con-\nsonant. Markov classi\xef\xac\x81ed 20,000 letters as V or C and computed the bigram and\ntrigram probability that a given letter would be a vowel given the previous one or\ntwo letters. Shannon (1948) applied n-grams to compute approximations to English\nword sequences. Based on Shannon\xe2\x80\x99s work, Markov models were commonly used in\nengineering, linguistic, and psychological work on modeling word sequences by the\n1950s. In a series of extremely in\xef\xac\x82uential papers starting with Chomsky (1956) and\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\nthat \xe2\x80\x9c\xef\xac\x81nite-state Markov processes\xe2\x80\x9d, while a possibly useful engineering heuristic,\nwere incapable of being a complete cognitive model of human grammatical knowl-\nedge. These arguments led many linguists and computational linguists to ignore\nwork in statistical modeling for decades.\n\nThe resurgence of n-gram models came from Jelinek and colleagues at the IBM\nThomas J. Watson Research Center, who were in\xef\xac\x82uenced by Shannon, and Baker\nat CMU, who was in\xef\xac\x82uenced by the work of Baum and colleagues. Independently\nthese two labs successfully used n-grams in their speech recognition systems (Baker 1975b,\nJelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). A trigram model was\nused in the IBM TANGORA speech recognition system in the 1970s, but the idea\nwas not written up until later.\n\nAdd-one smoothing derives from Laplace\xe2\x80\x99s 1812 law of succession and was \xef\xac\x81rst\napplied as an engineering solution to the zero-frequency problem by Jeffreys (1948)\n\n\x0cclass-based\nn-gram\n\nEXERCISES\n\n25\n\nbased on an earlier Add-K suggestion by Johnson (1932). Problems with the add-\none algorithm are summarized in Gale and Church (1994).\n\nA wide variety of different language modeling and smoothing techniques were\nproposed in the 80s and 90s, including Good-Turing discounting\xe2\x80\x94\xef\xac\x81rst applied to\nthe n-gram smoothing at IBM by Katz (N\xc2\xb4adas 1984, Church and Gale 1991)\xe2\x80\x94\nWitten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n-\ngram models that used information about word classes.\n\nStarting in the late 1990s, Chen and Goodman produced a highly in\xef\xac\x82uential\nseries of papers with a comparison of different language models (Chen and Good-\nman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).\nThey performed a number of carefully controlled experiments comparing differ-\nent discounting algorithms, cache models, class-based models, and other language\nmodel parameters. They showed the advantages of Modi\xef\xac\x81ed Interpolated Kneser-\nNey, which has since become the standard baseline for language modeling, espe-\ncially because they showed that caches and class-based models provided only minor\nadditional improvement. These papers are recommended for any reader with further\ninterest in language modeling.\n\nTwo commonly used toolkits for building language models are SRILM (Stolcke,\n2002) and KenLM (Hea\xef\xac\x81eld 2011, Hea\xef\xac\x81eld et al. 2013). Both are publicly available.\nSRILM offers a wider range of options and types of discounting, while KenLM is\noptimized for speed and memory size, making it possible to build web-scale lan-\nguage models.\n\nThe highest accuracy language models are neural network language models.\nThese solve a major problem with n-gram language models: the number of parame-\nters increases exponentially as the n-gram order increases, and n-grams have no way\nto generalize from training to test set. Neural language models instead project words\ninto a continuous space in which words with similar contexts have similar represen-\ntations. We\xe2\x80\x99ll introduce both feedforward language models (Bengio et al. 2006,\nSchwenk 2007) in Chapter 7, and recurrent language models (Mikolov, 2012) in\nChapter 9.\n\nExercises\n\n3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11).\nNow write out all the non-zero trigram probabilities for the I am Sam corpus\non page 4.\n\n3.2 Calculate the probability of the sentence i want chinese food. Give two\nprobabilities, one using Fig. 3.2 and the \xe2\x80\x98useful probabilities\xe2\x80\x99 just below it on\npage 6, and another using the add-1 smoothed table in Fig. 3.6. Assume the\nadditional add-1 smoothed probabilities P(i|<s>) = 0.19 and P(</s>|food) =\n0.40.\n\n3.3 Which of the two probabilities you computed in the previous exercise is higher,\n\nunsmoothed or smoothed? Explain why.\n\n3.4 We are given the following corpus, modi\xef\xac\x81ed from the one in the chapter:\n\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\n\n\x0c26 CHAPTER 3\n\n\xe2\x80\xa2 N-GRAM LANGUAGE MODELS\n\nUsing a bigram language model with add-one smoothing, what is P(Sam |\nam)? Include <s> and </s> in your counts just like any other token.\nSuppose we didn\xe2\x80\x99t use the end-symbol </s>. Train an unsmoothed bigram\ngrammar on the following training corpus without using the end-symbol </s>:\n\n3.5\n\n<s> a b\n<s> b b\n<s> b a\n<s> a a\n\n3.6\n\nDemonstrate that your bigram model does not assign a single probability dis-\ntribution across all sentence lengths by showing that the sum of the probability\nof the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the\nsum of the probability of all possible 3 word sentences over the alphabet {a,b}\nis also 1.0.\nSuppose we train a trigram language model with add-one smoothing on a\ngiven corpus. The corpus contains V word types. Express a formula for esti-\nmating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),\nin terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\nso on for bigrams and unigrams.\n\n3.7 We are given the following corpus, modi\xef\xac\x81ed from the one in the chapter:\n\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nIf we use linear interpolation smoothing between a maximum-likelihood bi-\ngram model and a maximum-likelihood unigram model with \xce\xbb1 = 1\n2 and \xce\xbb2 =\n2, what is P(Sam|am)? Include <s> and </s> in your counts just like any\n1\nother token.\n\n3.8 Write a program to compute unsmoothed unigrams and bigrams.\n3.9 Run your n-gram program on two different small corpora of your choice (you\nmight use email text or newsgroups). Now compare the statistics of the two\ncorpora. What are the differences in the most common unigrams between the\ntwo? How about interesting differences in bigrams?\n\n3.10 Add an option to your program to generate random sentences.\n3.11 Add an option to your program to compute the perplexity of a test set.\n3.12 Given a training set of 100 numbers consists of 91 zeros and 1 each of the\nother digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What\nis the unigram perplexity?\n\n\x0cAlgoet, P. H. and Cover, T. M. (1988). A sandwich proof of\nthe Shannon-McMillan-Breiman theorem. The Annals of\nProbability, 16(2), 899\xe2\x80\x93909.\n\nBahl, L. R., Jelinek, F., and Mercer, R. L. (1983). A max-\nimum likelihood approach to continuous speech recogni-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 5(2), 179\xe2\x80\x93190.\n\nBaker, J. K. (1975a). The DRAGON system \xe2\x80\x93 An overview.\nIEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing, ASSP-23(1), 24\xe2\x80\x9329.\n\nBaker, J. K. (1975b). Stochastic modeling for automatic\nIn Reddy, D. R. (Ed.), Speech\n\nspeech understanding.\nRecognition. Academic Press.\n\nBengio, Y., Schwenk, H., Sen\xc2\xb4ecal, J.-S., Morin, F., and Gau-\nvain, J.-L. (2006). Neural probabilistic language models.\nIn Innovations in Machine Learning, 137\xe2\x80\x93186. Springer.\nBlodgett, S. L. and O\xe2\x80\x99Connor, B. (2017). Racial disparity in\nnatural language processing: A case study of social media\nafrican-american english. In Fairness, Accountability, and\nTransparency in Machine Learning (FAT/ML) Workshop,\nKDD.\n\nBrants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.\n(2007). Large language models in machine translation. In\nEMNLP/CoNLL 2007.\n\nBuck, C., Hea\xef\xac\x81eld, K., and Van Ooyen, B. (2014). N-gram\ncounts and language models from the common crawl. In\nProceedings of LREC.\n\nChen, S. F. and Goodman, J. (1996). An empirical study of\nsmoothing techniques for language modeling. In ACL-96,\n310\xe2\x80\x93318.\n\nChen, S. F. and Goodman, J. (1998). An empirical study of\nsmoothing techniques for language modeling. Tech. rep.\nTR-10-98, Computer Science Group, Harvard University.\nChen, S. F. and Goodman, J. (1999). An empirical study of\nsmoothing techniques for language modeling. Computer\nSpeech and Language, 13, 359\xe2\x80\x93394.\n\nChomsky, N. (1956). Three models for the description of\nlanguage. IRE Transactions on Information Theory, 2(3),\n113\xe2\x80\x93124.\n\nChomsky, N. (1957). Syntactic Structures. Mouton, The\n\nHague.\n\nChurch, K. W. and Gale, W. A. (1991). A comparison of\nthe enhanced Good-Turing and deleted estimation methods\nfor estimating probabilities of English bigrams. Computer\nSpeech and Language, 5, 19\xe2\x80\x9354.\n\nChurch, K. W., Hart, T., and Gao, J. (2007). Compress-\nIn\n\ning trigram language models with Golomb coding.\nEMNLP/CoNLL 2007, 199\xe2\x80\x93207.\n\nCover, T. M. and Thomas, J. A. (1991). Elements of Infor-\n\nmation Theory. Wiley.\n\nFranz, A. and Brants, T. (2006). All our n-gram are belong to\nyou. http://googleresearch.blogspot.com/2006/\n08/all-our-n-gram-are-belong-to-you.html.\n\nGale, W. A. and Church, K. W. (1994). What is wrong\nwith adding one?. In Oostdijk, N. and de Haan, P. (Eds.),\nCorpus-Based Research into Language, 189\xe2\x80\x93198. Rodopi.\nGoodman, J. (2006). A bit of progress in language mod-\neling: Extended version. Tech. rep. MSR-TR-2001-72,\nMachine Learning and Applied Statistics Group, Microsoft\nResearch, Redmond, WA.\n\nExercises\n\n27\n\nHea\xef\xac\x81eld, K. (2011). KenLM: Faster and smaller language\nmodel queries. In Workshop on Statistical Machine Trans-\nlation, 187\xe2\x80\x93197.\n\nHea\xef\xac\x81eld, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.\n(2013). Scalable modi\xef\xac\x81ed Kneser-Ney language model es-\ntimation.. In ACL 2013, 690\xe2\x80\x93696.\n\nJeffreys, H. (1948). Theory of Probability (2nd Ed.). Claren-\n\ndon Press. Section 3.23.\n\nJelinek, F. (1976). Continuous speech recognition by statis-\n\ntical methods. Proceedings of the IEEE, 64(4), 532\xe2\x80\x93557.\n\nJelinek, F. (1990). Self-organized language modeling for\nIn Waibel, A. and Lee, K.-F. (Eds.),\nspeech recognition.\nReadings in Speech Recognition, 450\xe2\x80\x93506. Morgan Kauf-\nmann. Originally distributed as IBM technical report in\n1985.\n\nJelinek, F. and Mercer, R. L. (1980). Interpolated estimation\nof Markov source parameters from sparse data. In Gelsema,\nE. S. and Kanal, L. N. (Eds.), Proceedings, Workshop on\nPattern Recognition in Practice, 381\xe2\x80\x93397. North Holland.\nJohnson, W. E. (1932). Probability: deductive and inductive\n\nproblems (appendix to). Mind, 41(164), 421\xe2\x80\x93423.\n\nJurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke,\nA., Fosler, E., and Morgan, N. (1994). The Berkeley restau-\nrant project. In ICSLP-94, 2139\xe2\x80\x932142.\n\nJurgens, D., Tsvetkov, Y., and Jurafsky, D. (2017). Incorpo-\nrating dialectal variability for socially equitable language\nidenti\xef\xac\x81cation. In ACL 2017, 51\xe2\x80\x9357.\n\nKane, S. K., Morris, M. R., Paradiso, A., and Campbell, J.\n(2017). \xe2\x80\x9cat times avuncular and cantankerous, with the\nre\xef\xac\x82exes of a mongoose\xe2\x80\x9d: Understanding self-expression\nthrough augmentative and alternative communication de-\nvices. In CSCW 2017, 1166\xe2\x80\x931179.\n\nKneser, R. and Ney, H. (1995). Improved backing-off for M-\ngram language modeling. In ICASSP-95, Vol. 1, 181\xe2\x80\x93184.\nMarkov, A. A. (1913). Essai d\xe2\x80\x99une recherche statistique sur\nle texte du roman \xe2\x80\x9cEugene Onegin\xe2\x80\x9d illustrant la liaison des\nepreuve en chain (\xe2\x80\x98Example of a statistical investigation of\nthe text of \xe2\x80\x9cEugene Onegin\xe2\x80\x9d illustrating the dependence be-\ntween samples in chain\xe2\x80\x99). Izvistia Imperatorskoi Akademii\nNauk (Bulletin de l\xe2\x80\x99Acad\xc2\xb4emie Imp\xc2\xb4eriale des Sciences de\nSt.-P\xc2\xb4etersbourg), 7, 153\xe2\x80\x93162.\n\nMikolov, T. (2012). Statistical language models based on\nneural networks. Ph.D. thesis, Ph. D. thesis, Brno Univer-\nsity of Technology.\n\nMiller, G. A. and Chomsky, N. (1963). Finitary models of\nlanguage users. In Luce, R. D., Bush, R. R., and Galanter,\nE. (Eds.), Handbook of Mathematical Psychology, Vol. II,\n419\xe2\x80\x93491. John Wiley.\n\nMiller, G. A. and Selfridge, J. A. (1950). Verbal context\nand the recall of meaningful material. American Journal of\nPsychology, 63, 176\xe2\x80\x93185.\n\nN\xc2\xb4adas, A. (1984). Estimation of probabilities in the language\nmodel of the IBM speech recognition system. IEEE Trans-\nactions on Acoustics, Speech, Signal Processing, 32(4),\n859\xe2\x80\x93861.\n\nSchwenk, H. (2007). Continuous space language models.\n\nComputer Speech & Language, 21(3), 492\xe2\x80\x93518.\n\nShannon, C. E. (1948). A mathematical theory of commu-\nnication. Bell System Technical Journal, 27(3), 379\xe2\x80\x93423.\nContinued in the following volume.\n\n\x0c28 Chapter 3 \xe2\x80\xa2 N-gram Language Models\n\nShannon, C. E. (1951). Prediction and entropy of printed\n\nEnglish. Bell System Technical Journal, 30, 50\xe2\x80\x9364.\n\nStolcke, A. (1998). Entropy-based pruning of backoff lan-\nguage models. In Proc. DARPA Broadcast News Transcrip-\ntion and Understanding Workshop, 270\xe2\x80\x93274.\n\nStolcke, A. (2002). SRILM \xe2\x80\x93 an extensible language model-\n\ning toolkit. In ICSLP-02.\n\nTalbot, D. and Osborne, M. (2007). Smoothed Bloom Fil-\nter Language Models: Tera-Scale LMs on the Cheap. In\nEMNLP/CoNLL 2007, 468\xe2\x80\x93476.\n\nTrnka, K., Yarrington, D., McCaw, J., McCoy, K. F., and\nPennington, C. (2007). The effects of word prediction on\ncommunication rate for AAC. In NAACL-HLT 07, 173\xe2\x80\x93\n176.\n\nWitten, I. H. and Bell, T. C. (1991). The zero-frequency\nproblem: Estimating the probabilities of novel events in\nadaptive text compression. IEEE Transactions on Informa-\ntion Theory, 37(4), 1085\xe2\x80\x931094.\n\n\x0c'7.5 Neural Language Models\n\nAs our \xef\xac\x81rst application of neural networks, let\xe2\x80\x99s consider language modeling: pre-\ndicting upcoming words from prior word context.\n\nNeural net-based language models turn out to have many advantages over the n-\ngram language models of Chapter 3. Among these are that neural language models\ndon\xe2\x80\x99t need smoothing, they can handle much longer histories, and they can general-\nize over contexts of similar words. For a training set of a given size, a neural lan-\nguage model has much higher predictive accuracy than an n-gram language model.\nFurthermore, neural language models underlie many of the models we\xe2\x80\x99ll introduce\nfor tasks like machine translation, dialog, and language generation.\n\n\x0c16 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nOn the other hand, there is a cost for this improved performance: neural net\nlanguage models are strikingly slower to train than traditional language models, and\nso for many tasks an n-gram language model is still the right tool.\n\nIn this chapter we\xe2\x80\x99ll describe simple feedforward neural language models, \xef\xac\x81rst\nintroduced by Bengio et al. (2003). Modern neural language models are generally\nnot feedforward but recurrent, using the technology that we will introduce in Chap-\nter 9.\n\nA feedforward neural LM is a standard feedforward network that takes as input\nat time t a representation of some number of previous words (wt\xe2\x88\x921,wt\xe2\x88\x922, etc.) and\noutputs a probability distribution over possible next words. Thus\xe2\x80\x94like the n-gram\nLM\xe2\x80\x94the feedforward neural LM approximates the probability of a word given the\nentire prior context P(wt|wt\xe2\x88\x921\n\n) by approximating based on the N previous words:\n\n1\n\nP(wt|wt\xe2\x88\x921\n\n1\n\n) \xe2\x89\x88 P(wt|wt\xe2\x88\x921\n\nt\xe2\x88\x92N+1)\n\n(7.26)\n\nIn the following examples we\xe2\x80\x99ll use a 4-gram example, so we\xe2\x80\x99ll show a net to\n\nestimate the probability P(wt = i|wt\xe2\x88\x921,wt\xe2\x88\x922,wt\xe2\x88\x923).\n\n7.5.1 Embeddings\nIn neural language models, the prior context is represented by embeddings of the\nprevious words. Representing the prior context as embeddings, rather than by ex-\nact words as used in n-gram language models, allows neural language models to\ngeneralize to unseen data much better than n-gram language models. For example,\nsuppose we\xe2\x80\x99ve seen this sentence in training:\n\nI have to make sure when I get home to feed the cat.\n\nbut we\xe2\x80\x99ve never seen the word \xe2\x80\x9cdog\xe2\x80\x9d after the words \xe2\x80\x9cfeed the\xe2\x80\x9d. In our test set we\nare trying to predict what comes after the pre\xef\xac\x81x \xe2\x80\x9cI forgot when I got home to feed\nthe\xe2\x80\x9d.\n\nAn n-gram language model will predict \xe2\x80\x9ccat\xe2\x80\x9d, but not \xe2\x80\x9cdog\xe2\x80\x9d. But a neural LM,\nwhich can make use of the fact that \xe2\x80\x9ccat\xe2\x80\x9d and \xe2\x80\x9cdog\xe2\x80\x9d have similar embeddings, will\nbe able to assign a reasonably high probability to \xe2\x80\x9cdog\xe2\x80\x9d as well as \xe2\x80\x9ccat\xe2\x80\x9d, merely\nbecause they have similar vectors.\n\nLet\xe2\x80\x99s see how this works in practice. Let\xe2\x80\x99s assume we have an embedding dic-\ntionary E that gives us, for each word in our vocabulary V , the embedding for that\nword, perhaps precomputed by an algorithm like word2vec from Chapter 6.\n\nFig. 7.12 shows a sketch of this simpli\xef\xac\x81ed feedforward neural language model\nwith N=3; we have a moving window at time t with an embedding vector represent-\ning each of the 3 previous words (words wt\xe2\x88\x921, wt\xe2\x88\x922, and wt\xe2\x88\x923). These 3 vectors are\nconcatenated together to produce x, the input layer of a neural network whose output\nis a softmax with a probability distribution over words. Thus y42, the value of output\nnode 42 is the probability of the next word wt being V42, the vocabulary word with\nindex 42.\n\nThe model shown in Fig. 7.12 is quite suf\xef\xac\x81cient, assuming we learn the embed-\ndings separately by a method like the word2vec methods of Chapter 6. The method\nof using another algorithm to learn the embedding representations we use for input\nwords is called pretraining. If those pretrained embeddings are suf\xef\xac\x81cient for your\npurposes, then this is all you need.\n\nHowever, often we\xe2\x80\x99d like to learn the embeddings simultaneously with training\nthe network. This is true when whatever task the network is designed for (sentiment\n\npretraining\n\n\x0c7.5\n\n\xe2\x80\xa2 NEURAL LANGUAGE MODELS\n\n17\n\nFigure 7.12 A simpli\xef\xac\x81ed view of a feedforward neural language model moving through a text. At each\ntimestep t the network takes the 3 context words, converts each to a d-dimensional embedding, and concatenates\nthe 3 embeddings together to get the 1\xc3\x97 Nd unit input layer x for the network. These units are multiplied by\na weight matrix W and bias vector b and then an activation function to produce a hidden layer h, which is then\nmultiplied by another weight matrix U. (For graphic simplicity we don\xe2\x80\x99t show b in this and future pictures.)\nFinally, a softmax output layer predicts at each node i the probability that the next word wt will be vocabulary\nword Vi.\n(This picture is simpli\xef\xac\x81ed because it assumes we just look up in an embedding dictionary E the\nd-dimensional embedding vector for each word, precomputed by an algorithm like word2vec.)\n\none-hot vector\n\nclassi\xef\xac\x81cation, or translation, or parsing) places strong constraints on what makes a\ngood representation.\n\nLet\xe2\x80\x99s therefore show an architecture that allows the embeddings to be learned.\nTo do this, we\xe2\x80\x99ll add an extra layer to the network, and propagate the error all the\nway back to the embedding vectors, starting with embeddings with random values\nand slowly moving toward sensible representations.\nFor this to work at the input layer, instead of pre-trained embeddings, we\xe2\x80\x99re\ngoing to represent each of the N previous words as a one-hot vector of length |V|, i.e.,\nwith one dimension for each word in the vocabulary. A one-hot vector is a vector\nthat has one element equal to 1\xe2\x80\x94in the dimension corresponding to that word\xe2\x80\x99s\nindex in the vocabulary\xe2\x80\x94 while all the other elements are set to zero.\nThus in a one-hot representation for the word \xe2\x80\x9ctoothpaste\xe2\x80\x9d, supposing it happens\nto have index 5 in the vocabulary, x5 is one and and xi = 0 \xe2\x88\x80i (cid:54)= 5, as shown here:\n\n[0 0 0 0 1 0 0 ... 0 0 0 0]\n... |V|\n\n1 2 3 4 5 6 7 ...\n\nFig. 7.13 shows the additional layers needed to learn the embeddings during LM\ntraining. Here the N=3 context words are represented as 3 one-hot vectors, fully\nconnected to the embedding layer via 3 instantiations of the embedding matrix E.\nNote that we don\xe2\x80\x99t want to learn separate weight matrices for mapping each of the 3\nprevious words to the projection layer, we want one single embedding dictionary E\nthat\xe2\x80\x99s shared among these three. That\xe2\x80\x99s because over time, many different words will\nappear as wt\xe2\x88\x922 or wt\xe2\x88\x921, and we\xe2\x80\x99d like to just represent each word with one vector,\nwhichever context position it appears in. The embedding weight matrix E thus has\n\nOutput layer P(w|u)\n\n1\xe2\xa8\x89|V|\nU\n\n|V|\xe2\xa8\x89dh\n\ny1\n\n\xe2\x80\xa6\n\ny42\n\n\xe2\x80\xa6\n\ny|V|\n\nP(wt=V42|wt-3,wt-2,wt-3)\n\nHidden layer\n\n1\xe2\xa8\x89dh\n\nh1\n\nh2\n\nh3\n\nhdh\xe2\x80\xa6\n\ndh\xe2\xa8\x893d\n\nW\n\nProjection layer 1\xe2\xa8\x893d\nconcatenated embeddings\n\nfor context words\n\n...\n\nhole\n\nin\n\nembedding for\n\nword 35\nthe\nwt-3\n\nembedding for \n\nword 9925\nground\n\nwt-2\n\nembedding for \nword 45180\nthere\nwt-1\n\nword 42\nlived\nwt\n\n...\n\n\x0c18 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nFigure 7.13 Learning all the way back to embeddings. Notice that the embedding matrix E is shared among\nthe 3 context words.\n\nprojection layer\n\na row for each word, each a vector of d dimensions, and hence has dimensionality\nV \xc3\x97 d.\n\nLet\xe2\x80\x99s walk through the forward pass of Fig. 7.13.\n1. Select three embeddings from E: Given the three previous words, we look\nup their indices, create 3 one-hot vectors, and then multiply each by the em-\nbedding matrix E. Consider wt\xe2\x88\x923. The one-hot vector for \xe2\x80\x98the\xe2\x80\x99 (index 35) is\nmultiplied by the embedding matrix E, to give the \xef\xac\x81rst part of the \xef\xac\x81rst hidden\nlayer, called the projection layer. Since each row of the input matrix E is just\nan embedding for a word, and the input is a one-hot column vector xi for word\nVi, the projection layer for input w will be Exi = ei, the embedding for word i.\nWe now concatenate the three embeddings for the context words.\n\n2. Multiply by W: We now multiply by W (and add b) and pass through the\n\nrecti\xef\xac\x81ed linear (or other) activation function to get the hidden layer h.\n\n3. Multiply by U: h is now multiplied by U\n4. Apply softmax: After the softmax, each node i in the output layer estimates\nthe probability P(wt = i|wt\xe2\x88\x921,wt\xe2\x88\x922,wt\xe2\x88\x923)\n\nIn summary, if we use e to represent the projection layer, formed by concate-\nnating the 3 embeddings for the three context vectors, the equations for a neural\nlanguage model become:\n\ne = (Ex1,Ex2, ...,Ex)\nh = \xcf\x83 (We + b)\nz = Uh\ny = softmax(z)\n\n(7.27)\n(7.28)\n(7.29)\n(7.30)\n\nOutput layer \nP(w|context)\n\nHidden layer\n\n1\xe2\xa8\x89|V|\nU\nh1\n\n|V|\xe2\xa8\x89dh\n\n1\xe2\xa8\x89dh\n\ny1\n\n\xe2\x80\xa6\n\ny42\n\n\xe2\x80\xa6\n\ny|V|\n\nh2\n\nh3\n\nhdh\xe2\x80\xa6\n\ndh\xe2\xa8\x893d\n\nW\nProjection layer 1\xe2\xa8\x893d\n\nd\xe2\xa8\x89|V|\n\nE\n\nInput layer\none-hot vectors\n\n1\xe2\xa8\x89|V|\n\n1\n\n35\n\n|V|\n\n1\n\n9925\n\n|V|\n\n1\n\n45180\n\n|V|\n\n0 0\n\n1\n\n00\n\n0 0\n\n0\n\n1\n\n00\n\n0 0\n\n00\n\nindex\nword 35\n\nindex \n\nword 9925\n\n0\n\n1\nindex \n\nword 45180\n\n...\n\nhole\n\nin\n\nthe\nwt-3\n\nground\n\nwt-2\n\nthere\nwt-1\n\nP(wt=V42|wt-3,wt-2,wt-3)\n\nE is shared\nacross words\n\nword 42\nlived\nwt\n\n...\n\n\x0c7.6\n\n\xe2\x80\xa2 SUMMARY\n\n19\n\n7.5.2 Training the neural language model\nto set all the parameters \xce\xb8 = E,W,U,b, we do gradient\nTo train the model, i.e.\ndescent (Fig. ??), using error backpropagation on the computation graph to compute\nthe gradient. Training thus not only sets the weights W and U of the network, but\nalso as we\xe2\x80\x99re predicting upcoming words, we\xe2\x80\x99re learning the embeddings E for each\nwords that best predict upcoming words.\n\nGenerally training proceeds by taking as input a very long text, concatenating\nall the sentences, starting with random weights, and then iteratively moving through\nthe text predicting each word wt. At each word wt, the cross-entropy (negative log\nlikelihood) loss is:\n\nL = \xe2\x88\x92log p(wt|wt\xe2\x88\x921, ...,wt\xe2\x88\x92n+1)\n\n(7.31)\n\nThe gradient for this loss is then:\n\xce\xb8t+1 = \xce\xb8t \xe2\x88\x92 \xce\xb7\n\n\xe2\x88\x82 \xe2\x88\x92 log p(wt|wt\xe2\x88\x921, ...,wt\xe2\x88\x92n+1)\n\n\xe2\x88\x82\xce\xb8\n\n(7.32)\n\nThis gradient can be computed in any standard neural network framework which\nwill then backpropagate through U, W , b, E.\n\nTraining the parameters to minimize loss will result both in an algorithm for\nlanguage modeling (a word predictor) but also a new set of embeddings E that can\nbe used as word representations for other tasks.\n\n7.6 Summary\n\nto each unit in layer i + 1, and there are no cycles.\n\nneurons but now simply an abstract computational device.\n\n\xe2\x80\xa2 Neural networks are built out of neural units, originally inspired by human\n\xe2\x80\xa2 Each neural unit multiplies input values by a weight vector, adds a bias, and\nthen applies a non-linear activation function like sigmoid, tanh, or recti\xef\xac\x81ed\nlinear.\n\xe2\x80\xa2 In a fully-connected, feedforward network, each unit in layer i is connected\n\xe2\x80\xa2 The power of neural networks comes from the ability of early layers to learn\n\xe2\x80\xa2 Neural networks are trained by optimization algorithms like gradient de-\n\xe2\x80\xa2 Error backpropagation, backward differentiation on a computation graph,\n\xe2\x80\xa2 Neural language models use a neural network as a probabilistic classi\xef\xac\x81er, to\n\xe2\x80\xa2 Neural language models can use pretrained embeddings, or can learn embed-\n\ncompute the probability of the next word given the previous n words.\n\nis used to compute the gradients of the loss function for a network.\n\nrepresentations that can be utilized by later layers in the network.\n\nscent.\n\ndings from scratch in the process of language modeling.\n\nBibliographical and Historical Notes\n\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\nloch and Pitts, 1943), a simpli\xef\xac\x81ed model of the human neuron as a kind of com-\n\n\x0c20 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nconnectionist\n\nputing element that could be described in terms of propositional logic. By the late\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\nBernard Widrow at Stanford) developed research into neural networks; this phase\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\n\nThe \xef\xac\x81eld of neural networks declined after it was shown that a single percep-\ntron unit was unable to model functions as simple as XOR (Minsky and Papert,\n1969). While some small amount of work continued during the next two decades,\na major revival for the \xef\xac\x81eld didn\xe2\x80\x99t come until the 1980s, when practical tools for\nbuilding deeper networks like error backpropagation became widespread (Rumel-\nhart et al., 1986). During the 1980s a wide variety of neural network and related\narchitectures were developed, particularly for applications in psychology and cog-\nnitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986,\nRumelhart and McClelland 1986a, Elman 1990), for which the term connection-\nist or parallel distributed processing was often used (Feldman and Ballard 1982,\nSmolensky 1988). Many of the principles and techniques developed in this period\nare foundational to modern work, including the ideas of distributed representations\n(Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for com-\npositionality (Smolensky, 1990).\n\nBy the 1990s larger neural networks began to be applied to many practical lan-\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989,\nLeCun et al. 1990) and speech recognition (Morgan and Bourlard 1989, Morgan\nand Bourlard 1990). By the early 2000s, improvements in computer hardware and\nadvances in optimization and training techniques made it possible to train even larger\nand deeper networks, leading to the modern term deep learning (Hinton et al. 2006,\nBengio et al. 2007). We cover more related history in Chapter 9.\n\nThere are a number of excellent books on the subject. Goldberg (2017) has a\nsuperb and comprehensive coverage of neural networks for natural language pro-\ncessing. For neural networks in general see Goodfellow et al. (2016) and Nielsen\n(2015).\n\n\x0cBibliographical and Historical Notes\n\n21\n\nMorgan, N. and Bourlard, H. (1990). Continuous speech\nrecognition using multilayer perceptrons with hidden\nmarkov models. In ICASSP-90, 413\xe2\x80\x93416.\n\nNielsen, M. A. (2015). Neural networks and Deep learning.\n\nDetermination Press USA.\n\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., De-\nVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A.\n(2017). Automatic differentiation in pytorch. In NIPS-W.\nRosenblatt, F. (1958). The perceptron: A probabilistic model\nfor information storage and organization in the brain.. Psy-\nchological review, 65(6), 386\xe2\x80\x93408.\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986).\nLearning internal representations by error propagation. In\nRumelhart, D. E. and McClelland, J. L. (Eds.), Parallel\nDistributed Processing, Vol. 2, 318\xe2\x80\x93362. MIT Press.\n\nRumelhart, D. E. and McClelland, J. L. (1986a). On learn-\ning the past tense of English verbs. In Rumelhart, D. E. and\nMcClelland, J. L. (Eds.), Parallel Distributed Processing,\nVol. 2, 216\xe2\x80\x93271. MIT Press.\n\nRumelhart, D. E. and McClelland, J. L. (Eds.). (1986b). Par-\n\nallel Distributed Processing. MIT Press.\n\nRussell, S. and Norvig, P. (2002). Arti\xef\xac\x81cial Intelligence: A\n\nModern Approach (2nd Ed.). Prentice Hall.\n\nSmolensky, P. (1988). On the proper treatment of connec-\n\ntionism. Behavioral and brain sciences, 11(1), 1\xe2\x80\x9323.\n\nSmolensky, P. (1990). Tensor product variable binding and\nthe representation of symbolic structures in connectionist\nsystems. Arti\xef\xac\x81cial intelligence, 46(1-2), 159\xe2\x80\x93216.\n\nSrivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. R. (2014). Dropout: a simple way\nto prevent neural networks from over\xef\xac\x81tting.. JMLR, 15(1),\n1929\xe2\x80\x931958.\n\nWidrow, B. and Hoff, M. E. (1960). Adaptive switching cir-\ncuits. In IRE WESCON Convention Record, Vol. 4, 96\xe2\x80\x93\n104.\n\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,\nM., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-\nenberg, J., Man\xc2\xb4e, D., Monga, R., Moore, S., Murray, D.,\nOlah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,\nI., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\nVi\xc2\xb4egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke,\nM., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-\nscale machine learning on heterogeneous systems.. Soft-\nware available from tensor\xef\xac\x82ow.org.\n\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003).\nA neural probabilistic language model. Journal of machine\nlearning research, 3(Feb), 1137\xe2\x80\x931155.\n\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.\n(2007). Greedy layer-wise training of deep networks. In\nNIPS 2007, 153\xe2\x80\x93160.\n\nElman, J. L. (1990). Finding structure in time. Cognitive\n\nscience, 14(2), 179\xe2\x80\x93211.\n\nFeldman, J. A. and Ballard, D. H. (1982). Connectionist\nmodels and their properties. Cognitive Science, 6, 205\xe2\x80\x93\n254.\n\nGoldberg, Y. (2017). Neural Network Methods for Natural\nLanguage Processing, Vol. 10 of Synthesis Lectures on Hu-\nman Language Technologies. Morgan & Claypool.\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep\n\nLearning. MIT Press.\n\nHinton, G. E. (1986). Learning distributed representations\n\nof concepts. In COGSCI-86, 1\xe2\x80\x9312.\n\nHinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast\nlearning algorithm for deep belief nets. Neural computa-\ntion, 18(7), 1527\xe2\x80\x931554.\n\nHinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever,\nI., and Salakhutdinov, R. R. (2012).\nImproving neural\nnetworks by preventing co-adaptation of feature detectors.\narXiv preprint arXiv:1207.0580.\n\nKingma, D. and Ba, J. (2015). Adam: A method for stochas-\n\ntic optimization. In ICLR 2015.\n\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard,\nR. E., Hubbard, W., and Jackel, L. D. (1989). Backpropa-\ngation applied to handwritten zip code recognition. Neural\ncomputation, 1(4), 541\xe2\x80\x93551.\n\nLeCun, Y., Boser, B. E., Denker, J. S., Henderson, D.,\nHoward, R. E., Hubbard, W. E., and Jackel, L. D. (1990).\nHandwritten digit recognition with a back-propagation net-\nwork. In NIPS 1990, 396\xe2\x80\x93404.\n\nMcClelland, J. L. and Elman, J. L. (1986). The TRACE\nmodel of speech perception. Cognitive Psychology, 18, 1\xe2\x80\x93\n86.\n\nMcCulloch, W. S. and Pitts, W. (1943). A logical calculus of\nideas immanent in nervous activity. Bulletin of Mathemat-\nical Biophysics, 5, 115\xe2\x80\x93133. Reprinted in Neurocomput-\ning: Foundations of Research, ed. by J. A. Anderson and E\nRosenfeld. MIT Press 1988.\n\nMinsky, M. and Papert, S. (1969). Perceptrons. MIT Press.\nMorgan, N. and Bourlard, H. (1989). Generalization and\nparameter estimation in feedforward nets: Some experi-\nments. In Advances in neural information processing sys-\ntems, 630\xe2\x80\x93637.\n\n\x0c'