Speech and Language Processing Daniel Jurafsky James H Martin Copyright c 2019 All rights reserved Draft of October 2 2019 CHAPTER 5 Logistic Regression And how do you know that these fine begonias are not of equal importance Hercule Poirot in Agatha Christies The Mysterious Affair at Styles Detective stories are as littered with clues as texts are with words Yet for the poor reader it can be challenging to know how to weigh the authors clues in order to make the crucial classification task deciding whodunnit In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or cues and some particular outcome logistic regressionlogisticregression Indeed logistic regression is one of the most important analytic tools in the social and natural sciences In natural language processing logistic regression is the base line supervised machine learning algorithm for classification and also has a very close relationship with neural networks As we will see in Chapter 7 a neural net work can be viewed as a series of logistic regression classifiers stacked on top of each other Thus the classification and machine learning techniques introduced here will play an important role throughout the book Logistic regression can be used to classify an observation into one of two classes like positive sentiment and negative sentiment or into one of many classes Because the mathematics for the twoclass case is simpler well describe this special case of logistic regression first in the next few sections and then briefly summarize the use of multinomial logistic regression for more than two classes in Section 56 Well introduce the mathematics of logistic regression in the next few sections But lets begin with some highlevel issues Generative and Discriminative Classifiers The most important difference be tween naive Bayes and logistic regression is that logistic regression is a discrimina tive classifier while naive Bayes is a generative classifier These are two very different frameworks for how to build a machine learning model Consider a visual metaphor imagine were trying to distinguish dog images from cat images A generative model would have the goal of understanding what dogs look like and what cats look like You might literally ask such a model to generate ie draw a dog Given a test image the system then asks whether its the cat model or the dog model that better fits is less surprised by the image and chooses that as its label A discriminative model by contrast is only try ing to learn to distinguish the classes perhaps with out learning much about them So maybe all the dogs in the training data are wearing collars and the cats arent If that one feature neatly separates the classes the model is satisfied If you ask such a model what it knows about cats all it can say is that they dont wear collars 2 CHAPTER 5 LOGISTIC REGRESSION More formally recall that the naive Bayes assigns a class c to a document d not by directly computing Pcd but by computing a likelihood and a prior ĉ argmax cC likelihood Pdc prior Pc 51 A generative model like naive Bayes makes use of this likelihood term whichgenerativemodel expresses how to generate the features of a document if we knew it was of class c By contrast a discriminative model in this text categorization scenario attemptsdiscriminativemodel to directly compute Pcd Perhaps it will learn to assign a high weight to document features that directly improve its ability to discriminate between possible classes even if it couldnt generate an example of one of the classes Components of a probabilistic machine learning classifier Like naive Bayes logistic regression is a probabilistic classifier that makes use of supervised machine learning Machine learning classifiers require a training corpus of M inputoutput pairs xiyi Well use superscripts in parentheses to refer to individual instances in the training setfor sentiment classification each instance might be an individual document to be classified A machine learning system for classification then has four components 1 A feature representation of the input For each input observation xi this will be a vector of features x1x2 xn We will generally refer to feature i for input x j as x ji sometimes simplified as xi but we will also see the notation fi fix or for multiclass classification ficx 2 A classification function that computes ŷ the estimated class via pyx In the next section we will introduce the sigmoid and softmax tools for classifi cation 3 An objective function for learning usually involving minimizing error on training examples We will introduce the crossentropy loss function 4 An algorithm for optimizing the objective function We introduce the stochas tic gradient descent algorithm Logistic regression has two phases training we train the system specifically the weights w and b using stochastic gradient descent and the crossentropy loss test Given a test example x we compute pyx and return the higher probability label y 1 or y 0 51 Classification the sigmoid The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation Here we introduce the sigmoid classifier that will help us make this decision Consider a single input observation x which we will represent by a vector of features x1x2 xn well show sample features in the next subsection The clas sifier output y can be 1 meaning the observation is a member of the class or 0 the observation is not a member of the class We want to know the probability Py 1x that this observation is a member of the class So perhaps the decision 51 CLASSIFICATION THE SIGMOID 3 is positive sentiment versus negative sentiment the features represent counts of words in a document and Py 1x is the probability that the document has positive sentiment while and Py 0x is the probability that the document has negative sentiment Logistic regression solves this task by learning from a training set a vector of weights and a bias term Each weight wi is a real number and is associated with one of the input features xi The weight wi represents how important that input feature is to the classification decision and can be positive meaning the feature is associated with the class or negative meaning the feature is not associated with the class Thus we might expect in a sentiment task the word awesome to have a high positive weight and abysmal to have a very negative weight The bias term also called thebias term intercept is another real number thats added to the weighted inputsintercept To make a decision on a test instance after weve learned the weights in training the classifier first multiplies each xi by its weight wi sums up the weighted features and adds the bias term b The resulting single number z expresses the weighted sum of the evidence for the class z n i1 wixi b 52 In the rest of the book well represent such sums using the dot product notation fromdot product linear algebra The dot product of two vectors a and b written as a b is the sum of the products of the corresponding elements of each vector Thus the following is an equivalent formation to Eq 52 z w xb 53 But note that nothing in Eq 53 forces z to be a legal probability that is to lie between 0 and 1 In fact since weights are realvalued the output might even be negative z ranges from to Figure 51 The sigmoid function y 11ez takes a real value and maps it to the range 01 It is nearly linear around 0 but outlier values get squashed toward 0 or 1 To create a probability well pass z through the sigmoid function σz Thesigmoid sigmoid function named because it looks like an s is also called the logistic func tion and gives logistic regression its name The sigmoid has the following equationlogisticfunction shown graphically in Fig 51 y σz 1 1 ez 54 The sigmoid has a number of advantages it takes a realvalued number and maps it into the range 01 which is just what we want for a probability Because it is 4 CHAPTER 5 LOGISTIC REGRESSION nearly linear around 0 but has a sharp slope toward the ends it tends to squash outlier values toward 0 or 1 And its differentiable which as well see in Section 58 will be handy for learning Were almost there If we apply the sigmoid to the sum of the weighted features we get a number between 0 and 1 To make it a probability we just need to make sure that the two cases py 1 and py 0 sum to 1 We can do this as follows Py 1 σw xb 1 1 ewxb Py 0 1σw xb 1 1 1 ewxb ewxb 1 ewxb 55 Now we have an algorithm that given an instance x computes the probability Py 1x How do we make a decision For a test instance x we say yes if the probability Py 1x is more than 5 and no otherwise We call 5 the decision boundarydecisionboundary ŷ 1 if Py 1x 05 0 otherwise 511 Example sentiment classification Lets have an example Suppose we are doing binary sentiment classification on movie review text and we would like to know whether to assign the sentiment class or to a review document doc Well represent each input observation by the 6 features x1x6 of the input shown in the following table Fig 52 shows the features in a sample mini test document Var Definition Value in Fig 52 x1 countpositive lexicon doc 3 x2 countnegative lexicon doc 2 x3 1 if no doc 0 otherwise 1 x4 count1st and 2nd pronouns doc 3 x5 1 if doc 0 otherwise 0 x6 logword count of doc ln66 419 Lets assume for the moment that weve already learned a realvalued weight for each of these features and that the 6 weights corresponding to the 6 features are 255012052007 while b 01 Well discuss in the next section how the weights are learned The weight w1 for example indicates how important a feature the number of positive lexicon words great nice enjoyable etc is to a positive sentiment decision while w2 tells us the importance of negative lexicon words Note that w1 25 is positive while w2 50 meaning that negative words are negatively associated with a positive sentiment decision and are about twice as important as positive words 51 CLASSIFICATION THE SIGMOID 5 Its hokey There are virtually no surprises and the writing is secondrate So why was it so enjoyable For one thing the cast is great Another nice touch is the music I was overcome with the urge to get off the couch and start dancing It sucked me in and itll do the same to you x13 x6419 x31 x43x50 x22 Figure 52 A sample mini test document showing the extracted features in the vector x Given these 6 features and the input review x Px and Px can be com puted using Eq 55 px PY 1x σw xb σ255012052007 3213041901 σ833 070 56 px PY 0x 1σw xb 030 Logistic regression is commonly applied to all sorts of NLP tasks and any property of the input can be a feature Consider the task of period disambiguation deciding if a period is the end of a sentence or part of a word by classifying each period into one of two classes EOS endofsentence and notEOS We might use features like x1 below expressing that the current word is lower case and the class is EOS perhaps with a positive weight or that the current word is in our abbreviations dictionary Prof and the class is EOS perhaps with a negative weight A feature can also express a quite complex combination of properties For example a period following an upper case word is likely to be an EOS but if the word itself is St and the previous word is capitalized then the period is likely part of a shortening of the word street x1 1 if Casewi Lower 0 otherwise x2 1 if wi AcronymDict 0 otherwise x3 1 if wi St Casewi1 Cap 0 otherwise Designing features Features are generally designed by examining the training set with an eye to linguistic intuitions and the linguistic literature on the domain A careful error analysis on the training set or devset of an early version of a system often provides insights into features For some tasks it is especially helpful to build complex features that are combi nations of more primitive features We saw such a feature for period disambiguation above where a period on the word St was less likely to be the end of the sentence if the previous word was capitalized For logistic regression and naive Bayes these combination features or feature interactions have to be designed by handfeatureinteractions 6 CHAPTER 5 LOGISTIC REGRESSION For many tasks especially when feature values can reference specific words well need large numbers of features Often these are created automatically via fea ture templates abstract specifications of features For example a bigram templatefeaturetemplates for period disambiguation might create a feature for every pair of words that occurs before a period in the training set Thus the feature space is sparse since we only have to create a feature if that ngram exists in that position in the training set The feature is generally created as a hash from the string descriptions A user description of a feature as bigramAmerican breakfast is hashed into a unique integer i that becomes the feature number fi In order to avoid the extensive human effort of feature design recent research in NLP has focused on representation learning ways to learn features automatically in an unsupervised way from the input Well introduce methods for representation learning in Chapter 6 and Chapter 7 Choosing a classifier Logistic regression has a number of advantages over naive Bayes Naive Bayes has overly strong conditional independence assumptions Con sider two features which are strongly correlated in fact imagine that we just add the same feature f1 twice Naive Bayes will treat both copies of f1 as if they were sep arate multiplying them both in overestimating the evidence By contrast logistic regression is much more robust to correlated features if two features f1 and f2 are perfectly correlated regression will simply assign part of the weight to w1 and part to w2 Thus when there are many correlated features logistic regression will assign a more accurate probability than naive Bayes So logistic regression generally works better on larger documents or datasets and is a common default Despite the less accurate probabilities naive Bayes still often makes the correct classification decision Furthermore naive Bayes can work extremely well some times even better than logistic regression on very small datasets Ng and Jordan 2002 or short documents Wang and Manning 2012 Furthermore naive Bayes is easy to implement and very fast to train theres no optimization step So its still a reasonable approach to use in some situations 52 Learning in Logistic Regression How are the parameters of the model the weights w and bias b learned Logistic regression is an instance of supervised classification in which we know the correct label y either 0 or 1 for each observation x What the system produces via Eq 55 is ŷ the systems estimate of the true y We want to learn parameters meaning w and b that make ŷ for each training observation as close as possible to the true y This requires 2 components that we foreshadowed in the introduction to the chapter The first is a metric for how close the current label ŷ is to the true gold label y Rather than measure similarity we usually talk about the opposite of this the distance between the system output and the gold output and we call this distance the loss function or the cost function In the next section well introduce the lossloss function that is commonly used for logistic regression and also for neural networks the crossentropy loss The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function The standard algorithm for this is gradient descent well introduce the stochastic gradient descent algorithm in the following section 53 THE CROSSENTROPY LOSS FUNCTION 7 53 The crossentropy loss function We need a loss function that expresses for an observation x how close the classifier output ŷ σw xb is to the correct output y which is 0 or 1 Well call this Lŷy How much ŷ differs from the true y 57 We do this via a loss function that prefers the correct class labels of the train ing examples to be more likely This is called conditional maximum likelihood estimation we choose the parameters wb that maximize the log probability of the true y labels in the training data given the observations x The resulting loss function is the negative log likelihood loss generally called the crossentropy losscrossentropyloss Lets derive this loss function applied to a single observation x Wed like to learn weights that maximize the probability of the correct label pyx Since there are only two discrete outcomes 1 or 0 this is a Bernoulli distribution and we can express the probability pyx that our classifier produces for one observation as the following keeping in mind that if y1 Eq 58 simplifies to ŷ if y0 Eq 58 simplifies to 1 ŷ pyx ŷ y 1 ŷ1y 58 Now we take the log of both sides This will turn out to be handy mathematically and doesnt hurt us whatever values maximize a probability will also maximize the log of the probability log pyx log ŷ y 1 ŷ1y y log ŷ1 y log1 ŷ 59 Eq 59 describes a log likelihood that should be maximized In order to turn this into loss function something that we need to minimize well just flip the sign on Eq 59 The result is the crossentropy loss LCE LCEŷy log pyx y log ŷ1 y log1 ŷ 510 Finally we can plug in the definition of ŷ σw xb LCEwb y logσw xb1 y log1σw xb 511 Lets see if this loss function does the right thing for our example from Fig 52 We want the loss to be smaller if the models estimate is close to correct and bigger if the model is confused So first lets suppose the correct gold label for the sentiment example in Fig 52 is positive ie y 1 In this case our model is doing well since from Eq 56 it indeed gave the example a higher probability of being positive 69 than negative 31 If we plug σw xb 69 and y 1 into Eq 511 the right side of the equation drops out leading to the following loss LCEwb y logσw xb1 y log1σw xb logσw xb log69 37 8 CHAPTER 5 LOGISTIC REGRESSION By contrast lets pretend instead that the example in Fig 52 was actually negative ie y 0 perhaps the reviewer went on to say But bottom line the movie is terrible I beg you not to see it In this case our model is confused and wed want the loss to be higher Now if we plug y 0 and 1σw xb 31 from Eq 56 into Eq 511 the left side of the equation drops out LCEwb y logσw xb1 y log1σw xb log1σw xb log31 117 Sure enough the loss for the first classifier 37 is less than the loss for the second classifier 117 Why does minimizing this negative log probability do what we want A per fect classifier would assign probability 1 to the correct outcome y1 or y0 and probability 0 to the incorrect outcome That means the higher ŷ the closer it is to 1 the better the classifier the lower ŷ is the closer it is to 0 the worse the clas sifier The negative log of this probability is a convenient loss metric since it goes from 0 negative log of 1 no loss to infinity negative log of 0 infinite loss This loss function also ensures that as the probability of the correct answer is maximized the probability of the incorrect answer is minimized since the two sum to one any increase in the probability of the correct answer is coming at the expense of the in correct answer Its called the crossentropy loss because Eq 59 is also the formula for the crossentropy between the true probability distribution y and our estimated distribution ŷ Now we know what we want to minimize in the next section well see how to find the minimum 54 Gradient Descent Our goal with gradient descent is to find the optimal weights minimize the loss function weve defined for the model In Eq 512 below well explicitly represent the fact that the loss function L is parameterized by the weights which well refer to in machine learning in general as θ in the case of logistic regression θ wb θ̂ argmin θ 1 m m i1 LCEyixiθ 512 How shall we find the minimum of this or any loss function Gradient descent is a method that finds a minimum of a function by figuring out in which direction in the space of the parameters θ the functions slope is rising the most steeply and moving in the opposite direction The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom you might look around yourself 360 degrees find the direction where the ground is sloping the steepest and walk downhill in that direction For logistic regression this loss function is conveniently convex A convex funcconvex tion has just one minimum there are no local minima to get stuck in so gradient descent starting from any point is guaranteed to find the minimum By contrast 54 GRADIENT DESCENT 9 the loss for multilayer neural networks is nonconvex and gradient descent may get stuck in local minima for neural network training and never find the global opti mum Although the algorithm and the concept of gradient are designed for direction vectors lets first consider a visualization of the case where the parameter of our system is just a single scalar w shown in Fig 53 Given a random initialization of w at some value w1 and assuming the loss function L happened to have the shape in Fig 53 we need the algorithm to tell us whether at the next iteration we should move left making w2 smaller than w1 or right making w2 bigger than w1 to reach the minimum w Loss 0 w1 wmin slope of loss at w1 is negative goal one step of gradient descent Figure 53 The first step in iteratively finding the minimum of this loss function by moving w in the reverse direction from the slope of the function Since the slope is negative we need to move w in a positive direction to the right Here superscripts are used for learning steps so w1 means the initial value of w which is 0 w2 at the second step and so on The gradient descent algorithm answers this question by finding the gradientgradient of the loss function at the current point and moving in the opposite direction The gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function The gradient is a multivariable generalization of the slope so for a function of one variable like the one in Fig 53 we can informally think of the gradient as the slope The dotted line in Fig 53 shows the slope of this hypothetical loss function at point w w1 You can see that the slope of this dotted line is negative Thus to find the minimum gradient descent tells us to go in the opposite direction moving w in a positive direction The magnitude of the amount to move in gradient descent is the value of the slope d dw f xw weighted by a learning rate η A higher faster learning rate means thatlearning rate we should move w more on each step The change we make in our parameter is the learning rate times the gradient or the slope in our singlevariable example wt1 wt η d dw f xw 513 Now lets extend the intuition from a function of one scalar variable w to many variables because we dont just want to move left or right we want to know where in the Ndimensional space of the N parameters that make up θ we should move The gradient is just such a vector it expresses the directional components of the sharpest slope along each of those N dimensions If were just imagining two weight dimensions say for one weight w and one bias b the gradient might be a vector with two orthogonal components each of which tells us how much the ground slopes in the w dimension and in the b dimension Fig 54 shows a visualization 10 CHAPTER 5 LOGISTIC REGRESSION Costwb w b Figure 54 Visualization of the gradient vector in two dimensions w and b In an actual logistic regression the parameter vector w is much longer than 1 or 2 since the input feature vector x can be quite long and we need a weight wi for each xi For each dimensionvariable wi in w plus the bias b the gradient will have a component that tells us the slope with respect to that variable Essentially were asking How much would a small change in that variable wi influence the total loss function L In each dimension wi we express the slope as a partial derivative wi of the loss function The gradient is then defined as a vector of these partials Well represent ŷ as f xθ to make the dependence on θ more obvious θ L f xθy       w1 L f xθy w2 L f xθy wn L f xθy       514 The final equation for updating θ based on the gradient is thus θt1 θt ηL f xθy 515 541 The Gradient for Logistic Regression In order to update θ we need a definition for the gradient L f xθy Recall that for logistic regression the crossentropy loss function is LCEwb y logσw xb1 y log1σw xb 516 It turns out that the derivative of this function for one observation vector x is Eq 517 the interested reader can see Section 58 for the derivation of this equation LCEwb w j σw xb yx j 517 Note in Eq 517 that the gradient with respect to a single weight w j represents a very intuitive value the difference between the true y and our estimated ŷ σw xb for that observation multiplied by the corresponding input value x j 54 GRADIENT DESCENT 11 542 The Stochastic Gradient Descent Algorithm Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example and nudging θ in the right direction the opposite direction of the gradient Fig 55 shows the algorithm function STOCHASTIC GRADIENT DESCENTL f x y returns θ where L is the loss function f is a function parameterized by θ x is the set of training inputs x1 x2 xn y is the set of training outputs labels y1 y2 yn θ0 repeat til done see caption For each training tuple xi yi in random order 1 Optional for reporting How are we doing on this tuple Compute ŷ i f xiθ What is our estimated output ŷ Compute the loss Lŷ iyi How far off is ŷi from the true output yi 2 gθ L f xiθyi How should we move θ to maximize loss 3 θθ η g Go the other way instead return θ Figure 55 The stochastic gradient descent algorithm Step 1 computing the loss is used to report how well we are doing on the current tuple The algorithm can terminate when it converges or when the gradient ε or when progress halts for example when the loss starts going up on a heldout set The learning rate η is a parameter that must be adjusted If its too high the learner will take steps that are too large overshooting the minimum of the loss func tion If its too low the learner will take steps that are too small and take too long to get to the minimum It is common to begin the learning rate at a higher value and then slowly decrease it so that it is a function of the iteration k of training you will sometimes see the notation ηk to mean the value of the learning rate at iteration k 543 Working through an example Lets walk though a single step of the gradient descent algorithm Well use a sim plified version of the example in Fig 52 as it sees a single observation x whose correct value is y 1 this is a positive review and with only two features x1 3 count of positive lexicon words x2 2 count of negative lexicon words Lets assume the initial weights and bias in θ 0 are all set to 0 and the initial learning rate η is 01 w1 w2 b 0 η 01 The single update step requires that we compute the gradient multiplied by the learning rate θ t1 θ t ηθ L f xiθyi 12 CHAPTER 5 LOGISTIC REGRESSION In our mini example there are three parameters so the gradient vector has 3 dimen sions for w1 w2 and b We can compute the first gradient as follows wb    LCE wb w1 LCE wb w2 LCE wb b      σw xb yx1σw xb yx2 σw xb y     σ01x1σ01x2 σ01     05x105x2 05     1510 05   Now that we have a gradient we compute the new parameter vector θ 1 by moving θ 0 in the opposite direction from the gradient θ 2   w1w2 b   η   1510 05     151 05   So after one step of gradient descent the weights have shifted to be w1 15 w2 1 and b 05 Note that this observation x happened to be a positive example We would expect that after seeing more negative examples with high counts of negative words that the weight w2 would shift to have a negative value 544 Minibatch training Stochastic gradient descent is called stochastic because it chooses a single random example at a time moving the weights so as to improve performance on that single example That can result in very choppy movements so its common to compute the gradient over batches of training instances rather than a single instance For example in batch training we compute the gradient over the entire datasetbatch training By seeing so many examples batch training offers a superb estimate of which di rection to move the weights at the cost of spending a lot of time processing every single example in the training set to compute this perfect direction A compromise is minibatch training we train on a group of m examples perminibatch haps 512 or 1024 that is less than the whole dataset If m is the size of the dataset then we are doing batch gradient descent if m 1 we are back to doing stochas tic gradient descent Minibatch training also has the advantage of computational efficiency The minibatches can easily be vectorized choosing the size of the mini batch based on the computational resources This allows us to process all the exam ples in one minibatch in parallel and then accumulate the loss something thats not possible with individual or batch training We just need to define minibatch versions of the crossentropy loss function we defined in Section 53 and the gradient in Section 541 Lets extend the cross entropy loss for one example from Eq 510 to minibatches of size m Well continue to use the notation that xi and yi mean the ith training features and training label respectively We make the assumption that the training examples are independent log ptraining labels log m i1 pyixi 518 m i1 log pyixi 519 m i1 LCEŷiyi 520 55 REGULARIZATION 13 Now the cost function for the minibatch of m examples is the average loss for each example Costwb 1 m m i1 LCEŷiyi 1 m m i1 yi logσw xib1 yi log 1σw xib 521 The minibatch gradient is the average of the individual gradients from Eq 517 Costwb w j 1 m m i1 σw xib yi xij 522 55 Regularization Numquam ponenda est pluralitas sine necessitate Plurality should never be proposed unless needed William of Occam There is a problem with learning weights that make the model perfectly match the training data If a feature is perfectly predictive of the outcome because it happens to only occur in one class it will be assigned a very high weight The weights for features will attempt to perfectly fit details of the training set in fact too perfectly modeling noisy factors that just accidentally correlate with the class This problem is called overfitting A good model should be able to generalize well from the trainingoverfitting generalize data to the unseen test set but a model that overfits will have poor generalization To avoid overfitting a new regularization term Rθ is added to the objectiveregularization function in Eq 512 resulting in the following objective for a batch of m exam ples slightly rewritten from Eq 512 to be maximizing log probability rather than minimizing loss and removing the 1m term which doesnt affect the argmax θ̂ argmax θ m i1 logPyixiαRθ 523 The new regularization term Rθ is used to penalize large weights Thus a setting of the weights that matches the training data perfectly but uses many weights with high values to do sowill be penalized more than a setting that matches the data a little less well but does so using smaller weights There are two common ways to compute this regularization term Rθ L2 regularization is a quadratic function ofL2regularization the weight values named because it uses the square of the L2 norm of the weight values The L2 norm θ 2 is the same as the Euclidean distance of the vector θ from the origin If θ consists of n weights then Rθ θ 22 n j1 θ 2j 524 14 CHAPTER 5 LOGISTIC REGRESSION The L2 regularized objective function becomes θ̂ argmax θ m 1i logPyixi α n j1 θ 2j 525 L1 regularization is a linear function of the weight values named after the L1 normL1regularization W 1 the sum of the absolute values of the weights or Manhattan distance the Manhattan distance is the distance youd have to walk between two points in a city with a street grid like New York Rθ θ 1 n i1 θi 526 The L1 regularized objective function becomes θ̂ argmax θ m 1i logPyixi α n j1 θ j 527 These kinds of regularization come from statistics where L1 regularization is called lasso regression Tibshirani 1996 and L2 regularization is called ridge regressionlasso ridge and both are commonly used in language processing L2 regularization is easier to optimize because of its simple derivative the derivative of θ 2 is just 2θ while L1 regularization is more complex the derivative of θ is noncontinuous at zero But where L2 prefers weight vectors with many small weights L1 prefers sparse solutions with some larger weights but many more weights set to zero Thus L1 regularization leads to much sparser weight vectors that is far fewer features Both L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look L1 regularization can be viewed as a Laplace prior on the weights L2 regularization corresponds to assuming that weights are distributed according to a gaussian distribution with mean µ 0 In a gaussian or normal distribution the further away a value is from the mean the lower its probability scaled by the variance σ By using a gaussian prior on the weights we are saying that weights prefer to have the value 0 A gaussian for a weight θ j is 1 2πσ2j exp θ jµ j2 2σ2j 528 If we multiply each weight by a gaussian prior on the weight we are thus maximiz ing the following constraint θ̂ argmax θ M i1 Pyixi n j1 1 2πσ2j exp θ jµ j2 2σ2j 529 which in log space with µ 0 and assuming 2σ2 1 corresponds to θ̂ argmax θ m i1 logPyixiα n j1 θ 2j 530 which is in the same form as Eq 525 56 MULTINOMIAL LOGISTIC REGRESSION 15 56 Multinomial logistic regression Sometimes we need more than two classes Perhaps we might want to do 3way sentiment classification positive negative or neutral Or we could be classifying the part of speech of a word choosing from 10 30 or even 50 different parts of speech or assigning semantic labels like the named entities or semantic relations we will introduce in Chapter 18 In such cases we use multinomial logistic regression also called softmax re multinomial logistic regression gression or historically the maxent classifier In multinomial logistic regression the target y is a variable that ranges over more than two classes we want to know the probability of y being in each potential class c C py cx The multinomial logistic classifier uses a generalization of the sigmoid called the softmax function to compute the probability py cx The softmax functionsoftmax takes a vector z z1z2 zk of k arbitrary values and maps them to a probability distribution with each value in the range 01 and all the values summing to 1 Like the sigmoid it is an exponential function For a vector z of dimensionality k the softmax is defined as softmaxzi ezik j1 e z j 1 i k 531 The softmax of an input vector z z1z2 zk is thus a vector itself softmaxz ez1k i1 ezi ez2k i1 ezi ezkk i1 ezi 532 The denominator k i1 e zi is used to normalize all the values into probabilities Thus for example given a vector z 061115123211 the result softmaxz is 00550090000670100740010 Again like the sigmoid the input to the softmax will be the dot product between a weight vector w and an input vector x plus a bias But now well need separate weight vectors and bias for each of the K classes py cx e wc xbc k j1 ew j xb j 533 Like the sigmoid the softmax has the property of squashing values toward 0 or 1 Thus if one of the inputs is larger than the others it will tend to push its probability toward 1 and suppress the probabilities of the smaller inputs 16 CHAPTER 5 LOGISTIC REGRESSION 561 Features in Multinomial Logistic Regression For multiclass classification the input features need to be a function of both the observation x and the candidate output class c Thus instead of the notation xi fi or fix when were discussing features we will use the notation ficx meaning feature i for a particular class c for a given observation x In binary classification a positive weight on a feature pointed toward y1 and a negative weight toward y0 but in multiclass classification a feature could be evidence for or against an individual class Lets look at some sample features for a few NLP tasks to help understand this perhaps unintuitive use of features that are functions of both the observation x and the class c Suppose we are doing text classification and instead of binary classification our task is to assign one of the 3 classes or 0 neutral to a document Now a feature related to exclamation marks might have a negative weight for 0 documents and a positive weight for or documents Var Definition Wt f10x 1 if doc 0 otherwise 45 f1x 1 if doc 0 otherwise 26 f1x 1 if doc 0 otherwise 13 562 Learning in Multinomial Logistic Regression Multinomial logistic regression has a slightly different loss function than binary lo gistic regression because it uses the softmax rather than the sigmoid classifier The loss function for a single example x is the sum of the logs of the K output classes LCEŷy K k1 1y k log py kx K k1 1y k log e wkxbkK j1 e w j xb j 534 This makes use of the function 1 which evaluates to 1 if the condition in the brackets is true and to 0 otherwise The gradient for a single example turns out to be very similar to the gradient for logistic regression although we dont show the derivation here It is the difference between the value for the true class k which is 1 and the probability the classifier outputs for class k weighted by the value of the input xk LCE wk 1y k py kxxk 1y k e wkxbkK j1 e w j xb j xk 535 57 INTERPRETING MODELS 17 57 Interpreting models Often we want to know more than just the correct classification of an observation We want to know why the classifier made the decision it did That is we want our decision to be interpretable Interpretability can be hard to define strictly but theinterpretable core idea is that as humans we should know why our algorithms reach the conclu sions they do Because the features to logistic regression are often humandesigned one way to understand a classifiers decision is to understand the role each feature plays in the decision Logistic regression can be combined with statistical tests the likelihood ratio test or the Wald test investigating whether a particular feature is significant by one of these tests or inspecting its magnitude how large is the weight w associated with the feature can help us interpret why the classifier made the decision it makes This is enormously important for building transparent models Furthermore in addition to its use as a classifier logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables features In text classification perhaps we want to know if logically negative words no not never are more likely to be asso ciated with negative sentiment or if negative reviews of movies are more likely to discuss the cinematography However in doing so its necessary to control for po tential confounds other factors that might influence sentiment the movie genre the year it was made perhaps the length of the review in words Or we might be study ing the relationship between NLPextracted linguistic features and nonlinguistic outcomes hospital readmissions political outcomes or product sales but need to control for confounds the age of the patient the county of voting the brand of the product In such cases logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features 58 Advanced Deriving the Gradient Equation In this section we give the derivation of the gradient of the crossentropy loss func tion LCE for logistic regression Lets start with some quick calculus refreshers First the derivative of lnx d dx lnx 1 x 536 Second the very elegant derivative of the sigmoid dσz dz σz1σz 537 Finally the chain rule of derivatives Suppose we are computing the derivativechain rule of a composite function f x uvx The derivative of f x is the derivative of ux with respect to vx times the derivative of vx with respect to x d f dx du dv dv dx 538 First we want to know the derivative of the loss function with respect to a single weight w j well need to compute it for each weight and for the bias 18 CHAPTER 5 LOGISTIC REGRESSION LLwb w j w j y logσw xb1 y log1σw xb w j y logσw xb w j 1 y log 1σw xb 539 Next using the chain rule and relying on the derivative of log LLwb w j y σw xb w j σw xb 1 y 1σw xb w j 1σw xb 540 Rearranging terms LLwb w j y σw xb 1 y 1σw xb w j σw xb 541 And now plugging in the derivative of the sigmoid and using the chain rule one more time we end up with Eq 542 LLwb w j yσw xb σw xb1σw xb σw xb1σw xb w xb w j yσw xb σw xb1σw xb σw xb1σw xbx j yσw xbx j σw xb yx j 542 59 Summary This chapter introduced the logistic regression model of classification Logistic regression is a supervised machine learning classifier that extracts realvalued features from the input multiplies each by a weight sums them and passes the sum through a sigmoid function to generate a probability A threshold is used to make a decision Logistic regression can be used with two classes eg positive and negative sentiment or with multiple classes multinomial logistic regression for ex ample for nary text classification partofspeech labeling etc Multinomial logistic regression uses the softmax function to compute proba bilities The weights vector w and bias b are learned from a labeled training set via a loss function such as the crossentropy loss that must be minimized Minimizing this loss function is a convex optimization problem and iterative algorithms like gradient descent are used to find the optimal weights Regularization is used to avoid overfitting Logistic regression is also one of the most useful analytic tools because of its ability to transparently study the importance of individual features BIBLIOGRAPHICAL AND HISTORICAL NOTES 19 Bibliographical and Historical Notes Logistic regression was developed in the field of statistics where it was used for the analysis of binary data by the 1960s and was particularly common in medicine Cox 1969 Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation Sankoff and Labov 1979 Nonetheless logistic regression didnt become common in natural language pro cessing until the 1990s when it seems to have appeared simultaneously from two directions The first source was the neighboring fields of information retrieval and speech processing both of which had made use of regression and both of which lent many other statistical techniques to NLP Indeed a very early use of logistic regression for document routing was one of the first NLP applications to use LSI embeddings as word representations Schütze et al 1995 At the same time in the early 1990s logistic regression was developed and ap plied to NLP at IBM Research under the name maximum entropy modeling ormaximumentropy maxent Berger et al 1996 seemingly independent of the statistical literature Un der that name it was applied to language modeling Rosenfeld 1996 partofspeech tagging Ratnaparkhi 1996 parsing Ratnaparkhi 1997 coreference resolution Kehler 1997 and text classification Nigam et al 1999 More on classification can be found in machine learning textbooks Hastie et al 2001 Witten and Frank 2005 Bishop 2006 Murphy 2012 Exercises 20 Chapter 5 Logistic Regression Berger A Della Pietra S A and Della Pietra V J 1996 A maximum entropy approach to natural language process ing Computational Linguistics 221 3971 Bishop C M 2006 Pattern recognition and machine learning Springer Cox D 1969 Analysis of Binary Data Chapman and Hall London Hastie T Tibshirani R J and Friedman J H 2001 The Elements of Statistical Learning Springer Kehler A 1997 Probabilistic coreference in information extraction In EMNLP 1997 163173 Murphy K P 2012 Machine learning A probabilistic perspective MIT press Ng A Y and Jordan M I 2002 On discriminative vs generative classifiers A comparison of logistic regression and naive bayes In NIPS 14 841848 Nigam K Lafferty J D and McCallum A 1999 Using maximum entropy for text classification In IJCAI99 work shop on machine learning for information filtering 6167 Ratnaparkhi A 1996 A maximum entropy partofspeech tagger In EMNLP 1996 133142 Ratnaparkhi A 1997 A linear observed time statistical parser based on maximum entropy models In EMNLP 1997 110 Rosenfeld R 1996 A maximum entropy approach to adaptive statistical language modeling Computer Speech and Language 10 187228 Sankoff D and Labov W 1979 On the uses of variable rules Language in society 823 189222 Schütze H Hull D A and Pedersen J 1995 A com parison of classifiers and document representations for the routing problem In SIGIR95 229237 Tibshirani R J 1996 Regression shrinkage and selection via the lasso Journal of the Royal Statistical Society Series B Methodological 581 267288 Wang S and Manning C D 2012 Baselines and bigrams Simple good sentiment and topic classification In ACL 2012 9094 Witten I H and Frank E 2005 Data Mining Practical Machine Learning Tools and Techniques 2nd Ed Mor gan Kaufmann
