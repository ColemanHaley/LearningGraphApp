b"Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved.\n\nDraft of October 2, 2019.\n\nCopyright c(cid:13) 2019.\n\nAll\n\nCHAPTER\n\n5 Logistic Regression\n\nlogistic\nregression\n\n\xe2\x80\x9dAnd how do you know that these \xef\xac\x81ne begonias are not of equal importance?\xe2\x80\x9d\nHercule Poirot, in Agatha Christie\xe2\x80\x99s The Mysterious Affair at Styles\nDetective stories are as littered with clues as texts are with words. Yet for the\npoor reader it can be challenging to know how to weigh the author\xe2\x80\x99s clues in order\nto make the crucial classi\xef\xac\x81cation task: deciding whodunnit.\n\nIn this chapter we introduce an algorithm that is admirably suited for discovering\nthe link between features or cues and some particular outcome: logistic regression.\nIndeed, logistic regression is one of the most important analytic tools in the social\nand natural sciences. In natural language processing, logistic regression is the base-\nline supervised machine learning algorithm for classi\xef\xac\x81cation, and also has a very\nclose relationship with neural networks. As we will see in Chapter 7, a neural net-\nwork can be viewed as a series of logistic regression classi\xef\xac\x81ers stacked on top of\neach other. Thus the classi\xef\xac\x81cation and machine learning techniques introduced here\nwill play an important role throughout the book.\n\nLogistic regression can be used to classify an observation into one of two classes\n(like \xe2\x80\x98positive sentiment\xe2\x80\x99 and \xe2\x80\x98negative sentiment\xe2\x80\x99), or into one of many classes.\nBecause the mathematics for the two-class case is simpler, we\xe2\x80\x99ll describe this special\ncase of logistic regression \xef\xac\x81rst in the next few sections, and then brie\xef\xac\x82y summarize\nthe use of multinomial logistic regression for more than two classes in Section 5.6.\nWe\xe2\x80\x99ll introduce the mathematics of logistic regression in the next few sections.\n\nBut let\xe2\x80\x99s begin with some high-level issues.\nGenerative and Discriminative Classi\xef\xac\x81ers: The most important difference be-\ntween naive Bayes and logistic regression is that logistic regression is a discrimina-\ntive classi\xef\xac\x81er while naive Bayes is a generative classi\xef\xac\x81er.\n\nThese are two very different frameworks for how\nto build a machine learning model. Consider a visual\nmetaphor:\nimagine we\xe2\x80\x99re trying to distinguish dog\nimages from cat images. A generative model would\nhave the goal of understanding what dogs look like\nand what cats look like. You might literally ask such\na model to \xe2\x80\x98generate\xe2\x80\x99, i.e. draw, a dog. Given a test\nimage, the system then asks whether it\xe2\x80\x99s the cat model or the dog model that better\n\xef\xac\x81ts (is less surprised by) the image, and chooses that as its label.\n\nA discriminative model, by contrast, is only try-\ning to learn to distinguish the classes (perhaps with-\nout learning much about them). So maybe all the\ndogs in the training data are wearing collars and the\ncats aren\xe2\x80\x99t. If that one feature neatly separates the\nclasses, the model is satis\xef\xac\x81ed.\nIf you ask such a\nmodel what it knows about cats all it can say is that\nthey don\xe2\x80\x99t wear collars.\n\n\x0c2 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\nMore formally, recall that the naive Bayes assigns a class c to a document d not\n\nby directly computing P(c|d) but by computing a likelihood and a prior\n\n(cid:122) (cid:125)(cid:124) (cid:123)\n\nlikelihood\nP(d|c)\n\n(cid:122)(cid:125)(cid:124)(cid:123)\n\nprior\nP(c)\n\n\xcb\x86c = argmax\n\nc\xe2\x88\x88C\n\n(5.1)\n\ngenerative\nmodel\n\ndiscriminative\nmodel\n\nA generative model like naive Bayes makes use of this likelihood term, which\nexpresses how to generate the features of a document if we knew it was of class c.\nBy contrast a discriminative model in this text categorization scenario attempts\nto directly compute P(c|d). Perhaps it will learn to assign a high weight to document\nfeatures that directly improve its ability to discriminate between possible classes,\neven if it couldn\xe2\x80\x99t generate an example of one of the classes.\nComponents of a probabilistic machine learning classi\xef\xac\x81er: Like naive Bayes,\nlogistic regression is a probabilistic classi\xef\xac\x81er that makes use of supervised machine\nlearning. Machine learning classi\xef\xac\x81ers require a training corpus of M input/output\npairs (x(i),y(i)). (We\xe2\x80\x99ll use superscripts in parentheses to refer to individual instances\nin the training set\xe2\x80\x94for sentiment classi\xef\xac\x81cation each instance might be an individual\ndocument to be classi\xef\xac\x81ed). A machine learning system for classi\xef\xac\x81cation then has\nfour components:\n\n1. A feature representation of the input. For each input observation x(i), this\nwill be a vector of features [x1,x2, ...,xn]. We will generally refer to feature\ni for input x( j) as x( j)\n, sometimes simpli\xef\xac\x81ed as xi, but we will also see the\ni\nnotation fi, fi(x), or, for multiclass classi\xef\xac\x81cation, fi(c,x).\n2. A classi\xef\xac\x81cation function that computes \xcb\x86y, the estimated class, via p(y|x). In\nthe next section we will introduce the sigmoid and softmax tools for classi\xef\xac\x81-\ncation.\n\n3. An objective function for learning, usually involving minimizing error on\n\ntraining examples. We will introduce the cross-entropy loss function\n\n4. An algorithm for optimizing the objective function. We introduce the stochas-\n\ntic gradient descent algorithm.\n\nLogistic regression has two phases:\ntraining: we train the system (speci\xef\xac\x81cally the weights w and b) using stochastic\ntest: Given a test example x we compute p(y|x) and return the higher probability\n\ngradient descent and the cross-entropy loss.\n\nlabel y = 1 or y = 0.\n\n5.1 Classi\xef\xac\x81cation: the sigmoid\n\nThe goal of binary logistic regression is to train a classi\xef\xac\x81er that can make a binary\ndecision about the class of a new input observation. Here we introduce the sigmoid\nclassi\xef\xac\x81er that will help us make this decision.\n\nConsider a single input observation x, which we will represent by a vector of\nfeatures [x1,x2, ...,xn] (we\xe2\x80\x99ll show sample features in the next subsection). The clas-\nsi\xef\xac\x81er output y can be 1 (meaning the observation is a member of the class) or 0\n(the observation is not a member of the class). We want to know the probability\nP(y = 1|x) that this observation is a member of the class. So perhaps the decision\n\n\x0cbias term\nintercept\n\n5.1\n\n\xe2\x80\xa2 CLASSIFICATION: THE SIGMOID\n\n3\n\nis \xe2\x80\x9cpositive sentiment\xe2\x80\x9d versus \xe2\x80\x9cnegative sentiment\xe2\x80\x9d, the features represent counts\nof words in a document, and P(y = 1|x) is the probability that the document has\npositive sentiment, while and P(y = 0|x) is the probability that the document has\nnegative sentiment.\n\nLogistic regression solves this task by learning, from a training set, a vector of\nweights and a bias term. Each weight wi is a real number, and is associated with one\nof the input features xi. The weight wi represents how important that input feature is\nto the classi\xef\xac\x81cation decision, and can be positive (meaning the feature is associated\nwith the class) or negative (meaning the feature is not associated with the class).\nThus we might expect in a sentiment task the word awesome to have a high positive\nweight, and abysmal to have a very negative weight. The bias term, also called the\nintercept, is another real number that\xe2\x80\x99s added to the weighted inputs.\n\nTo make a decision on a test instance\xe2\x80\x94 after we\xe2\x80\x99ve learned the weights in\ntraining\xe2\x80\x94 the classi\xef\xac\x81er \xef\xac\x81rst multiplies each xi by its weight wi, sums up the weighted\nfeatures, and adds the bias term b. The resulting single number z expresses the\nweighted sum of the evidence for the class.\n\n(cid:32) n(cid:88)\n\n(cid:33)\n\nz =\n\nwixi\n\n+ b\n\n(5.2)\n\ni=1\n\ndot product\n\nIn the rest of the book we\xe2\x80\x99ll represent such sums using the dot product notation from\nlinear algebra. The dot product of two vectors a and b, written as a\xc2\xb7 b is the sum of\nthe products of the corresponding elements of each vector. Thus the following is an\nequivalent formation to Eq. 5.2:\n\nz = w\xc2\xb7 x + b\n\n(5.3)\n\nBut note that nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie\nbetween 0 and 1. In fact, since weights are real-valued, the output might even be\nnegative; z ranges from \xe2\x88\x92\xe2\x88\x9e to \xe2\x88\x9e.\n\nFigure 5.1 The sigmoid function y = 1\nIt is nearly linear around 0 but outlier values get squashed toward 0 or 1.\n\n1+e\xe2\x88\x92z takes a real value and maps it to the range [0,1].\n\nsigmoid\n\nlogistic\nfunction\n\nTo create a probability, we\xe2\x80\x99ll pass z through the sigmoid function, \xcf\x83 (z). The\nsigmoid function (named because it looks like an s) is also called the logistic func-\ntion, and gives logistic regression its name. The sigmoid has the following equation,\nshown graphically in Fig. 5.1:\n\ny = \xcf\x83 (z) =\n\n1\n\n1 + e\xe2\x88\x92z\n\n(5.4)\n\nThe sigmoid has a number of advantages; it takes a real-valued number and maps\nit into the range [0,1], which is just what we want for a probability. Because it is\n\n\x0c4 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\nnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier\nvalues toward 0 or 1. And it\xe2\x80\x99s differentiable, which as we\xe2\x80\x99ll see in Section 5.8 will\nbe handy for learning.\n\nWe\xe2\x80\x99re almost there. If we apply the sigmoid to the sum of the weighted features,\nwe get a number between 0 and 1. To make it a probability, we just need to make\nsure that the two cases, p(y = 1) and p(y = 0), sum to 1. We can do this as follows:\n\nP(y = 1) = \xcf\x83 (w\xc2\xb7 x + b)\n\n=\n\n1\n\n1 + e\xe2\x88\x92(w\xc2\xb7x+b)\n\nP(y = 0) = 1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)\n\n= 1\xe2\x88\x92\n\n1\n\n1 + e\xe2\x88\x92(w\xc2\xb7x+b)\ne\xe2\x88\x92(w\xc2\xb7x+b)\n1 + e\xe2\x88\x92(w\xc2\xb7x+b)\n\n=\n\n(5.5)\n\ndecision\nboundary\n\nNow we have an algorithm that given an instance x computes the probability P(y =\n1|x). How do we make a decision? For a test instance x, we say yes if the probability\nP(y = 1|x) is more than .5, and no otherwise. We call .5 the decision boundary:\n\n(cid:26) 1 if P(y = 1|x) > 0.5\n\n0 otherwise\n\n\xcb\x86y =\n\n5.1.1 Example: sentiment classi\xef\xac\x81cation\nLet\xe2\x80\x99s have an example. Suppose we are doing binary sentiment classi\xef\xac\x81cation on\nmovie review text, and we would like to know whether to assign the sentiment class\n+ or \xe2\x88\x92 to a review document doc. We\xe2\x80\x99ll represent each input observation by the 6\nfeatures x1...x6 of the input shown in the following table; Fig. 5.2 shows the features\nin a sample mini test document.\n\ncount(positive lexicon) \xe2\x88\x88 doc)\ncount(negative lexicon) \xe2\x88\x88 doc)\n\n(cid:26) 1 if \xe2\x80\x9cno\xe2\x80\x9d \xe2\x88\x88 doc\n(cid:26) 1 if \xe2\x80\x9c!\xe2\x80\x9d \xe2\x88\x88 doc\n\nVar De\xef\xac\x81nition\nx1\nx2\nx3\nx4\nx5\nx6\n\n0 otherwise\n\n0 otherwise\n\nlog(word count of doc)\n\ncount(1st and 2nd pronouns \xe2\x88\x88 doc)\n\nValue in Fig. 5.2\n3\n2\n\n1\n3\n\n0\nln(66) = 4.19\n\nLet\xe2\x80\x99s assume for the moment that we\xe2\x80\x99ve already learned a real-valued weight for\neach of these features, and that the 6 weights corresponding to the 6 features are\n[2.5,\xe2\x88\x925.0,\xe2\x88\x921.2,0.5,2.0,0.7], while b = 0.1. (We\xe2\x80\x99ll discuss in the next section how\nthe weights are learned.) The weight w1, for example indicates how important a\nfeature the number of positive lexicon words (great, nice, enjoyable, etc.)\nis to\na positive sentiment decision, while w2 tells us the importance of negative lexicon\nwords. Note that w1 = 2.5 is positive, while w2 = \xe2\x88\x925.0, meaning that negative words\nare negatively associated with a positive sentiment decision, and are about twice as\nimportant as positive words.\n\n\x0c5.1\n\n\xe2\x80\xa2 CLASSIFICATION: THE SIGMOID\n\n5\n\nFigure 5.2 A sample mini test document showing the extracted features in the vector x.\n\nGiven these 6 features and the input review x, P(+|x) and P(\xe2\x88\x92|x) can be com-\n\nputed using Eq. 5.5:\np(+|x) = P(Y = 1|x) = \xcf\x83 (w\xc2\xb7 x + b)\n\n= \xcf\x83 ([2.5,\xe2\x88\x925.0,\xe2\x88\x921.2,0.5,2.0,0.7]\xc2\xb7 [3,2,1,3,0,4.19] + 0.1)\n= \xcf\x83 (.833)\n= 0.70\n\n(5.6)\n\np(\xe2\x88\x92|x) = P(Y = 0|x) = 1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)\n\n= 0.30\n\nLogistic regression is commonly applied to all sorts of NLP tasks, and any property\nof the input can be a feature. Consider the task of period disambiguation: deciding\nif a period is the end of a sentence or part of a word, by classifying each period\ninto one of two classes EOS (end-of-sentence) and not-EOS. We might use features\nlike x1 below expressing that the current word is lower case and the class is EOS\n(perhaps with a positive weight), or that the current word is in our abbreviations\ndictionary (\xe2\x80\x9cProf.\xe2\x80\x9d) and the class is EOS (perhaps with a negative weight). A feature\ncan also express a quite complex combination of properties. For example a period\nfollowing an upper case word is likely to be an EOS, but if the word itself is St. and\nthe previous word is capitalized, then the period is likely part of a shortening of the\nword street.\n\n0 otherwise\n\n(cid:26) 1 if \xe2\x80\x9cCase(wi) = Lower\xe2\x80\x9d\n(cid:26) 1 if \xe2\x80\x9cwi \xe2\x88\x88 AcronymDict\xe2\x80\x9d\n(cid:26) 1 if \xe2\x80\x9cwi = St. & Case(wi\xe2\x88\x921) = Cap\xe2\x80\x9d\n\n0 otherwise\n\n0 otherwise\n\nx1 =\n\nx2 =\n\nx3 =\n\nDesigning features: Features are generally designed by examining the training\nset with an eye to linguistic intuitions and the linguistic literature on the domain. A\ncareful error analysis on the training set or devset of an early version of a system\noften provides insights into features.\n\nFor some tasks it is especially helpful to build complex features that are combi-\nnations of more primitive features. We saw such a feature for period disambiguation\nabove, where a period on the word St. was less likely to be the end of the sentence\nif the previous word was capitalized. For logistic regression and naive Bayes these\ncombination features or feature interactions have to be designed by hand.\n\nfeature\ninteractions\n\nx2=2\n\nx3=1\n\n It's hokey . There are virtually no surprises , and the writing is second-rate . \nSo why was it so enjoyable  ? For one thing , the cast is\n great . Another nice touch is the music . I was overcome with the urge to get off\n the couch and start dancing .  It sucked me in , and it'll do the same to you  .\n\nx1=3\n\nx5=0\n\nx6=4.19\n\nx4=3\n\n\x0c6 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\nfeature\ntemplates\n\nFor many tasks (especially when feature values can reference speci\xef\xac\x81c words)\nwe\xe2\x80\x99ll need large numbers of features. Often these are created automatically via fea-\nture templates, abstract speci\xef\xac\x81cations of features. For example a bigram template\nfor period disambiguation might create a feature for every pair of words that occurs\nbefore a period in the training set. Thus the feature space is sparse, since we only\nhave to create a feature if that n-gram exists in that position in the training set. The\nfeature is generally created as a hash from the string descriptions. A user description\nof a feature as, \xe2\x80\x9cbigram(American breakfast)\xe2\x80\x9d is hashed into a unique integer i that\nbecomes the feature number fi.\n\nIn order to avoid the extensive human effort of feature design, recent research in\nNLP has focused on representation learning: ways to learn features automatically\nin an unsupervised way from the input. We\xe2\x80\x99ll introduce methods for representation\nlearning in Chapter 6 and Chapter 7.\n\nChoosing a classi\xef\xac\x81er Logistic regression has a number of advantages over naive\nBayes. Naive Bayes has overly strong conditional independence assumptions. Con-\nsider two features which are strongly correlated; in fact, imagine that we just add the\nsame feature f1 twice. Naive Bayes will treat both copies of f1 as if they were sep-\narate, multiplying them both in, overestimating the evidence. By contrast, logistic\nregression is much more robust to correlated features; if two features f1 and f2 are\nperfectly correlated, regression will simply assign part of the weight to w1 and part\nto w2. Thus when there are many correlated features, logistic regression will assign\na more accurate probability than naive Bayes. So logistic regression generally works\nbetter on larger documents or datasets and is a common default.\n\nDespite the less accurate probabilities, naive Bayes still often makes the correct\nclassi\xef\xac\x81cation decision. Furthermore, naive Bayes can work extremely well (some-\ntimes even better than logistic regression) on very small datasets (Ng and Jordan,\n2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is\neasy to implement and very fast to train (there\xe2\x80\x99s no optimization step). So it\xe2\x80\x99s still a\nreasonable approach to use in some situations.\n\n5.2 Learning in Logistic Regression\n\nHow are the parameters of the model, the weights w and bias b, learned? Logistic\nregression is an instance of supervised classi\xef\xac\x81cation in which we know the correct\nlabel y (either 0 or 1) for each observation x. What the system produces via Eq. 5.5\nis \xcb\x86y, the system\xe2\x80\x99s estimate of the true y. We want to learn parameters (meaning w\nand b) that make \xcb\x86y for each training observation as close as possible to the true y .\n\nThis requires 2 components that we foreshadowed in the introduction to the\nchapter. The \xef\xac\x81rst is a metric for how close the current label ( \xcb\x86y) is to the true gold\nlabel y. Rather than measure similarity, we usually talk about the opposite of this:\nthe distance between the system output and the gold output, and we call this distance\nthe loss function or the cost function. In the next section we\xe2\x80\x99ll introduce the loss\nfunction that is commonly used for logistic regression and also for neural networks,\nthe cross-entropy loss.\n\nThe second thing we need is an optimization algorithm for iteratively updating\nthe weights so as to minimize this loss function. The standard algorithm for this is\ngradient descent; we\xe2\x80\x99ll introduce the stochastic gradient descent algorithm in the\nfollowing section.\n\nloss\n\n\x0c5.3 The cross-entropy loss function\n\n5.3\n\n\xe2\x80\xa2 THE CROSS-ENTROPY LOSS FUNCTION\n\n7\n\ncross-entropy\nloss\n\nWe need a loss function that expresses, for an observation x, how close the classi\xef\xac\x81er\noutput ( \xcb\x86y = \xcf\x83 (w\xc2\xb7 x + b)) is to the correct output (y, which is 0 or 1). We\xe2\x80\x99ll call this:\n\nL( \xcb\x86y,y) = How much \xcb\x86y differs from the true y\n\n(5.7)\n\nWe do this via a loss function that prefers the correct class labels of the train-\ning examples to be more likely. This is called conditional maximum likelihood\nestimation: we choose the parameters w,b that maximize the log probability of\nthe true y labels in the training data given the observations x. The resulting loss\nfunction is the negative log likelihood loss, generally called the cross-entropy loss.\nLet\xe2\x80\x99s derive this loss function, applied to a single observation x. We\xe2\x80\x99d like to\nlearn weights that maximize the probability of the correct label p(y|x). Since there\nare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can\nexpress the probability p(y|x) that our classi\xef\xac\x81er produces for one observation as\nthe following (keeping in mind that if y=1, Eq. 5.8 simpli\xef\xac\x81es to \xcb\x86y; if y=0, Eq. 5.8\nsimpli\xef\xac\x81es to 1\xe2\x88\x92 \xcb\x86y):\n\np(y|x) = \xcb\x86y y (1\xe2\x88\x92 \xcb\x86y)1\xe2\x88\x92y\n\n(5.8)\n\nNow we take the log of both sides. This will turn out to be handy mathematically,\nand doesn\xe2\x80\x99t hurt us; whatever values maximize a probability will also maximize the\nlog of the probability:\n\nlog p(y|x) = log(cid:2) \xcb\x86y y (1\xe2\x88\x92 \xcb\x86y)1\xe2\x88\x92y(cid:3)\n\n= ylog \xcb\x86y + (1\xe2\x88\x92 y)log(1\xe2\x88\x92 \xcb\x86y)\n\n(5.9)\n\nEq. 5.9 describes a log likelihood that should be maximized. In order to turn this\ninto loss function (something that we need to minimize), we\xe2\x80\x99ll just \xef\xac\x82ip the sign on\nEq. 5.9. The result is the cross-entropy loss LCE:\n\nLCE ( \xcb\x86y,y) = \xe2\x88\x92log p(y|x) = \xe2\x88\x92 [ylog \xcb\x86y + (1\xe2\x88\x92 y)log(1\xe2\x88\x92 \xcb\x86y)]\n\n(5.10)\n\nFinally, we can plug in the de\xef\xac\x81nition of \xcb\x86y = \xcf\x83 (w\xc2\xb7 x + b):\n\nLCE (w,b) = \xe2\x88\x92 [ylog\xcf\x83 (w\xc2\xb7 x + b) + (1\xe2\x88\x92 y)log (1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b))]\n\n(5.11)\n\nLet\xe2\x80\x99s see if this loss function does the right thing for our example from Fig. 5.2. We\nwant the loss to be smaller if the model\xe2\x80\x99s estimate is close to correct, and bigger if\nthe model is confused. So \xef\xac\x81rst let\xe2\x80\x99s suppose the correct gold label for the sentiment\nexample in Fig. 5.2 is positive, i.e., y = 1. In this case our model is doing well, since\nfrom Eq. 5.6 it indeed gave the example a higher probability of being positive (.69)\nthan negative (.31). If we plug \xcf\x83 (w\xc2\xb7 x + b) = .69 and y = 1 into Eq. 5.11, the right\nside of the equation drops out, leading to the following loss:\n\nLCE (w,b) =\n=\n\n=\n\n=\n\n\xe2\x88\x92[ylog\xcf\x83 (w\xc2\xb7 x + b) + (1\xe2\x88\x92 y)log (1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b))]\n\xe2\x88\x92 [log\xcf\x83 (w\xc2\xb7 x + b)]\n\xe2\x88\x92log(.69)\n.37\n\n\x0c8 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\nBy contrast, let\xe2\x80\x99s pretend instead that the example in Fig. 5.2 was actually negative,\ni.e. y = 0 (perhaps the reviewer went on to say \xe2\x80\x9cBut bottom line, the movie is\nterrible! I beg you not to see it!\xe2\x80\x9d). In this case our model is confused and we\xe2\x80\x99d want\nthe loss to be higher. Now if we plug y = 0 and 1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b) = .31 from Eq. 5.6\ninto Eq. 5.11, the left side of the equation drops out:\n\nLCE (w,b) =\n=\n\n=\n\n=\n\n\xe2\x88\x92[ylog\xcf\x83 (w\xc2\xb7 x + b)+(1\xe2\x88\x92 y)log (1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b))]\n\n\xe2\x88\x92 [log (1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b))]\n\xe2\x88\x92 log (.31)\n1.17\n\nSure enough, the loss for the \xef\xac\x81rst classi\xef\xac\x81er (.37) is less than the loss for the second\nclassi\xef\xac\x81er (1.17).\n\nWhy does minimizing this negative log probability do what we want? A per-\nfect classi\xef\xac\x81er would assign probability 1 to the correct outcome (y=1 or y=0) and\nprobability 0 to the incorrect outcome. That means the higher \xcb\x86y (the closer it is to\n1), the better the classi\xef\xac\x81er; the lower \xcb\x86y is (the closer it is to 0), the worse the clas-\nsi\xef\xac\x81er. The negative log of this probability is a convenient loss metric since it goes\nfrom 0 (negative log of 1, no loss) to in\xef\xac\x81nity (negative log of 0, in\xef\xac\x81nite loss). This\nloss function also ensures that as the probability of the correct answer is maximized,\nthe probability of the incorrect answer is minimized; since the two sum to one, any\nincrease in the probability of the correct answer is coming at the expense of the in-\ncorrect answer. It\xe2\x80\x99s called the cross-entropy loss, because Eq. 5.9 is also the formula\nfor the cross-entropy between the true probability distribution y and our estimated\ndistribution \xcb\x86y.\n\nNow we know what we want to minimize; in the next section, we\xe2\x80\x99ll see how to\n\n\xef\xac\x81nd the minimum.\n\n5.4 Gradient Descent\n\nOur goal with gradient descent is to \xef\xac\x81nd the optimal weights: minimize the loss\nfunction we\xe2\x80\x99ve de\xef\xac\x81ned for the model. In Eq. 5.12 below, we\xe2\x80\x99ll explicitly represent\nthe fact that the loss function L is parameterized by the weights, which we\xe2\x80\x99ll refer to\nin machine learning in general as \xce\xb8 (in the case of logistic regression \xce\xb8 = w,b):\n\n\xcb\x86\xce\xb8 = argmin\n\n\xce\xb8\n\n1\nm\n\nm(cid:88)\n\ni=1\n\nLCE (y(i),x(i);\xce\xb8 )\n\n(5.12)\n\nHow shall we \xef\xac\x81nd the minimum of this (or any) loss function? Gradient descent\nis a method that \xef\xac\x81nds a minimum of a function by \xef\xac\x81guring out in which direction\n(in the space of the parameters \xce\xb8) the function\xe2\x80\x99s slope is rising the most steeply,\nand moving in the opposite direction. The intuition is that if you are hiking in a\ncanyon and trying to descend most quickly down to the river at the bottom, you might\nlook around yourself 360 degrees, \xef\xac\x81nd the direction where the ground is sloping the\nsteepest, and walk downhill in that direction.\n\nFor logistic regression, this loss function is conveniently convex. A convex func-\ntion has just one minimum; there are no local minima to get stuck in, so gradient\ndescent starting from any point is guaranteed to \xef\xac\x81nd the minimum. (By contrast,\n\nconvex\n\n\x0c5.4\n\n\xe2\x80\xa2 GRADIENT DESCENT\n\n9\n\nthe loss for multi-layer neural networks is non-convex, and gradient descent may\nget stuck in local minima for neural network training and never \xef\xac\x81nd the global opti-\nmum.)\n\nAlthough the algorithm (and the concept of gradient) are designed for direction\nvectors, let\xe2\x80\x99s \xef\xac\x81rst consider a visualization of the case where the parameter of our\nsystem is just a single scalar w, shown in Fig. 5.3.\n\nGiven a random initialization of w at some value w1, and assuming the loss\nfunction L happened to have the shape in Fig. 5.3, we need the algorithm to tell us\nwhether at the next iteration we should move left (making w2 smaller than w1) or\nright (making w2 bigger than w1) to reach the minimum.\n\ngradient\n\nlearning rate\n\nFigure 5.3 The \xef\xac\x81rst step in iteratively \xef\xac\x81nding the minimum of this loss function, by moving\nw in the reverse direction from the slope of the function. Since the slope is negative, we need\nto move w in a positive direction, to the right. Here superscripts are used for learning steps,\nso w1 means the initial value of w (which is 0), w2 at the second step, and so on.\n\nThe gradient descent algorithm answers this question by \xef\xac\x81nding the gradient\nof the loss function at the current point and moving in the opposite direction. The\ngradient of a function of many variables is a vector pointing in the direction of the\ngreatest increase in a function. The gradient is a multi-variable generalization of the\nslope, so for a function of one variable like the one in Fig. 5.3, we can informally\nthink of the gradient as the slope. The dotted line in Fig. 5.3 shows the slope of this\nhypothetical loss function at point w = w1. You can see that the slope of this dotted\nline is negative. Thus to \xef\xac\x81nd the minimum, gradient descent tells us to go in the\nopposite direction: moving w in a positive direction.\n\nThe magnitude of the amount to move in gradient descent is the value of the slope\nd\ndw f (x;w) weighted by a learning rate \xce\xb7. A higher (faster) learning rate means that\nwe should move w more on each step. The change we make in our parameter is the\nlearning rate times the gradient (or the slope, in our single-variable example):\n\nwt+1 = wt \xe2\x88\x92 \xce\xb7\n\nd\ndw\n\nf (x;w)\n\n(5.13)\n\nNow let\xe2\x80\x99s extend the intuition from a function of one scalar variable w to many\nvariables, because we don\xe2\x80\x99t just want to move left or right, we want to know where\nin the N-dimensional space (of the N parameters that make up \xce\xb8) we should move.\nThe gradient is just such a vector; it expresses the directional components of the\nsharpest slope along each of those N dimensions. If we\xe2\x80\x99re just imagining two weight\ndimensions (say for one weight w and one bias b), the gradient might be a vector with\ntwo orthogonal components, each of which tells us how much the ground slopes in\nthe w dimension and in the b dimension. Fig. 5.4 shows a visualization:\n\nLoss\n\nslope of loss at w1 \n\nis negative\n\none step\nof gradient\n\ndescent\n\nw1\n0\n\nwmin\n(goal)\n\nw\n\n\x0c10 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\nFigure 5.4 Visualization of the gradient vector in two dimensions w and b.\n\nIn an actual logistic regression, the parameter vector w is much longer than 1 or\n2, since the input feature vector x can be quite long, and we need a weight wi for\neach xi. For each dimension/variable wi in w (plus the bias b), the gradient will have\na component that tells us the slope with respect to that variable. Essentially we\xe2\x80\x99re\nasking: \xe2\x80\x9cHow much would a small change in that variable wi in\xef\xac\x82uence the total loss\nfunction L?\xe2\x80\x9d\n\nIn each dimension wi, we express the slope as a partial derivative \xe2\x88\x82\n\xe2\x88\x82 wi\n\nof the loss\nfunction. The gradient is then de\xef\xac\x81ned as a vector of these partials. We\xe2\x80\x99ll represent \xcb\x86y\nas f (x;\xce\xb8 ) to make the dependence on \xce\xb8 more obvious:\n\n\xe2\x88\x87\xce\xb8 L( f (x;\xce\xb8 ),y)) =\n\n\xef\xa3\xae\xef\xa3\xaf\xef\xa3\xaf\xef\xa3\xaf\xef\xa3\xaf\xef\xa3\xb0\n\n\xe2\x88\x82\n\xe2\x88\x82 w1\n\xe2\x88\x82\n\xe2\x88\x82 w2\n\n\xe2\x88\x82\n\xe2\x88\x82 wn\n\n\xef\xa3\xb9\xef\xa3\xba\xef\xa3\xba\xef\xa3\xba\xef\xa3\xba\xef\xa3\xbb\n\nL( f (x;\xce\xb8 ),y)\nL( f (x;\xce\xb8 ),y)\n\n...\n\nL( f (x;\xce\xb8 ),y)\n\nThe \xef\xac\x81nal equation for updating \xce\xb8 based on the gradient is thus\n\n\xce\xb8t+1 = \xce\xb8t \xe2\x88\x92 \xce\xb7\xe2\x88\x87L( f (x;\xce\xb8 ),y)\n\n5.4.1 The Gradient for Logistic Regression\nIn order to update \xce\xb8, we need a de\xef\xac\x81nition for the gradient \xe2\x88\x87L( f (x;\xce\xb8 ),y). Recall that\nfor logistic regression, the cross-entropy loss function is:\n\nLCE (w,b) = \xe2\x88\x92 [ylog\xcf\x83 (w\xc2\xb7 x + b) + (1\xe2\x88\x92 y)log (1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b))]\n\n(5.16)\n\nIt turns out that the derivative of this function for one observation vector x is\nEq. 5.17 (the interested reader can see Section 5.8 for the derivation of this equation):\n\n\xe2\x88\x82 LCE (w,b)\n\n\xe2\x88\x82 w j\n\n= [\xcf\x83 (w\xc2\xb7 x + b)\xe2\x88\x92 y]x j\n\n(5.17)\n\nNote in Eq. 5.17 that the gradient with respect to a single weight w j represents a\nvery intuitive value: the difference between the true y and our estimated \xcb\x86y = \xcf\x83 (w\xc2\xb7\nx + b) for that observation, multiplied by the corresponding input value x j.\n\n(5.14)\n\n(5.15)\n\nCost(w,b)\n\nw\n\nb\n\n\x0c5.4\n\n\xe2\x80\xa2 GRADIENT DESCENT\n\n11\n\n5.4.2 The Stochastic Gradient Descent Algorithm\nStochastic gradient descent is an online algorithm that minimizes the loss function\nby computing its gradient after each training example, and nudging \xce\xb8 in the right\ndirection (the opposite direction of the gradient). Fig. 5.5 shows the algorithm.\n\nfunction STOCHASTIC GRADIENT DESCENT(L(), f (), x, y) returns \xce\xb8\n\nf is a function parameterized by \xce\xb8\nx is the set of training inputs x(1), x(2), ..., x(n)\ny is the set of training outputs (labels) y(1), y(2), ..., y(n)\n\n# where: L is the loss function\n#\n#\n#\n\xce\xb8 \xe2\x86\x900\nrepeat til done # see caption\n\nFor each training tuple (x(i), y(i)) (in random order)\n\n1. Optional (for reporting):\n\nCompute \xcb\x86y (i) = f (x(i);\xce\xb8 )\nCompute the loss L( \xcb\x86y (i),y(i)) # How far off is \xcb\x86y(i)) from the true output y(i)?\n\n# How are we doing on this tuple?\n# What is our estimated output \xcb\x86y?\n\n2. g\xe2\x86\x90\xe2\x88\x87\xce\xb8 L( f (x(i);\xce\xb8 ),y(i))\n3. \xce\xb8 \xe2\x86\x90\xce\xb8 \xe2\x88\x92 \xce\xb7 g\n\n# How should we move \xce\xb8 to maximize loss?\n# Go the other way instead\n\nreturn \xce\xb8\n\nFigure 5.5 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used\nto report how well we are doing on the current tuple. The algorithm can terminate when it\nconverges (or when the gradient < \xce\xb5), or when progress halts (for example when the loss\nstarts going up on a held-out set).\n\nThe learning rate \xce\xb7 is a parameter that must be adjusted. If it\xe2\x80\x99s too high, the\nlearner will take steps that are too large, overshooting the minimum of the loss func-\ntion. If it\xe2\x80\x99s too low, the learner will take steps that are too small, and take too long to\nget to the minimum. It is common to begin the learning rate at a higher value, and\nthen slowly decrease it, so that it is a function of the iteration k of training; you will\nsometimes see the notation \xce\xb7k to mean the value of the learning rate at iteration k.\n\n5.4.3 Working through an example\nLet\xe2\x80\x99s walk though a single step of the gradient descent algorithm. We\xe2\x80\x99ll use a sim-\npli\xef\xac\x81ed version of the example in Fig. 5.2 as it sees a single observation x, whose\ncorrect value is y = 1 (this is a positive review), and with only two features:\n\nx1 = 3\nx2 = 2\n\n(count of positive lexicon words)\n(count of negative lexicon words)\n\nLet\xe2\x80\x99s assume the initial weights and bias in \xce\xb8 0 are all set to 0, and the initial learning\nrate \xce\xb7 is 0.1:\n\nw1 = w2 = b = 0\n\n\xce\xb7 = 0.1\n\nThe single update step requires that we compute the gradient, multiplied by the\nlearning rate\n\n\xce\xb8 t+1 = \xce\xb8 t \xe2\x88\x92 \xce\xb7\xe2\x88\x87\xce\xb8 L( f (x(i);\xce\xb8 ),y(i))\n\n\x0c12 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\n\xef\xa3\xae\xef\xa3\xaf\xef\xa3\xb0 \xe2\x88\x82 LCE (w,b)\n\n\xe2\x88\x82 LCE (w,b)\n\xe2\x88\x82 LCE (w,b)\n\n\xe2\x88\x82 w1\n\xe2\x88\x82 w2\n\xe2\x88\x82 b\n\n\xef\xa3\xb9\xef\xa3\xba\xef\xa3\xbb =\n\n\xe2\x88\x87w,b =\n\nIn our mini example there are three parameters, so the gradient vector has 3 dimen-\nsions, for w1, w2, and b. We can compute the \xef\xac\x81rst gradient as follows:\n\n\xef\xa3\xb9\xef\xa3\xbb =\n\n\xef\xa3\xae\xef\xa3\xb0 \xe2\x88\x921.5\n\n\xe2\x88\x921.0\n\xe2\x88\x920.5\n\n\xef\xa3\xb9\xef\xa3\xbb\n\n(\xcf\x83 (w\xc2\xb7 x + b)\xe2\x88\x92 y)x2\n\xcf\x83 (w\xc2\xb7 x + b)\xe2\x88\x92 y\n\n\xef\xa3\xae\xef\xa3\xb0 (\xcf\x83 (w\xc2\xb7 x + b)\xe2\x88\x92 y)x1\n\xef\xa3\xb9\xef\xa3\xbb =\n\xef\xa3\xae\xef\xa3\xb0 (\xcf\x83 (0)\xe2\x88\x92 1)x1\n\xef\xa3\xae\xef\xa3\xb0 w1\n\xef\xa3\xae\xef\xa3\xb0 \xe2\x88\x921.5\n\xef\xa3\xb9\xef\xa3\xbb\xe2\x88\x92 \xce\xb7\n\xef\xa3\xb9\xef\xa3\xbb =\n\n(\xcf\x83 (0)\xe2\x88\x92 1)x2\n\xcf\x83 (0)\xe2\x88\x92 1\n\n\xce\xb8 2 =\n\n\xe2\x88\x920.5x2\n\xe2\x88\x920.5\n\n\xef\xa3\xb9\xef\xa3\xbb =\n\xef\xa3\xae\xef\xa3\xb0 \xe2\x88\x920.5x1\n\xef\xa3\xae\xef\xa3\xb0 .15\n\xef\xa3\xb9\xef\xa3\xbb\n\n\xe2\x88\x921.0\n\xe2\x88\x920.5\n\n.1\n.05\n\nw2\nb\n\nNow that we have a gradient, we compute the new parameter vector \xce\xb8 1 by moving\n\xce\xb8 0 in the opposite direction from the gradient:\n\nbatch training\n\nmini-batch\n\nSo after one step of gradient descent, the weights have shifted to be: w1 = .15,\nw2 = .1, and b = .05.\n\nNote that this observation x happened to be a positive example. We would expect\nthat after seeing more negative examples with high counts of negative words, that\nthe weight w2 would shift to have a negative value.\n\n5.4.4 Mini-batch training\nStochastic gradient descent is called stochastic because it chooses a single random\nexample at a time, moving the weights so as to improve performance on that single\nexample. That can result in very choppy movements, so it\xe2\x80\x99s common to compute the\ngradient over batches of training instances rather than a single instance.\n\nFor example in batch training we compute the gradient over the entire dataset.\nBy seeing so many examples, batch training offers a superb estimate of which di-\nrection to move the weights, at the cost of spending a lot of time processing every\nsingle example in the training set to compute this perfect direction.\n\nA compromise is mini-batch training: we train on a group of m examples (per-\nhaps 512, or 1024) that is less than the whole dataset. (If m is the size of the dataset,\nthen we are doing batch gradient descent; if m = 1, we are back to doing stochas-\ntic gradient descent). Mini-batch training also has the advantage of computational\nef\xef\xac\x81ciency. The mini-batches can easily be vectorized, choosing the size of the mini-\nbatch based on the computational resources. This allows us to process all the exam-\nples in one mini-batch in parallel and then accumulate the loss, something that\xe2\x80\x99s not\npossible with individual or batch training.\n\nWe just need to de\xef\xac\x81ne mini-batch versions of the cross-entropy loss function\nwe de\xef\xac\x81ned in Section 5.3 and the gradient in Section 5.4.1. Let\xe2\x80\x99s extend the cross-\nentropy loss for one example from Eq. 5.10 to mini-batches of size m. We\xe2\x80\x99ll continue\nto use the notation that x(i) and y(i) mean the ith training features and training label,\nrespectively. We make the assumption that the training examples are independent:\n\nlog p(training labels) = log\n\nm(cid:89)\nm(cid:88)\nm(cid:88)\n\ni=1\n\n=\n\ni=1\n= \xe2\x88\x92\n\np(y(i)|x(i))\n\ni=1\nlog p(y(i)|x(i))\n\nLCE ( \xcb\x86y(i),y(i))\n\n(5.18)\n\n(5.19)\n\n(5.20)\n\n\x0c5.5\n\n\xe2\x80\xa2 REGULARIZATION\n\n13\n\nNow the cost function for the mini-batch of m examples is the average loss for each\nexample:\n\nCost(w,b) =\n\nLCE ( \xcb\x86y(i),y(i))\n\nm(cid:88)\nm(cid:88)\n\ni=1\n\ni=1\n\n1\nm\n= \xe2\x88\x92 1\nm\n\ny(i) log\xcf\x83 (w\xc2\xb7 x(i) + b) + (1\xe2\x88\x92 y(i))log\n\nThe mini-batch gradient is the average of the individual gradients from Eq. 5.17:\n\n(cid:16)\n\n(cid:17)\n\n1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x(i) + b)\n\n(5.21)\n\nx(i)\nj\n\n(5.22)\n\n(cid:104)\n\xcf\x83 (w\xc2\xb7 x(i) + b)\xe2\x88\x92 y(i)(cid:105)\n\nm(cid:88)\n\ni=1\n\n\xe2\x88\x82Cost(w,b)\n\n\xe2\x88\x82 w j\n\n=\n\n1\nm\n\n5.5 Regularization\n\nNumquam ponenda est pluralitas sine necessitate\n\xe2\x80\x98Plurality should never be proposed unless needed\xe2\x80\x99\nWilliam of Occam\n\nover\xef\xac\x81tting\ngeneralize\nregularization\n\nL2\nregularization\n\nThere is a problem with learning weights that make the model perfectly match the\ntraining data. If a feature is perfectly predictive of the outcome because it happens\nto only occur in one class, it will be assigned a very high weight. The weights for\nfeatures will attempt to perfectly \xef\xac\x81t details of the training set, in fact too perfectly,\nmodeling noisy factors that just accidentally correlate with the class. This problem is\ncalled over\xef\xac\x81tting. A good model should be able to generalize well from the training\ndata to the unseen test set, but a model that over\xef\xac\x81ts will have poor generalization.\n\nTo avoid over\xef\xac\x81tting, a new regularization term R(\xce\xb8 ) is added to the objective\nfunction in Eq. 5.12, resulting in the following objective for a batch of m exam-\nples (slightly rewritten from Eq. 5.12 to be maximizing log probability rather than\nminimizing loss, and removing the 1\n\nm term which doesn\xe2\x80\x99t affect the argmax):\n\nlogP(y(i)|x(i))\xe2\x88\x92 \xce\xb1R(\xce\xb8 )\n\n(5.23)\n\nm(cid:88)\n\ni=1\n\n\xcb\x86\xce\xb8 = argmax\n\n\xce\xb8\n\nThe new regularization term R(\xce\xb8 ) is used to penalize large weights. Thus a setting\nof the weights that matches the training data perfectly\xe2\x80\x94 but uses many weights with\nhigh values to do so\xe2\x80\x94will be penalized more than a setting that matches the data a\nlittle less well, but does so using smaller weights. There are two common ways to\ncompute this regularization term R(\xce\xb8 ). L2 regularization is a quadratic function of\nthe weight values, named because it uses the (square of the) L2 norm of the weight\nvalues. The L2 norm, ||\xce\xb8||2, is the same as the Euclidean distance of the vector \xce\xb8\nfrom the origin. If \xce\xb8 consists of n weights, then:\n\nR(\xce\xb8 ) = ||\xce\xb8||2\n\n2 =\n\n\xce\xb8 2\nj\n\n(5.24)\n\nn(cid:88)\n\nj=1\n\n\x0c14 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\nL1\nregularization\n\nlasso\nridge\n\n(cid:34) m(cid:88)\n\n1=i\n\n(cid:34) m(cid:88)\n\n1=i\n\nThe L2 regularized objective function becomes:\n\n\xcb\x86\xce\xb8 = argmax\n\n\xce\xb8\n\nlogP(y(i)|x(i))\n\n(cid:35)\n\nn(cid:88)\n\nj=1\n\n\xce\xb8 2\nj\n\n\xe2\x88\x92 \xce\xb1\n\n(5.25)\n\nL1 regularization is a linear function of the weight values, named after the L1 norm\n||W||1, the sum of the absolute values of the weights, or Manhattan distance (the\nManhattan distance is the distance you\xe2\x80\x99d have to walk between two points in a city\nwith a street grid like New York):\n\nR(\xce\xb8 ) = ||\xce\xb8||1 =\n\n|\xce\xb8i|\n\nn(cid:88)\n\ni=1\n\n(5.26)\n\n(5.27)\n\n(5.28)\n\n(5.29)\n\nThe L1 regularized objective function becomes:\n\n\xcb\x86\xce\xb8 = argmax\n\n\xce\xb8\n\nlogP(y(i)|x(i))\n\n(cid:35)\n\nn(cid:88)\n\nj=1\n\n|\xce\xb8 j|\n\n\xe2\x88\x92 \xce\xb1\n\nThese kinds of regularization come from statistics, where L1 regularization is called\nlasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression,\nand both are commonly used in language processing. L2 regularization is easier to\noptimize because of its simple derivative (the derivative of \xce\xb8 2 is just 2\xce\xb8), while\nL1 regularization is more complex (the derivative of |\xce\xb8| is non-continuous at zero).\nBut where L2 prefers weight vectors with many small weights, L1 prefers sparse\nsolutions with some larger weights but many more weights set to zero. Thus L1\nregularization leads to much sparser weight vectors, that is, far fewer features.\n\nBoth L1 and L2 regularization have Bayesian interpretations as constraints on\nthe prior of how weights should look. L1 regularization can be viewed as a Laplace\nprior on the weights. L2 regularization corresponds to assuming that weights are\ndistributed according to a gaussian distribution with mean \xc2\xb5 = 0.\nIn a gaussian\nor normal distribution, the further away a value is from the mean, the lower its\nprobability (scaled by the variance \xcf\x83). By using a gaussian prior on the weights, we\nare saying that weights prefer to have the value 0. A gaussian for a weight \xce\xb8 j is\n\n(cid:32)\n\xe2\x88\x92 (\xce\xb8 j \xe2\x88\x92 \xc2\xb5 j)2\n\n(cid:33)\n\n2\xcf\x83 2\nj\n\n1(cid:113)\n\n2\xcf\x80\xcf\x83 2\nj\n\nexp\n\nIf we multiply each weight by a gaussian prior on the weight, we are thus maximiz-\ning the following constraint:\n\n\xcb\x86\xce\xb8 = argmax\n\n\xce\xb8\n\nP(y(i)|x(i))\xc3\x97\n\ni=1\n\nj=1\n\nexp\n\n2\xcf\x80\xcf\x83 2\nj\n\nn(cid:89)\n\n1(cid:113)\n\n(cid:32)\n\xe2\x88\x92 (\xce\xb8 j \xe2\x88\x92 \xc2\xb5 j)2\n\n(cid:33)\n\n2\xcf\x83 2\nj\n\nM(cid:89)\n\nwhich in log space, with \xc2\xb5 = 0, and assuming 2\xcf\x83 2 = 1, corresponds to\n\nm(cid:88)\n\nn(cid:88)\n\n\xcb\x86\xce\xb8 = argmax\n\n\xce\xb8\n\nlogP(y(i)|x(i))\xe2\x88\x92 \xce\xb1\n\ni=1\n\nj=1\n\n\xce\xb8 2\nj\n\n(5.30)\n\nwhich is in the same form as Eq. 5.25.\n\n\x0c5.6 Multinomial logistic regression\n\n5.6\n\n\xe2\x80\xa2 MULTINOMIAL LOGISTIC REGRESSION\n\n15\n\nmultinomial\nlogistic\nregression\n\nsoftmax\n\nSometimes we need more than two classes. Perhaps we might want to do 3-way\nsentiment classi\xef\xac\x81cation (positive, negative, or neutral). Or we could be classifying\nthe part of speech of a word (choosing from 10, 30, or even 50 different parts of\nspeech), or assigning semantic labels like the named entities or semantic relations\nwe will introduce in Chapter 18.\n\nIn such cases we use multinomial logistic regression, also called softmax re-\ngression (or, historically, the maxent classi\xef\xac\x81er). In multinomial logistic regression\nthe target y is a variable that ranges over more than two classes; we want to know\nthe probability of y being in each potential class c \xe2\x88\x88 C, p(y = c|x).\nThe multinomial logistic classi\xef\xac\x81er uses a generalization of the sigmoid, called\nthe softmax function, to compute the probability p(y = c|x). The softmax function\ntakes a vector z = [z1,z2, ...,zk] of k arbitrary values and maps them to a probability\ndistribution, with each value in the range (0,1), and all the values summing to 1.\nLike the sigmoid, it is an exponential function.\n\nFor a vector z of dimensionality k, the softmax is de\xef\xac\x81ned as:\n\nThe softmax of an input vector z = [z1,z2, ...,zk] is thus a vector itself:\n\nsoftmax(zi) =\n\nezi(cid:80)k\n\nj=1 ez j\n\n1 \xe2\x89\xa4 i \xe2\x89\xa4 k\n\n(cid:34)\n\nez1(cid:80)k\n\ni=1 ezi\n\n,\n\nez2(cid:80)k\n\ni=1 ezi\n\n, ...,\n\nezk(cid:80)k\n\ni=1 ezi\n\n(cid:35)\n\n(5.31)\n\n(5.32)\n\ni=1 ezi is used to normalize all the values into probabilities.\n\nsoftmax(z) =\n\nThe denominator(cid:80)k\n\nThus for example given a vector:\n\nz = [0.6,1.1,\xe2\x88\x921.5,1.2,3.2,\xe2\x88\x921.1]\n\nthe result softmax(z) is\n\n[0.055,0.090,0.0067,0.10,0.74,0.010]\n\nAgain like the sigmoid, the input to the softmax will be the dot product between\na weight vector w and an input vector x (plus a bias). But now we\xe2\x80\x99ll need separate\nweight vectors (and bias) for each of the K classes.\n\np(y = c|x) =\n\newc \xc2\xb7 x + bc\nk(cid:88)\new j \xc2\xb7 x + b j\n\nj=1\n\n(5.33)\n\nLike the sigmoid, the softmax has the property of squashing values toward 0 or 1.\nThus if one of the inputs is larger than the others, it will tend to push its probability\ntoward 1, and suppress the probabilities of the smaller inputs.\n\n\x0c16 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\n5.6.1 Features in Multinomial Logistic Regression\nFor multiclass classi\xef\xac\x81cation the input features need to be a function of both the\nobservation x and the candidate output class c. Thus instead of the notation xi, fi\nor fi(x), when we\xe2\x80\x99re discussing features we will use the notation fi(c,x), meaning\nfeature i for a particular class c for a given observation x.\n\nIn binary classi\xef\xac\x81cation, a positive weight on a feature pointed toward y=1 and\na negative weight toward y=0, but in multiclass classi\xef\xac\x81cation a feature could be\nevidence for or against an individual class.\n\nLet\xe2\x80\x99s look at some sample features for a few NLP tasks to help understand this\nperhaps unintuitive use of features that are functions of both the observation x and\nthe class c.\nSuppose we are doing text classi\xef\xac\x81cation, and instead of binary classi\xef\xac\x81cation our\ntask is to assign one of the 3 classes +, \xe2\x88\x92, or 0 (neutral) to a document. Now a\nfeature related to exclamation marks might have a negative weight for 0 documents,\nand a positive weight for + or \xe2\x88\x92 documents:\n\nVar\nf1(0,x)\n\nf1(+,x)\nf1(\xe2\x88\x92,x)\n\nDe\xef\xac\x81nition\n\n(cid:26) 1 if \xe2\x80\x9c!\xe2\x80\x9d \xe2\x88\x88 doc\n(cid:26) 1 if \xe2\x80\x9c!\xe2\x80\x9d \xe2\x88\x88 doc\n(cid:26) 1 if \xe2\x80\x9c!\xe2\x80\x9d \xe2\x88\x88 doc\n\n0 otherwise\n\n0 otherwise\n\n0 otherwise\n\nWt\n\xe2\x88\x924.5\n\n2.6\n\n1.3\n\n5.6.2 Learning in Multinomial Logistic Regression\nMultinomial logistic regression has a slightly different loss function than binary lo-\ngistic regression because it uses the softmax rather than the sigmoid classi\xef\xac\x81er. The\nloss function for a single example x is the sum of the logs of the K output classes:\n\nK(cid:88)\nK(cid:88)\n\nk=1\n\nk=1\n\nLCE ( \xcb\x86y,y) = \xe2\x88\x92\n\n= \xe2\x88\x92\n\n1{y = k}log p(y = k|x)\n\n1{y = k}log\n\n(cid:80)K\newk\xc2\xb7x+bk\nj=1 ew j\xc2\xb7x+b j\n\n(5.34)\n\nThis makes use of the function 1{} which evaluates to 1 if the condition in the\nbrackets is true and to 0 otherwise.\n\nThe gradient for a single example turns out to be very similar to the gradient for\nlogistic regression, although we don\xe2\x80\x99t show the derivation here. It is the difference\nbetween the value for the true class k (which is 1) and the probability the classi\xef\xac\x81er\noutputs for class k, weighted by the value of the input xk:\n\n\xe2\x88\x82 LCE\n\xe2\x88\x82 wk\n\n(cid:32)\n\n= \xe2\x88\x92(1{y = k}\xe2\x88\x92 p(y = k|x))xk\n1{y = k}\xe2\x88\x92 ewk\xc2\xb7x+bk\n\n(cid:80)K\nj=1 ew j\xc2\xb7x+b j\n\n= \xe2\x88\x92\n\n(cid:33)\n\nxk\n\n(5.35)\n\n\x0c5.7\n\nInterpreting models\n\n5.7\n\n\xe2\x80\xa2\n\nINTERPRETING MODELS\n\n17\n\ninterpretable\n\nOften we want to know more than just the correct classi\xef\xac\x81cation of an observation.\nWe want to know why the classi\xef\xac\x81er made the decision it did. That is, we want our\ndecision to be interpretable. Interpretability can be hard to de\xef\xac\x81ne strictly, but the\ncore idea is that as humans we should know why our algorithms reach the conclu-\nsions they do. Because the features to logistic regression are often human-designed,\none way to understand a classi\xef\xac\x81er\xe2\x80\x99s decision is to understand the role each feature\nplays in the decision. Logistic regression can be combined with statistical tests (the\nlikelihood ratio test, or the Wald test); investigating whether a particular feature is\nsigni\xef\xac\x81cant by one of these tests, or inspecting its magnitude (how large is the weight\nw associated with the feature?) can help us interpret why the classi\xef\xac\x81er made the\ndecision it makes. This is enormously important for building transparent models.\n\nFurthermore, in addition to its use as a classi\xef\xac\x81er, logistic regression in NLP and\nmany other \xef\xac\x81elds is widely used as an analytic tool for testing hypotheses about the\neffect of various explanatory variables (features). In text classi\xef\xac\x81cation, perhaps we\nwant to know if logically negative words (no, not, never) are more likely to be asso-\nciated with negative sentiment, or if negative reviews of movies are more likely to\ndiscuss the cinematography. However, in doing so it\xe2\x80\x99s necessary to control for po-\ntential confounds: other factors that might in\xef\xac\x82uence sentiment (the movie genre, the\nyear it was made, perhaps the length of the review in words). Or we might be study-\ning the relationship between NLP-extracted linguistic features and non-linguistic\noutcomes (hospital readmissions, political outcomes, or product sales), but need to\ncontrol for confounds (the age of the patient, the county of voting, the brand of the\nproduct). In such cases, logistic regression allows us to test whether some feature is\nassociated with some outcome above and beyond the effect of other features.\n\n5.8 Advanced: Deriving the Gradient Equation\n\nIn this section we give the derivation of the gradient of the cross-entropy loss func-\ntion LCE for logistic regression. Let\xe2\x80\x99s start with some quick calculus refreshers.\nFirst, the derivative of ln(x):\n\nd\ndx\n\nln(x) =\n\n1\nx\n\nSecond, the (very elegant) derivative of the sigmoid:\n\nd\xcf\x83 (z)\ndz = \xcf\x83 (z)(1\xe2\x88\x92 \xcf\x83 (z))\n\n(5.36)\n\n(5.37)\n\nchain rule\n\nFinally, the chain rule of derivatives. Suppose we are computing the derivative\nof a composite function f (x) = u(v(x)). The derivative of f (x) is the derivative of\nu(x) with respect to v(x) times the derivative of v(x) with respect to x:\n\nd f\ndx =\n\ndu\ndv\n\n\xc2\xb7 dv\ndx\n\n(5.38)\n\nFirst, we want to know the derivative of the loss function with respect to a single\n\nweight w j (we\xe2\x80\x99ll need to compute it for each weight, and for the bias):\n\n\x0c18 CHAPTER 5\n\n\xe2\x80\xa2 LOGISTIC REGRESSION\n\n\xe2\x88\x82 LL(w,b)\n\n\xe2\x88\x82 w j\n\n=\n\n\xe2\x88\x82\n\xe2\x88\x82 w j\n\n(cid:20) \xe2\x88\x82\n\n\xe2\x88\x92 [ylog\xcf\x83 (w\xc2\xb7 x + b) + (1\xe2\x88\x92 y)log (1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b))]\n\n= \xe2\x88\x92\n\nylog\xcf\x83 (w\xc2\xb7 x + b) +\n\n\xe2\x88\x82\n\xe2\x88\x82 w j\n\n\xe2\x88\x82 w j\n\n(1\xe2\x88\x92 y)log [1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)]\n\n(cid:21)\n\nNext, using the chain rule, and relying on the derivative of log:\n\xe2\x88\x82 LL(w,b)\n\n1\xe2\x88\x92 y\n\ny\n\n= \xe2\x88\x92\n\n\xcf\x83 (w\xc2\xb7 x + b)\n\n\xe2\x88\x82\n\xe2\x88\x82 w j\n\n\xcf\x83 (w\xc2\xb7 x + b)\xe2\x88\x92\n\n1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)\n\n\xe2\x88\x82 w j\n\nRearranging terms:\n\xe2\x88\x82 LL(w,b)\n\n\xe2\x88\x82 w j\n\n(cid:20)\n\n= \xe2\x88\x92\n\ny\n\n\xcf\x83 (w\xc2\xb7 x + b)\n\n\xe2\x88\x92\n\n1\xe2\x88\x92 y\n\n1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)\n\n(cid:21) \xe2\x88\x82\n\n\xe2\x88\x82 w j\n\n(5.39)\n\n\xe2\x88\x82\n\xe2\x88\x82 w j\n\n1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)\n(5.40)\n\n\xcf\x83 (w\xc2\xb7 x + b)\n\n(5.41)\n\nAnd now plugging in the derivative of the sigmoid, and using the chain rule one\nmore time, we end up with Eq. 5.42:\n\xe2\x88\x82 LL(w,b)\n\ny\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)\n\n\xcf\x83 (w\xc2\xb7 x + b)[1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)]\n\n= \xe2\x88\x92\n\n\xcf\x83 (w\xc2\xb7 x + b)[1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)]\n\n\xe2\x88\x82 (w\xc2\xb7 x + b)\n\n(cid:20)\n(cid:20)\n\n(cid:21)\n(cid:21)\n\n\xcf\x83 (w\xc2\xb7 x + b)[1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)]x j\n\ny\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)\n\n\xcf\x83 (w\xc2\xb7 x + b)[1\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)]\n\n= \xe2\x88\x92\n= \xe2\x88\x92[y\xe2\x88\x92 \xcf\x83 (w\xc2\xb7 x + b)]x j\n= [\xcf\x83 (w\xc2\xb7 x + b)\xe2\x88\x92 y]x j\n\n\xe2\x88\x82 w j\n\n(5.42)\n\n\xe2\x88\x82 w j\n\n5.9 Summary\n\nThis chapter introduced the logistic regression model of classi\xef\xac\x81cation.\n\n\xe2\x80\xa2 Logistic regression is a supervised machine learning classi\xef\xac\x81er that extracts\nreal-valued features from the input, multiplies each by a weight, sums them,\nand passes the sum through a sigmoid function to generate a probability. A\nthreshold is used to make a decision.\n\xe2\x80\xa2 Logistic regression can be used with two classes (e.g., positive and negative\nsentiment) or with multiple classes (multinomial logistic regression, for ex-\nample for n-ary text classi\xef\xac\x81cation, part-of-speech labeling, etc.).\n\xe2\x80\xa2 Multinomial logistic regression uses the softmax function to compute proba-\n\xe2\x80\xa2 The weights (vector w and bias b) are learned from a labeled training set via a\n\xe2\x80\xa2 Minimizing this loss function is a convex optimization problem, and iterative\n\xe2\x80\xa2 Regularization is used to avoid over\xef\xac\x81tting.\n\xe2\x80\xa2 Logistic regression is also one of the most useful analytic tools, because of its\n\nalgorithms like gradient descent are used to \xef\xac\x81nd the optimal weights.\n\nloss function, such as the cross-entropy loss, that must be minimized.\n\nbilities.\n\nability to transparently study the importance of individual features.\n\n\x0cBibliographical and Historical Notes\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\n\n19\n\nLogistic regression was developed in the \xef\xac\x81eld of statistics, where it was used for\nthe analysis of binary data by the 1960s, and was particularly common in medicine\n(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one\nof the formal foundations of the study of linguistic variation (Sankoff and Labov,\n1979).\n\nNonetheless, logistic regression didn\xe2\x80\x99t become common in natural language pro-\ncessing until the 1990s, when it seems to have appeared simultaneously from two\ndirections. The \xef\xac\x81rst source was the neighboring \xef\xac\x81elds of information retrieval and\nspeech processing, both of which had made use of regression, and both of which\nlent many other statistical techniques to NLP. Indeed a very early use of logistic\nregression for document routing was one of the \xef\xac\x81rst NLP applications to use (LSI)\nembeddings as word representations (Sch\xc2\xa8utze et al., 1995).\n\nAt the same time in the early 1990s logistic regression was developed and ap-\nplied to NLP at IBM Research under the name maximum entropy modeling or\nmaxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-\nder that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech\ntagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution\n(Kehler, 1997), and text classi\xef\xac\x81cation (Nigam et al., 1999).\n\nMore on classi\xef\xac\x81cation can be found in machine learning textbooks (Hastie et al. 2001,\n\nWitten and Frank 2005, Bishop 2006, Murphy 2012).\n\nmaximum\nentropy\n\nExercises\n\n\x0c20 Chapter 5 \xe2\x80\xa2 Logistic Regression\n\nBerger, A., Della Pietra, S. A., and Della Pietra, V. J. (1996).\nA maximum entropy approach to natural language process-\ning. Computational Linguistics, 22(1), 39\xe2\x80\x9371.\n\nBishop, C. M. (2006). Pattern recognition and machine\n\nlearning. Springer.\n\nCox, D. (1969). Analysis of Binary Data. Chapman and Hall,\n\nLondon.\n\nHastie, T., Tibshirani, R. J., and Friedman, J. H. (2001). The\n\nElements of Statistical Learning. Springer.\n\nKehler, A. (1997). Probabilistic coreference in information\n\nextraction. In EMNLP 1997, 163\xe2\x80\x93173.\n\nMurphy, K. P. (2012). Machine learning: A probabilistic\n\nperspective. MIT press.\n\nNg, A. Y. and Jordan, M. I. (2002). On discriminative vs.\ngenerative classi\xef\xac\x81ers: A comparison of logistic regression\nand naive bayes. In NIPS 14, 841\xe2\x80\x93848.\n\nNigam, K., Lafferty, J. D., and McCallum, A. (1999). Using\nmaximum entropy for text classi\xef\xac\x81cation. In IJCAI-99 work-\nshop on machine learning for information \xef\xac\x81ltering, 61\xe2\x80\x9367.\nRatnaparkhi, A. (1996). A maximum entropy part-of-speech\n\ntagger. In EMNLP 1996, 133\xe2\x80\x93142.\n\nRatnaparkhi, A. (1997). A linear observed time statistical\nIn EMNLP\n\nparser based on maximum entropy models.\n1997, 1\xe2\x80\x9310.\n\nRosenfeld, R. (1996). A maximum entropy approach to\nadaptive statistical language modeling. Computer Speech\nand Language, 10, 187\xe2\x80\x93228.\n\nSankoff, D. and Labov, W. (1979). On the uses of variable\n\nrules. Language in society, 8(2-3), 189\xe2\x80\x93222.\n\nSch\xc2\xa8utze, H., Hull, D. A., and Pedersen, J. (1995). A com-\nparison of classi\xef\xac\x81ers and document representations for the\nrouting problem. In SIGIR-95, 229\xe2\x80\x93237.\n\nTibshirani, R. J. (1996). Regression shrinkage and selection\nvia the lasso. Journal of the Royal Statistical Society. Series\nB (Methodological), 58(1), 267\xe2\x80\x93288.\n\nWang, S. and Manning, C. D. (2012). Baselines and bigrams:\nIn ACL\n\nSimple, good sentiment and topic classi\xef\xac\x81cation.\n2012, 90\xe2\x80\x9394.\n\nWitten, I. H. and Frank, E. (2005). Data Mining: Practical\nMachine Learning Tools and Techniques (2nd Ed.). Mor-\ngan Kaufmann.\n\n\x0c"