b'Speech and Language Processing. Daniel Jurafsky & James H. Martin.\nrights reserved.\n\nDraft of October 2, 2019.\n\nCopyright c(cid:13) 2019.\n\nAll\n\nCHAPTER\n\n7 Neural Networks and Neural\n\nLanguage Models\n\n\xe2\x80\x9c[M]achines of this character can behave in a very complicated manner when\nthe number of units is large.\xe2\x80\x9d\n\nAlan Turing (1948) \xe2\x80\x9cIntelligent Machines\xe2\x80\x9d, page 6\n\nfeedforward\n\ndeep learning\n\nNeural networks are a fundamental computational tool for language process-\ning, and a very old one. They are called neural because their origins lie in the\nMcCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpli\xef\xac\x81ed model of the\nhuman neuron as a kind of computing element that could be described in terms of\npropositional logic. But the modern use in language processing no longer draws on\nthese early biological inspirations.\n\nInstead, a modern neural network is a network of small computing units, each\nof which takes a vector of input values and produces a single output value. In this\nchapter we introduce the neural net applied to classi\xef\xac\x81cation. The architecture we\nintroduce is called a feedforward network because the computation proceeds iter-\natively from one layer of units to the next. The use of modern neural nets is often\ncalled deep learning, because modern networks are often deep (have many layers).\nNeural networks share much of the same mathematics as logistic regression. But\nneural networks are a more powerful classi\xef\xac\x81er than logistic regression, and indeed a\nminimal neural network (technically one with a single \xe2\x80\x98hidden layer\xe2\x80\x99) can be shown\nto learn any function.\n\nNeural net classi\xef\xac\x81ers are different from logistic regression in another way. With\nlogistic regression, we applied the regression classi\xef\xac\x81er to many different tasks by\ndeveloping many rich kinds of feature templates based on domain knowledge. When\nworking with neural networks, it is more common to avoid most uses of rich hand-\nderived features, instead building neural networks that take raw words as inputs\nand learn to induce features as part of the process of learning to classify. We saw\nexamples of this kind of representation learning for embeddings in Chapter 6. Nets\nthat are very deep are particularly good at representation learning. For that reason\ndeep neural nets are the right tool for large scale problems that offer suf\xef\xac\x81cient data\nto learn features automatically.\n\nIn this chapter we\xe2\x80\x99ll introduce feedforward networks as classi\xef\xac\x81ers, and also ap-\nply them to the simple task of language modeling: assigning probabilities to word\nsequences and predicting upcoming words. In subsequent chapters we\xe2\x80\x99ll introduce\nmany other aspects of neural models, such as recurrent neural networks (Chap-\nter 9), encoder-decoder models, attention and the Transformer (Chapter 10).\n\n\x0c2 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\n7.1 Units\n\nbias term\n\nThe building block of a neural network is a single computational unit. A unit takes\na set of real valued numbers as input, performs some computation on them, and\nproduces an output.\n\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\ntional term in the sum called a bias term. Given a set of inputs x1...xn, a unit has\na set of corresponding weights w1...wn and a bias b, so the weighted sum z can be\nrepresented as:\n\n(cid:88)\n\nz = b +\n\nwixi\n\n(7.1)\n\ni\n\nvector\n\nOften it\xe2\x80\x99s more convenient to express this weighted sum using vector notation; recall\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thus\nwe\xe2\x80\x99ll talk about z in terms of a weight vector w, a scalar bias b, and an input vector\nx, and we\xe2\x80\x99ll replace the sum with the convenient dot product:\n\nz = w\xc2\xb7 x + b\n\n(7.2)\n\nactivation\n\nAs de\xef\xac\x81ned in Eq. 7.2, z is just a real valued number.\n\nFinally, instead of using z, a linear function of x, as the output, neural units\napply a non-linear function f to z. We will refer to the output of this function as\nthe activation value for the unit, a. Since we are just modeling a single unit, the\nactivation for the node is in fact the \xef\xac\x81nal output of the network, which we\xe2\x80\x99ll generally\ncall y. So the value y is de\xef\xac\x81ned as:\n\ny = a = f (z)\n\nWe\xe2\x80\x99ll discuss three popular non-linear functions f () below (the sigmoid, the tanh,\nand the recti\xef\xac\x81ed linear ReLU) but it\xe2\x80\x99s pedagogically convenient to start with the\nsigmoid function since we saw it in Chapter 5:\n\nsigmoid\n\ny = \xcf\x83 (z) =\n\n1\n\n1 + e\xe2\x88\x92z\n\n(7.3)\n\nThe sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output\ninto the range [0,1], which is useful in squashing outliers toward 0 or 1. And it\xe2\x80\x99s\ndifferentiable, which as we saw in Section ?? will be handy for learning.\n\nFigure 7.1 The sigmoid function takes a real value and maps it to the range [0,1]. It is\nnearly linear around 0 but outlier values get squashed toward 0 or 1.\n\n\x0c7.1\n\n\xe2\x80\xa2 UNITS\n\n3\n\nSubstituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit:\n\ny = \xcf\x83 (w\xc2\xb7 x + b) =\n\n1\n\n1 + exp(\xe2\x88\x92(w\xc2\xb7 x + b))\n\n(7.4)\n\nFig. 7.2 shows a \xef\xac\x81nal schematic of a basic neural unit. In this example the unit\ntakes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each\nvalue by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then\npasses the resulting sum through a sigmoid function to result in a number between 0\nand 1.\n\nFigure 7.2 A neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a\nweight for an input clamped at +1) and producing an output y. We include some convenient\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\nthis case the output of the unit y is the same as a, but in deeper networks we\xe2\x80\x99ll reserve y to\nmean the \xef\xac\x81nal output of the entire network, leaving a as the activation of an individual node.\n\nLet\xe2\x80\x99s walk through an example just to get an intuition. Let\xe2\x80\x99s suppose we have a\n\nunit with the following weight vector and bias:\n\nw = [0.2,0.3,0.9]\nb = 0.5\n\nWhat would this unit do with the following input vector:\n\nx = [0.5,0.6,0.1]\n\nThe resulting output y would be:\ny = \xcf\x83 (w\xc2\xb7 x + b) =\n\n1\n\n1 + e\xe2\x88\x92(w\xc2\xb7x+b)\n\n=\n\n1\n\n1 + e\xe2\x88\x92(.5\xe2\x88\x97.2+.6\xe2\x88\x97.3+.1\xe2\x88\x97.9+.5)\n\n= e\xe2\x88\x920.87 = .70\n\ntanh\n\nIn practice, the sigmoid is not commonly used as an activation function. A function\nthat is very similar but almost always better is the tanh function shown in Fig. 7.3a;\ntanh is a variant of the sigmoid that ranges from -1 to +1:\n\ny =\n\nez \xe2\x88\x92 e\xe2\x88\x92z\nez + e\xe2\x88\x92z\n\n(7.5)\n\nReLU\n\nThe simplest activation function, and perhaps the most commonly used, is the rec-\nti\xef\xac\x81ed linear unit, also called the ReLU, shown in Fig. 7.3b. It\xe2\x80\x99s just the same as x\nwhen x is positive, and 0 otherwise:\n\ny = max(x,0)\n\n(7.6)\n\ny\na\n\xcf\x83\n\n\xe2\x88\x91\n\nz\n\nw1\nx1\n\nw2\nx2\n\nw3\n\nb\nx3\n\n+1\n\n\x0c4 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nFigure 7.3 The tanh and ReLU activation functions.\n\n(a)\n\n(b)\n\nsaturated\n\nThese activation functions have different properties that make them useful for differ-\nent language applications or network architectures. For example the recti\xef\xac\x81er func-\ntion has nice properties that result from it being very close to linear. In the sigmoid\nor tanh functions, very high values of z result in values of y that are saturated, i.e.,\nextremely close to 1, which causes problems for learning. Recti\xef\xac\x81ers don\xe2\x80\x99t have this\nproblem, since the output of values close to 1 also approaches 1 in a nice gentle\nlinear way. By contrast, the tanh function has the nice properties of being smoothly\ndifferentiable and mapping outlier values toward the mean.\n\n7.2 The XOR problem\n\nEarly in the history of neural networks it was realized that the power of neural net-\nworks, as with the real neurons that inspired them, comes from combining these\nunits into larger networks.\n\nOne of the most clever demonstrations of the need for multi-layer networks was\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsome very simple functions of its input. Consider the task of computing elementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthe truth tables for those functions:\n\nAND\n\nx1 x2 y\n0\n0\n0\n0\n0\n1\n1\n1\n\n0\n1\n0\n1\n\nOR\n\nx1 x2 y\n0\n0\n1\n0\n1\n1\n1\n1\n\n0\n1\n0\n1\n\nXOR\n\nx1 x2 y\n0\n0\n1\n0\n1\n1\n1\n0\n\n0\n1\n0\n1\n\nperceptron\n\nThis example was \xef\xac\x81rst shown for the perceptron, which is a very simple neural\nunit that has a binary output and does not have a non-linear activation function. The\noutput y of a perceptron is 0 or 1, and is computed as follows (using the same weight\nw, input x, and bias b as in Eq. 7.2):\n\n(cid:26) 0,\n\n1,\n\ny =\n\nif w\xc2\xb7 x + b \xe2\x89\xa4 0\nif w\xc2\xb7 x + b > 0\n\n(7.7)\n\n\x0c7.2\n\n\xe2\x80\xa2 THE XOR PROBLEM 5\n\nIt\xe2\x80\x99s very easy to build a perceptron that can compute the logical AND and OR\n\nfunctions of its binary inputs; Fig. 7.4 shows the necessary weights.\n\n(a)\n\n(b)\n\nFigure 7.4 The weights w and bias b for perceptrons for computing logical functions. The\ninputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied\nwith the bias weight b. (a) logical AND, showing weights w1 = 1 and w2 = 1 and bias weight\nb = \xe2\x88\x921. (b) logical OR, showing weights w1 = 1 and w2 = 1 and bias weight b = 0. These\nweights/biases are just one from an in\xef\xac\x81nite number of possible sets of weights and biases that\nwould implement the functions.\n\nIt turns out, however, that it\xe2\x80\x99s not possible to build a perceptron to compute\n\nlogical XOR! (It\xe2\x80\x99s worth spending a moment to give it a try!)\n\nThe intuition behind this important result relies on understanding that a percep-\ntron is a linear classi\xef\xac\x81er. For a two-dimensional input x1 and x2, the perception\nequation, w1x1 + w2x2 + b = 0 is the equation of a line. (We can see this by putting\nit in the standard linear format: x2 = \xe2\x88\x92(w1/w2)x1 \xe2\x88\x92 b.) This line acts as a decision\nboundary in two-dimensional space in which the output 0 is assigned to all inputs\nlying on one side of the line, and the output 1 to all input points lying on the other\nside of the line. If we had more than 2 inputs, the decision boundary becomes a\nhyperplane instead of a line, but the idea is the same, separating the space into two\ncategories.\n\nFig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\nby one possible set of parameters for an AND and an OR classi\xef\xac\x81er. Notice that there\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separable\nfunction. Of course we could draw a boundary with a curve, or some other function,\nbut not a single line.\n\n7.2.1 The solution: neural networks\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\nculated by a layered network of units. Let\xe2\x80\x99s see an example of how to do this from\nGoodfellow et al. (2016) that computes XOR using two layers of ReLU-based units.\nFig. 7.6 shows a \xef\xac\x81gure with the input being processed by two layers of neural units.\nThe middle layer (called h) has two units, and the output layer (called y) has one\nunit. A set of weights and biases are shown for each ReLU that correctly computes\nthe XOR function.\n\nLet\xe2\x80\x99s walk through what happens with the input x = [0 0]. If we multiply each\ninput value by the appropriate weight, sum, and then add the bias b, we get the\nvector [0 -1], and we then apply the recti\xef\xac\x81ed linear transformation to give the output\nof the h layer as [0 0]. Now we once again multiply by the weights, sum, and add\nthe bias (0 in this case) resulting in the value 0. The reader should work through the\ncomputation of the remaining 3 possible input pairs to see that the resulting y values\nare 1 for the inputs [0 1] and [1 0] and 0 for [0 0] and [1 1].\n\ndecision\nboundary\n\nlinearly\nseparable\n\nx1\n\nx2\n+1\n\n1\n1\n-1\n\nx1\n\nx2\n+1\n\n1\n1\n0\n\n\x0c6 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nFigure 7.5 The functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the\ny axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\n(2002).\n\nFigure 7.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in\ntwo layers; we\xe2\x80\x99ve called them h1, h2 (h for \xe2\x80\x9chidden layer\xe2\x80\x9d) and y1. As before, the numbers\non the arrows represent the weights w for each unit, and we represent the bias b as a weight\non a unit clamped to +1, with the bias weights/units in gray.\n\nIt\xe2\x80\x99s also instructive to look at the intermediate results, the outputs of the two\nhidden nodes h0 and h1. We showed in the previous paragraph that the h vector for\nthe inputs x = [0 0] was [0 0]. Fig. 7.7b shows the values of the h layer for all 4\ninputs. Notice that hidden representations of the two input points x = [0 1] and x\n= [1 0] (the two cases with XOR output = 1) are merged to the single point h = [1\n0]. The merger makes it easy to linearly separate the positive and negative cases\nof XOR. In other words, we can view the hidden layer of the network as forming a\nrepresentation for the input.\n\nIn this example we just stipulated the weights in Fig. 7.6. But for real examples\nthe weights for neural networks are learned automatically using the error backprop-\nagation algorithm to be introduced in Section 7.4. That means the hidden layers will\nlearn to form useful representations. This intuition, that neural networks can auto-\nmatically learn useful representations of the input, is one of their key advantages,\nand one that we will return to again and again in later chapters.\n\nNote that the solution to the XOR problem requires a network of units with non-\nlinear activation functions. A network made up of simple linear (perceptron) units\ncannot solve the XOR problem. This is because a network formed by many layers\nof purely linear units can always be reduced (shown to be computationally identical\n\nx2\n\n1\n\n0\n\n0\n\nx2\n\n1\n\n0\n\n0\n\nx2\n\n1\n\n0\n\n0\n\n?\n\nx1\n\n1\n\nx1\n\n1\n\nx1\n\n1\n\na)  x1 AND x2\n\nb)  x1 OR x2\n\nc)  x1 XOR x2\n\ny1\n\n1\n\n1\n\nh1\n\n1\nx1\n\n-2\n\n0\n\nh2\n\n1\nx2\n\n1\n\n0\n\n-1\n\n+1\n\n+1\n\n\x0c7.3\n\n\xe2\x80\xa2 FEED-FORWARD NEURAL NETWORKS\n\n7\n\nFigure 7.7 The hidden layer forming a new representation of the input. Here is the rep-\nresentation of the hidden layer, h, compared to the original input representation x. Notice\nthat the input point [0 1] has been collapsed with the input point [1 0], making it possible to\nlinearly separate the positive and negative cases of XOR. After Goodfellow et al. (2016).\n\nto) a single layer of linear units with appropriate weights, and we\xe2\x80\x99ve already shown\n(visually, in Fig. 7.5) that a single unit cannot solve the XOR problem.\n\n7.3 Feed-Forward Neural Networks\n\nfeedforward\nnetwork\n\nmulti-layer\nperceptrons\nMLP\n\nhidden layer\n\nfully-connected\n\nLet\xe2\x80\x99s now walk through a slightly more formal presentation of the simplest kind of\nneural network, the feedforward network. A feedforward network is a multilayer\nnetwork in which the units are connected with no cycles; the outputs from units in\neach layer are passed to units in the next higher layer, and no outputs are passed\nback to lower layers. (In Chapter 9 we\xe2\x80\x99ll introduce networks with cycles, called\nrecurrent neural networks.)\n\nFor historical reasons multilayer networks, especially feedforward networks, are\nsometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,\nsince the units in modern multilayer networks aren\xe2\x80\x99t perceptrons (perceptrons are\npurely linear, but modern networks are made up of units with non-linearities like\nsigmoids), but at some point the name stuck.\n\nSimple feedforward networks have three kinds of nodes:\n\ninput units, hidden\n\nunits, and output units. Fig. 7.8 shows a picture.\n\nThe input units are simply scalar values just as we saw in Fig. 7.2.\nThe core of the neural network is the hidden layer formed of hidden units,\neach of which is a neural unit as described in Section 7.1, taking a weighted sum of\nits inputs and then applying a non-linearity. In the standard architecture, each layer\nis fully-connected, meaning that each unit in each layer takes as input the outputs\nfrom all the units in the previous layer, and there is a link between every pair of units\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\n\nRecall that a single hidden unit has parameters w (the weight vector) and b (the\nbias scalar). We represent the parameters for the entire hidden layer by combining\nthe weight vector wi and bias bi for each unit i into a single weight matrix W and\na single bias vector b for the whole layer (see Fig. 7.8). Each element Wi j of the\nweight matrix W represents the weight of the connection from the ith input unit xi to\n\nx1\n\n1\n\n0\n\n0\n\nh1\n\n1\n\n0\n\n0\n\n1\n\nh0\n\n2\n\nx0\n\n1\n\na) The original x space\n\nb) The new h space\n\n\x0c8 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nFigure 7.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,\nand one input layer (the input layer is usually not counted when enumerating layers).\n\nthe jth hidden unit h j.\n\nThe advantage of using a single matrix W for the weights of the entire layer is\nthat now the hidden layer computation for a feedforward network can be done very\nef\xef\xac\x81ciently with simple matrix operations. In fact, the computation only has three\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\nand applying the activation function g (such as the sigmoid, tanh, or ReLU activation\nfunction de\xef\xac\x81ned above).\n\nThe output of the hidden layer, the vector h, is thus the following, using the\n\nsigmoid function \xcf\x83:\n\nh = \xcf\x83 (W x + b)\n\n(7.8)\n\nNotice that we\xe2\x80\x99re applying the \xcf\x83 function here to a vector, while in Eq. 7.3 it was\napplied to a scalar. We\xe2\x80\x99re thus allowing \xcf\x83 (\xc2\xb7), and indeed any activation function\ng(\xc2\xb7), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].\n\nLet\xe2\x80\x99s introduce some constants to represent the dimensionalities of these vectors\nand matrices. We\xe2\x80\x99ll refer to the input layer as layer 0 of the network, and have n0\nrepresent the number of inputs, so x is a vector of real numbers of dimension n0,\nor more formally x \xe2\x88\x88 Rn0. Let\xe2\x80\x99s call the hidden layer layer 1 and the output layer\nlayer 2. The hidden layer has dimensionality n1, so h \xe2\x88\x88 Rn1 and also b \xe2\x88\x88 Rn1 (since\neach hidden unit can take a different bias value). And the weight matrix W has\ndimensionality W \xe2\x88\x88 Rn1\xc3\x97n0.\n\nTake a moment to convince yourself that the matrix multiplication in Eq. 7.8 will\n\ncompute the value of each h j as \xcf\x83(cid:0)(cid:80)nx\n\nAs we saw in Section 7.2, the resulting value h (for hidden but also for hypoth-\nesis) forms a representation of the input. The role of the output layer is to take\nthis new representation h and compute a \xef\xac\x81nal output. This output could be a real-\nvalued number, but in many cases the goal of the network is to make some sort of\nclassi\xef\xac\x81cation decision, and so we will focus on the case of classi\xef\xac\x81cation.\n\nIf we are doing a binary task like sentiment classi\xef\xac\x81cation, we might have a single\noutput node, and its value y is the probability of positive versus negative sentiment.\nIf we are doing multinomial classi\xef\xac\x81cation, such as assigning a part-of-speech tag, we\nmight have one output node for each potential part-of-speech, whose output value\nis the probability of that part-of-speech, and the values of all the output nodes must\nsum to one. The output layer thus gives a probability distribution across the output\n\ni=1 wi jxi + b j\n\n(cid:1).\n\nU\n\nh1\n\nW\n\ny1\n\ny2\n\n\xe2\x80\xa6\n\nyn2\n\nh2\n\nh3\n\nhn1\xe2\x80\xa6\n\nx1\n\nx2\n\n\xe2\x80\xa6\n\nxn0\n\nb\n\n+1\n\n\x0cnormalizing\n\nsoftmax\n\n7.3\n\n\xe2\x80\xa2 FEED-FORWARD NEURAL NETWORKS\n\n9\n\nnodes.\n\nLet\xe2\x80\x99s see how this happens. Like the hidden layer, the output layer has a weight\nmatrix (let\xe2\x80\x99s call it U), but some models don\xe2\x80\x99t include a bias vector b in the output\nlayer, so we\xe2\x80\x99ll simplify by eliminating the bias vector in this example. The weight\nmatrix is multiplied by its input vector (h) to produce the intermediate output z.\n\nz = Uh\n\nThere are n2 output nodes, so z \xe2\x88\x88 Rn2, weight matrix U has dimensionality U \xe2\x88\x88\nRn2\xc3\x97n1, and element Ui j is the weight from unit j in the hidden layer to unit i in the\noutput layer.\n\nHowever, z can\xe2\x80\x99t be the output of the classi\xef\xac\x81er, since it\xe2\x80\x99s a vector of real-valued\nnumbers, while what we need for classi\xef\xac\x81cation is a vector of probabilities. There is\na convenient function for normalizing a vector of real values, by which we mean\nconverting it to a vector that encodes a probability distribution (all the numbers lie\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ?? of\nChapter 5. For a vector z of dimensionality d, the softmax is de\xef\xac\x81ned as:\n\nsoftmax(zi) =\n\n1 \xe2\x89\xa4 i \xe2\x89\xa4 d\n\n(7.9)\n\nezi(cid:80)d\n\nj=1 ez j\n\nThus for example given a vector z=[0.6 1.1 -1.5 1.2 3.2 -1.1], softmax(z) is [0.055\n0.090 0.0067 0.10 0.74 0.010].\n\nYou may recall that softmax was exactly what is used to create a probability\ndistribution from a vector of real-valued numbers (computed from summing weights\ntimes features) in logistic regression in Chapter 5.\n\nThat means we can think of a neural network classi\xef\xac\x81er with one hidden layer\nas building a vector h which is a hidden layer representation of the input, and then\nrunning standard logistic regression on the features that the network develops in h.\nBy contrast, in Chapter 5 the features were mainly designed by hand via feature\ntemplates. So a neural network is like logistic regression, but (a) with many layers,\nsince a deep neural network is like layer after layer of logistic regression classi\xef\xac\x81ers,\nand (b) rather than forming the features by feature templates, the prior layers of the\nnetwork induce the feature representations themselves.\n\nHere are the \xef\xac\x81nal equations for a feedforward network with a single hidden layer,\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-\nized by weight matrices W and U and a bias vector b:\n\nh = \xcf\x83 (W x + b)\nz = Uh\ny = softmax(z)\n\n(7.10)\n\nWe\xe2\x80\x99ll call this network a 2-layer network (we traditionally don\xe2\x80\x99t count the input\nlayer when numbering layers, but do count the output layer). So by this terminology\nlogistic regression is a 1-layer network.\n\nLet\xe2\x80\x99s now set up some notation to make it easier to talk about deeper networks\nof depth more than 2. We\xe2\x80\x99ll use superscripts in square brackets to mean layer num-\nbers, starting at 0 for the input layer. So W [1] will mean the weight matrix for the\n(\xef\xac\x81rst) hidden layer, and b[1] will mean the bias vector for the (\xef\xac\x81rst) hidden layer. n j\nwill mean the number of units at layer j. We\xe2\x80\x99ll use g(\xc2\xb7) to stand for the activation\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\nfor output layers. We\xe2\x80\x99ll use a[i] to mean the output from layer i, and z[i] to mean the\n\n\x0c10 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\ncombination of weights and biases W [i]a[i\xe2\x88\x921] +b[i]. The 0th layer is for inputs, so the\ninputs x we\xe2\x80\x99ll refer to more generally as a[0].\n\nThus we can re-represent our 2-layer net from Eq. 7.10 as follows:\n\nz[1] = W [1]a[0] + b[1]\na[1] = g[1](z[1])\nz[2] = W [2]a[1] + b[2]\na[2] = g[2](z[2])\n\n\xcb\x86y = a[2]\n\n(7.11)\n\nNote that with this notation, the equations for the computation done at each layer are\nthe same. The algorithm for computing the forward step in an n-layer feedforward\nnetwork, given the input vector a[0] is thus simply:\n\nfor i in 1..n\n\nz[i] = W [i] a[i\xe2\x88\x921] + b[i]\na[i] = g[i](z[i])\n\n\xcb\x86y = a[n]\nThe activation functions g(\xc2\xb7) are generally different at the \xef\xac\x81nal layer. Thus g[2]\nmight be softmax for multinomial classi\xef\xac\x81cation or sigmoid for binary classi\xef\xac\x81cation,\nwhile ReLU or tanh might be the activation function g(\xc2\xb7) at the internal layers.\n\n7.4 Training Neural Nets\n\nA feedforward neural net is an instance of supervised machine learning in which we\nknow the correct output y for each observation x. What the system produces, via\nEq. 7.11, is \xcb\x86y, the system\xe2\x80\x99s estimate of the true y. The goal of the training procedure\nis to learn parameters W [i] and b[i] for each layer i that make \xcb\x86y for each training\nobservation as close as possible to the true y.\n\nIn general, we do all this by drawing on the methods we introduced in Chapter 5\nfor logistic regression, so the reader should be comfortable with that chapter before\nproceeding.\n\nFirst, we\xe2\x80\x99ll need a loss function that models the distance between the system\noutput and the gold output, and it\xe2\x80\x99s common to use the loss function used for logistic\nregression, the cross-entropy loss.\n\nSecond, to \xef\xac\x81nd the parameters that minimize this loss function, we\xe2\x80\x99ll use the\n\ngradient descent optimization algorithm introduced in Chapter 5.\n\nThird, gradient descent requires knowing the gradient of the loss function, the\nvector that contains the partial derivative of the loss function with respect to each of\nthe parameters. Here is one part where learning for neural networks is more complex\nthan for logistic logistic regression. In logistic regression, for each observation we\ncould directly compute the derivative of the loss function with respect to an individ-\nual w or b. But for neural networks, with millions of parameters in many layers, it\xe2\x80\x99s\nmuch harder to see how to compute the partial derivative of some weight in layer 1\nwhen the loss is attached to some much later layer. How do we partial out the loss\nover all those intermediate layers?\n\nThe answer is the algorithm called error backpropagation or reverse differen-\n\ntiation.\n\n\x0c7.4\n\n\xe2\x80\xa2 TRAINING NEURAL NETS\n\n11\n\ncross-entropy\nloss\n\n7.4.1 Loss function\nThe cross-entropy loss that is used in neural networks is the same one we saw for\nlogistic regression.\n\nIn fact, if the neural network is being used as a binary classi\xef\xac\x81er, with the sig-\nmoid at the \xef\xac\x81nal layer, the loss function is exactly the same as we saw with logistic\nregression in Eq. ??:\n\nLCE ( \xcb\x86y,y) = \xe2\x88\x92log p(y|x) = \xe2\x88\x92 [ylog \xcb\x86y + (1\xe2\x88\x92 y)log(1\xe2\x88\x92 \xcb\x86y)]\n\n(7.12)\n\nWhat about if the neural network is being used as a multinomial classi\xef\xac\x81er? Let y be\na vector over the C classes representing the true output probability distribution. The\ncross-entropy loss here is\n\nLCE ( \xcb\x86y,y) = \xe2\x88\x92\n\nyi log \xcb\x86yi\n\n(7.13)\n\ni=1\n\nWe can simplify this equation further. Assume this is a hard classi\xef\xac\x81cation task,\nmeaning that only one class is the correct one, and that there is one output unit in y\nfor each class. If the true class is i, then y is a vector where yi = 1 and y j = 0 \xe2\x88\x80 j (cid:54)= i.\nA vector like this, with one value=1 and the rest 0, is called a one-hot vector. Now\nlet \xcb\x86y be the vector output from the network. The sum in Eq. 7.13 will be 0 except\nfor the true class. Hence the cross-entropy loss is simply the log probability of the\ncorrect class, and we therefore also call this the negative log likelihood loss:\n\nLCE ( \xcb\x86y,y) = \xe2\x88\x92log \xcb\x86yi\n\n(7.14)\n\nnegative log\nlikelihood loss\n\nC(cid:88)\n\nPlugging in the softmax formula from Eq. 7.9, and with K the number of classes:\n\nLCE ( \xcb\x86y,y) = \xe2\x88\x92log\n\nezi(cid:80)K\n\nj=1 ez j\n\n(7.15)\n\n7.4.2 Computing the Gradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregression is), we could simply use the derivative of the loss that we used for logistic\nregression in Eq. 7.16 (and derived in Section ??):\n\n\xe2\x88\x82 LCE (w,b)\n\n\xe2\x88\x82 w j\n\n= ( \xcb\x86y\xe2\x88\x92 y) x j\n= (\xcf\x83 (w\xc2\xb7 x + b)\xe2\x88\x92 y) x j\n\n(7.16)\n\nOr for a network with one hidden layer and softmax output, we could use the deriva-\ntive of the softmax loss from Eq. ??:\n\n\xe2\x88\x82 LCE\n\xe2\x88\x82 wk\n\n(cid:32)\n= (1{y = k}\xe2\x88\x92 p(y = k|x))xk\n1{y = k}\xe2\x88\x92 ewk\xc2\xb7x+bk\n\n(cid:80)K\nj=1 ew j\xc2\xb7x+b j\n\n=\n\n(cid:33)\n\nxk\n\n(7.17)\n\n\x0c12 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nerror back-\npropagation\n\nBut these derivatives only give correct updates for one weight layer: the last one!\nFor deep networks, computing the gradients for each weight is much more complex,\nsince we are computing the derivative with respect to weight parameters that appear\nall the way back in the very early layers of the network, even though the loss is\ncomputed only at the very end of the network.\n\nThe solution to computing this gradient is an algorithm called error backprop-\nagation or backprop (Rumelhart et al., 1986). While backprop was invented spe-\ncially for neural networks, it turns out to be the same as a more general procedure\ncalled backward differentiation, which depends on the notion of computation\ngraphs. Let\xe2\x80\x99s see how that works in the next subsection.\n\n7.4.3 Computation Graphs\nA computation graph is a representation of the process of computing a mathematical\nexpression, in which the computation is broken down into separate operations, each\nof which is modeled as a node in a graph.\n\nConsider computing the function L(a,b,c) = c(a + 2b). If we make each of the\ncomponent addition and multiplication operations explicit, and add names (d and e)\nfor the intermediate outputs, the resulting series of computations is:\n\nd = 2\xe2\x88\x97 b\ne = a + d\nL = c\xe2\x88\x97 e\n\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.9. The simplest use of computation graphs is to compute the value of the\nfunction with some given inputs. In the \xef\xac\x81gure, we\xe2\x80\x99ve assumed the inputs a = 3,\nb = 1, c = \xe2\x88\x922, and we\xe2\x80\x99ve shown the result of the forward pass to compute the re-\nsult L(3,1,\xe2\x88\x922) = 10. In the forward pass of a computation graph, we apply each\noperation left to right, passing the outputs of each computation as the input to the\nnext node.\n\nFigure 7.9 Computation graph for the function L(a,b,c) = c(a + 2b), with values for input\nnodes a = 3, b = 1, c = \xe2\x88\x922, showing the forward pass computation of L.\n\n7.4.4 Backward differentiation on computation graphs\nThe importance of the computation graph comes from the backward pass, which\nis used to compute the derivatives that we\xe2\x80\x99ll need for the weight update. In this\nexample our goal is to compute the derivative of the output function L with respect\n\nforward pass\ne=5\ne=d+a\n\nd=2\nd = 2b\n\nL=-10\nL=ce\n\n3\na\n1\nb\n-2\nc\n\n\x0cchain rule\n\n7.4\n\n\xe2\x80\xa2 TRAINING NEURAL NETS\n\n13\n\nto each of the input variables, i.e., \xe2\x88\x82 L\nmuch a small change in a affects L.\n\n\xe2\x88\x82 a , \xe2\x88\x82 L\n\n\xe2\x88\x82 b , and \xe2\x88\x82 L\n\n\xe2\x88\x82 c . The derivative \xe2\x88\x82 L\n\n\xe2\x88\x82 a , tells us how\n\nBackwards differentiation makes use of the chain rule in calculus. Suppose we\nare computing the derivative of a composite function f (x) = u(v(x)). The derivative\nof f (x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with\nrespect to x:\n\nd f\ndx =\n\ndu\ndv\n\n\xc2\xb7 dv\ndx\n\n(7.18)\n\nThe chain rule extends to more than two functions. If computing the derivative of a\ncomposite function f (x) = u(v(w(x))), the derivative of f (x) is:\n\nd f\ndx =\n\ndu\ndv\n\n\xc2\xb7 dv\ndw\n\n\xc2\xb7 dw\ndx\n\n(7.19)\n\nLet\xe2\x80\x99s now compute the 3 derivatives we need. Since in the computation graph\n\nL = ce, we can directly compute the derivative \xe2\x88\x82 L\n\xe2\x88\x82 c :\n\n\xe2\x88\x82 L\n\xe2\x88\x82 c = e\n\nFor the other two, we\xe2\x80\x99ll need to use the chain rule:\n\n\xe2\x88\x82 L\n\xe2\x88\x82 a =\n\xe2\x88\x82 L\n\xe2\x88\x82 b =\n\n\xe2\x88\x82 L\n\xe2\x88\x82 e\n\xe2\x88\x82 L\n\xe2\x88\x82 e\n\n\xe2\x88\x82 e\n\xe2\x88\x82 a\n\xe2\x88\x82 e\n\xe2\x88\x82 d\n\n\xe2\x88\x82 d\n\xe2\x88\x82 b\n\n(7.20)\n\n(7.21)\n\nEq. 7.21 thus requires \xef\xac\x81ve intermediate derivatives: \xe2\x88\x82 L\n\n\xe2\x88\x82 d , and \xe2\x88\x82 d\n\xe2\x88\x82 b ,\nwhich are as follows (making use of the fact that the derivative of a sum is the sum\nof the derivatives):\n\n\xe2\x88\x82 e , \xe2\x88\x82 L\n\n\xe2\x88\x82 a, \xe2\x88\x82 e\n\n\xe2\x88\x82 c , \xe2\x88\x82 e\n\nL = ce :\n\ne = a + d :\n\nd = 2b :\n\n\xe2\x88\x82 L\n\xe2\x88\x82 c = e\n\xe2\x88\x82 e\n\xe2\x88\x82 d = 1\n\n\xe2\x88\x82 L\n\xe2\x88\x82 e = c,\n\xe2\x88\x82 e\n\xe2\x88\x82 a = 1,\n\xe2\x88\x82 d\n\xe2\x88\x82 b = 2\n\n\xe2\x88\x82 c and \xe2\x88\x82 L\n\nIn the backward pass, we compute each of these partials along each edge of the graph\nfrom right to left, multiplying the necessary partials to result in the \xef\xac\x81nal derivative\nwe need. Thus we begin by annotating the \xef\xac\x81nal node with \xe2\x88\x82 L\n\xe2\x88\x82 L = 1. Moving to the\nleft, we then compute \xe2\x88\x82 L\n\xe2\x88\x82 e , and so on, until we have annotated the graph all\nthe way to the input variables. The forward pass conveniently already will have\ncomputed the values of the forward intermediate variables we need (like d and e)\nto compute these derivatives. Fig. 7.10 shows the backward pass. At each node we\nneed to compute the local partial derivative with respect to the parent, multiply it by\nthe partial derivative that is being passed down from the parent, and then pass it to\nthe child.\n\nBackward differentiation for a neural network\nOf course computation graphs for real neural networks are much more complex.\nFig. 7.11 shows a sample computation graph for a 2-layer neural network with n0 =\n\n\x0c14 CHAPTER 7\n\n\xe2\x80\xa2 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS\n\nFigure 7.10 Computation graph for the function L(a,b,c) = c(a + 2b), showing the back-\nward pass computation of \xe2\x88\x82 L\n\n\xe2\x88\x82 a , \xe2\x88\x82 L\n\n\xe2\x88\x82 b , and \xe2\x88\x82 L\n\xe2\x88\x82 c .\n\n2, n1 = 2, and n2 = 1, assuming binary classi\xef\xac\x81cation and hence using a sigmoid\noutput unit for simplicity. The function that the computation graph is computing is:\n\nz[1] = W [1]x + b[1]\na[1] = ReLU(z[1])\nz[2] = W [2]a[1] + b[2]\na[2] = \xcf\x83 (z[2])\n\n\xcb\x86y = a[2]\n\n(7.22)\n\nFigure 7.11 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer)\nwith two input dimensions and 2 hidden dimensions.\n\nThe weights that need updating (those for which we need to know the partial\nderivative of the loss function) are shown in orange. In order to do the backward\npass, we\xe2\x80\x99ll need to know the derivatives of all the functions in the graph. We already\nsaw in Section ?? the derivative of the sigmoid \xcf\x83:\nd\xcf\x83 (z)\ndz = \xcf\x83 (z)(1\xe2\x88\x92 \xcf\x83 (z))\n\n(7.23)\n\na=3\na\n\xe2\x88\x82L=-2\n\xe2\x88\x82a\nb=1\nb\n\xe2\x88\x82L=-4\n\xe2\x88\x82b\n\nc=-2\n\nc\n\xe2\x88\x82L=5\n\xe2\x88\x82c\n\n\xe2\x88\x82e =1\n\xe2\x88\x82a\n\n\xe2\x88\x82d =2\n\xe2\x88\x82b\n\nd=2\nd = 2b\n\xe2\x88\x82L=-2\n\xe2\x88\x82d\n\ne=5\ne=d+a\n\xe2\x88\x82L=-2\n\xe2\x88\x82e\n\n\xe2\x88\x82e =1\n\xe2\x88\x82d\n\n\xe2\x88\x82L =5\n\xe2\x88\x82c\n\n \n\n\xe2\x88\x82L =-2\n\xe2\x88\x82e\n\nL=-10\nL=ce\n\xe2\x88\x82L=1\n\xe2\x88\x82L\n\nbackward pass\n\nx1\n\nx2\n\n*\n\n*\n\n*\n\n*\n\nw[1]\n11\nw[1]\n21\n\nb[1]\n\nw[1]\n12\nw[1]\n22\n\nb[1]\n\nz[1] = \n\n+\n\na[1] = \nReLU\n\nz[1] = \n\n+\n\na[1] = \nReLU\n\n*\n\n*\n\nw[2]\n11\n\n \nw[2]\n21\n\nb[2]\n\nz[2] = \n\n+\n\na[2] = \xcf\x83\n\nL (a[2],y)\n\n\x0c7.5\n\n\xe2\x80\xa2 NEURAL LANGUAGE MODELS\n\n15\n\nWe\xe2\x80\x99ll also need the derivatives of each of the other activation functions. The\n\nderivative of tanh is:\n\nd tanh(z)\n\ndz\n\n= 1\xe2\x88\x92 tanh2(z)\n\nThe derivative of the ReLU is\n\nd ReLU(z)\n\ndz\n\n(cid:26) 0 f or x < 0\n\n1 f or x \xe2\x89\xa5 0\n\n=\n\n(7.24)\n\n(7.25)\n\n7.4.5 More details on learning\nOptimization in neural networks is a non-convex optimization problem, more com-\nplex than for logistic regression, and for that and other reasons there are many best\npractices for successful learning.\n\nFor logistic regression we can initialize gradient descent with all the weights and\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\nweights with small random numbers. It\xe2\x80\x99s also helpful to normalize the input values\nto have 0 mean and unit variance.\n\nVarious forms of regularization are used to prevent over\xef\xac\x81tting. One of the most\nimportant is dropout: randomly dropping some units and their connections from\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning\nof hyperparameters is also important. The parameters of a neural network are the\nweights W and biases b; those are learned by gradient descent. The hyperparameters\nare things that are chosen by the algorithm designer; optimal values are tuned on a\ndevset rather than by gradient descent learning on the training set. Hyperparameters\ninclude the learning rate \xce\xb7, the mini-batch size, the model architecture (the number\nof layers, the number of hidden nodes per layer, the choice of activation functions),\nhow to regularize, and so on. Gradient descent itself also has many architectural\nvariants such as Adam (Kingma and Ba, 2015).\n\nFinally, most modern neural networks are built using computation graph for-\nmalisms that make it easy and natural to do gradient computation and parallelization\nonto vector-based GPUs (Graphic Processing Units). Pytorch (Paszke et al., 2017)\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\nreader should consult a neural network textbook for further details; some sugges-\ntions are at the end of the chapter.\n\ndropout\n\nhyperparameter\n\n