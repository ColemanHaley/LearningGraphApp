Speech and Language Processing Daniel Jurafsky James H Martin Copyright c 2019 All rights reserved Draft of October 2 2019 CHAPTER 7 Neural Networks and NeuralLanguage Models Machines of this character can behave in a very complicated manner when the number of units is large Alan Turing 1948 Intelligent Machines page 6 Neural networks are a fundamental computational tool for language process ing and a very old one They are called neural because their origins lie in the McCullochPitts neuron McCulloch and Pitts 1943 a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic But the modern use in language processing no longer draws on these early biological inspirations Instead a modern neural network is a network of small computing units each of which takes a vector of input values and produces a single output value In this chapter we introduce the neural net applied to classification The architecture we introduce is called a feedforward network because the computation proceeds iterfeedforward atively from one layer of units to the next The use of modern neural nets is often called deep learning because modern networks are often deep have many layersdeep learning Neural networks share much of the same mathematics as logistic regression But neural networks are a more powerful classifier than logistic regression and indeed a minimal neural network technically one with a single hidden layer can be shown to learn any function Neural net classifiers are different from logistic regression in another way With logistic regression we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge When working with neural networks it is more common to avoid most uses of rich hand derived features instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify We saw examples of this kind of representation learning for embeddings in Chapter 6 Nets that are very deep are particularly good at representation learning For that reason deep neural nets are the right tool for large scale problems that offer sufficient data to learn features automatically In this chapter well introduce feedforward networks as classifiers and also ap ply them to the simple task of language modeling assigning probabilities to word sequences and predicting upcoming words In subsequent chapters well introduce many other aspects of neural models such as recurrent neural networks Chap ter 9 encoderdecoder models attention and the Transformer Chapter 10 2 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS 71 Units The building block of a neural network is a single computational unit A unit takes a set of real valued numbers as input performs some computation on them and produces an output At its heart a neural unit is taking a weighted sum of its inputs with one addi tional term in the sum called a bias term Given a set of inputs x1xn a unit hasbias term a set of corresponding weights w1wn and a bias b so the weighted sum z can be represented as z b i wixi 71 Often its more convenient to express this weighted sum using vector notation recall from linear algebra that a vector is at heart just a list or array of numbers Thusvector well talk about z in terms of a weight vector w a scalar bias b and an input vector x and well replace the sum with the convenient dot product z w xb 72 As defined in Eq 72 z is just a real valued number Finally instead of using z a linear function of x as the output neural units apply a nonlinear function f to z We will refer to the output of this function as the activation value for the unit a Since we are just modeling a single unit theactivation activation for the node is in fact the final output of the network which well generally call y So the value y is defined as y a f z Well discuss three popular nonlinear functions f below the sigmoid the tanh and the rectified linear ReLU but its pedagogically convenient to start with the sigmoid function since we saw it in Chapter 5sigmoid y σz 1 1 ez 73 The sigmoid shown in Fig 71 has a number of advantages it maps the output into the range 01 which is useful in squashing outliers toward 0 or 1 And its differentiable which as we saw in Section will be handy for learning Figure 71 The sigmoid function takes a real value and maps it to the range 01 It is nearly linear around 0 but outlier values get squashed toward 0 or 1 71 UNITS 3 Substituting Eq 72 into Eq 73 gives us the output of a neural unit y σw xb 1 1 expw xb 74 Fig 72 shows a final schematic of a basic neural unit In this example the unit takes 3 input values x1x2 and x3 and computes a weighted sum multiplying each value by a weight w1 w2 and w3 respectively adds them to a bias term b and then passes the resulting sum through a sigmoid function to result in a number between 0 and 1 x1 x2 x3 y w1 w2 w3 b σ 1 z a Figure 72 A neural unit taking 3 inputs x1 x2 and x3 and a bias b that we represent as a weight for an input clamped at 1 and producing an output y We include some convenient intermediate variables the output of the summation z and the output of the sigmoid a In this case the output of the unit y is the same as a but in deeper networks well reserve y to mean the final output of the entire network leaving a as the activation of an individual node Lets walk through an example just to get an intuition Lets suppose we have a unit with the following weight vector and bias w 020309 b 05 What would this unit do with the following input vector x 050601 The resulting output y would be y σw xb 1 1 ewxb 1 1 e5263195 e087 70 In practice the sigmoid is not commonly used as an activation function A function that is very similar but almost always better is the tanh function shown in Fig 73atanh tanh is a variant of the sigmoid that ranges from 1 to 1 y ez ez ez ez 75 The simplest activation function and perhaps the most commonly used is the rec tified linear unit also called the ReLU shown in Fig 73b Its just the same as xReLU when x is positive and 0 otherwise y maxx0 76 4 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS a b Figure 73 The tanh and ReLU activation functions These activation functions have different properties that make them useful for differ ent language applications or network architectures For example the rectifier func tion has nice properties that result from it being very close to linear In the sigmoid or tanh functions very high values of z result in values of y that are saturated iesaturated extremely close to 1 which causes problems for learning Rectifiers dont have this problem since the output of values close to 1 also approaches 1 in a nice gentle linear way By contrast the tanh function has the nice properties of being smoothly differentiable and mapping outlier values toward the mean 72 The XOR problem Early in the history of neural networks it was realized that the power of neural net works as with the real neurons that inspired them comes from combining these units into larger networks One of the most clever demonstrations of the need for multilayer networks was the proof by Minsky and Papert 1969 that a single neural unit cannot compute some very simple functions of its input Consider the task of computing elementary logical functions of two inputs like AND OR and XOR As a reminder here are the truth tables for those functions AND OR XOR x1 x2 y x1 x2 y x1 x2 y 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 This example was first shown for the perceptron which is a very simple neuralperceptron unit that has a binary output and does not have a nonlinear activation function The output y of a perceptron is 0 or 1 and is computed as follows using the same weight w input x and bias b as in Eq 72 y 0 if w xb 0 1 if w xb 0 77 72 THE XOR PROBLEM 5 Its very easy to build a perceptron that can compute the logical AND and OR functions of its binary inputs Fig 74 shows the necessary weights x1 x2 1 1 1 1 x1 x2 1 0 1 1 a b Figure 74 The weights w and bias b for perceptrons for computing logical functions The inputs are shown as x1 and x2 and the bias as a special node with value 1 which is multiplied with the bias weight b a logical AND showing weights w1 1 and w2 1 and bias weight b 1 b logical OR showing weights w1 1 and w2 1 and bias weight b 0 These weightsbiases are just one from an infinite number of possible sets of weights and biases that would implement the functions It turns out however that its not possible to build a perceptron to compute logical XOR Its worth spending a moment to give it a try The intuition behind this important result relies on understanding that a percep tron is a linear classifier For a twodimensional input x1 and x2 the perception equation w1x1 w2x2 b 0 is the equation of a line We can see this by putting it in the standard linear format x2 w1w2x1b This line acts as a decision boundary in twodimensional space in which the output 0 is assigned to all inputsdecisionboundary lying on one side of the line and the output 1 to all input points lying on the other side of the line If we had more than 2 inputs the decision boundary becomes a hyperplane instead of a line but the idea is the same separating the space into two categories Fig 75 shows the possible logical inputs 00 01 10 and 11 and the line drawn by one possible set of parameters for an AND and an OR classifier Notice that there is simply no way to draw a line that separates the positive cases of XOR 01 and 10 from the negative cases 00 and 11 We say that XOR is not a linearly separablelinearlyseparable function Of course we could draw a boundary with a curve or some other function but not a single line 721 The solution neural networks While the XOR function cannot be calculated by a single perceptron it can be cal culated by a layered network of units Lets see an example of how to do this from Goodfellow et al 2016 that computes XOR using two layers of ReLUbased units Fig 76 shows a figure with the input being processed by two layers of neural units The middle layer called h has two units and the output layer called y has one unit A set of weights and biases are shown for each ReLU that correctly computes the XOR function Lets walk through what happens with the input x 0 0 If we multiply each input value by the appropriate weight sum and then add the bias b we get the vector 0 1 and we then apply the rectified linear transformation to give the output of the h layer as 0 0 Now we once again multiply by the weights sum and add the bias 0 in this case resulting in the value 0 The reader should work through the computation of the remaining 3 possible input pairs to see that the resulting y values are 1 for the inputs 0 1 and 1 0 and 0 for 0 0 and 1 1 6 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS 0 0 1 1 x1 x2 0 0 1 1 x1 x2 0 0 1 1 x1 x2 a x1 AND x2 b x1 OR x2 c x1 XOR x2 Figure 75 The functions AND OR and XOR represented with input x1 on the xaxis and input x2 on the y axis Filled circles represent perceptron outputs of 1 and white circles perceptron outputs of 0 There is no way to draw a line that correctly separates the two categories for XOR Figure styled after Russell and Norvig 2002 x1 x2 h1 h2 y1 1 1 11 1 1 2 01 1 0 Figure 76 XOR solution after Goodfellow et al 2016 There are three ReLU units in two layers weve called them h1 h2 h for hidden layer and y1 As before the numbers on the arrows represent the weights w for each unit and we represent the bias b as a weight on a unit clamped to 1 with the bias weightsunits in gray Its also instructive to look at the intermediate results the outputs of the two hidden nodes h0 and h1 We showed in the previous paragraph that the h vector for the inputs x 0 0 was 0 0 Fig 77b shows the values of the h layer for all 4 inputs Notice that hidden representations of the two input points x 0 1 and x 1 0 the two cases with XOR output 1 are merged to the single point h 1 0 The merger makes it easy to linearly separate the positive and negative cases of XOR In other words we can view the hidden layer of the network as forming a representation for the input In this example we just stipulated the weights in Fig 76 But for real examples the weights for neural networks are learned automatically using the error backprop agation algorithm to be introduced in Section 74 That means the hidden layers will learn to form useful representations This intuition that neural networks can auto matically learn useful representations of the input is one of their key advantages and one that we will return to again and again in later chapters Note that the solution to the XOR problem requires a network of units with non linear activation functions A network made up of simple linear perceptron units cannot solve the XOR problem This is because a network formed by many layers of purely linear units can always be reduced shown to be computationally identical 73 FEEDFORWARD NEURAL NETWORKS 7 0 0 1 1 x0 x1 a The original x space 0 0 1 1 h0 h1 2 b The new h space Figure 77 The hidden layer forming a new representation of the input Here is the rep resentation of the hidden layer h compared to the original input representation x Notice that the input point 0 1 has been collapsed with the input point 1 0 making it possible to linearly separate the positive and negative cases of XOR After Goodfellow et al 2016 to a single layer of linear units with appropriate weights and weve already shown visually in Fig 75 that a single unit cannot solve the XOR problem 73 FeedForward Neural Networks Lets now walk through a slightly more formal presentation of the simplest kind of neural network the feedforward network A feedforward network is a multilayerfeedforwardnetwork network in which the units are connected with no cycles the outputs from units in each layer are passed to units in the next higher layer and no outputs are passed back to lower layers In Chapter 9 well introduce networks with cycles called recurrent neural networks For historical reasons multilayer networks especially feedforward networks are sometimes called multilayer perceptrons or MLPs this is a technical misnomermultilayerperceptrons MLP since the units in modern multilayer networks arent perceptrons perceptrons are purely linear but modern networks are made up of units with nonlinearities like sigmoids but at some point the name stuck Simple feedforward networks have three kinds of nodes input units hidden units and output units Fig 78 shows a picture The input units are simply scalar values just as we saw in Fig 72 The core of the neural network is the hidden layer formed of hidden unitshidden layer each of which is a neural unit as described in Section 71 taking a weighted sum of its inputs and then applying a nonlinearity In the standard architecture each layer is fullyconnected meaning that each unit in each layer takes as input the outputsfullyconnected from all the units in the previous layer and there is a link between every pair of units from two adjacent layers Thus each hidden unit sums over all the input units Recall that a single hidden unit has parameters w the weight vector and b the bias scalar We represent the parameters for the entire hidden layer by combining the weight vector wi and bias bi for each unit i into a single weight matrix W and a single bias vector b for the whole layer see Fig 78 Each element Wi j of the weight matrix W represents the weight of the connection from the ith input unit xi to 8 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS x1 x2 h1 h2 y1 xn0 h3 hn1 1 b U W y2 yn2 Figure 78 A simple 2layer feedforward network with one hidden layer one output layer and one input layer the input layer is usually not counted when enumerating layers the jth hidden unit h j The advantage of using a single matrix W for the weights of the entire layer is that now the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations In fact the computation only has three steps multiplying the weight matrix by the input vector x adding the bias vector b and applying the activation function g such as the sigmoid tanh or ReLU activation function defined above The output of the hidden layer the vector h is thus the following using the sigmoid function σ h σWxb 78 Notice that were applying the σ function here to a vector while in Eq 73 it was applied to a scalar Were thus allowing σ and indeed any activation function g to apply to a vector elementwise so gz1z2z3 gz1gz2gz3 Lets introduce some constants to represent the dimensionalities of these vectors and matrices Well refer to the input layer as layer 0 of the network and have n0 represent the number of inputs so x is a vector of real numbers of dimension n0 or more formally x Rn0 Lets call the hidden layer layer 1 and the output layer layer 2 The hidden layer has dimensionality n1 so h Rn1 and also b Rn1 since each hidden unit can take a different bias value And the weight matrix W has dimensionality W Rn1n0 Take a moment to convince yourself that the matrix multiplication in Eq 78 will compute the value of each h j as σ nx i1 wi jxi b j As we saw in Section 72 the resulting value h for hidden but also for hypoth esis forms a representation of the input The role of the output layer is to take this new representation h and compute a final output This output could be a real valued number but in many cases the goal of the network is to make some sort of classification decision and so we will focus on the case of classification If we are doing a binary task like sentiment classification we might have a single output node and its value y is the probability of positive versus negative sentiment If we are doing multinomial classification such as assigning a partofspeech tag we might have one output node for each potential partofspeech whose output value is the probability of that partofspeech and the values of all the output nodes must sum to one The output layer thus gives a probability distribution across the output 73 FEEDFORWARD NEURAL NETWORKS 9 nodes Lets see how this happens Like the hidden layer the output layer has a weight matrix lets call it U but some models dont include a bias vector b in the output layer so well simplify by eliminating the bias vector in this example The weight matrix is multiplied by its input vector h to produce the intermediate output z z Uh There are n2 output nodes so z Rn2 weight matrix U has dimensionality U Rn2n1 and element Ui j is the weight from unit j in the hidden layer to unit i in the output layer However z cant be the output of the classifier since its a vector of realvalued numbers while what we need for classification is a vector of probabilities There is a convenient function for normalizing a vector of real values by which we meannormalizing converting it to a vector that encodes a probability distribution all the numbers lie between 0 and 1 and sum to 1 the softmax function that we saw on page ofsoftmax Chapter 5 For a vector z of dimensionality d the softmax is defined as softmaxzi ezid j1 e z j 1 i d 79 Thus for example given a vector z06 11 15 12 32 11 softmaxz is 0055 0090 00067 010 074 0010 You may recall that softmax was exactly what is used to create a probability distribution from a vector of realvalued numbers computed from summing weights times features in logistic regression in Chapter 5 That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input and then running standard logistic regression on the features that the network develops in h By contrast in Chapter 5 the features were mainly designed by hand via feature templates So a neural network is like logistic regression but a with many layers since a deep neural network is like layer after layer of logistic regression classifiers and b rather than forming the features by feature templates the prior layers of the network induce the feature representations themselves Here are the final equations for a feedforward network with a single hidden layer which takes an input vector x outputs a probability distribution y and is parameter ized by weight matrices W and U and a bias vector b h σWxb z Uh y softmaxz 710 Well call this network a 2layer network we traditionally dont count the input layer when numbering layers but do count the output layer So by this terminology logistic regression is a 1layer network Lets now set up some notation to make it easier to talk about deeper networks of depth more than 2 Well use superscripts in square brackets to mean layer num bers starting at 0 for the input layer So W 1 will mean the weight matrix for the first hidden layer and b1 will mean the bias vector for the first hidden layer n j will mean the number of units at layer j Well use g to stand for the activation function which will tend to be ReLU or tanh for intermediate layers and softmax for output layers Well use ai to mean the output from layer i and zi to mean the 10 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS combination of weights and biases W iai1bi The 0th layer is for inputs so the inputs x well refer to more generally as a0 Thus we can rerepresent our 2layer net from Eq 710 as follows z1 W 1a0b1 a1 g1z1 z2 W 2a1b2 a2 g2z2 ŷ a2 711 Note that with this notation the equations for the computation done at each layer are the same The algorithm for computing the forward step in an nlayer feedforward network given the input vector a0 is thus simply for i in 1n zi W i ai1 bi ai gizi ŷ an The activation functions g are generally different at the final layer Thus g2 might be softmax for multinomial classification or sigmoid for binary classification while ReLU or tanh might be the activation function g at the internal layers 74 Training Neural Nets A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x What the system produces via Eq 711 is ŷ the systems estimate of the true y The goal of the training procedure is to learn parameters W i and bi for each layer i that make ŷ for each training observation as close as possible to the true y In general we do all this by drawing on the methods we introduced in Chapter 5 for logistic regression so the reader should be comfortable with that chapter before proceeding First well need a loss function that models the distance between the system output and the gold output and its common to use the loss function used for logistic regression the crossentropy loss Second to find the parameters that minimize this loss function well use the gradient descent optimization algorithm introduced in Chapter 5 Third gradient descent requires knowing the gradient of the loss function the vector that contains the partial derivative of the loss function with respect to each of the parameters Here is one part where learning for neural networks is more complex than for logistic logistic regression In logistic regression for each observation we could directly compute the derivative of the loss function with respect to an individ ual w or b But for neural networks with millions of parameters in many layers its much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer How do we partial out the loss over all those intermediate layers The answer is the algorithm called error backpropagation or reverse differen tiation 74 TRAINING NEURAL NETS 11 741 Loss function The crossentropy loss that is used in neural networks is the same one we saw forcrossentropyloss logistic regression In fact if the neural network is being used as a binary classifier with the sig moid at the final layer the loss function is exactly the same as we saw with logistic regression in Eq LCEŷy log pyx y log ŷ1 y log1 ŷ 712 What about if the neural network is being used as a multinomial classifier Let y be a vector over the C classes representing the true output probability distribution The crossentropy loss here is LCEŷy C i1 yi log ŷi 713 We can simplify this equation further Assume this is a hard classification task meaning that only one class is the correct one and that there is one output unit in y for each class If the true class is i then y is a vector where yi 1 and y j 0 j 6 i A vector like this with one value1 and the rest 0 is called a onehot vector Now let ŷ be the vector output from the network The sum in Eq 713 will be 0 except for the true class Hence the crossentropy loss is simply the log probability of the correct class and we therefore also call this the negative log likelihood lossnegative loglikelihood loss LCEŷy log ŷi 714 Plugging in the softmax formula from Eq 79 and with K the number of classes LCEŷy log eziK j1 e z j 715 742 Computing the Gradient How do we compute the gradient of this loss function Computing the gradient requires the partial derivative of the loss function with respect to each parameter For a network with one weight layer and sigmoid output which is what logistic regression is we could simply use the derivative of the loss that we used for logistic regression in Eq 716 and derived in Section LCEwb w j ŷ y x j σw xb y x j 716 Or for a network with one hidden layer and softmax output we could use the deriva tive of the softmax loss from Eq LCE wk 1y k py kxxk 1y k e wkxbkK j1 e w j xb j xk 717 12 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS But these derivatives only give correct updates for one weight layer the last one For deep networks computing the gradients for each weight is much more complex since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network even though the loss is computed only at the very end of the network The solution to computing this gradient is an algorithm called error backprop agation or backprop Rumelhart et al 1986 While backprop was invented speerror backpropagation cially for neural networks it turns out to be the same as a more general procedure called backward differentiation which depends on the notion of computation graphs Lets see how that works in the next subsection 743 Computation Graphs A computation graph is a representation of the process of computing a mathematical expression in which the computation is broken down into separate operations each of which is modeled as a node in a graph Consider computing the function Labc ca2b If we make each of the component addition and multiplication operations explicit and add names d and e for the intermediate outputs the resulting series of computations is d 2b e ad L c e We can now represent this as a graph with nodes for each operation and di rected edges showing the outputs from each operation as the inputs to the next as in Fig 79 The simplest use of computation graphs is to compute the value of the function with some given inputs In the figure weve assumed the inputs a 3 b 1 c 2 and weve shown the result of the forward pass to compute the re sult L312 10 In the forward pass of a computation graph we apply each operation left to right passing the outputs of each computation as the input to the next node eda d 2b Lce 3 1 2 e5 d2 L10 forward pass a b c Figure 79 Computation graph for the function Labc ca2b with values for input nodes a 3 b 1 c 2 showing the forward pass computation of L 744 Backward differentiation on computation graphs The importance of the computation graph comes from the backward pass which is used to compute the derivatives that well need for the weight update In this example our goal is to compute the derivative of the output function L with respect 74 TRAINING NEURAL NETS 13 to each of the input variables ie La L b and L c The derivative L a tells us how much a small change in a affects L Backwards differentiation makes use of the chain rule in calculus Suppose wechain rule are computing the derivative of a composite function f x uvx The derivative of f x is the derivative of ux with respect to vx times the derivative of vx with respect to x d f dx du dv dv dx 718 The chain rule extends to more than two functions If computing the derivative of a composite function f x uvwx the derivative of f x is d f dx du dv dv dw dw dx 719 Lets now compute the 3 derivatives we need Since in the computation graph L ce we can directly compute the derivative Lc L c e 720 For the other two well need to use the chain rule L a L e e a L b L e e d d b 721 Eq 721 thus requires five intermediate derivatives Le L c e a e d and d b which are as follows making use of the fact that the derivative of a sum is the sum of the derivatives L ce L e c L c e e ad e a 1 e d 1 d 2b d b 2 In the backward pass we compute each of these partials along each edge of the graph from right to left multiplying the necessary partials to result in the final derivative we need Thus we begin by annotating the final node with LL 1 Moving to the left we then compute Lc and L e and so on until we have annotated the graph all the way to the input variables The forward pass conveniently already will have computed the values of the forward intermediate variables we need like d and e to compute these derivatives Fig 710 shows the backward pass At each node we need to compute the local partial derivative with respect to the parent multiply it by the partial derivative that is being passed down from the parent and then pass it to the child Backward differentiation for a neural network Of course computation graphs for real neural networks are much more complex Fig 711 shows a sample computation graph for a 2layer neural network with n0 14 CHAPTER 7 NEURAL NETWORKS AND NEURAL LANGUAGE MODELS eda d 2b Lce a3 b1 e5 d2 L10 L1L L4b L2d a b c L2a L5c L 2eL2e e 1d L 5c d 2b e 1a backward pass c2 Figure 710 Computation graph for the function Labc ca2b showing the back ward pass computation of La L b and L c 2 n1 2 and n2 1 assuming binary classification and hence using a sigmoid output unit for simplicity The function that the computation graph is computing is z1 W 1xb1 a1 ReLUz1 z2 W 2a1b2 a2 σz2 ŷ a2 722 z2 a 2 σ a1 ReLU z1 b1 x1 x2 a1 ReLU z1 b1 w211 w111 w121 w112 w1 22 b2 w221 L a2y Figure 711 Sample computation graph for a simple 2layer neural net 1 hidden layer with two input dimensions and 2 hidden dimensions The weights that need updating those for which we need to know the partial derivative of the loss function are shown in orange In order to do the backward pass well need to know the derivatives of all the functions in the graph We already saw in Section the derivative of the sigmoid σ dσz dz σz1σz 723 
