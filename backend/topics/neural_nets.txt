Speech and Language Processing . Daniel Jurafsky & James H . Martin . Copyright c © 2019 . All rights reserved . Draft of October 2 , 2019 . CHAPTER 7 Neural Networks and NeuralLanguage Models “ [ M ] achines of this character can behave in a very complicated manner when the number of units is large . ” Alan Turing ( 1948 ) “ Intelligent Machines ” , page 6 Neural networks are a fundamental computational tool for language process - ing , and a very old one . They are called neural because their origins lie in the McCulloch-Pitts neuron ( McCulloch and Pitts , 1943 ) , a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic . But the modern use in language processing no longer draws on these early biological inspirations . Instead , a modern neural network is a network of small computing units , each of which takes a vector of input values and produces a single output value . In this chapter we introduce the neural net applied to classification . The architecture we introduce is called a feedforward network because the computation proceeds iter-feedforward atively from one layer of units to the next . The use of modern neural nets is often called deep learning , because modern networks are often deep ( have many layers ) . deep learning Neural networks share much of the same mathematics as logistic regression . But neural networks are a more powerful classifier than logistic regression , and indeed a minimal neural network ( technically one with a single ‘ hidden layer ’ ) can be shown to learn any function . Neural net classifiers are different from logistic regression in another way . With logistic regression , we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge . When working with neural networks , it is more common to avoid most uses of rich hand - derived features , instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify . We saw examples of this kind of representation learning for embeddings in Chapter 6 . Nets that are very deep are particularly good at representation learning . For that reason deep neural nets are the right tool for large scale problems that offer sufficient data to learn features automatically . In this chapter we’ll introduce feedforward networks as classifiers , and also ap - ply them to the simple task of language modeling : assigning probabilities to word sequences and predicting upcoming words . In subsequent chapters we’ll introduce many other aspects of neural models , such as recurrent neural networks ( Chap - ter 9 ) , encoder-decoder models , attention and the Transformer ( Chapter 10 ) . 2 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS 7.1 Units The building block of a neural network is a single computational unit . A unit takes a set of real valued numbers as input , performs some computation on them , and produces an output . At its heart , a neural unit is taking a weighted sum of its inputs , with one addi - tional term in the sum called a bias term . Given a set of inputs x1 . . . xn , a unit hasbias term a set of corresponding weights w1 . . . wn and a bias b , so the weighted sum z can be represented as : z = b + ∑ i wixi ( 7.1 ) Often it’s more convenient to express this weighted sum using vector notation ; recall from linear algebra that a vector is , at heart , just a list or array of numbers . Thusvector we’ll talk about z in terms of a weight vector w , a scalar bias b , and an input vector x , and we’ll replace the sum with the convenient dot product : z = w · x + b ( 7.2 ) As defined in Eq . 7.2 , z is just a real valued number . Finally , instead of using z , a linear function of x , as the output , neural units apply a non-linear function f to z . We will refer to the output of this function as the activation value for the unit , a . Since we are just modeling a single unit , theactivation activation for the node is in fact the final output of the network , which we’ll generally call y . So the value y is defined as : y = a = f ( z ) We’ll discuss three popular non-linear functions f ( ) below ( the sigmoid , the tanh , and the rectified linear ReLU ) but it’s pedagogically convenient to start with the sigmoid function since we saw it in Chapter 5 : sigmoid y = σ ( z ) = 1 1 + e − z ( 7.3 ) The sigmoid ( shown in Fig . 7.1 ) has a number of advantages ; it maps the output into the range [ 0,1 ] , which is useful in squashing outliers toward 0 or 1 . And it’s differentiable , which as we saw in Section ? ? will be handy for learning . Figure 7.1 The sigmoid function takes a real value and maps it to the range [ 0,1 ] . It is nearly linear around 0 but outlier values get squashed toward 0 or 1 . 7.1 • UNITS 3 Substituting Eq . 7.2 into Eq . 7.3 gives us the output of a neural unit : y = σ ( w · x + b ) = 1 1 + exp ( − ( w · x + b ) ) ( 7.4 ) Fig . 7.2 shows a final schematic of a basic neural unit . In this example the unit takes 3 input values x1 , x2 , and x3 , and computes a weighted sum , multiplying each value by a weight ( w1 , w2 , and w3 , respectively ) , adds them to a bias term b , and then passes the resulting sum through a sigmoid function to result in a number between 0 and 1 . x1 x2 x3 y w1 w2 w3 ∑ b σ + 1 z a Figure 7.2 A neural unit , taking 3 inputs x1 , x2 , and x3 ( and a bias b that we represent as a weight for an input clamped at + 1 ) and producing an output y . We include some convenient intermediate variables : the output of the summation , z , and the output of the sigmoid , a . In this case the output of the unit y is the same as a , but in deeper networks we’ll reserve y to mean the final output of the entire network , leaving a as the activation of an individual node . Let’s walk through an example just to get an intuition . Let’s suppose we have a unit with the following weight vector and bias : w = [ 0.2,0.3,0.9 ] b = 0.5 What would this unit do with the following input vector : x = [ 0.5,0.6,0.1 ] The resulting output y would be : y = σ ( w · x + b ) = 1 1 + e − ( w·x + b ) = 1 1 + e − ( . 5 ∗ . 2 + . 6 ∗ . 3 + . 1 ∗ . 9 + . 5 ) = e − 0.87 = . 70 In practice , the sigmoid is not commonly used as an activation function . A function that is very similar but almost always better is the tanh function shown in Fig . 7.3a ; tanh tanh is a variant of the sigmoid that ranges from - 1 to + 1 : y = ez − e − z ez + e − z ( 7.5 ) The simplest activation function , and perhaps the most commonly used , is the rec - tified linear unit , also called the ReLU , shown in Fig . 7.3b . It’s just the same as xReLU when x is positive , and 0 otherwise : y = max ( x , 0 ) ( 7.6 ) 4 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS ( a ) ( b ) Figure 7.3 The tanh and ReLU activation functions . These activation functions have different properties that make them useful for differ - ent language applications or network architectures . For example the rectifier func - tion has nice properties that result from it being very close to linear . In the sigmoid or tanh functions , very high values of z result in values of y that are saturated , i.e . , saturated extremely close to 1 , which causes problems for learning . Rectifiers don’t have this problem , since the output of values close to 1 also approaches 1 in a nice gentle linear way . By contrast , the tanh function has the nice properties of being smoothly differentiable and mapping outlier values toward the mean . 7.2 The XOR problem Early in the history of neural networks it was realized that the power of neural net - works , as with the real neurons that inspired them , comes from combining these units into larger networks . One of the most clever demonstrations of the need for multi-layer networks was the proof by Minsky and Papert ( 1969 ) that a single neural unit cannot compute some very simple functions of its input . Consider the task of computing elementary logical functions of two inputs , like AND , OR , and XOR . As a reminder , here are the truth tables for those functions : AND OR XOR x1 x2 y x1 x2 y x1 x2 y 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 This example was first shown for the perceptron , which is a very simple neuralperceptron unit that has a binary output and does not have a non-linear activation function . The output y of a perceptron is 0 or 1 , and is computed as follows ( using the same weight w , input x , and bias b as in Eq . 7.2 ) : y = { 0 , if w · x + b ≤ 0 1 , if w · x + b > 0 ( 7.7 ) 7.2 • THE XOR PROBLEM 5 It’s very easy to build a perceptron that can compute the logical AND and OR functions of its binary inputs ; Fig . 7.4 shows the necessary weights . x1 x2 + 1 - 1 1 1 x1 x2 + 1 0 1 1 ( a ) ( b ) Figure 7.4 The weights w and bias b for perceptrons for computing logical functions . The inputs are shown as x1 and x2 and the bias as a special node with value + 1 which is multiplied with the bias weight b . ( a ) logical AND , showing weights w1 = 1 and w2 = 1 and bias weight b = − 1 . ( b ) logical OR , showing weights w1 = 1 and w2 = 1 and bias weight b = 0 . These weights / biases are just one from an infinite number of possible sets of weights and biases that would implement the functions . It turns out , however , that it’s not possible to build a perceptron to compute logical XOR ! ( It’s worth spending a moment to give it a try ! ) The intuition behind this important result relies on understanding that a percep - tron is a linear classifier . For a two-dimensional input x1 and x2 , the perception equation , w1x1 + w2x2 + b = 0 is the equation of a line . ( We can see this by putting it in the standard linear format : x2 = − ( w1 / w2 ) x1 − b . ) This line acts as a decision boundary in two-dimensional space in which the output 0 is assigned to all inputsdecisionboundary lying on one side of the line , and the output 1 to all input points lying on the other side of the line . If we had more than 2 inputs , the decision boundary becomes a hyperplane instead of a line , but the idea is the same , separating the space into two categories . Fig . 7.5 shows the possible logical inputs ( 00 , 01 , 10 , and 11 ) and the line drawn by one possible set of parameters for an AND and an OR classifier . Notice that there is simply no way to draw a line that separates the positive cases of XOR ( 01 and 10 ) from the negative cases ( 00 and 11 ) . We say that XOR is not a linearly separablelinearlyseparable function . Of course we could draw a boundary with a curve , or some other function , but not a single line . 7.2.1 The solution : neural networks While the XOR function cannot be calculated by a single perceptron , it can be cal - culated by a layered network of units . Let’s see an example of how to do this from Goodfellow et al . ( 2016 ) that computes XOR using two layers of ReLU-based units . Fig . 7.6 shows a figure with the input being processed by two layers of neural units . The middle layer ( called h ) has two units , and the output layer ( called y ) has one unit . A set of weights and biases are shown for each ReLU that correctly computes the XOR function . Let’s walk through what happens with the input x = [ 0 0 ] . If we multiply each input value by the appropriate weight , sum , and then add the bias b , we get the vector [ 0 - 1 ] , and we then apply the rectified linear transformation to give the output of the h layer as [ 0 0 ] . Now we once again multiply by the weights , sum , and add the bias ( 0 in this case ) resulting in the value 0 . The reader should work through the computation of the remaining 3 possible input pairs to see that the resulting y values are 1 for the inputs [ 0 1 ] and [ 1 0 ] and 0 for [ 0 0 ] and [ 1 1 ] . 6 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS 0 0 1 1 x1 x2 0 0 1 1 x1 x2 0 0 1 1 x1 x2 a ) x1 AND x2 b ) x1 OR x2 c ) x1 XOR x2 ? Figure 7.5 The functions AND , OR , and XOR , represented with input x1 on the x-axis and input x2 on the y axis . Filled circles represent perceptron outputs of 1 , and white circles perceptron outputs of 0 . There is no way to draw a line that correctly separates the two categories for XOR . Figure styled after Russell and Norvig ( 2002 ) . x1 x2 h1 h2 y1 + 1 1 - 11 1 1 - 2 01 + 1 0 Figure 7.6 XOR solution after Goodfellow et al . ( 2016 ) . There are three ReLU units , in two layers ; we’ve called them h1 , h2 ( h for “ hidden layer ” ) and y1 . As before , the numbers on the arrows represent the weights w for each unit , and we represent the bias b as a weight on a unit clamped to + 1 , with the bias weights / units in gray . It’s also instructive to look at the intermediate results , the outputs of the two hidden nodes h0 and h1 . We showed in the previous paragraph that the h vector for the inputs x = [ 0 0 ] was [ 0 0 ] . Fig . 7.7b shows the values of the h layer for all 4 inputs . Notice that hidden representations of the two input points x = [ 0 1 ] and x = [ 1 0 ] ( the two cases with XOR output = 1 ) are merged to the single point h = [ 1 0 ] . The merger makes it easy to linearly separate the positive and negative cases of XOR . In other words , we can view the hidden layer of the network as forming a representation for the input . In this example we just stipulated the weights in Fig . 7.6 . But for real examples the weights for neural networks are learned automatically using the error backprop - agation algorithm to be introduced in Section 7.4 . That means the hidden layers will learn to form useful representations . This intuition , that neural networks can auto - matically learn useful representations of the input , is one of their key advantages , and one that we will return to again and again in later chapters . Note that the solution to the XOR problem requires a network of units with non - linear activation functions . A network made up of simple linear ( perceptron ) units cannot solve the XOR problem . This is because a network formed by many layers of purely linear units can always be reduced ( shown to be computationally identical 7.3 • FEED-FORWARD NEURAL NETWORKS 7 0 0 1 1 x0 x1 a ) The original x space 0 0 1 1 h0 h1 2 b ) The new h space Figure 7.7 The hidden layer forming a new representation of the input . Here is the rep - resentation of the hidden layer , h , compared to the original input representation x . Notice that the input point [ 0 1 ] has been collapsed with the input point [ 1 0 ] , making it possible to linearly separate the positive and negative cases of XOR . After Goodfellow et al . ( 2016 ) . to ) a single layer of linear units with appropriate weights , and we’ve already shown ( visually , in Fig . 7.5 ) that a single unit cannot solve the XOR problem . 7.3 Feed-Forward Neural Networks Let’s now walk through a slightly more formal presentation of the simplest kind of neural network , the feedforward network . A feedforward network is a multilayerfeedforwardnetwork network in which the units are connected with no cycles ; the outputs from units in each layer are passed to units in the next higher layer , and no outputs are passed back to lower layers . ( In Chapter 9 we’ll introduce networks with cycles , called recurrent neural networks . ) For historical reasons multilayer networks , especially feedforward networks , are sometimes called multi-layer perceptrons ( or MLPs ) ; this is a technical misnomer , multi-layerperceptrons MLP since the units in modern multilayer networks aren’t perceptrons ( perceptrons are purely linear , but modern networks are made up of units with non-linearities like sigmoids ) , but at some point the name stuck . Simple feedforward networks have three kinds of nodes : input units , hidden units , and output units . Fig . 7.8 shows a picture . The input units are simply scalar values just as we saw in Fig . 7.2 . The core of the neural network is the hidden layer formed of hidden units , hidden layer each of which is a neural unit as described in Section 7.1 , taking a weighted sum of its inputs and then applying a non-linearity . In the standard architecture , each layer is fully-connected , meaning that each unit in each layer takes as input the outputsfully-connected from all the units in the previous layer , and there is a link between every pair of units from two adjacent layers . Thus each hidden unit sums over all the input units . Recall that a single hidden unit has parameters w ( the weight vector ) and b ( the bias scalar ) . We represent the parameters for the entire hidden layer by combining the weight vector wi and bias bi for each unit i into a single weight matrix W and a single bias vector b for the whole layer ( see Fig . 7.8 ) . Each element Wi j of the weight matrix W represents the weight of the connection from the ith input unit xi to 8 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS x1 x2 h1 h2 y1 xn0 … h3 hn1 … + 1 b … U W y2 yn2 Figure 7.8 A simple 2-layer feedforward network , with one hidden layer , one output layer , and one input layer ( the input layer is usually not counted when enumerating layers ) . the jth hidden unit h j . The advantage of using a single matrix W for the weights of the entire layer is that now the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations . In fact , the computation only has three steps : multiplying the weight matrix by the input vector x , adding the bias vector b , and applying the activation function g ( such as the sigmoid , tanh , or ReLU activation function defined above ) . The output of the hidden layer , the vector h , is thus the following , using the sigmoid function σ : h = σ ( Wx + b ) ( 7.8 ) Notice that we’re applying the σ function here to a vector , while in Eq . 7.3 it was applied to a scalar . We’re thus allowing σ ( · ) , and indeed any activation function g ( · ) , to apply to a vector element-wise , so g [ z1 , z2 , z3 ] = [ g ( z1 ) , g ( z2 ) , g ( z3 ) ] . Let’s introduce some constants to represent the dimensionalities of these vectors and matrices . We’ll refer to the input layer as layer 0 of the network , and have n0 represent the number of inputs , so x is a vector of real numbers of dimension n0 , or more formally x ∈ Rn0 . Let’s call the hidden layer layer 1 and the output layer layer 2 . The hidden layer has dimensionality n1 , so h ∈ Rn1 and also b ∈ Rn1 ( since each hidden unit can take a different bias value ) . And the weight matrix W has dimensionality W ∈ Rn1 × n0 . Take a moment to convince yourself that the matrix multiplication in Eq . 7.8 will compute the value of each h j as σ ( ∑ nx i = 1 wi jxi + b j ) . As we saw in Section 7.2 , the resulting value h ( for hidden but also for hypoth - esis ) forms a representation of the input . The role of the output layer is to take this new representation h and compute a final output . This output could be a real - valued number , but in many cases the goal of the network is to make some sort of classification decision , and so we will focus on the case of classification . If we are doing a binary task like sentiment classification , we might have a single output node , and its value y is the probability of positive versus negative sentiment . If we are doing multinomial classification , such as assigning a part-of-speech tag , we might have one output node for each potential part-of-speech , whose output value is the probability of that part-of-speech , and the values of all the output nodes must sum to one . The output layer thus gives a probability distribution across the output 7.3 • FEED-FORWARD NEURAL NETWORKS 9 nodes . Let’s see how this happens . Like the hidden layer , the output layer has a weight matrix ( let’s call it U ) , but some models don’t include a bias vector b in the output layer , so we’ll simplify by eliminating the bias vector in this example . The weight matrix is multiplied by its input vector ( h ) to produce the intermediate output z . z = Uh There are n2 output nodes , so z ∈ Rn2 , weight matrix U has dimensionality U ∈ Rn2 × n1 , and element Ui j is the weight from unit j in the hidden layer to unit i in the output layer . However , z can’t be the output of the classifier , since it’s a vector of real-valued numbers , while what we need for classification is a vector of probabilities . There is a convenient function for normalizing a vector of real values , by which we meannormalizing converting it to a vector that encodes a probability distribution ( all the numbers lie between 0 and 1 and sum to 1 ) : the softmax function that we saw on page ? ? ofsoftmax Chapter 5 . For a vector z of dimensionality d , the softmax is defined as : softmax ( zi ) = ezi ∑ d j = 1 e z j 1 ≤ i ≤ d ( 7.9 ) Thus for example given a vector z =[ 0.6 1.1 - 1.5 1.2 3.2 - 1.1 ] , softmax ( z ) is [ 0.055 0.090 0.0067 0.10 0.74 0.010 ] . You may recall that softmax was exactly what is used to create a probability distribution from a vector of real-valued numbers ( computed from summing weights times features ) in logistic regression in Chapter 5 . That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input , and then running standard logistic regression on the features that the network develops in h . By contrast , in Chapter 5 the features were mainly designed by hand via feature templates . So a neural network is like logistic regression , but ( a ) with many layers , since a deep neural network is like layer after layer of logistic regression classifiers , and ( b ) rather than forming the features by feature templates , the prior layers of the network induce the feature representations themselves . Here are the final equations for a feedforward network with a single hidden layer , which takes an input vector x , outputs a probability distribution y , and is parameter - ized by weight matrices W and U and a bias vector b : h = σ ( Wx + b ) z = Uh y = softmax ( z ) ( 7.10 ) We’ll call this network a 2-layer network ( we traditionally don’t count the input layer when numbering layers , but do count the output layer ) . So by this terminology logistic regression is a 1-layer network . Let’s now set up some notation to make it easier to talk about deeper networks of depth more than 2 . We’ll use superscripts in square brackets to mean layer num - bers , starting at 0 for the input layer . So W [ 1 ] will mean the weight matrix for the ( first ) hidden layer , and b [ 1 ] will mean the bias vector for the ( first ) hidden layer . n j will mean the number of units at layer j . We’ll use g ( · ) to stand for the activation function , which will tend to be ReLU or tanh for intermediate layers and softmax for output layers . We’ll use a [ i ] to mean the output from layer i , and z [ i ] to mean the 10 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS combination of weights and biases W [ i ] a [ i − 1 ] + b [ i ] . The 0th layer is for inputs , so the inputs x we’ll refer to more generally as a [ 0 ] . Thus we can re-represent our 2-layer net from Eq . 7.10 as follows : z [ 1 ] = W [ 1 ] a [ 0 ] + b [ 1 ] a [ 1 ] = g [ 1 ] ( z [ 1 ] ) z [ 2 ] = W [ 2 ] a [ 1 ] + b [ 2 ] a [ 2 ] = g [ 2 ] ( z [ 2 ] ) ŷ = a [ 2 ] ( 7.11 ) Note that with this notation , the equations for the computation done at each layer are the same . The algorithm for computing the forward step in an n-layer feedforward network , given the input vector a [ 0 ] is thus simply : for i in 1 . . n z [ i ] = W [ i ] a [ i − 1 ] + b [ i ] a [ i ] = g [ i ] ( z [ i ] ) ŷ = a [ n ] The activation functions g ( · ) are generally different at the final layer . Thus g [ 2 ] might be softmax for multinomial classification or sigmoid for binary classification , while ReLU or tanh might be the activation function g ( · ) at the internal layers . 7.4 Training Neural Nets A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x . What the system produces , via Eq . 7.11 , is ŷ , the system’s estimate of the true y . The goal of the training procedure is to learn parameters W [ i ] and b [ i ] for each layer i that make ŷ for each training observation as close as possible to the true y . In general , we do all this by drawing on the methods we introduced in Chapter 5 for logistic regression , so the reader should be comfortable with that chapter before proceeding . First , we’ll need a loss function that models the distance between the system output and the gold output , and it’s common to use the loss function used for logistic regression , the cross-entropy loss . Second , to find the parameters that minimize this loss function , we’ll use the gradient descent optimization algorithm introduced in Chapter 5 . Third , gradient descent requires knowing the gradient of the loss function , the vector that contains the partial derivative of the loss function with respect to each of the parameters . Here is one part where learning for neural networks is more complex than for logistic logistic regression . In logistic regression , for each observation we could directly compute the derivative of the loss function with respect to an individ - ual w or b . But for neural networks , with millions of parameters in many layers , it’s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer . How do we partial out the loss over all those intermediate layers ? The answer is the algorithm called error backpropagation or reverse differen - tiation . 7.4 • TRAINING NEURAL NETS 11 7.4.1 Loss function The cross-entropy loss that is used in neural networks is the same one we saw forcross-entropyloss logistic regression . In fact , if the neural network is being used as a binary classifier , with the sig - moid at the final layer , the loss function is exactly the same as we saw with logistic regression in Eq . ? ? : LCE ( ŷ , y ) = − log p ( y | x ) = − [ y log ŷ + ( 1 − y ) log ( 1 − ŷ ) ] ( 7.12 ) What about if the neural network is being used as a multinomial classifier ? Let y be a vector over the C classes representing the true output probability distribution . The cross-entropy loss here is LCE ( ŷ , y ) = − C ∑ i = 1 yi log ŷi ( 7.13 ) We can simplify this equation further . Assume this is a hard classification task , meaning that only one class is the correct one , and that there is one output unit in y for each class . If the true class is i , then y is a vector where yi = 1 and y j = 0 ∀ j 6 = i . A vector like this , with one value = 1 and the rest 0 , is called a one-hot vector . Now let ŷ be the vector output from the network . The sum in Eq . 7.13 will be 0 except for the true class . Hence the cross-entropy loss is simply the log probability of the correct class , and we therefore also call this the negative log likelihood loss : negative loglikelihood loss LCE ( ŷ , y ) = − log ŷi ( 7.14 ) Plugging in the softmax formula from Eq . 7.9 , and with K the number of classes : LCE ( ŷ , y ) = − log ezi ∑ K j = 1 e z j ( 7.15 ) 7.4.2 Computing the Gradient How do we compute the gradient of this loss function ? Computing the gradient requires the partial derivative of the loss function with respect to each parameter . For a network with one weight layer and sigmoid output ( which is what logistic regression is ) , we could simply use the derivative of the loss that we used for logistic regression in Eq . 7.16 ( and derived in Section ? ? ) : ∂ LCE ( w , b ) ∂ w j = ( ŷ − y ) x j = ( σ ( w · x + b ) − y ) x j ( 7.16 ) Or for a network with one hidden layer and softmax output , we could use the deriva - tive of the softmax loss from Eq . ? ? : ∂ LCE ∂ wk = ( 1 { y = k } − p ( y = k | x ) ) xk = ( 1 { y = k } − e wk·x + bk ∑ K j = 1 e w j · x + b j ) xk ( 7.17 ) 12 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS But these derivatives only give correct updates for one weight layer : the last one ! For deep networks , computing the gradients for each weight is much more complex , since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network , even though the loss is computed only at the very end of the network . The solution to computing this gradient is an algorithm called error backprop - agation or backprop ( Rumelhart et al . , 1986 ) . While backprop was invented spe-error back-propagation cially for neural networks , it turns out to be the same as a more general procedure called backward differentiation , which depends on the notion of computation graphs . Let’s see how that works in the next subsection . 7.4.3 Computation Graphs A computation graph is a representation of the process of computing a mathematical expression , in which the computation is broken down into separate operations , each of which is modeled as a node in a graph . Consider computing the function L ( a , b , c ) = c ( a + 2b ) . If we make each of the component addition and multiplication operations explicit , and add names ( d and e ) for the intermediate outputs , the resulting series of computations is : d = 2 ∗ b e = a + d L = c ∗ e We can now represent this as a graph , with nodes for each operation , and di - rected edges showing the outputs from each operation as the inputs to the next , as in Fig . 7.9 . The simplest use of computation graphs is to compute the value of the function with some given inputs . In the figure , we’ve assumed the inputs a = 3 , b = 1 , c = − 2 , and we’ve shown the result of the forward pass to compute the re - sult L ( 3,1 , − 2 ) = 10 . In the forward pass of a computation graph , we apply each operation left to right , passing the outputs of each computation as the input to the next node . e = d + a d = 2b L = ce 3 1 - 2 e = 5 d = 2 L = - 10 forward pass a b c Figure 7.9 Computation graph for the function L ( a , b , c ) = c ( a + 2b ) , with values for input nodes a = 3 , b = 1 , c = − 2 , showing the forward pass computation of L . 7.4.4 Backward differentiation on computation graphs The importance of the computation graph comes from the backward pass , which is used to compute the derivatives that we’ll need for the weight update . In this example our goal is to compute the derivative of the output function L with respect 7.4 • TRAINING NEURAL NETS 13 to each of the input variables , i.e . , ∂ L ∂ a , ∂ L ∂ b , and ∂ L ∂ c . The derivative ∂ L ∂ a , tells us how much a small change in a affects L . Backwards differentiation makes use of the chain rule in calculus . Suppose wechain rule are computing the derivative of a composite function f ( x ) = u ( v ( x ) ) . The derivative of f ( x ) is the derivative of u ( x ) with respect to v ( x ) times the derivative of v ( x ) with respect to x : d f dx = du dv · dv dx ( 7.18 ) The chain rule extends to more than two functions . If computing the derivative of a composite function f ( x ) = u ( v ( w ( x ) ) ) , the derivative of f ( x ) is : d f dx = du dv · dv dw · dw dx ( 7.19 ) Let’s now compute the 3 derivatives we need . Since in the computation graph L = ce , we can directly compute the derivative ∂ L ∂ c : ∂ L ∂ c = e ( 7.20 ) For the other two , we’ll need to use the chain rule : ∂ L ∂ a = ∂ L ∂ e ∂ e ∂ a ∂ L ∂ b = ∂ L ∂ e ∂ e ∂ d ∂ d ∂ b ( 7.21 ) Eq . 7.21 thus requires five intermediate derivatives : ∂ L ∂ e , ∂ L ∂ c , ∂ e ∂ a , ∂ e ∂ d , and ∂ d ∂ b , which are as follows ( making use of the fact that the derivative of a sum is the sum of the derivatives ) : L = ce : ∂ L ∂ e = c , ∂ L ∂ c = e e = a + d : ∂ e ∂ a = 1 , ∂ e ∂ d = 1 d = 2b : ∂ d ∂ b = 2 In the backward pass , we compute each of these partials along each edge of the graph from right to left , multiplying the necessary partials to result in the final derivative we need . Thus we begin by annotating the final node with ∂ L ∂ L = 1 . Moving to the left , we then compute ∂ L ∂ c and ∂ L ∂ e , and so on , until we have annotated the graph all the way to the input variables . The forward pass conveniently already will have computed the values of the forward intermediate variables we need ( like d and e ) to compute these derivatives . Fig . 7.10 shows the backward pass . At each node we need to compute the local partial derivative with respect to the parent , multiply it by the partial derivative that is being passed down from the parent , and then pass it to the child . Backward differentiation for a neural network Of course computation graphs for real neural networks are much more complex . Fig . 7.11 shows a sample computation graph for a 2-layer neural network with n0 = 14 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS e = d + a d = 2b L = ce a = 3 b = 1 e = 5 d = 2 L = - 10 ∂ L = 1 ∂ L ∂ L = - 4 ∂ b ∂ L = - 2 ∂ d a b c ∂ L = - 2 ∂ a ∂ L = 5 ∂ c ∂ L = - 2 ∂ e ∂ L = - 2 ∂ e ∂ e = 1 ∂ d ∂ L = 5 ∂ c ∂ d = 2 ∂ b ∂ e = 1 ∂ a backward pass c = - 2 Figure 7.10 Computation graph for the function L ( a , b , c ) = c ( a + 2b ) , showing the back - ward pass computation of ∂ L ∂ a , ∂ L ∂ b , and ∂ L ∂ c . 2 , n1 = 2 , and n2 = 1 , assuming binary classification and hence using a sigmoid output unit for simplicity . The function that the computation graph is computing is : z [ 1 ] = W [ 1 ] x + b [ 1 ] a [ 1 ] = ReLU ( z [ 1 ] ) z [ 2 ] = W [ 2 ] a [ 1 ] + b [ 2 ] a [ 2 ] = σ ( z [ 2 ] ) ŷ = a [ 2 ] ( 7.22 ) z [ 2 ] = + a [ 2 ] = σ a [ 1 ] = ReLU z [ 1 ] = + b [ 1 ] * * * * x1 x2 a [ 1 ] = ReLU z [ 1 ] = + b [ 1 ] * * w [ 2 ] 11 w [ 1 ] 11 w [ 1 ] 21 w [ 1 ] 12 w [ 1 ] 22 b [ 2 ] w [ 2 ] 21 L ( a [ 2 ] , y ) Figure 7.11 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer ) with two input dimensions and 2 hidden dimensions . The weights that need updating ( those for which we need to know the partial derivative of the loss function ) are shown in orange . In order to do the backward pass , we’ll need to know the derivatives of all the functions in the graph . We already saw in Section ? ? the derivative of the sigmoid σ : dσ ( z ) dz = σ ( z ) ( 1 − σ ( z ) ) ( 7.23 )
