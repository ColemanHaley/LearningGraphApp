b'Chapter 18\n\nMachine translation\n\nMachine translation (MT) is one of the \xe2\x80\x9choly grail\xe2\x80\x9d problems in arti\xef\xac\x81cial intelligence,\nwith the potential to transform society by facilitating communication between people\nanywhere in the world. As a result, MT has received signi\xef\xac\x81cant attention and funding\nsince the early 1950s. However, it has proved remarkably challenging, and while there\nhas been substantial progress towards usable MT systems \xe2\x80\x94 especially for high-resource\nlanguage pairs like English-French \xe2\x80\x94 we are still far from translation systems that match\nthe nuance and depth of human translations.\n\n18.1 Machine translation as a task\n\nMachine translation can be formulated as an optimization problem:\n\n\xcb\x86w(t) = argmax\n\n\xce\xa8(w(s), w(t)),\n\nw(t)\n\n[18.1]\n\nwhere w(s) is a sentence in a source language, w(t) is a sentence in the target language,\nand \xce\xa8 is a scoring function. As usual, this formalism requires two components: a decod-\ning algorithm for computing \xcb\x86w(t), and a learning algorithm for estimating the parameters\nof the scoring function \xce\xa8.\n\nDecoding is dif\xef\xac\x81cult for machine translation because of the huge space of possible\ntranslations. We have faced large label spaces before: for example, in sequence labeling,\nthe set of possible label sequences is exponential in the length of the input. In these cases,\nit was possible to search the space quickly by introducing locality assumptions: for ex-\nample, that each tag depends only on its predecessor, or that each production depends\nonly on its parent. In machine translation, no such locality assumptions seem possible:\nhuman translators reword, reorder, and rearrange words; they replace single words with\nmulti-word phrases, and vice versa. This \xef\xac\x82exibility means that in even relatively simple\n\n431\n\n\x0c432\n\nCHAPTER 18. MACHINE TRANSLATION\n\nFigure 18.1: The Vauquois Pyramid\n\ntranslation models, decoding is NP-hard (Knight, 1999). Approaches for dealing with this\ncomplexity are described in \xc2\xa7 18.4.\n\nEstimating translation models is dif\xef\xac\x81cult as well. Labeled translation data usually\n\ncomes in the form parallel sentences, e.g.,\n\nw(s) =A Vinay le gusta las manzanas.\nw(t) =Vinay likes apples.\n\nA useful feature function would note the translation pairs (gusta, likes), (manzanas, apples),\nand even (Vinay, Vinay). But this word-to-word alignment is not given in the data. One\nsolution is to treat this alignment as a latent variable; this is the approach taken by clas-\nsical statistical machine translation (SMT) systems, described in \xc2\xa7 18.2. Another solution\nis to model the relationship between w(t) and w(s) through a more complex and expres-\nsive function; this is the approach taken by neural machine translation (NMT) systems,\ndescribed in \xc2\xa7 18.3.\n\nThe Vauquois Pyramid is a theory of how translation should be done. At the lowest\nlevel, the translation system operates on individual words, but the horizontal distance\nat this level is large, because languages express ideas differently. If we can move up the\ntriangle to syntactic structure, the distance for translation is reduced; we then need only\nproduce target-language text from the syntactic representation, which can be as simple\nas reading off a tree. Further up the triangle lies semantics; translating between semantic\nrepresentations should be easier still, but mapping between semantics and surface text is a\ndif\xef\xac\x81cult, unsolved problem. At the top of the triangle is interlingua, a semantic represen-\ntation that is so generic that it is identical across all human languages. Philosophers de-\nbate whether such a thing as interlingua is really possible (e.g., Derrida, 1985). While the\n\xef\xac\x81rst-order logic representations discussed in chapter 12 might be thought to be language\nindependent, they are built on an inventory of predicates that are suspiciously similar to\nEnglish words (Nirenburg and Wilks, 2001). Nonetheless, the idea of linking translation\n\nJacob Eisenstein. Draft of November 13, 2018.\n\ninterlingua\n\nsemantics\n\nsyntax\n\ntext\n\nsource\n\ntarget\n\n\x0c18.1. MACHINE TRANSLATION AS A TASK\n\n433\n\nAdequate? Fluent?\nTo Vinay it like Python\nyes\nVinay debugs memory leaks no\nVinay likes Python\nyes\n\nno\nyes\nyes\n\nTable 18.1: Adequacy and \xef\xac\x82uency for translations of the Spanish sentence A Vinay le gusta\nPython.\n\nand semantic understanding may still be a promising path, if the resulting translations\nbetter preserve the meaning of the original text.\n\n18.1.1 Evaluating translations\nThere are two main criteria for a translation, summarized in Table 18.1.\n\n\xe2\x80\xa2 Adequacy: The translation w(t) should adequately re\xef\xac\x82ect the linguistic content of\nw(s). For example, if w(s) = A Vinay le gusta Python, the reference translation is\nw(t) = Vinay likes Python. However, the gloss, or word-for-word translation w(t) =\nTo Vinay it like Python is also considered adequate because it contains all the relevant\ncontent. The output w(t) = Vinay debugs memory leaks is not adequate.\n\n\xe2\x80\xa2 Fluency: The translation w(t) should read like \xef\xac\x82uent text in the target language. By\nthis criterion, the gloss w(t) = To Vinay it like Python will score poorly, and w(t) =\nVinay debugs memory leaks will be preferred.\n\nAutomated evaluations of machine translations typically merge both of these criteria,\nby comparing the system translation with one or more reference translations, produced\nby professional human translators. The most popular quantitative metric is BLEU (bilin-\ngual evaluation understudy; Papineni et al., 2002), which is based on n-gram precision:\nwhat fraction of n-grams in the system translation appear in the reference? Speci\xef\xac\x81cally,\nfor each n-gram length, the precision is de\xef\xac\x81ned as,\n\npn =\n\nnumber of n-grams appearing in both reference and hypothesis translations\n\nnumber of n-grams appearing in the hypothesis translation\n\n.\n\n[18.2]\n\nThe n-gram precisions for three hypothesis translations are shown in Figure 18.2.\n\nThe BLEU score is then based on the average, exp 1\n\nn=1 log pn. Two modi\xef\xac\x81cations\nof Equation 18.2 are necessary: (1) to avoid computing log 0, all precisions are smoothed\nto ensure that they are positive; (2) each n-gram in the reference can be used at most\nonce, so that to to to to to to does not achieve p1 = 1 against the reference to be or not to\nbe. Furthermore, precision-based metrics are biased in favor of short translations, which\n\nN(cid:80)N\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c434\n\nCHAPTER 18. MACHINE TRANSLATION\n\nTranslation\n\np1\n\np2\n\np3\n\np4 BP BLEU\n\nReference Vinay likes programming in Python\n\nSys1\nSys2\nSys3\n\nTo Vinay it like to program Python\nVinay likes Python\nVinay likes programming in his pajamas\n\n2\n7\n3\n3\n4\n6\n\n0\n1\n2\n3\n5\n\n0\n0\n2\n4\n\n0\n0\n1\n3\n\n1\n.51\n1\n\n.21\n.33\n.76\n\nFigure 18.2: A reference translation and three system outputs. For each output, pn indi-\ncates the precision at each n-gram, and BP indicates the brevity penalty.\n\ncan achieve high scores by minimizing the denominator in [18.2]. To avoid this issue, a\nbrevity penalty is applied to translations that are shorter than the reference. This penalty\nis indicated as \xe2\x80\x9cBP\xe2\x80\x9d in Figure 18.2.\n\nAutomated metrics like BLEU have been validated by correlation with human judg-\nments of translation quality. Nonetheless, it is not dif\xef\xac\x81cult to construct examples in which\nthe BLEU score is high, yet the translation is dis\xef\xac\x82uent or carries a completely different\nmeaning from the original. To give just one example, consider the problem of translating\npronouns. Because pronouns refer to speci\xef\xac\x81c entities, a single incorrect pronoun can oblit-\nerate the semantics of the original sentence. Existing state-of-the-art systems generally\ndo not attempt the reasoning necessary to correctly resolve pronominal anaphora (Hard-\nmeier, 2012). Despite the importance of pronouns for semantics, they have a marginal\nimpact on BLEU, which may help to explain why existing systems do not make a greater\neffort to translate them correctly.\n\nFairness and bias The problem of pronoun translation intersects with issues of fairness\nand bias. In many languages, such as Turkish, the third person singular pronoun is gender\nneutral. Today\xe2\x80\x99s state-of-the-art systems produce the following Turkish-English transla-\ntions (Caliskan et al., 2017):\n\n(18.1) O\nHe\n(18.2) O\n\nShe\n\nbir\nis a\nbir\nis a\n\ndoktor.\ndoctor.\nhem\xc2\xb8sire.\nnurse.\n\nThe same problem arises for other professions that have stereotypical genders, such as\nengineers, soldiers, and teachers, and for other languages that have gender-neutral pro-\nnouns. This bias was not directly programmed into the translation model; it arises from\nstatistical tendencies in existing datasets. This highlights a general problem with data-\ndriven approaches, which can perpetuate biases that negatively impact disadvantaged\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.1. MACHINE TRANSLATION AS A TASK\n\n435\n\ngroups. Worse, machine learning can amplify biases in data (Bolukbasi et al., 2016): if a\ndataset has even a slight tendency towards men as doctors, the resulting translation model\nmay produce translations in which doctors are always he, and nurses are always she.\n\nOther metrics A range of other automated metrics have been proposed for machine\ntranslation. One potential weakness of BLEU is that it only measures precision; METEOR\nis a weighted F -MEASURE, which is a combination of recall and precision (see \xc2\xa7 4.4.1).\nTranslation Error Rate (TER) computes the string edit distance (see \xc2\xa7 9.1.4) between the\nreference and the hypothesis (Snover et al., 2006). For language pairs like English and\nJapanese, there are substantial differences in word order, and word order errors are not\nsuf\xef\xac\x81ciently captured by n-gram based metrics. The RIBES metric applies rank correla-\ntion to measure the similarity in word order between the system and reference transla-\ntions (Isozaki et al., 2010).\n\n18.1.2 Data\n\nData-driven approaches to machine translation rely primarily on parallel corpora, which\nare translations at the sentence level. Early work focused on government records, in which\n\xef\xac\x81ne-grained of\xef\xac\x81cial translations are often required. For example, the IBM translation sys-\ntems were based on the proceedings of the Canadian Parliament, called Hansards, which\nare recorded in English and French (Brown et al., 1990). The growth of the European\nUnion led to the development of the EuroParl corpus, which spans 21 European lan-\nguages (Koehn, 2005). While these datasets helped to launch the \xef\xac\x81eld of machine transla-\ntion, they are restricted to narrow domains and a formal speaking style, limiting their ap-\nplicability to other types of text. As more resources are committed to machine translation,\nnew translation datasets have been commissioned. This has broadened the scope of avail-\nable data to news,1 movie subtitles,2 social media (Ling et al., 2013), dialogues (Fordyce,\n2007), TED talks (Paul et al., 2010), and scienti\xef\xac\x81c research articles (Nakazawa et al., 2016).\nDespite this growing set of resources, the main bottleneck in machine translation data\nis the need for parallel corpora that are aligned at the sentence level. Many languages have\nsizable parallel corpora with some high-resource language, but not with each other. The\nhigh-resource language can then be used as a \xe2\x80\x9cpivot\xe2\x80\x9d or \xe2\x80\x9cbridge\xe2\x80\x9d (Boitet, 1988; Utiyama\nand Isahara, 2007): for example, De Gispert and Marino (2006) use Spanish as a bridge for\ntranslation between Catalan and English. For most of the 6000 languages spoken today,\nthe only source of translation data remains the Judeo-Christian Bible (Resnik et al., 1999).\nWhile relatively small, at less than a million tokens, the Bible has been translated into\nmore than 2000 languages, far outpacing any other corpus. Some research has explored\n\n1https://catalog.ldc.upenn.edu/LDC2010T10,\n\nhttp://www.statmt.org/wmt15/\n\ntranslation-task.html\n\n2http://opus.nlpl.eu/\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c436\n\nCHAPTER 18. MACHINE TRANSLATION\n\nthe possibility of automatically identifying parallel sentence pairs from unaligned parallel\ntexts, such as web pages and Wikipedia articles (Kilgarriff and Grefenstette, 2003; Resnik\nand Smith, 2003; Adafre and De Rijke, 2006). Another approach is to create large parallel\ncorpora through crowdsourcing (Zaidan and Callison-Burch, 2011).\n\n18.2 Statistical machine translation\n\nThe previous section introduced adequacy and \xef\xac\x82uency as the two main criteria for ma-\nchine translation. A natural modeling approach is to represent them with separate scores,\n\n\xce\xa8(w(s), w(t)) = \xce\xa8A(w(s), w(t)) + \xce\xa8F (w(t)).\n\n[18.3]\n\nThe \xef\xac\x82uency score \xce\xa8F need not even consider the source sentence; it only judges w(t) on\nwhether it is \xef\xac\x82uent in the target language. This decomposition is advantageous because\nit makes it possible to estimate the two scoring functions on separate data. While the\nadequacy model must be estimated from aligned sentences \xe2\x80\x94 which are relatively expen-\nsive and rare \xe2\x80\x94 the \xef\xac\x82uency model can be estimated from monolingual text in the target\nlanguage. Large monolingual corpora are now available in many languages, thanks to\nresources such as Wikipedia.\n\nAn elegant justi\xef\xac\x81cation of the decomposition in Equation 18.3 is provided by the noisy\n\nchannel model, in which each scoring function is a log probability:\n\n\xce\xa8A(w(s), w(t)) (cid:44) log pS|T (w(s) | w(t))\n\n\xce\xa8F (w(t)) (cid:44) log pT (w(t))\n\n\xce\xa8(w(s), w(t)) = log pS|T (w(s) | w(t)) + log pT (w(t)) = log pS,T (w(s), w(t)).\n\n[18.4]\n[18.5]\n[18.6]\n\nBy setting the scoring functions equal to the logarithms of the prior and likelihood, their\nsum is equal to log pS,T , which is the logarithm of the joint probability of the source and\ntarget. The sentence \xcb\x86w(t) that maximizes this joint probability is also the maximizer of the\nconditional probability pT|S, making it the most likely target language sentence, condi-\ntioned on the source.\n\nThe noisy channel model can be justi\xef\xac\x81ed by a generative story. The target text is orig-\ninally generated from a probability model pT . It is then encoded in a \xe2\x80\x9cnoisy channel\xe2\x80\x9d\npS|T , which converts it to a string in the source language. In decoding, we apply Bayes\xe2\x80\x99\nrule to recover the string w(t) that is maximally likely under the conditional probability\npT|S. Under this interpretation, the target probability pT is just a language model, and\ncan be estimated using any of the techniques from chapter 6. The only remaining learning\nproblem is to estimate the translation model pS|T .\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.2. STATISTICAL MACHINE TRANSLATION\n\n437\n\nFigure 18.3: An example word-to-word alignment\n\n18.2.1 Statistical translation modeling\nThe simplest decomposition of the translation model is word-to-word: each word in the\nsource should be aligned to a word in the translation. This approach presupposes an\nalignment A(w(s), w(t)), which contains a list of pairs of source and target tokens. For\nexample, given w(s) = A Vinay le gusta Python and w(t) = Vinay likes Python, one possible\nword-to-word alignment is,\n\nA(w(s), w(t)) = {(A, \xe2\x88\x85), (Vinay, Vinay), (le, likes), (gusta, likes), (Python,Python)}.\n\n[18.7]\n\nThis alignment is shown in Figure 18.3. Another, less promising, alignment is:\n\nA(w(s), w(t)) = {(A, Vinay), (Vinay, likes), (le, Python), (gusta, \xe2\x88\x85), (Python, \xe2\x88\x85)}.\n\n[18.8]\n\nEach alignment contains exactly one tuple for each word in the source, which serves to\nexplain how the source word could be translated from the target, as required by the trans-\nlation probability pS|T . If no appropriate word in the target can be identi\xef\xac\x81ed for a source\nword, it is aligned to \xe2\x88\x85 \xe2\x80\x94 as is the case for the Spanish function word a in the example,\nwhich glosses to the English word to. Words in the target can align with multiple words\nin the source, so that the target word likes can align to both le and gusta in the source.\n\nThe joint probability of the alignment and the translation can be de\xef\xac\x81ned conveniently\n\nas,\n\np(w(s),A | w(t)) =\n\n=\n\np(w(s)\n\nm , am | w(t)\n\nam, m, M (s), M (t))\n\np(am | m, M (s), M (t)) \xc3\x97 p(w(s)\n\nm | w(t)\nam).\n\n[18.9]\n\n[18.10]\n\nM (s)(cid:89)m=1\nM (s)(cid:89)m=1\n\nThis probability model makes two key assumptions:\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\nA\n\nVinay\n\nle\n\ngusta\n\npython\n\nVinay\n\nlikes\n\npython\n\n\x0cM (s)(cid:89)m=1\n\nM (s)(cid:89)m=1\n\n438\n\nCHAPTER 18. MACHINE TRANSLATION\n\n\xe2\x80\xa2 The alignment probability factors across tokens,\n\np(A | w(s), w(t)) =\n\np(am | m, M (s), M (t)).\n\n[18.11]\n\nThis means that each alignment decision is independent of the others, and depends\nonly on the index m, and the sentence lengths M (s) and M (t).\n\n\xe2\x80\xa2 The translation probability also factors across tokens,\n\np(w(s) | w(t),A) =\n\np(w(s)\n\nm | w(t)\nam),\n\n[18.12]\n\nso that each word in w(s) depends only on its aligned word in w(t). This means that\ntranslation is word-to-word, ignoring context. The hope is that the target language\nmodel p(w(t)) will correct any dis\xef\xac\x82uencies that arise from word-to-word translation.\n\nTo translate with such a model, we could sum or max over all possible alignments,\n\np(w(s), w(t)) =(cid:88)A\n\np(w(s), w(t),A)\n\n=p(w(t))(cid:88)A\np(A) \xc3\x97 p(w(s) | w(t),A)\n\xe2\x89\xa5p(w(t)) maxA p(A) \xc3\x97 p(w(s) | w(t),A).\n\n[18.13]\n\n[18.14]\n\n[18.15]\n\nThe term p(A) de\xef\xac\x81nes the prior probability over alignments. A series of alignment\nmodels with increasingly relaxed independence assumptions was developed by researchers\nat IBM in the 1980s and 1990s, known as IBM Models 1-6 (Och and Ney, 2003).\nIBM\nModel 1 makes the strongest independence assumption:\n1\n\np(am | m, M (s), M (t)) =\n\n.\n\nM (t)\n\n[18.16]\n\nIn this model, every alignment is equally likely. This is almost surely wrong, but it re-\nsults in a convex learning objective, yielding a good initialization for the more complex\nalignment models (Brown et al., 1993; Koehn, 2009).\n\n18.2.2 Estimation\nLet us de\xef\xac\x81ne the parameter \xce\xb8u\xe2\x86\x92v as the probability of translating target word u to source\nword v. If word-to-word alignments were annotated, these probabilities could be com-\nputed from relative frequencies,\n\n\xcb\x86\xce\xb8u\xe2\x86\x92v =\n\ncount(u, v)\ncount(u)\n\n,\n\n[18.17]\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.2. STATISTICAL MACHINE TRANSLATION\n\n439\n\nwhere count(u, v) is the count of instances in which word v was aligned to word u in\nthe training set, and count(u) is the total count of the target word u. The smoothing\ntechniques mentioned in chapter 6 can help to reduce the variance of these probability\nestimates.\n\nConversely, if we had an accurate translation model, we could estimate the likelihood\n\nof each alignment decision,\n\nqm(am | w(s), w(t)) \xe2\x88\x9d p(am | m, M (s), M (t)) \xc3\x97 p(w(s)\n\nm | w(t)\nam),\n\n[18.18]\n\nwhere qm(am | w(s), w(t)) is a measure of our con\xef\xac\x81dence in aligning source word w(s)\nm\nto target word w(t)\nam. The relative frequencies could then be computed from the expected\ncounts,\n\nEq [count(u, v)]\n\n\xcb\x86\xce\xb8u\xe2\x86\x92v =\n\ncount(u)\nqm(am | w(s), w(t)) \xc3\x97 \xce\xb4(w(s)\n\nm = v) \xc3\x97 \xce\xb4(w(t)\n\nam = u).\n\n[18.19]\n\n[18.20]\n\nEq [count(u, v)] =(cid:88)m\n\nThe expectation-maximization (EM) algorithm proceeds by iteratively updating qm\nand \xcb\x86\xce\x98. The algorithm is described in general form in chapter 5. For statistical machine\ntranslation, the steps of the algorithm are:\n\n1. E-step: Update beliefs about word alignment using Equation 18.18.\n\n2. M-step: Update the translation model using Equations 18.19 and 18.20.\n\nAs discussed in chapter 5, the expectation maximization algorithm is guaranteed to con-\nverge, but not to a global optimum. However, for IBM Model 1, it can be shown that EM\noptimizes a convex objective, and global optimality is guaranteed. For this reason, IBM\nModel 1 is often used as an initialization for more complex alignment models. For more\ndetail, see Koehn (2009).\n\n18.2.3 Phrase-based translation\n\nReal translations are not word-to-word substitutions. One reason is that many multiword\nexpressions are not translated literally, as shown in this example from French:\n\n(18.3) Nous\n\nallons\nwill\n\nprendre\ntake\n\nun\na\n\nverre\nglass\n\nWe\nWe\xe2\x80\x99ll have a drink\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c440\n\nCHAPTER 18. MACHINE TRANSLATION\n\nFigure 18.4: A phrase-based alignment between French and English, corresponding to\nexample (18.3)\n\nThe line we will take a glass is the word-for-word gloss of the French sentence; the transla-\ntion we\xe2\x80\x99ll have a drink is shown on the third line. Such examples are dif\xef\xac\x81cult for word-to-\nword translation models, since they require translating prendre to have and verre to drink.\nThese translations are only correct in the context of these speci\xef\xac\x81c phrases.\n\nPhrase-based translation generalizes on word-based models by building translation\ntables and alignments between multiword spans. (These \xe2\x80\x9cphrases\xe2\x80\x9d are not necessarily\nsyntactic constituents like the noun phrases and verb phrases described in chapters 9 and\n10.) The generalization from word-based translation is surprisingly straightforward: the\ntranslation tables can now condition on multi-word units, and can assign probabilities to\nmulti-word units; alignments are mappings from spans to spans, ((i, j), (k, (cid:96))), so that\n\np(w(s) | w(t),A) = (cid:89)((i,j),(k,(cid:96)))\xe2\x88\x88A\n\npw(s)|w(t)({w(s)\n\ni+1, w(s)\n\ni+2, . . . , w(s)\n\nj } | {w(t)\n\nk+1, w(t)\n\nk+2, . . . , w(t)\n\n(cid:96) }).\n[18.21]\n\nThe phrase alignment ((i, j), (k, (cid:96))) indicates that the span w(s)\ni+1:j is the translation of the\nspan w(t)\nk+1:(cid:96). An example phrasal alignment is shown in Figure 18.4. Note that the align-\nment set A is required to cover all of the tokens in the source, just as in word-based trans-\nlation. The probability model pw(s)|w(t) must now include translations for all phrase pairs,\nwhich can be learned from expectation-maximization just as in word-based statistical ma-\nchine translation.\n\nJacob Eisenstein. Draft of November 13, 2018.\n\nNous\n\nallons\n\nprendre\n\nune\n\nverre\n\nWe\xe2\x80\x99ll\n\nhave\n\na\n\ndrink\n\n\x0c18.2. STATISTICAL MACHINE TRANSLATION\n\n441\n\n*Syntax-based translation\n\n18.2.4\nThe Vauquois Pyramid (Figure 18.1) suggests that translation might be easier if we take a\nhigher-level view. One possibility is to incorporate the syntactic structure of the source,\nthe target, or both. This is particularly promising for language pairs that consistent syn-\ntactic differences. For example, English adjectives almost always precede the nouns that\nthey modify, while in Romance languages such as French and Spanish, the adjective often\nfollows the noun: thus, angry \xef\xac\x81sh would translate to pez (\xef\xac\x81sh) enojado (angry) in Spanish.\nIn word-to-word translation, these reorderings cause the alignment model to be overly\npermissive. It is not that the order of any pair of English words can be reversed when\ntranslating into Spanish, but only adjectives and nouns within a noun phrase. Similar\nissues arise when translating between verb-\xef\xac\x81nal languages such as Japanese (in which\nverbs usually follow the subject and object), verb-initial languages like Tagalog and clas-\nsical Arabic, and verb-medial languages such as English.\n\nAn elegant solution is to link parsing and translation in a synchronous context-free\ngrammar (SCFG; Chiang, 2007).3 An SCFG is a set of productions of the form X \xe2\x86\x92 (\xce\xb1, \xce\xb2,\xe2\x88\xbc),\nwhere X is a non-terminal, \xce\xb1 and \xce\xb2 are sequences of terminals or non-terminals, and \xe2\x88\xbc\nis a one-to-one alignment of items in \xce\xb1 with items in \xce\xb2. English-Spanish adjective-noun\nordering can be handled by a set of synchronous productions, e.g.,\n\nNP \xe2\x86\x92 (DET1 NN2 JJ3, DET1 JJ3 NN2),\n\n[18.22]\n\nwith subscripts indicating the alignment between the Spanish (left) and English (right)\nparts of the right-hand side. Terminal productions yield translation pairs,\n\nJJ \xe2\x86\x92 (enojado1,\n\nangry1).\n\n[18.23]\n\nA synchronous derivation begins with the start symbol S, and derives a pair of sequences\nof terminal symbols.\n\nGiven an SCFG in which each production yields at most two symbols in each lan-\nguage (Chomsky Normal Form; see \xc2\xa7 9.2.1), a sentence can be parsed using only the CKY\nalgorithm (chapter 10). The resulting derivation also includes productions in the other\nlanguage, all the way down to the surface form. Therefore, SCFGs make translation very\nsimilar to parsing. In a weighted SCFG, the log probability log pS|T can be computed from\nthe sum of the log-probabilities of the productions. However, combining SCFGs with a\ntarget language model is computationally expensive, necessitating approximate search\nalgorithms (Huang and Chiang, 2007).\n\nSynchronous context-free grammars are an example of tree-to-tree translation, be-\ncause they model the syntactic structure of both the target and source language. In string-\nto-tree translation, string elements are translated into constituent tree fragments, which\n3Earlier approaches to syntactic machine translation includes syntax-driven transduction (Lewis II and\n\nStearns, 1968) and stochastic inversion transduction grammars (Wu, 1997).\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c442\n\nCHAPTER 18. MACHINE TRANSLATION\n\nare then assembled into a translation (Yamada and Knight, 2001; Galley et al., 2004); in\ntree-to-string translation, the source side is parsed, and then transformed into a string on\nthe target side (Liu et al., 2006). A key question for syntax-based translation is the extent\nto which we phrasal constituents align across translations (Fox, 2002), because this gov-\nerns the extent to which we can rely on monolingual parsers and treebanks. For more on\nsyntax-based machine translation, see the monograph by Williams et al. (2016).\n\n18.3 Neural machine translation\n\nNeural network models for machine translation are based on the encoder-decoder archi-\ntecture (Cho et al., 2014). The encoder network converts the source language sentence into\na vector or matrix representation; the decoder network then converts the encoding into a\nsentence in the target language.\n\nz =ENCODE(w(s))\n\nw(t) | w(s) \xe2\x88\xbcDECODE(z),\n\n[18.24]\n[18.25]\n\nwhere the second line means that the function DECODE(z) de\xef\xac\x81nes the conditional proba-\nbility p(w(t) | w(s)).\n\nThe decoder is typically a recurrent neural network, which generates the target lan-\nguage sentence one word at a time, while recurrently updating a hidden state. The en-\ncoder and decoder networks are trained end-to-end from parallel sentences. If the output\nlayer of the decoder is a logistic function, then the entire architecture can be trained to\nmaximize the conditional log-likelihood,\n\np(w(t)\n\nM (t)(cid:88)m=1\n1:m\xe2\x88\x921, w(s)) \xe2\x88\x9d exp(cid:16)\xce\xb2\n\nlog p(w(t) | w(s)) =\nm | w(t)\nm\xe2\x88\x921 is a recurrent function of the previously generated text\n\nm | w(t)\nm\xe2\x88\x921(cid:17)\nm \xc2\xb7 h(t)\n\n1:m\xe2\x88\x921, z)\n\n[18.27]\n\n[18.26]\n\nw(t)\n\np(w(t)\n\nwhere the hidden state h(t)\nw(t)\n\n1:m\xe2\x88\x921 and the encoding z. The second line is equivalent to writing,\n\n[18.28]\nwhere \xce\xb2 \xe2\x88\x88 R(V (t)\xc3\x97K) is the matrix of output word vectors for the V (t) words in the target\nlanguage vocabulary.\n\n1:m\xe2\x88\x921, w(s) \xe2\x88\xbc SoftMax(cid:16)\xce\xb2 \xc2\xb7 h(t)\n\nm\xe2\x88\x921(cid:17) ,\n\nm | w(t)\nw(t)\n\nThe simplest encoder-decoder architecture is the sequence-to-sequence model (Sutskever\n\net al., 2014). In this model, the encoder is set to the \xef\xac\x81nal hidden state of a long short-term\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.3. NEURAL MACHINE TRANSLATION\n\n443\n\nFigure 18.5: A deep bidirectional LSTM encoder\n\nmemory (LSTM) (see \xc2\xa7 6.3.3) on the source sentence:\nm , h(s)\n\nm\xe2\x88\x921)\n\nm =LSTM(x(s)\nh(s)\nz (cid:44)h(s)\n\nM (s),\n\n[18.29]\n[18.30]\n\nwhere x(s)\nthe initial hidden state for the decoder LSTM:\n\nm is the embedding of source language word w(s)\n\nm . The encoding then provides\n\nh(t)\n0 =z\nh(t)\nm =LSTM(x(t)\n\nm , h(t)\n\nm\xe2\x88\x921),\n\n[18.31]\n[18.32]\n\nwhere x(t)\n\nm is the embedding of the target language word w(t)\nm .\n\nSequence-to-sequence translation is nothing more than wiring together two LSTMs:\none to read the source, and another to generate the target. To make the model work well,\nsome additional tweaks are needed:\n\n\xe2\x80\xa2 Most notably, the model works much better if the source sentence is reversed, read-\ning from the end of the sentence back to the beginning. In this way, the words at the\nbeginning of the source have the greatest impact on the encoding z, and therefore\nimpact the words at the beginning of the target sentence. Later work on more ad-\nvanced encoding models, such as neural attention (see \xc2\xa7 18.3.1), has eliminated the\nneed for reversing the source sentence.\n\xe2\x80\xa2 The encoder and decoder can be implemented as deep LSTMs, with multiple layers\nm at layer i is treated\n\nof hidden states. As shown in Figure 18.5, each hidden state h(s,i)\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\nh(s,D)\nm\xe2\x88\x921\n\nh(s,D)\n\nm\n\nh(s,D)\nm+1\n\n. . .\n\n. . .\n\n. . .\n\nh(s,2)\nm\xe2\x88\x921\n\nh(s,1)\nm\xe2\x88\x921\n\nx(s)\n\nm\xe2\x88\x921\n\nh(s,2)\n\nm\n\nh(s,1)\n\nm\n\nx(s)\nm\n\nh(s,2)\nm+1\n\nh(s,1)\nm+1\n\nx(s)\n\nm+1\n\n\x0c444\n\nCHAPTER 18. MACHINE TRANSLATION\n\nas the input to an LSTM at layer i + 1:\n\nh(s,1)\nm =LSTM(x(s)\n=LSTM(h(s,i)\nh(s,i+1)\n\nm , h(s)\nm , h(s,i+1)\n\nm\xe2\x88\x921)\nm\xe2\x88\x921\n\nm\n\n),\n\n\xe2\x88\x80i \xe2\x89\xa5 1.\n\n[18.33]\n[18.34]\n\nThe original work on sequence-to-sequence translation used four layers; in 2016,\nGoogle\xe2\x80\x99s commercial machine translation system used eight layers (Wu et al., 2016).4\n\xe2\x80\xa2 Signi\xef\xac\x81cant improvements can be obtained by creating an ensemble of translation\nmodels, each trained from a different random initialization. For an ensemble of size\nN, the per-token decoding probability is set equal to,\n\np(w(t) | z, w(t)\n\n1:m\xe2\x88\x921) =\n\n1\nN\n\nN(cid:88)i=1\n\npi(w(t) | z, w(t)\n\n1:m\xe2\x88\x921),\n\n[18.35]\n\nwhere pi is the decoding probability for model i. Each translation model in the\nensemble includes its own encoder and decoder networks.\n\n\xe2\x80\xa2 The original sequence-to-sequence model used a fairly standard training setup: stochas-\n\ntic gradient descent with an exponentially decreasing learning rate after the \xef\xac\x81rst \xef\xac\x81ve\nepochs; mini-batches of 128 sentences, chosen to have similar length so that each\nsentence on the batch will take roughly the same amount of time to process; gradi-\nent clipping (see \xc2\xa7 3.3.4) to ensure that the norm of the gradient never exceeds some\nprede\xef\xac\x81ned value.\n\n18.3.1 Neural attention\nThe sequence-to-sequence model discussed in the previous section was a radical depar-\nture from statistical machine translation, in which each word or phrase in the target lan-\nguage is conditioned on a single word or phrase in the source language. Both approaches\nhave advantages. Statistical translation leverages the idea of compositionality \xe2\x80\x94 transla-\ntions of large units should be based on the translations of their component parts \xe2\x80\x94 and\nthis seems crucial if we are to scale translation to longer units of text. But the translation\nof each word or phrase often depends on the larger context, and encoder-decoder models\ncapture this context at the sentence level.\n\nIs it possible for translation to be both contextualized and compositional? One ap-\nproach is to augment neural translation with an attention mechanism. The idea of neural\nattention was described in \xc2\xa7 17.5, but its application to translation bears further discus-\nsion. In general, attention can be thought of as using a query to select from a memory\nof key-value pairs. However, the query, keys, and values are all vectors, and the entire\n\n4Google reports that this system took six days to train for English-French translation, using 96 NVIDIA\n\nK80 GPUs, which would have cost roughly half a million dollars at the time.\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.3. NEURAL MACHINE TRANSLATION\n\n445\n\nFigure 18.6: A general view of neural attention. The dotted box indicates that each \xce\xb1m\xe2\x86\x92n\ncan be viewed as a gate on value n.\n\noperation is differentiable. For each key n in the memory, we compute a score \xcf\x88\xce\xb1(m, n)\nwith respect to the query m. That score is a function of the compatibility of the key and\nthe query, and can be computed using a small feedforward neural network. The vector\nof scores is passed through an activation function, such as softmax. The output of this\nactivation function is a vector of non-negative numbers [\xce\xb1m\xe2\x86\x921, \xce\xb1m\xe2\x86\x922, . . . , \xce\xb1m\xe2\x86\x92N ](cid:62), with\nlength N equal to the size of the memory. Each value in the memory vn is multiplied by\nthe attention \xce\xb1m\xe2\x86\x92n; the sum of these scaled values is the output. This process is shown in\nFigure 18.6. In the extreme case that \xce\xb1m\xe2\x86\x92n = 1 and \xce\xb1m\xe2\x86\x92n(cid:48) = 0 for all other n(cid:48), then the\nattention mechanism simply selects the value vn from the memory.\n\nNeural attention makes it possible to integrate alignment into the encoder-decoder ar-\nchitecture. Rather than encoding the entire source sentence into a \xef\xac\x81xed length vector z,\nit can be encoded into a matrix Z \xe2\x88\x88 RK\xc3\x97M (S), where K is the dimension of the hidden\nstate, and M (S) is the number of tokens in the source input. Each column of Z represents\nthe state of a recurrent neural network over the source sentence. These vectors are con-\nstructed from a bidirectional LSTM (see \xc2\xa7 7.6), which can be a deep network as shown in\nFigure 18.5. These columns are both the keys and the values in the attention mechanism.\nAt each step m in decoding, the attentional state is computed by executing a query,\n\nwhich is equal to the state of the decoder, h(t)\n\nm . The resulting compatibility scores are,\n\n\xcf\x88\xce\xb1(m, n) =v\xce\xb1 \xc2\xb7 tanh(\xce\x98\xce\xb1[h(t)\n\nm ; h(s)\n\nn ]).\n\n[18.36]\n\nThe function \xcf\x88 is thus a two layer feedforward neural network, with weights v\xce\xb1 on the\noutput layer, and weights \xce\x98\xce\xb1 on the input layer. To convert these scores into atten-\ntion weights, we apply an activation function, which can be vector-wise softmax or an\nelement-wise sigmoid:\n\nSoftmax attention\n\n\xce\xb1m\xe2\x86\x92n =\n\nexp \xcf\x88\xce\xb1(m, n)\n(cid:80)M (s)\nn(cid:48)=1 exp \xcf\x88\xce\xb1(m, n(cid:48))\n\n[18.37]\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\nOutput\n\nactivation\n\n\xce\xb1\n\nQuery\n\n\xcf\x88\xce\xb1\n\nKey\n\nValue\n\n\x0c446\n\nCHAPTER 18. MACHINE TRANSLATION\n\nSigmoid attention\n\n\xce\xb1m\xe2\x86\x92n = \xcf\x83 (\xcf\x88\xce\xb1(m, n))\n\n[18.38]\n\nThe attention \xce\xb1 is then used to compute a context vector cm by taking a weighted\n\naverage over the columns of Z,\n\ncm =\n\nM (s)(cid:88)n=1\n\n\xce\xb1m\xe2\x86\x92nzn,\n\n[18.39]\n\nwhere \xce\xb1m\xe2\x86\x92n \xe2\x88\x88 [0, 1] is the amount of attention from word m of the target to word n of the\nsource. The context vector can be incorporated into the decoder\xe2\x80\x99s word output probability\nmodel, by adding another layer to the decoder (Luong et al., 2015):\n\n\xcb\x9ch(t)\n\nm ; cm](cid:17)\nm = tanh(cid:16)\xce\x98c[h(t)\nm(cid:19) .\nm+1 \xc2\xb7 \xcb\x9ch(t)\n\n1:m, w(s)) \xe2\x88\x9d exp(cid:18)\xce\xb2\n\nw(t)\n\np(w(t)\n\nm+1 | w(t)\n\n[18.40]\n\n[18.41]\n\nHere the decoder state h(t)\nto compute a \xef\xac\x81nal output vector \xcb\x9ch(t)\ndecoder recurrence in a similar manner (Bahdanau et al., 2014).\n\nm is concatenated with the context vector, forming the input\nm . The context vector can be incorporated into the\n\n*Neural machine translation without recurrence\n\n18.3.2\nIn the encoder-decoder model, attention\xe2\x80\x99s \xe2\x80\x9ckeys and values\xe2\x80\x9d are the hidden state repre-\nsentations in the encoder network, z, and the \xe2\x80\x9cqueries\xe2\x80\x9d are state representations in the\ndecoder network h(t). It is also possible to completely eliminate recurrence from neural\ntranslation, by applying self-attention (Lin et al., 2017; Kim et al., 2017) within the en-\ncoder and decoder, as in the transformer architecture (Vaswani et al., 2017). For level i,\nthe basic equations of the encoder side of the transformer are:\n\nz(i)\nm =\n\nM (s)(cid:88)n=1\n\nm\xe2\x86\x92n(\xce\x98vh(i\xe2\x88\x921)\n\xce\xb1(i)\nm =\xce\x982 ReLU(cid:16)\xce\x981z(i)\n\nn\n\nh(i)\n\nm + b1(cid:17) + b2.\n\n)\n\n[18.42]\n\n[18.43]\n\nFor each token m at level i, we compute self-attention over the entire source sentence.\nThe keys, values, and queries are all projections of the vector h(i\xe2\x88\x921): for example, in Equa-\ntion 18.42, the value vn is the projection \xce\x98vh(i\xe2\x88\x921)\nm\xe2\x86\x92n are com-\nn\nputed using a scaled form of softmax attention,\n\n. The attention scores \xce\xb1(i)\n\n\xce\xb1m\xe2\x86\x92n \xe2\x88\x9d exp(\xcf\x88\xce\xb1(m, n)/M ),\n\n[18.44]\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.3. NEURAL MACHINE TRANSLATION\n\n447\n\nm from h(i\xe2\x88\x921). The key, value,\nFigure 18.7: The transformer encoder\xe2\x80\x99s computation of z(i)\nand query are shown for token m \xe2\x88\x92 1. For example, \xcf\x88(i)\n\xce\xb1 (m, m \xe2\x88\x92 1) is computed from\nthe key \xce\x98kh(i\xe2\x88\x921)\nm , and the gate \xce\xb1(i)\nm\xe2\x86\x92m\xe2\x88\x921 operates on the value\n\xce\x98vh(i\xe2\x88\x921)\nm\xe2\x88\x921 . The \xef\xac\x81gure shows a minimal version of the architecture, with a single atten-\ntion head. With multiple heads, it is possible to attend to different properties of multiple\nwords.\n\nm\xe2\x88\x921 and the query \xce\x98qh(i\xe2\x88\x921)\n\nwhere M is the length of the input. This encourages the attention to be more evenly\ndispersed across the input. Self-attention is applied across multiple \xe2\x80\x9cheads\xe2\x80\x9d, each using\ndifferent projections of h(i\xe2\x88\x921) to form the keys, values, and queries. This architecture is\nshown in Figure 18.7. The output of the self-attentional layer is the representation z(i)\nm ,\nwhich is then passed through a two-layer feed-forward network, yielding the input to the\nnext layer, h(i). This self-attentional architecture can be applied in the decoder as well,\nbut this requires that there is zero attention to future words: \xce\xb1m\xe2\x86\x92n = 0 for all n > m.\n\nTo ensure that information about word order in the source is integrated into the model,\nthe encoder augments the base layer of the network with positional encodings of the\nindices of each word in the source. These encodings are vectors for each position m \xe2\x88\x88\n{1, 2, . . . , M}. The transformer sets these encodings equal to a set of sinusoidal functions\nof m,\n\ne2i\xe2\x88\x921(m) = sin(m/(10000\n\ne2i(m) = cos(m/(10000\n\n2i\nKe ))\n2i\nKe )),\n\n\xe2\x88\x80i \xe2\x88\x88 {1, 2, . . . , Ke/2}\n\n[18.45]\n\n[18.46]\n\nwhere e2i(m) is the value at element 2i of the encoding for index m. As we progress\nthrough the encoding, the sinusoidal functions have progressively narrower bandwidths.\nThis enables the model to learn to attend by relative positions of words. The positional\nencodings are concatenated with the word embeddings xm at the base layer of the model.5\n5The transformer architecture relies on several additional tricks, including layer normalization (see\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\nz(i)\n\n\xce\xb1(i)\nm\xe2\x86\x92\n\n\xcf\x88(i)\n\xce\xb1 (m,\xc2\xb7)\n\nv\n\nh(i\xe2\x88\x921)\n\nk\n\nq\n\nm \xe2\x88\x92 1\n\nm\n\nm + 1\n\n\x0c448\n\nCHAPTER 18. MACHINE TRANSLATION\n\nFigure 18.8: Translation with unknown words. The system outputs unk to indicate words\nthat are outside its vocabulary. Figure adapted from Luong et al. (2015).\n\nConvolutional neural networks (see \xc2\xa7 3.4) have also been applied as encoders in neu-\nral machine translation (Gehring et al., 2017). For each word w(s)\nm , a convolutional network\ncomputes a representation h(s)\nm from the embeddings of the word and its neighbors. This\nprocedure is applied several times, creating a deep convolutional network. The recurrent\ndecoder then computes a set of attention weights over these convolutional representa-\ntions, using the decoder\xe2\x80\x99s hidden state h(t) as the queries. This attention vector is used\nto compute a weighted average over the outputs of another convolutional neural network\nof the source, yielding an averaged representation cm, which is then fed into the decoder.\nAs with the transformer, speed is the main advantage over recurrent encoding models;\nanother similarity is that word order information is approximated through the use of po-\nsitional encodings.6\n\n18.3.3 Out-of-vocabulary words\nThus far, we have treated translation as a problem at the level of words or phrases. For\nwords that do not appear in the training data, all such models will struggle. There are\ntwo main reasons for the presence of out-of-vocabulary (OOV) words:\n\n\xe2\x80\xa2 New proper nouns, such as family names or organizations, are constantly arising \xe2\x80\x94\nparticularly in the news domain. The same is true, to a lesser extent, for technical\nterminology. This issue is shown in Figure 18.8.\n\n\xe2\x80\xa2 In many languages, words have complex internal structure, known as morphology.\nAn example is German, which uses compounding to form nouns like Abwasserbe-\nhandlungsanlage (sewage water treatment plant; example from Sennrich et al. (2016)).\n\xc2\xa7 3.3.4), residual connections around the nonlinear activations (see \xc2\xa7 3.2.2), and a non-monotonic learning\nrate schedule.\n\n6A recent evaluation found that best performance was obtained by using a recurrent network for the\ndecoder, and a transformer for the encoder (Chen et al., 2018). The transformer was also found to signi\xef\xac\x81cantly\noutperform a convolutional neural network.\n\nJacob Eisenstein. Draft of November 13, 2018.\n\nSource: The\n\necotax\n\nportico\n\nin Pont-de-buis was taken\n\ndown\n\non Thursday morning\n\nReference: Le portique \xc2\xb4ecotaxe de Pont-de-buis\n\na\n\n\xc2\xb4et\xc2\xb4e\n\nd\xc2\xb4emont\xc2\xb4e\n\njeudi\n\nmatin\n\nSystem:\n\nLe\n\nunk\n\nde\n\nunk\n\n`a unk\n\na\n\n\xc2\xb4et\xc2\xb4e\n\npris\n\nle\n\njeudi\n\nmatin\n\n\x0c18.4. DECODING\n\n449\n\nWhile compounds could in principle be addressed by better tokenization (see \xc2\xa7 8.4),\nother morphological processes involve more complex transformations of subword\nunits.\n\nNames and technical terms can be handled in a postprocessing step: after \xef\xac\x81rst identi-\nfying alignments between unknown words in the source and target, we can look up each\naligned source word in a dictionary, and choose a replacement (Luong et al., 2015). If the\nword does not appear in the dictionary, it is likely to be a proper noun, and can be copied\ndirectly from the source to the target. This approach can also be integrated directly into\nthe translation model, rather than applying it as a postprocessing step (Jean et al., 2015).\nWords with complex internal structure can be handled by translating subword units\nrather than entire words. A popular technique for identifying subword units is byte-pair\nencoding (BPE; Gage, 1994; Sennrich et al., 2016). The initial vocabulary is de\xef\xac\x81ned as the\nset of characters used in the text. The most common character bigram is then merged into\na new symbol, the vocabulary is updated, and the merging operation is applied again. For\nexample, given the dictionary {\xef\xac\x81sh, \xef\xac\x81shed, want, wanted, bike, biked}, we would \xef\xac\x81rst form\nthe subword unit ed, since this character bigram appears in three of the six words. Next,\nthere are several bigrams that each appear in a pair of words: \xef\xac\x81, is, sh, wa, an, etc. These can\nbe merged in any order. By iterating this process, we eventually reach the segmentation,\n{\xef\xac\x81sh, \xef\xac\x81sh+ed, want, want+ed, bik+e, bik+ed}. At this point, there are no bigrams that appear\nmore than once. In real data, merging is performed until the number of subword units\nreaches some prede\xef\xac\x81ned threshold, such as 104.\n\nEach subword unit is treated as a token for translation, in both the encoder (source\nside) and decoder (target side). BPE can be applied jointly to the union of the source and\ntarget vocabularies, identifying subword units that appear in both languages. For lan-\nguages that have different scripts, such as English and Russian, transliteration between\nthe scripts should be applied \xef\xac\x81rst.7\n\n18.4 Decoding\n\nGiven a trained translation model, the decoding task is:\n\n\xcb\x86w(t) = argmax\nw\xe2\x88\x88V\xe2\x88\x97\n\n\xce\xa8(w, w(s)),\n\n[18.47]\n\nIt is not possible to\nwhere w(t) is a sequence of tokens from the target vocabulary V.\nef\xef\xac\x81ciently obtain exact solutions to the decoding problem, for even minimally effective\n\n7Transliteration is crucial for converting names and other foreign words between languages that do not\nshare a single script, such as English and Japanese. It is typically approached using the \xef\xac\x81nite-state methods\ndiscussed in chapter 9 (Knight and Graehl, 1998).\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c450\n\nCHAPTER 18. MACHINE TRANSLATION\n\nmodels in either statistical or neural machine translation. Today\xe2\x80\x99s state-of-the-art transla-\ntion systems use beam search (see \xc2\xa7 11.3.1), which is an incremental decoding algorithm\nthat maintains a small constant number of competitive hypotheses. Such greedy approxi-\nmations are reasonably effective in practice, and this may be in part because the decoding\nobjective is only loosely correlated with measures of translation quality, so that exact op-\ntimization of [18.47] may not greatly improve the resulting translations.\n\nDecoding in neural machine translation is simpler than in phrase-based statistical ma-\n\nchine translation.8 The scoring function \xce\xa8 is de\xef\xac\x81ned,\n\nM (t)(cid:88)m=1\n\n\xce\xa8(w(t), w(s)) =\n\n\xcf\x88(w(t)\n\nm ; w(t)\n\n1:m\xe2\x88\x921, z)\n\n\xcf\x88(w(t); w(t)\n\n1:m\xe2\x88\x921, z) =\xce\xb2\n\nm \xc2\xb7 h(t)\nw(t)\n\nm \xe2\x88\x92 log(cid:88)w\xe2\x88\x88V\n\nexp(cid:16)\xce\xb2w \xc2\xb7 h(t)\nm(cid:17) ,\n\n[18.48]\n\n[18.49]\n\nwhere z is the encoding of the source sentence w(s), and h(t)\nz and the decoding history w(t)\nmodel, where z is a matrix encoding of the source.\n\nm is a function of the encoding\n1:m\xe2\x88\x921. This formulation subsumes the attentional translation\n\nNow consider the incremental decoding algorithm,\n\n\xcb\x86w(t)\n\nm = argmax\n\nw\xe2\x88\x88V \xcf\x88(w; \xcb\x86w(t)\n\n1:m\xe2\x88\x921, z), m = 1, 2, . . .\n\n[18.50]\n\n1:m\xe2\x88\x921.\n\nThis algorithm selects the best target language word at position m, assuming that it has\nalready generated the sequence \xcb\x86w(t)\n(Termination can be handled by augmenting\nthe vocabulary V with a special end-of-sequence token, (cid:4).) The incremental algorithm\nis likely to produce a suboptimal solution to the optimization problem de\xef\xac\x81ned in Equa-\ntion 18.47, because selecting the highest-scoring word at position m can set the decoder\non a \xe2\x80\x9cgarden path,\xe2\x80\x9d in which there are no good choices at some later position n > m. We\nmight hope for some dynamic programming solution, as in sequence labeling (\xc2\xa7 7.3). But\nthe Viterbi algorithm and its relatives rely on a Markov decomposition of the objective\nfunction into a sum of local scores: for example, scores can consider locally adjacent tags\n(ym, ym\xe2\x88\x921), but not the entire tagging history y1:m. This decomposition is not applicable\nto recurrent neural networks, because the hidden state h(t)\nm is impacted by the entire his-\ntory w(t)\n1:m; this sensitivity to long-range context is precisely what makes recurrent neural\nnetworks so effective.9 In fact, it can be shown that decoding from any recurrent neural\nnetwork is NP-complete (Siegelmann and Sontag, 1995; Chen et al., 2018).\n\n8For more on decoding in phrase-based statistical models, see Koehn (2009).\n9Note that this problem does not impact RNN-based sequence labeling models (see \xc2\xa7 7.6). This is because\n\nthe tags produced by these models do not affect the recurrent state.\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.5. TRAINING TOWARDS THE EVALUATION METRIC\n\n451\n\nBeam search Beam search is a general technique for avoiding search errors when ex-\nhaustive search is impossible; it was \xef\xac\x81rst discussed in \xc2\xa7 11.3.1. Beam search can be seen\nas a variant of the incremental decoding algorithm sketched in Equation 18.50, but at\neach step m, a set of K different hypotheses are kept on the beam. For each hypothesis\nk \xe2\x88\x88 {1, 2, . . . , K}, we compute both the current score(cid:80)M (t)\nk,1:m\xe2\x88\x921, z) as well as\nthe current hidden state h(t)\nk . At each step in the beam search, the K top-scoring children\nof each hypothesis currently on the beam are \xe2\x80\x9cexpanded\xe2\x80\x9d, and the beam is updated. For\na detailed description of beam search for RNN decoding, see Graves (2012).\n\nm=1 \xcf\x88(w(t)\n\nk,m; w(t)\n\nLearning and search Conventionally, the learning algorithm is trained to predict the\nright token in the translation, conditioned on the translation history being correct. But\nif decoding must be approximate, then we might do better by modifying the learning\nalgorithm to be robust to errors in the translation history. Scheduled sampling does this\nby training on histories that sometimes come from the ground truth, and sometimes come\nfrom the model\xe2\x80\x99s own output (Bengio et al., 2015).10 As training proceeds, the training\nwheels come off: we increase the fraction of tokens that come from the model rather than\nthe ground truth. Another approach is to train on an objective that relates directly to beam\nsearch performance (Wiseman et al., 2016). Reinforcement learning has also been applied\nto decoding of RNN-based translation models, making it possible to directly optimize\ntranslation metrics such as BLEU (Ranzato et al., 2016).\n\n18.5 Training towards the evaluation metric\n\nIn likelihood-based training, the objective is the maximize the probability of a parallel\ncorpus. However, translations are not evaluated in terms of likelihood: metrics like BLEU\nconsider only the correctness of a single output translation, and not the range of prob-\nabilities that the model assigns. It might therefore be better to train translation models\nto achieve the highest BLEU score possible \xe2\x80\x94 to the extent that we believe BLEU mea-\nsures translation quality. Unfortunately, BLEU and related metrics are not friendly for\noptimization: they are discontinuous, non-differentiable functions of the parameters of\nthe translation model.\n\nConsider an error function \xe2\x88\x86( \xcb\x86w(t), w(t)), which measures the discrepancy between the\nsystem translation \xcb\x86w(t) and the reference translation w(t); this function could be based on\nBLEU or any other metric on translation quality. One possible criterion would be to select\n\n10Scheduled sampling builds on earlier work on learning to search (Daum\xc2\xb4e III et al., 2009; Ross et al.,\n\n2011), which are also described in \xc2\xa7 15.2.4.\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c452\n\nCHAPTER 18. MACHINE TRANSLATION\n\nthe parameters \xce\xb8 that minimize the error of the system\xe2\x80\x99s preferred translation,\n\n\xcb\x86w(t) = argmax\n\n\xce\xa8(w(t), w(s); \xce\xb8)\n\nw(t)\n\n\xcb\x86\xce\xb8 = argmin\n\n\xe2\x88\x86( \xcb\x86w(t), w(s))\n\n\xce\xb8\n\n[18.51]\n\n[18.52]\n\nHowever, identifying the top-scoring translation \xcb\x86w(t) is usually intractable, as described\nin the previous section. In minimum error-rate training (MERT), \xcb\x86w(t) is selected from a\nset of candidate translations Y(w(s)); this is typically a strict subset of all possible transla-\ntions, so that it is only possible to optimize an approximation to the true error rate (Och\nand Ney, 2003).\n\nA further issue is that the objective function in Equation 18.52 is discontinuous and\nnon-differentiable, due to the argmax over translations: an in\xef\xac\x81nitesimal change in the\nparameters \xce\xb8 could cause another translation to be selected, with a completely different\nerror. To address this issue, we can instead minimize the risk, which is de\xef\xac\x81ned as the\nexpected error rate,\n\nR(\xce\xb8) =E \xcb\x86w(t)|w(s);\xce\xb8[\xe2\x88\x86( \xcb\x86w(t), w(t))]\n\n= (cid:88)\xcb\x86w(t)\xe2\x88\x88Y(w(s))\n\np( \xcb\x86w(t) | w(s)) \xc3\x97 \xe2\x88\x86( \xcb\x86w(t), w(t)).\n\n[18.53]\n\n[18.54]\n\nMinimum risk training minimizes the sum of R(\xce\xb8) across all instances in the training set.\n\nThe risk can be generalized by exponentiating the translation probabilities,\n\n\xcb\x9cp(w(t); \xce\xb8, \xce\xb1) \xe2\x88\x9d(cid:16)p(w(t) | w(s); \xce\xb8)(cid:17)\xce\xb1\n\n\xcb\x9cR(\xce\xb8) = (cid:88)\xcb\x86w(t)\xe2\x88\x88Y(w(s))\n\n\xcb\x9cp( \xcb\x86w(t) | w(s); \xce\xb1, \xce\xb8) \xc3\x97 \xe2\x88\x86( \xcb\x86w(t), w(t))\n\n[18.55]\n\n[18.56]\n\nwhere Y(w(s)) is now the set of all possible translations for w(s). Exponentiating the prob-\nabilities in this way is known as annealing (Smith and Eisner, 2006). When \xce\xb1 = 1, then\n\xcb\x9cR(\xce\xb8) = R(\xce\xb8); when \xce\xb1 = \xe2\x88\x9e, then \xcb\x9cR(\xce\xb8) is equivalent to the sum of the errors of the maxi-\nmum probability translations for each sentence in the dataset.\n\nClearly the set of candidate translations Y(w(s)) is too large to explicitly sum over.\nBecause the error function \xe2\x88\x86 generally does not decompose into smaller parts, there is\nno ef\xef\xac\x81cient dynamic programming solution to sum over this set. We can approximate\nthe sum(cid:80) \xcb\x86w(t)\xe2\x88\x88Y(w(s)) with a sum over a \xef\xac\x81nite number of samples, {w(t)\n2 , . . . , w(t)\nK }.\nIf these samples were drawn uniformly at random, then the (annealed) risk would be\n\n1 , w(t)\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.5. TRAINING TOWARDS THE EVALUATION METRIC\n\n453\n\napproximated as (Shen et al., 2016),\n\n\xcb\x9cp(w(t)\nk\n\n| w(s); \xce\xb8, \xce\xb1) \xc3\x97 \xe2\x88\x86(w(t)\n\nk , w(t))\n\n\xcb\x9cR(\xce\xb8) \xe2\x89\x88\n\nZ =\n\n1\nZ\n\nK(cid:88)k=1\nK(cid:88)k=1\n\n\xcb\x9cp(w(t)\nk\n\n| w(s); \xce\xb8, \xce\xb1).\n\n[18.57]\n\n[18.58]\n\nShen et al. (2016) report that performance plateaus at K = 100 for minimum risk training\nof neural machine translation.\n\nUniform sampling over the set of all possible translations is undesirable, because most\ntranslations have very low probability. A solution from Monte Carlo estimation is impor-\ntance sampling, in which we draw samples from a proposal distribution q(w(s)). This\ndistribution can be set equal to the current translation model p(w(t) | w(s); \xce\xb8). Each sam-\nple is then weighted by an importance score, \xcf\x89k =\n. The effect of this weighting\nis to correct for any mismatch between the proposal distribution q and the true distribu-\ntion \xcb\x9cp. The risk can then be approximated as,\n\nk |w(s))\nk ;w(s))\n\n\xcb\x9cp(w(t)\nq(w(t)\n\nw(t)\nk \xe2\x88\xbcq(w(s))\n\xcb\x9cp(w(t)\n| w(s))\nk\n\xcf\x89k =\nq(w(t)\nk ; w(s))\n1\nk=1 \xcf\x89k\n\n\xcb\x9cR(\xce\xb8) \xe2\x89\x88\n\nK(cid:88)k=1\n\n(cid:80)K\n\n\xcf\x89k \xc3\x97 \xe2\x88\x86(w(t)\n\nk , w(t)).\n\n[18.59]\n\n[18.60]\n\n[18.61]\n\nImportance sampling will generally give a more accurate approximation than uniform\nsampling. The only formal requirement is that the proposal assigns non-zero probability\nto every w(t) \xe2\x88\x88 Y(w(s)). For more on importance sampling and related methods, see\nRobert and Casella (2013).\n\nAdditional resources\n\nA complete textbook on machine translation is available from Koehn (2009). While this\nbook precedes recent work on neural translation, a more recent draft chapter on neural\ntranslation models is also available (Koehn, 2017). Neubig (2017) provides a compre-\nhensive tutorial on neural machine translation, starting from \xef\xac\x81rst principles. The course\nnotes from Cho (2015) are also useful. Several neural machine translation libraries are\navailable: LAMTRAM is an implementation of neural machine translation in DYNET (Neu-\nbig et al., 2017); OPENNMT (Klein et al., 2017) and FAIRSEQ are available in PYTORCH;\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c454\n\nCHAPTER 18. MACHINE TRANSLATION\n\nTENSOR2TENSOR is an implementation of several of the Google translation models in TEN-\nSORFLOW (Abadi et al., 2016).\n\nLiterary translation is especially challenging, even for expert human translators. Mes-\n\nsud (2014) describes some of these issues in her review of an English translation of L\xe2\x80\x99\xc2\xb4etranger,\nthe 1942 French novel by Albert Camus.11 She compares the new translation by Sandra\nSmith against earlier translations by Stuart Gilbert and Matthew Ward, focusing on the\ndif\xef\xac\x81culties presented by a single word in the \xef\xac\x81rst sentence:\n\nThen, too, Smith has reconsidered the book\xe2\x80\x99s famous opening. Camus\xe2\x80\x99s\noriginal is deceptively simple: \xe2\x80\x9cAujourd\xe2\x80\x99hui, maman est morte.\xe2\x80\x9d Gilbert in\xef\xac\x82u-\nenced generations by offering us \xe2\x80\x9cMother died today\xe2\x80\x9d\xe2\x80\x94inscribing in Meur-\nsault [the narrator] from the outset a formality that could be construed as\nheartlessness. But maman, after all, is intimate and affectionate, a child\xe2\x80\x99s name\nfor his mother. Matthew Ward concluded that it was essentially untranslatable\n(\xe2\x80\x9cmom\xe2\x80\x9d or \xe2\x80\x9cmummy\xe2\x80\x9d being not quite apt), and left it in the original French:\n\xe2\x80\x9cMaman died today.\xe2\x80\x9d There is a clear logic in this choice; but as Smith has\nexplained, in an interview in The Guardian, maman \xe2\x80\x9cdidn\xe2\x80\x99t really tell the reader\nanything about the connotation.\xe2\x80\x9d She, instead, has translated the sentence as\n\xe2\x80\x9cMy mother died today.\xe2\x80\x9d\n\nI chose \xe2\x80\x9cMy mother\xe2\x80\x9d because I thought about how someone would\ntell another person that his mother had died. Meursault is speaking\nto the reader directly. \xe2\x80\x9cMy mother died today\xe2\x80\x9d seemed to me the\nway it would work, and also implied the closeness of \xe2\x80\x9cmaman\xe2\x80\x9d you\nget in the French.\n\nElsewhere in the book, she has translated maman as \xe2\x80\x9cmama\xe2\x80\x9d \xe2\x80\x94 again, striving\nto come as close as possible to an actual, colloquial word that will carry the\nsame connotations as maman does in French.\n\nThe passage is a reminder that while the quality of machine translation has improved\ndramatically in recent years, expert human translations draw on considerations that are\nbeyond the ken of any contemporary computational approach.\n\nExercises\n\n1. Using Google translate or another online service, translate the following example\n\ninto two different languages of your choice:\n\n11The book review is currently available online at http://www.nybooks.com/articles/2014/06/\n\n05/camus-new-letranger/.\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c18.5. TRAINING TOWARDS THE EVALUATION METRIC\n\n455\n\n(18.4)\n\nIt is not down on any map; true places never are.\n\nThen translate each result back into English. Which is closer to the original? Can\nyou explain the differences?\n\n2. Compute the unsmoothed n-gram precisions p1 . . . p4 for the two back-translations\nin the previous problem, using the original source as the reference. Your n-grams\nshould include punctuation, and you should segment conjunctions like it\xe2\x80\x99s into two\ntokens.\n\n3. You are given the following dataset of translations from \xe2\x80\x9csimple\xe2\x80\x9d to \xe2\x80\x9cdif\xef\xac\x81cult\xe2\x80\x9d En-\n\nglish:\n\n(18.5)\n\na. Kids\n\nChildren\n\nlike\nadore\n\ncats.\nfelines.\n\nb. Cats\n\nFelines\n\nhats.\nfedoras.\n\nEstimate a word-to-word statistical translation model from simple English (source)\nto dif\xef\xac\x81cult English (target), using the expectation-maximization as described in \xc2\xa7 18.2.2.\nCompute two iterations of the algorithm by hand, starting from a uniform transla-\nM (t) .\ntion model, and using the simple alignment model p(am | m, M (s), M (t)) = 1\nHint: in the \xef\xac\x81nal M-step, you will want to switch from fractions to decimals.\n\n4. Building on the previous problem, what will be the converged translation proba-\nbility table? Can you state a general condition about the data, under which this\ntranslation model will fail in the way that it fails here?\n\n5. Propose a simple alignment model that would make it possible to recover the correct\n\ntranslation probabilities from the toy dataset in the previous two problems.\n\n6. Let (cid:96)(t)\n\nm+1 represent the loss at word m + 1 of the target, and let h(s)\n\nn represent the hid-\nden state at word n of the source. Write the expression for the derivative \xe2\x88\x82(cid:96)(t)\nin the\nm+1\n\xe2\x88\x82h(s)\nn\nsequence-to-sequence translation model expressed in Equations [18.29-18.32]. You\nmay assume that both the encoder and decoder are one-layer LSTMs. In general,\nhow many terms are on the shortest backpropagation path from (cid:96)(t)\n\nm+1 to h(s)\nn ?\n\n7. Now consider the neural attentional model from \xc2\xa7 18.3.1, with sigmoid attention.\nThe derivative \xe2\x88\x82(cid:96)(t)\nis the sum of many paths through the computation graph;\nidentify the shortest such path. You may assume that the initial state of the decoder\nrecurrence h(t)\n0\n\nis not tied to the \xef\xac\x81nal state of the encoder recurrence h(s)\n\nm+1\n\xe2\x88\x82zn\n\nM (s).\n\nUnder contract with MIT Press, shared under CC-BY-NC-ND license.\n\n\x0c456\n\nCHAPTER 18. MACHINE TRANSLATION\n\n8. Apply byte-pair encoding for the vocabulary it, unit, unite, until no bigram appears\n\nmore than once.\n\n9. This problem relates to the complexity of machine translation. Suppose you have\nan oracle that returns the list of words to include in the translation, so that your\nonly task is to order the words. Furthermore, suppose that the scoring function\nm\xe2\x88\x921). Show that the problem\nof \xef\xac\x81nding the optimal translation is NP-complete, by reduction from a well-known\nproblem.\n\nover orderings is a sum over bigrams,(cid:80)M\n\nm=1 \xcf\x88(w(t)\n\nm , w(t)\n\n10. Hand-design an attentional recurrent translation model that simply copies the input\nfrom the source to the target. You may assume an arbitrarily large hidden state, and\nyou may assume that there is a \xef\xac\x81nite maximum input length M. Specify all the\nweights such that the maximum probability translation of any source is the source\nitself. Hint: it is simplest to use the Elman recurrence hm = f (\xce\x98hm\xe2\x88\x921 + xm) rather\nthan an LSTM.\n\n11. Give a synchronized derivation (\xc2\xa7 18.2.4) for the Spanish-English translation,\n\n(18.6) El\n\npez\n\xef\xac\x81sh\n\nenojado\nangry\n\natacado.\nThe\nattacked.\nThe angry \xef\xac\x81sh attacked.\n\nAs above, the second line shows a word-for-word gloss, and the third line shows\nthe desired translation. Use the synchronized production rule in [18.22], and design\nthe other production rules necessary to derive this sentence pair. You may derive\n(atacado, attacked) directly from VP.\n\nJacob Eisenstein. Draft of November 13, 2018.\n\n\x0c'