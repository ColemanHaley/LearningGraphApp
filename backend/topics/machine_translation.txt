Chapter 18 Machine translation Machine translation MT is one of the holy grail problems in artificial intelligence with the potential to transform society by facilitating communication between people anywhere in the world As a result MT has received significant attention and funding since the early 1950s However it has proved remarkably challenging and while there has been substantial progress towards usable MT systems especially for highresource language pairs like EnglishFrench we are still far from translation systems that match the nuance and depth of human translations 181 Machine translation as a task Machine translation can be formulated as an optimization problem ŵt argmax wt Ψwswt 181 where ws is a sentence in a source language wt is a sentence in the target language and Ψ is a scoring function As usual this formalism requires two components a decod ing algorithm for computing ŵt and a learning algorithm for estimating the parameters of the scoring function Ψ Decoding is difficult for machine translation because of the huge space of possible translations We have faced large label spaces before for example in sequence labeling the set of possible label sequences is exponential in the length of the input In these cases it was possible to search the space quickly by introducing locality assumptions for ex ample that each tag depends only on its predecessor or that each production depends only on its parent In machine translation no such locality assumptions seem possible human translators reword reorder and rearrange words they replace single words with multiword phrases and vice versa This flexibility means that in even relatively simple 431 432 CHAPTER 18 MACHINE TRANSLATION source target text syntax semantics interlingua Figure 181 The Vauquois Pyramid translation models decoding is NPhard Knight 1999 Approaches for dealing with this complexity are described in 184 Estimating translation models is difficult as well Labeled translation data usually comes in the form parallel sentences eg ws A Vinay le gusta las manzanas wt Vinay likes apples A useful feature function would note the translation pairs gusta likes manzanas apples and even Vinay Vinay But this wordtoword alignment is not given in the data One solution is to treat this alignment as a latent variable this is the approach taken by clas sical statistical machine translation SMT systems described in 182 Another solution is to model the relationship between wt and ws through a more complex and expres sive function this is the approach taken by neural machine translation NMT systems described in 183 The Vauquois Pyramid is a theory of how translation should be done At the lowest level the translation system operates on individual words but the horizontal distance at this level is large because languages express ideas differently If we can move up the triangle to syntactic structure the distance for translation is reduced we then need only produce targetlanguage text from the syntactic representation which can be as simple as reading off a tree Further up the triangle lies semantics translating between semantic representations should be easier still but mapping between semantics and surface text is a difficult unsolved problem At the top of the triangle is interlingua a semantic represen tation that is so generic that it is identical across all human languages Philosophers de bate whether such a thing as interlingua is really possible eg Derrida 1985 While the firstorder logic representations discussed in chapter 12 might be thought to be language independent they are built on an inventory of predicates that are suspiciously similar to English words Nirenburg and Wilks 2001 Nonetheless the idea of linking translation Jacob Eisenstein Draft of November 13 2018 181 MACHINE TRANSLATION AS A TASK 433 Adequate Fluent To Vinay it like Python yes no Vinay debugs memory leaks no yes Vinay likes Python yes yes Table 181 Adequacy and fluency for translations of the Spanish sentence A Vinay le gusta Python and semantic understanding may still be a promising path if the resulting translations better preserve the meaning of the original text 1811 Evaluating translations There are two main criteria for a translation summarized in Table 181 Adequacy The translation wt should adequately reflect the linguistic content of ws For example if ws A Vinay le gusta Python the reference translation is wt Vinay likes Python However the gloss or wordforword translation wt To Vinay it like Python is also considered adequate because it contains all the relevant content The output wt Vinay debugs memory leaks is not adequate Fluency The translation wt should read like fluent text in the target language By this criterion the gloss wt To Vinay it like Python will score poorly and wt Vinay debugs memory leaks will be preferred Automated evaluations of machine translations typically merge both of these criteria by comparing the system translation with one or more reference translations produced by professional human translators The most popular quantitative metric is BLEU bilin gual evaluation understudy Papineni et al 2002 which is based on ngram precision what fraction of ngrams in the system translation appear in the reference Specifically for each ngram length the precision is defined as pn number of ngrams appearing in both reference and hypothesis translations number of ngrams appearing in the hypothesis translation 182 The ngram precisions for three hypothesis translations are shown in Figure 182 The BLEU score is then based on the average exp 1N N n1 log pn Two modifications of Equation 182 are necessary 1 to avoid computing log 0 all precisions are smoothed to ensure that they are positive 2 each ngram in the reference can be used at most once so that to to to to to to does not achieve p1 1 against the reference to be or not to be Furthermore precisionbased metrics are biased in favor of short translations which Under contract with MIT Press shared under CCBYNCND license 434 CHAPTER 18 MACHINE TRANSLATION Translation p1 p2 p3 p4 BP BLEU Reference Vinay likes programming in Python Sys1 To Vinay it like to program Python 27 0 0 0 1 21 Sys2 Vinay likes Python 33 1 2 0 0 51 33 Sys3 Vinay likes programming in his pajamas 46 3 5 2 4 1 3 1 76 Figure 182 A reference translation and three system outputs For each output pn indi cates the precision at each ngram and BP indicates the brevity penalty can achieve high scores by minimizing the denominator in 182 To avoid this issue a brevity penalty is applied to translations that are shorter than the reference This penalty is indicated as BP in Figure 182 Automated metrics like BLEU have been validated by correlation with human judg ments of translation quality Nonetheless it is not difficult to construct examples in which the BLEU score is high yet the translation is disfluent or carries a completely different meaning from the original To give just one example consider the problem of translating pronouns Because pronouns refer to specific entities a single incorrect pronoun can oblit erate the semantics of the original sentence Existing stateoftheart systems generally do not attempt the reasoning necessary to correctly resolve pronominal anaphora Hard meier 2012 Despite the importance of pronouns for semantics they have a marginal impact on BLEU which may help to explain why existing systems do not make a greater effort to translate them correctly Fairness and bias The problem of pronoun translation intersects with issues of fairness and bias In many languages such as Turkish the third person singular pronoun is gender neutral Todays stateoftheart systems produce the following TurkishEnglish transla tions Caliskan et al 2017 181 O He bir is a doktor doctor 182 O She bir is a hemşire nurse The same problem arises for other professions that have stereotypical genders such as engineers soldiers and teachers and for other languages that have genderneutral pro nouns This bias was not directly programmed into the translation model it arises from statistical tendencies in existing datasets This highlights a general problem with data driven approaches which can perpetuate biases that negatively impact disadvantaged Jacob Eisenstein Draft of November 13 2018 181 MACHINE TRANSLATION AS A TASK 435 groups Worse machine learning can amplify biases in data Bolukbasi et al 2016 if a dataset has even a slight tendency towards men as doctors the resulting translation model may produce translations in which doctors are always he and nurses are always she Other metrics A range of other automated metrics have been proposed for machine translation One potential weakness of BLEU is that it only measures precision METEOR is a weighted F MEASURE which is a combination of recall and precision see 441 Translation Error Rate TER computes the string edit distance see 914 between the reference and the hypothesis Snover et al 2006 For language pairs like English and Japanese there are substantial differences in word order and word order errors are not sufficiently captured by ngram based metrics The RIBES metric applies rank correla tion to measure the similarity in word order between the system and reference transla tions Isozaki et al 2010 1812 Data Datadriven approaches to machine translation rely primarily on parallel corpora which are translations at the sentence level Early work focused on government records in which finegrained official translations are often required For example the IBM translation sys tems were based on the proceedings of the Canadian Parliament called Hansards which are recorded in English and French Brown et al 1990 The growth of the European Union led to the development of the EuroParl corpus which spans 21 European lan guages Koehn 2005 While these datasets helped to launch the field of machine transla tion they are restricted to narrow domains and a formal speaking style limiting their ap plicability to other types of text As more resources are committed to machine translation new translation datasets have been commissioned This has broadened the scope of avail able data to news1 movie subtitles2 social media Ling et al 2013 dialogues Fordyce 2007 TED talks Paul et al 2010 and scientific research articles Nakazawa et al 2016 Despite this growing set of resources the main bottleneck in machine translation data is the need for parallel corpora that are aligned at the sentence level Many languages have sizable parallel corpora with some highresource language but not with each other The highresource language can then be used as a pivot or bridge Boitet 1988 Utiyama and Isahara 2007 for example De Gispert and Marino 2006 use Spanish as a bridge for translation between Catalan and English For most of the 6000 languages spoken today the only source of translation data remains the JudeoChristian Bible Resnik et al 1999 While relatively small at less than a million tokens the Bible has been translated into more than 2000 languages far outpacing any other corpus Some research has explored 1httpscatalogldcupenneduLDC2010T10 httpwwwstatmtorgwmt15 translationtaskhtml 2httpopusnlpleu Under contract with MIT Press shared under CCBYNCND license 436 CHAPTER 18 MACHINE TRANSLATION the possibility of automatically identifying parallel sentence pairs from unaligned parallel texts such as web pages and Wikipedia articles Kilgarriff and Grefenstette 2003 Resnik and Smith 2003 Adafre and De Rijke 2006 Another approach is to create large parallel corpora through crowdsourcing Zaidan and CallisonBurch 2011 182 Statistical machine translation The previous section introduced adequacy and fluency as the two main criteria for ma chine translation A natural modeling approach is to represent them with separate scores Ψwswt ΨAw swt ΨF w t 183 The fluency score ΨF need not even consider the source sentence it only judges wt on whether it is fluent in the target language This decomposition is advantageous because it makes it possible to estimate the two scoring functions on separate data While the adequacy model must be estimated from aligned sentences which are relatively expen sive and rare the fluency model can be estimated from monolingual text in the target language Large monolingual corpora are now available in many languages thanks to resources such as Wikipedia An elegant justification of the decomposition in Equation 183 is provided by the noisy channel model in which each scoring function is a log probability ΨAw swt log pST w s wt 184 ΨF w t log pT w t 185 Ψwswt log pST w s wt log pT wt log pST wswt 186 By setting the scoring functions equal to the logarithms of the prior and likelihood their sum is equal to log pST which is the logarithm of the joint probability of the source and target The sentence ŵt that maximizes this joint probability is also the maximizer of the conditional probability pT S making it the most likely target language sentence condi tioned on the source The noisy channel model can be justified by a generative story The target text is orig inally generated from a probability model pT It is then encoded in a noisy channel pST which converts it to a string in the source language In decoding we apply Bayes rule to recover the string wt that is maximally likely under the conditional probability pT S Under this interpretation the target probability pT is just a language model and can be estimated using any of the techniques from chapter 6 The only remaining learning problem is to estimate the translation model pST Jacob Eisenstein Draft of November 13 2018 182 STATISTICAL MACHINE TRANSLATION 437 A V in ay le gu st a py th on Vinay likes python Figure 183 An example wordtoword alignment 1821 Statistical translation modeling The simplest decomposition of the translation model is wordtoword each word in the source should be aligned to a word in the translation This approach presupposes an alignment Awswt which contains a list of pairs of source and target tokens For example given ws A Vinay le gusta Python and wt Vinay likes Python one possible wordtoword alignment is Awswt A Vinay Vinay le likes gusta likes PythonPython 187 This alignment is shown in Figure 183 Another less promising alignment is Awswt A Vinay Vinay likes le Python gusta Python 188 Each alignment contains exactly one tuple for each word in the source which serves to explain how the source word could be translated from the target as required by the trans lation probability pST If no appropriate word in the target can be identified for a source word it is aligned to as is the case for the Spanish function word a in the example which glosses to the English word to Words in the target can align with multiple words in the source so that the target word likes can align to both le and gusta in the source The joint probability of the alignment and the translation can be defined conveniently as pwsA wt Ms m1 pwsm am wtam mM sM t 189 Ms m1 pam mM sM t pwsm wtam 1810 This probability model makes two key assumptions Under contract with MIT Press shared under CCBYNCND license 438 CHAPTER 18 MACHINE TRANSLATION The alignment probability factors across tokens pA wswt Ms m1 pam mM sM t 1811 This means that each alignment decision is independent of the others and depends only on the index m and the sentence lengths M s and M t The translation probability also factors across tokens pws wtA Ms m1 pwsm wtam 1812 so that each word inws depends only on its aligned word inwt This means that translation is wordtoword ignoring context The hope is that the target language model pwt will correct any disfluencies that arise from wordtoword translation To translate with such a model we could sum or max over all possible alignments pwswt A pwswtA 1813 pwt A pA pws wtA 1814 pwt max A pA pws wtA 1815 The term pA defines the prior probability over alignments A series of alignment models with increasingly relaxed independence assumptions was developed by researchers at IBM in the 1980s and 1990s known as IBM Models 16 Och and Ney 2003 IBM Model 1 makes the strongest independence assumption pam mM sM t 1 M t 1816 In this model every alignment is equally likely This is almost surely wrong but it re sults in a convex learning objective yielding a good initialization for the more complex alignment models Brown et al 1993 Koehn 2009 1822 Estimation Let us define the parameter θuv as the probability of translating target word u to source word v If wordtoword alignments were annotated these probabilities could be com puted from relative frequencies θ̂uv countu v countu 1817 Jacob Eisenstein Draft of November 13 2018 182 STATISTICAL MACHINE TRANSLATION 439 where countu v is the count of instances in which word v was aligned to word u in the training set and countu is the total count of the target word u The smoothing techniques mentioned in chapter 6 can help to reduce the variance of these probability estimates Conversely if we had an accurate translation model we could estimate the likelihood of each alignment decision qmam wswt pam mM sM t pwsm wtam 1818 where qmam wswt is a measure of our confidence in aligning source word wsm to target word wtam The relative frequencies could then be computed from the expected counts θ̂uv Eq countu v countu 1819 Eq countu v m qmam wswt δwsm v δwtam u 1820 The expectationmaximization EM algorithm proceeds by iteratively updating qm and Θ̂ The algorithm is described in general form in chapter 5 For statistical machine translation the steps of the algorithm are 1 Estep Update beliefs about word alignment using Equation 1818 2 Mstep Update the translation model using Equations 1819 and 1820 As discussed in chapter 5 the expectation maximization algorithm is guaranteed to con verge but not to a global optimum However for IBM Model 1 it can be shown that EM optimizes a convex objective and global optimality is guaranteed For this reason IBM Model 1 is often used as an initialization for more complex alignment models For more detail see Koehn 2009 1823 Phrasebased translation Real translations are not wordtoword substitutions One reason is that many multiword expressions are not translated literally as shown in this example from French 183 Nous We allons will prendre take un a verre glass Well have a drink Under contract with MIT Press shared under CCBYNCND license 440 CHAPTER 18 MACHINE TRANSLATION N ou s al lo ns pr en dr e un e ve rr e Well have a drink Figure 184 A phrasebased alignment between French and English corresponding to example 183 The line we will take a glass is the wordforword gloss of the French sentence the transla tion well have a drink is shown on the third line Such examples are difficult for wordto word translation models since they require translating prendre to have and verre to drink These translations are only correct in the context of these specific phrases Phrasebased translation generalizes on wordbased models by building translation tables and alignments between multiword spans These phrases are not necessarily syntactic constituents like the noun phrases and verb phrases described in chapters 9 and 10 The generalization from wordbased translation is surprisingly straightforward the translation tables can now condition on multiword units and can assign probabilities to multiword units alignments are mappings from spans to spans i j k so that pws wtA ijkA pwswtw s i1 w s i2 w s j w t k1 w t k2 w t 1821 The phrase alignment i j k indicates that the span wsi1j is the translation of the span wtk1 An example phrasal alignment is shown in Figure 184 Note that the align ment set A is required to cover all of the tokens in the source just as in wordbased trans lation The probability model pwswt must now include translations for all phrase pairs which can be learned from expectationmaximization just as in wordbased statistical ma chine translation Jacob Eisenstein Draft of November 13 2018 182 STATISTICAL MACHINE TRANSLATION 441 1824 Syntaxbased translation The Vauquois Pyramid Figure 181 suggests that translation might be easier if we take a higherlevel view One possibility is to incorporate the syntactic structure of the source the target or both This is particularly promising for language pairs that consistent syn tactic differences For example English adjectives almost always precede the nouns that they modify while in Romance languages such as French and Spanish the adjective often follows the noun thus angry fish would translate to pez fish enojado angry in Spanish In wordtoword translation these reorderings cause the alignment model to be overly permissive It is not that the order of any pair of English words can be reversed when translating into Spanish but only adjectives and nouns within a noun phrase Similar issues arise when translating between verbfinal languages such as Japanese in which verbs usually follow the subject and object verbinitial languages like Tagalog and clas sical Arabic and verbmedial languages such as English An elegant solution is to link parsing and translation in a synchronous contextfree grammar SCFG Chiang 20073 An SCFG is a set of productions of the formX α β where X is a nonterminal α and β are sequences of terminals or nonterminals and is a onetoone alignment of items in α with items in β EnglishSpanish adjectivenoun ordering can be handled by a set of synchronous productions eg NP DET1 NN2 JJ3 DET1 JJ3 NN2 1822 with subscripts indicating the alignment between the Spanish left and English right parts of the righthand side Terminal productions yield translation pairs JJ enojado1 angry1 1823 A synchronous derivation begins with the start symbol S and derives a pair of sequences of terminal symbols Given an SCFG in which each production yields at most two symbols in each lan guage Chomsky Normal Form see 921 a sentence can be parsed using only the CKY algorithm chapter 10 The resulting derivation also includes productions in the other language all the way down to the surface form Therefore SCFGs make translation very similar to parsing In a weighted SCFG the log probability log pST can be computed from the sum of the logprobabilities of the productions However combining SCFGs with a target language model is computationally expensive necessitating approximate search algorithms Huang and Chiang 2007 Synchronous contextfree grammars are an example of treetotree translation be cause they model the syntactic structure of both the target and source language In string totree translation string elements are translated into constituent tree fragments which 3Earlier approaches to syntactic machine translation includes syntaxdriven transduction Lewis II and Stearns 1968 and stochastic inversion transduction grammars Wu 1997 Under contract with MIT Press shared under CCBYNCND license 442 CHAPTER 18 MACHINE TRANSLATION are then assembled into a translation Yamada and Knight 2001 Galley et al 2004 in treetostring translation the source side is parsed and then transformed into a string on the target side Liu et al 2006 A key question for syntaxbased translation is the extent to which we phrasal constituents align across translations Fox 2002 because this gov erns the extent to which we can rely on monolingual parsers and treebanks For more on syntaxbased machine translation see the monograph by Williams et al 2016 183 Neural machine translation Neural network models for machine translation are based on the encoderdecoder archi tecture Cho et al 2014 The encoder network converts the source language sentence into a vector or matrix representation the decoder network then converts the encoding into a sentence in the target language z ENCODEws 1824 wt ws DECODEz 1825 where the second line means that the function DECODEz defines the conditional proba bility pwt ws The decoder is typically a recurrent neural network which generates the target lan guage sentence one word at a time while recurrently updating a hidden state The en coder and decoder networks are trained endtoend from parallel sentences If the output layer of the decoder is a logistic function then the entire architecture can be trained to maximize the conditional loglikelihood log pwt ws Mt m1 pwtm wt1m1 z 1826 pwtm wt1m1ws exp β w t m htm1 1827 where the hidden state htm1 is a recurrent function of the previously generated text w t 1m1 and the encoding z The second line is equivalent to writing wtm wt1m1ws SoftMax β htm1 1828 where β RV tK is the matrix of output word vectors for the V t words in the target language vocabulary The simplest encoderdecoder architecture is the sequencetosequence model Sutskever et al 2014 In this model the encoder is set to the final hidden state of a long shortterm Jacob Eisenstein Draft of November 13 2018 183 NEURAL MACHINE TRANSLATION 443 h sD m1 h sD m h sD m1 h s2 m1 h s2 m h s2 m1 h s1 m1 h s1 m h s1 m1 x s m1 x s m x s m1 Figure 185 A deep bidirectional LSTM encoder memory LSTM see 633 on the source sentence hsm LSTMx s m h s m1 1829 z hs Ms 1830 where xsm is the embedding of source language word w s m The encoding then provides the initial hidden state for the decoder LSTM h t 0 z 1831 htm LSTMx t m h t m1 1832 where xtm is the embedding of the target language word w t m Sequencetosequence translation is nothing more than wiring together two LSTMs one to read the source and another to generate the target To make the model work well some additional tweaks are needed Most notably the model works much better if the source sentence is reversed read ing from the end of the sentence back to the beginning In this way the words at the beginning of the source have the greatest impact on the encoding z and therefore impact the words at the beginning of the target sentence Later work on more ad vanced encoding models such as neural attention see 1831 has eliminated the need for reversing the source sentence The encoder and decoder can be implemented as deep LSTMs with multiple layers of hidden states As shown in Figure 185 each hidden state hsim at layer i is treated Under contract with MIT Press shared under CCBYNCND license 444 CHAPTER 18 MACHINE TRANSLATION as the input to an LSTM at layer i 1 hs1m LSTMx s m h s m1 1833 hsi1m LSTMh si m h si1 m1 i 1 1834 The original work on sequencetosequence translation used four layers in 2016 Googles commercial machine translation system used eight layers Wu et al 20164 Significant improvements can be obtained by creating an ensemble of translation models each trained from a different random initialization For an ensemble of size N the pertoken decoding probability is set equal to pwt zwt1m1 1 N N i1 piw t zwt1m1 1835 where pi is the decoding probability for model i Each translation model in the ensemble includes its own encoder and decoder networks The original sequencetosequence model used a fairly standard training setup stochas tic gradient descent with an exponentially decreasing learning rate after the first five epochs minibatches of 128 sentences chosen to have similar length so that each sentence on the batch will take roughly the same amount of time to process gradi ent clipping see 334 to ensure that the norm of the gradient never exceeds some predefined value 1831 Neural attention The sequencetosequence model discussed in the previous section was a radical depar ture from statistical machine translation in which each word or phrase in the target lan guage is conditioned on a single word or phrase in the source language Both approaches have advantages Statistical translation leverages the idea of compositionality transla tions of large units should be based on the translations of their component parts and this seems crucial if we are to scale translation to longer units of text But the translation of each word or phrase often depends on the larger context and encoderdecoder models capture this context at the sentence level Is it possible for translation to be both contextualized and compositional One ap proach is to augment neural translation with an attention mechanism The idea of neural attention was described in 175 but its application to translation bears further discus sion In general attention can be thought of as using a query to select from a memory of keyvalue pairs However the query keys and values are all vectors and the entire 4Google reports that this system took six days to train for EnglishFrench translation using 96 NVIDIA K80 GPUs which would have cost roughly half a million dollars at the time Jacob Eisenstein Draft of November 13 2018 183 NEURAL MACHINE TRANSLATION 445 Output activation α Query ψα Key Value Figure 186 A general view of neural attention The dotted box indicates that each αmn can be viewed as a gate on value n operation is differentiable For each key n in the memory we compute a score ψαmn with respect to the query m That score is a function of the compatibility of the key and the query and can be computed using a small feedforward neural network The vector of scores is passed through an activation function such as softmax The output of this activation function is a vector of nonnegative numbers αm1 αm2 αmN with length N equal to the size of the memory Each value in the memory vn is multiplied by the attention αmn the sum of these scaled values is the output This process is shown in Figure 186 In the extreme case that αmn 1 and αmn 0 for all other n then the attention mechanism simply selects the value vn from the memory Neural attention makes it possible to integrate alignment into the encoderdecoder ar chitecture Rather than encoding the entire source sentence into a fixed length vector z it can be encoded into a matrix Z RKMS where K is the dimension of the hidden state and M S is the number of tokens in the source input Each column of Z represents the state of a recurrent neural network over the source sentence These vectors are con structed from a bidirectional LSTM see 76 which can be a deep network as shown in Figure 185 These columns are both the keys and the values in the attention mechanism At each step m in decoding the attentional state is computed by executing a query which is equal to the state of the decoder htm The resulting compatibility scores are ψαmn vα tanhΘαhtm hsn 1836 The function ψ is thus a two layer feedforward neural network with weights vα on the output layer and weights Θα on the input layer To convert these scores into atten tion weights we apply an activation function which can be vectorwise softmax or an elementwise sigmoid Softmax attention αmn expψαmnMs n1 expψαmn 1837 Under contract with MIT Press shared under CCBYNCND license 446 CHAPTER 18 MACHINE TRANSLATION Sigmoid attention αmn σ ψαmn 1838 The attention α is then used to compute a context vector cm by taking a weighted average over the columns of Z cm Ms n1 αmnzn 1839 where αmn 0 1 is the amount of attention from word m of the target to word n of the source The context vector can be incorporated into the decoders word output probability model by adding another layer to the decoder Luong et al 2015 h̃tm tanh Θch t m cm 1840 pwtm1 w t 1mw s exp β w t m1 h̃tm 1841 Here the decoder state htm is concatenated with the context vector forming the input to compute a final output vector h̃tm The context vector can be incorporated into the decoder recurrence in a similar manner Bahdanau et al 2014 1832 Neural machine translation without recurrence In the encoderdecoder model attentions keys and values are the hidden state repre sentations in the encoder network z and the queries are state representations in the decoder network ht It is also possible to completely eliminate recurrence from neural translation by applying selfattention Lin et al 2017 Kim et al 2017 within the en coder and decoder as in the transformer architecture Vaswani et al 2017 For level i the basic equations of the encoder side of the transformer are zim Ms n1 αimnΘvh i1 n 1842 him Θ2 ReLU Θ1z i m b1 b2 1843 For each token m at level i we compute selfattention over the entire source sentence The keys values and queries are all projections of the vector hi1 for example in Equa tion 1842 the value vn is the projection Θvh i1 n The attention scores α i mn are com puted using a scaled form of softmax attention αmn expψαmnM 1844 Jacob Eisenstein Draft of November 13 2018 183 NEURAL MACHINE TRANSLATION 447 zi α i m ψ i α m hi1 m 1 m m 1 k q v Figure 187 The transformer encoders computation of zim from hi1 The key value and query are shown for token m 1 For example ψiα mm 1 is computed from the key Θkh i1 m1 and the query Θqh i1 m and the gate α i mm1 operates on the value Θvh i1 m1 The figure shows a minimal version of the architecture with a single atten tion head With multiple heads it is possible to attend to different properties of multiple words where M is the length of the input This encourages the attention to be more evenly dispersed across the input Selfattention is applied across multiple heads each using different projections of hi1 to form the keys values and queries This architecture is shown in Figure 187 The output of the selfattentional layer is the representation zim which is then passed through a twolayer feedforward network yielding the input to the next layer hi This selfattentional architecture can be applied in the decoder as well but this requires that there is zero attention to future words αmn 0 for all n m To ensure that information about word order in the source is integrated into the model the encoder augments the base layer of the network with positional encodings of the indices of each word in the source These encodings are vectors for each position m 1 2 M The transformer sets these encodings equal to a set of sinusoidal functions of m e2i1m sinm10000 2i Ke 1845 e2im cosm10000 2i Ke i 1 2 Ke2 1846 where e2im is the value at element 2i of the encoding for index m As we progress through the encoding the sinusoidal functions have progressively narrower bandwidths This enables the model to learn to attend by relative positions of words The positional encodings are concatenated with the word embeddings xm at the base layer of the model5 5The transformer architecture relies on several additional tricks including layer normalization see Under contract with MIT Press shared under CCBYNCND license 448 CHAPTER 18 MACHINE TRANSLATION Source The ecotax portico in Pontdebuis was taken down on Thursday morning Reference Le portique écotaxe de Pontdebuis a été démonté jeudi matin System Le unk de unk à unk a été pris le jeudi matin Figure 188 Translation with unknown words The system outputs unk to indicate words that are outside its vocabulary Figure adapted from Luong et al 2015 Convolutional neural networks see 34 have also been applied as encoders in neu ral machine translation Gehring et al 2017 For each wordwsm a convolutional network computes a representation hsm from the embeddings of the word and its neighbors This procedure is applied several times creating a deep convolutional network The recurrent decoder then computes a set of attention weights over these convolutional representa tions using the decoders hidden state ht as the queries This attention vector is used to compute a weighted average over the outputs of another convolutional neural network of the source yielding an averaged representation cm which is then fed into the decoder As with the transformer speed is the main advantage over recurrent encoding models another similarity is that word order information is approximated through the use of po sitional encodings6 1833 Outofvocabulary words Thus far we have treated translation as a problem at the level of words or phrases For words that do not appear in the training data all such models will struggle There are two main reasons for the presence of outofvocabulary OOV words New proper nouns such as family names or organizations are constantly arising particularly in the news domain The same is true to a lesser extent for technical terminology This issue is shown in Figure 188 In many languages words have complex internal structure known as morphology An example is German which uses compounding to form nouns like Abwasserbe handlungsanlage sewage water treatment plant example from Sennrich et al 2016 334 residual connections around the nonlinear activations see 322 and a nonmonotonic learning rate schedule 6A recent evaluation found that best performance was obtained by using a recurrent network for the decoder and a transformer for the encoder Chen et al 2018 The transformer was also found to significantly outperform a convolutional neural network Jacob Eisenstein Draft of November 13 2018 184 DECODING 449 While compounds could in principle be addressed by better tokenization see 84 other morphological processes involve more complex transformations of subword units Names and technical terms can be handled in a postprocessing step after first identi fying alignments between unknown words in the source and target we can look up each aligned source word in a dictionary and choose a replacement Luong et al 2015 If the word does not appear in the dictionary it is likely to be a proper noun and can be copied directly from the source to the target This approach can also be integrated directly into the translation model rather than applying it as a postprocessing step Jean et al 2015 Words with complex internal structure can be handled by translating subword units rather than entire words A popular technique for identifying subword units is bytepair encoding BPE Gage 1994 Sennrich et al 2016 The initial vocabulary is defined as the set of characters used in the text The most common character bigram is then merged into a new symbol the vocabulary is updated and the merging operation is applied again For example given the dictionary fish fished want wanted bike biked we would first form the subword unit ed since this character bigram appears in three of the six words Next there are several bigrams that each appear in a pair of words fi is sh wa an etc These can be merged in any order By iterating this process we eventually reach the segmentation fish fishedwantwanted bike biked At this point there are no bigrams that appear more than once In real data merging is performed until the number of subword units reaches some predefined threshold such as 104 Each subword unit is treated as a token for translation in both the encoder source side and decoder target side BPE can be applied jointly to the union of the source and target vocabularies identifying subword units that appear in both languages For lan guages that have different scripts such as English and Russian transliteration between the scripts should be applied first7 184 Decoding Given a trained translation model the decoding task is ŵt argmax wV Ψwws 1847 where wt is a sequence of tokens from the target vocabulary V It is not possible to efficiently obtain exact solutions to the decoding problem for even minimally effective 7Transliteration is crucial for converting names and other foreign words between languages that do not share a single script such as English and Japanese It is typically approached using the finitestate methods discussed in chapter 9 Knight and Graehl 1998 Under contract with MIT Press shared under CCBYNCND license 450 CHAPTER 18 MACHINE TRANSLATION models in either statistical or neural machine translation Todays stateoftheart transla tion systems use beam search see 1131 which is an incremental decoding algorithm that maintains a small constant number of competitive hypotheses Such greedy approxi mations are reasonably effective in practice and this may be in part because the decoding objective is only loosely correlated with measures of translation quality so that exact op timization of 1847 may not greatly improve the resulting translations Decoding in neural machine translation is simpler than in phrasebased statistical ma chine translation8 The scoring function Ψ is defined Ψwtws Mt m1 ψwtm w t 1m1 z 1848 ψwtw t 1m1 z βwtm htm log wV exp βw htm 1849 where z is the encoding of the source sentencews and htm is a function of the encoding z and the decoding historywt1m1 This formulation subsumes the attentional translation model where z is a matrix encoding of the source Now consider the incremental decoding algorithm ŵtm argmax wV ψw ŵ t 1m1 z m 1 2 1850 This algorithm selects the best target language word at position m assuming that it has already generated the sequence ŵt1m1 Termination can be handled by augmenting the vocabulary V with a special endofsequence token The incremental algorithm is likely to produce a suboptimal solution to the optimization problem defined in Equa tion 1847 because selecting the highestscoring word at position m can set the decoder on a garden path in which there are no good choices at some later position n m We might hope for some dynamic programming solution as in sequence labeling 73 But the Viterbi algorithm and its relatives rely on a Markov decomposition of the objective function into a sum of local scores for example scores can consider locally adjacent tags ym ym1 but not the entire tagging history y1m This decomposition is not applicable to recurrent neural networks because the hidden state htm is impacted by the entire his tory wt1m this sensitivity to longrange context is precisely what makes recurrent neural networks so effective9 In fact it can be shown that decoding from any recurrent neural network is NPcomplete Siegelmann and Sontag 1995 Chen et al 2018 8For more on decoding in phrasebased statistical models see Koehn 2009 9Note that this problem does not impact RNNbased sequence labeling models see 76 This is because the tags produced by these models do not affect the recurrent state Jacob Eisenstein Draft of November 13 2018 185 TRAINING TOWARDS THE EVALUATION METRIC 451 Beam search Beam search is a general technique for avoiding search errors when ex haustive search is impossible it was first discussed in 1131 Beam search can be seen as a variant of the incremental decoding algorithm sketched in Equation 1850 but at each step m a set of K different hypotheses are kept on the beam For each hypothesis k 1 2 K we compute both the current scoreMtm1 ψw t kmw t k1m1 z as well as the current hidden state htk At each step in the beam search the K topscoring children of each hypothesis currently on the beam are expanded and the beam is updated For a detailed description of beam search for RNN decoding see Graves 2012 Learning and search Conventionally the learning algorithm is trained to predict the right token in the translation conditioned on the translation history being correct But if decoding must be approximate then we might do better by modifying the learning algorithm to be robust to errors in the translation history Scheduled sampling does this by training on histories that sometimes come from the ground truth and sometimes come from the models own output Bengio et al 201510 As training proceeds the training wheels come off we increase the fraction of tokens that come from the model rather than the ground truth Another approach is to train on an objective that relates directly to beam search performance Wiseman et al 2016 Reinforcement learning has also been applied to decoding of RNNbased translation models making it possible to directly optimize translation metrics such as BLEU Ranzato et al 2016 185 Training towards the evaluation metric In likelihoodbased training the objective is the maximize the probability of a parallel corpus However translations are not evaluated in terms of likelihood metrics like BLEU consider only the correctness of a single output translation and not the range of prob abilities that the model assigns It might therefore be better to train translation models to achieve the highest BLEU score possible to the extent that we believe BLEU mea sures translation quality Unfortunately BLEU and related metrics are not friendly for optimization they are discontinuous nondifferentiable functions of the parameters of the translation model Consider an error function ŵtwt which measures the discrepancy between the system translation ŵt and the reference translationwt this function could be based on BLEU or any other metric on translation quality One possible criterion would be to select 10Scheduled sampling builds on earlier work on learning to search Daumé III et al 2009 Ross et al 2011 which are also described in 1524 Under contract with MIT Press shared under CCBYNCND license 452 CHAPTER 18 MACHINE TRANSLATION the parameters θ that minimize the error of the systems preferred translation ŵt argmax wt Ψwtwsθ 1851 θ̂ argmin θ ŵtws 1852 However identifying the topscoring translation ŵt is usually intractable as described in the previous section In minimum errorrate training MERT ŵt is selected from a set of candidate translations Yws this is typically a strict subset of all possible transla tions so that it is only possible to optimize an approximation to the true error rate Och and Ney 2003 A further issue is that the objective function in Equation 1852 is discontinuous and nondifferentiable due to the argmax over translations an infinitesimal change in the parameters θ could cause another translation to be selected with a completely different error To address this issue we can instead minimize the risk which is defined as the expected error rate Rθ Eŵtwsθŵ twt 1853 ŵtYws pŵt wsŵtwt 1854 Minimum risk training minimizes the sum ofRθ across all instances in the training set The risk can be generalized by exponentiating the translation probabilities p̃wtθ α pwt wsθ α 1855 R̃θ ŵtYws p̃ŵt wsαθŵtwt 1856 where Yws is now the set of all possible translations forws Exponentiating the prob abilities in this way is known as annealing Smith and Eisner 2006 When α 1 then R̃θ Rθ when α then R̃θ is equivalent to the sum of the errors of the maxi mum probability translations for each sentence in the dataset Clearly the set of candidate translations Yws is too large to explicitly sum over Because the error function generally does not decompose into smaller parts there is no efficient dynamic programming solution to sum over this set We can approximate the sum ŵtYws with a sum over a finite number of samples w t 1 w t 2 w t K If these samples were drawn uniformly at random then the annealed risk would be Jacob Eisenstein Draft of November 13 2018 185 TRAINING TOWARDS THE EVALUATION METRIC 453 approximated as Shen et al 2016 R̃θ 1 Z K k1 p̃w t k wsθ αw t k w t 1857 Z K k1 p̃w t k wsθ α 1858 Shen et al 2016 report that performance plateaus at K 100 for minimum risk training of neural machine translation Uniform sampling over the set of all possible translations is undesirable because most translations have very low probability A solution from Monte Carlo estimation is impor tance sampling in which we draw samples from a proposal distribution qws This distribution can be set equal to the current translation model pwt wsθ Each sam ple is then weighted by an importance score ωk p̃w t k w s qw t k w s The effect of this weighting is to correct for any mismatch between the proposal distribution q and the true distribu tion p̃ The risk can then be approximated as w t k qws 1859 ωk p̃w t k ws qw t k w s 1860 R̃θ 1K k1 ωk K k1 ωk wtk wt 1861 Importance sampling will generally give a more accurate approximation than uniform sampling The only formal requirement is that the proposal assigns nonzero probability to every wt Yws For more on importance sampling and related methods see Robert and Casella 2013 Additional resources A complete textbook on machine translation is available from Koehn 2009 While this book precedes recent work on neural translation a more recent draft chapter on neural translation models is also available Koehn 2017 Neubig 2017 provides a compre hensive tutorial on neural machine translation starting from first principles The course notes from Cho 2015 are also useful Several neural machine translation libraries are available LAMTRAM is an implementation of neural machine translation in DYNET Neu big et al 2017 OPENNMT Klein et al 2017 and FAIRSEQ are available in PYTORCH Under contract with MIT Press shared under CCBYNCND license 454 CHAPTER 18 MACHINE TRANSLATION TENSOR2TENSOR is an implementation of several of the Google translation models in TEN SORFLOW Abadi et al 2016 Literary translation is especially challenging even for expert human translators Mes sud 2014 describes some of these issues in her review of an English translation of Létranger the 1942 French novel by Albert Camus11 She compares the new translation by Sandra Smith against earlier translations by Stuart Gilbert and Matthew Ward focusing on the difficulties presented by a single word in the first sentence Then too Smith has reconsidered the books famous opening Camuss original is deceptively simple Aujourdhui maman est morte Gilbert influ enced generations by offering us Mother died todayinscribing in Meur sault the narrator from the outset a formality that could be construed as heartlessness But maman after all is intimate and affectionate a childs name for his mother Matthew Ward concluded that it was essentially untranslatable mom or mummy being not quite apt and left it in the original French Maman died today There is a clear logic in this choice but as Smith has explained in an interview in The Guardian maman didnt really tell the reader anything about the connotation She instead has translated the sentence as My mother died today I chose My mother because I thought about how someone would tell another person that his mother had died Meursault is speaking to the reader directly My mother died today seemed to me the way it would work and also implied the closeness of maman you get in the French Elsewhere in the book she has translated maman as mama again striving to come as close as possible to an actual colloquial word that will carry the same connotations as maman does in French The passage is a reminder that while the quality of machine translation has improved dramatically in recent years expert human translations draw on considerations that are beyond the ken of any contemporary computational approach Exercises 1 Using Google translate or another online service translate the following example into two different languages of your choice 11The book review is currently available online at httpwwwnybookscomarticles201406 05camusnewletranger Jacob Eisenstein Draft of November 13 2018 185 TRAINING TOWARDS THE EVALUATION METRIC 455 184 It is not down on any map true places never are Then translate each result back into English Which is closer to the original Can you explain the differences 2 Compute the unsmoothed ngram precisions p1 p4 for the two backtranslations in the previous problem using the original source as the reference Your ngrams should include punctuation and you should segment conjunctions like its into two tokens 3 You are given the following dataset of translations from simple to difficult En glish 185 a Kids Children like adore cats felines b Cats Felines hats fedoras Estimate a wordtoword statistical translation model from simple English source to difficult English target using the expectationmaximization as described in 1822 Compute two iterations of the algorithm by hand starting from a uniform transla tion model and using the simple alignment model pam mM sM t 1Mt Hint in the final Mstep you will want to switch from fractions to decimals 4 Building on the previous problem what will be the converged translation proba bility table Can you state a general condition about the data under which this translation model will fail in the way that it fails here 5 Propose a simple alignment model that would make it possible to recover the correct translation probabilities from the toy dataset in the previous two problems 6 Let tm1 represent the loss at wordm1 of the target and let h s n represent the hid den state at word n of the source Write the expression for the derivative t m1 h s n in the sequencetosequence translation model expressed in Equations 18291832 You may assume that both the encoder and decoder are onelayer LSTMs In general how many terms are on the shortest backpropagation path from tm1 to h s n 7 Now consider the neural attentional model from 1831 with sigmoid attention The derivative t m1 zn is the sum of many paths through the computation graph identify the shortest such path You may assume that the initial state of the decoder recurrence ht0 is not tied to the final state of the encoder recurrence h s Ms Under contract with MIT Press shared under CCBYNCND license 456 CHAPTER 18 MACHINE TRANSLATION 8 Apply bytepair encoding for the vocabulary it unit unite until no bigram appears more than once 9 This problem relates to the complexity of machine translation Suppose you have an oracle that returns the list of words to include in the translation so that your only task is to order the words Furthermore suppose that the scoring function over orderings is a sum over bigrams M m1 ψw t m w t m1 Show that the problem of finding the optimal translation is NPcomplete by reduction from a wellknown problem 10 Handdesign an attentional recurrent translation model that simply copies the input from the source to the target You may assume an arbitrarily large hidden state and you may assume that there is a finite maximum input length M Specify all the weights such that the maximum probability translation of any source is the source itself Hint it is simplest to use the Elman recurrence hm fΘhm1 xm rather than an LSTM 11 Give a synchronized derivation 1824 for the SpanishEnglish translation 186 El The pez fish enojado angry atacado attacked The angry fish attacked As above the second line shows a wordforword gloss and the third line shows the desired translation Use the synchronized production rule in 1822 and design the other production rules necessary to derive this sentence pair You may derive atacado attacked directly from VP Jacob Eisenstein Draft of November 13 2018