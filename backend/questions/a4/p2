2 . Analyzing NMT Systems ( 30 points ) ( a ) ( 12 points ) Here we present a series of errors we found in the outputs 2 of our NMT model ( which is the same as the one you just trained ) . For each example of a Spanish source sentence , reference ( i.e . , ‘ gold ’ ) English translation , and NMT ( i.e . , ‘ model ’ ) English translation , please : 1 . Identify the error in the NMT translation . 2 . Provide possible reason ( s ) why the model may have made the error ( either due to a specific linguistic construct or a specific model limitation ) . 3 . Describe one possible way we might alter the NMT system to fix the observed error . There are more than one possible fixes for an error . For example , it could be tweaking the size of the hidden layers or changing the attention mechanism . Below are the translations that you should analyze as described above . Note that out-of-vocabulary words are underlined . Rest assured that you don’t need to know Spanish to answer these questions . You just need to know English ! The Spanish words in these questions are similar enough to English that you can mostly see the alignments . If you are uncertain about some words , please feel free to use resources like Google Translate to look them up . i . ( 2 points ) Source Sentence : Aqúı otro de mis favoritos , “ La noche estrellada ” . Reference Translation : So another one of my favorites , “ The Starry Night ” . NMT Translation : Here’s another favorite of my favorites , “ The Starry Night ” . ii . ( 2 points ) Source Sentence : Ustedes saben que lo que yo hago es escribir para los niños , y , de hecho , probablemente soy el autor para niños , ms ledo en los EEUU . Reference Translation : You know , what I do is write for children , and I’m probably America’s most widely read children’s author , in fact . NMT Translation : You know what I do is write for children , and in fact , I’m probably the author for children , more reading in the U.S . iii . ( 2 points ) Source Sentence : Un amigo me hizo eso – Richard Bolingbroke . Reference Translation : A friend of mine did that – Richard Bolingbroke . NMT Translation : A friend of mine did that – Richard < unk > iv . ( 2 points ) Source Sentence : Solo tienes que dar vuelta a la manzana para verlo como una epifańıa . Reference Translation : You’ve just got to go around the block to see it as an epiphany . NMT Translation : You just have to go back to the apple to see it as an epiphany . v . ( 2 points ) Source Sentence : Ella salvó mi vida al permitirme entrar al baño de la sala de profesores . Reference Translation : She saved my life by letting me go to the bathroom in the teachers ’ lounge . NMT Translation : She saved my life by letting me go to the bathroom in the women’s room . 2The data is from TED talks . CS 224n Assignment 4 Page 6 of 7 vi . ( 2 points ) Source Sentence : Eso es más de 100,000 hectáreas . Reference Translation : That’s more than 250 thousand acres . NMT Translation : That’s over 100,000 acres . ( b ) ( 4 points ) Now it is time to explore the outputs of the model that you have trained ! The test-set translations your model produced in question 1-i should be located in outputs / test outputs . txt . Please identify 2 examples of errors that your model produced . 3 The two examples you find should be different error types from one another and different error types than the examples provided in the previous question . For each example you should : 1 . Write the source sentence in Spanish . The source sentences are in the en es data / test.es . 2 . Write the reference English translation . The reference translations are in the en es data / test . en . 3 . Write your NMT model’s English translation . The model-translated sentences are in the outputs / test outputs . txt . 4 . Identify the error in the NMT translation . 5 . Provide a reason why the model may have made the error ( either due to a specific linguistic construct or specific model limitations ) . 6 . Describe one possible way we might alter the NMT system to fix the observed error . ( c ) ( 14 points ) BLEU score is the most commonly used automatic evaluation metric for NMT systems . It is usually calculated across the entire test set , but here we will consider BLEU defined for a single example . 4 Suppose we have a source sentence s , a set of k reference translations r1 , . . . , rk , and a candidate translation c . To compute the BLEU score of c , we first compute the modified n-gram precision pn of c , for each of n = 1 , 2 , 3 , 4 , where n is the n in n-gram : pn = ∑ ngram ∈ c min ( max i = 1 , . . . , k Countri ( ngram ) , Countc ( ngram ) ) ∑ ngram ∈ c Countc ( ngram ) ( 15 ) Here , for each of the n-grams that appear in the candidate translation c , we count the maxi - mum number of times it appears in any one reference translation , capped by the number of times it appears in c ( this is the numerator ) . We divide this by the number of n-grams in c ( denominator ) . Next , we compute the brevity penalty BP . Let len ( c ) be the length of c and let len ( r ) be the length of the reference translation that is closest to len ( c ) ( in the case of two equally-close reference translation lengths , choose len ( r ) as the shorter one ) . BP = { 1 if len ( c ) ≥ len ( r ) exp ( 1 − len ( r ) len ( c ) ) otherwise ( 16 ) Lastly , the BLEU score for candidate c with respect to r1 , . . . , rk is : BLEU = BP × exp ( 4 ∑ n = 1 λn log pn ) ( 17 ) where λ1 , λ2 , λ3 , λ4 are weights that sum to 1 . The log here is natural log . 3An ‘ error ’ is not just a NMT translation that doesn’t match the reference translation . There must be something wrong with the NMT translation , in your opinion . 4This definition of sentence-level BLEU score matches the sentence bleu ( ) function in the nltk Python package . Note that the NLTK function is sensitive to capitalization . In this question , all text is lowercased , so capitalization is irrelevant . http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu CS 224n Assignment 4 Page 7 of 7 i . ( 5 points ) Please consider this example : Source Sentence s : el amor todo lo puede Reference Translation r1 : love can always find a way Reference Translation r2 : love makes anything possible NMT Translation c1 : the love can always do NMT Translation c2 : love can make anything possible Please compute the BLEU scores for c1 and c2 . Let λi = 0.5 for i ∈ { 1 , 2 } and λi = 0 for i ∈ { 3 , 4 } ( this means we ignore 3-grams and 4-grams , i.e . , don’t compute p3 or p4 ) . When computing BLEU scores , show your working ( i.e . , show your computed values for p1 , p2 , len ( c ) , len ( r ) and BP ) . Note that the BLEU scores can be expressed between 0 and 1 or between 0 and 100 . The code is using the 0 to 100 scale while in this question we are using the 0 to 1 scale . Which of the two NMT translations is considered the better translation according to the BLEU Score ? Do you agree that it is the better translation ? ii . ( 5 points ) Our hard drive was corrupted and we lost Reference Translation r2 . Please recom - pute BLEU scores for c1 and c2 , this time with respect to r1 only . Which of the two NMT translations now receives the higher BLEU score ? Do you agree that it is the better translation ? iii . ( 2 points ) Due to data availability , NMT systems are often evaluated with respect to only a single reference translation . Please explain ( in a few sentences ) why this may be problematic . iv . ( 2 points ) List two advantages and two disadvantages of BLEU , compared to human evaluation , as an evaluation metric for Machine Translation . Submission Instructions You shall submit this assignment on GradeScope as two submissions – one for “ Assignment 4 [ coding ] ” and another for ‘ Assignment 4 [ written ] ” : 1 . Run the collect submission.sh script on Azure to produce your assignment4 . zip file . You can use scp to transfer files between Azure and your local computer . 2 . Upload your assignment4 . zip file to GradeScope to “ Assignment 4 [ coding ] ” . 3 . Upload your written solutions to GradeScope to “ Assignment 4 [ written ] ” . When you submit your assignment , make sure to tag all the pages for each problem according to Gradescope’s submission directions . Points will be deducted if the submission is not correctly tagged .
