Part 1 : Count-Based Word Vectors ( 10 points ) Most word vector models start from the following idea : You shall know a word by the company it keeps ( Firth , J . R . 1957:11 ( https://en.wikipedia.org/wiki/John_Rupert_Firth ) ) Many word vector implementations are driven by the idea that similar words , i.e . , ( near ) synonyms , will be used in similar contexts . As a result , similar words will often be spoken or written along with a shared subset of words , i.e . , contexts . By examining these contexts , we can try to develop embeddings for our words . With this intuition in mind , many " old school " approaches to constructing word vectors relied on word counts . Here we elaborate upon one of those strategies , co - occurrence matrices ( for more information , see here ( http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf ) or here ( https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285 ) ) . exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 2 of 18 3/30/2020 , 9:13 PM exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 3 of 18 3/30/2020 , 9:13 PM Co-Occurrence A co-occurrence matrix counts how often things co-occur in some environment . Given some word occurring in the document , we consider the context window surrounding . Supposing our fixed window size is , then this is the preceding and subsequent words in that document , i.e . words and . We build a co - occurrence matrix , which is a symmetric word-by-word matrix in which is the number of times appears inside ' s window among all documents . Example : Co-Occurrence with Fixed Window of n = 1 : Document 1 : " all that glitters is not gold " Document 2 : " all is well that ends well " * < START > all that glitters is not gold well ends < END > < START > 0 2 0 0 0 0 0 0 0 0 all 2 0 1 0 1 0 0 0 0 0 that 0 1 0 1 0 0 0 1 1 0 glitters 0 0 1 0 1 0 0 0 0 0 is 0 1 0 1 0 1 0 1 0 0 not 0 0 0 0 1 0 1 0 0 0 gold 0 0 0 0 0 1 0 0 0 1 well 0 0 1 0 1 0 0 0 1 1 ends 0 0 1 0 0 0 0 1 0 0 < END > 0 0 0 0 0 0 1 1 0 0 Note : In NLP , we often add < START > and < END > tokens to represent the beginning and end of sentences , paragraphs or documents . In thise case we imagine < START > and < END > tokens encapsulating each document , e.g . , " < START > All that glitters is not gold < END > " , and include these tokens in our co-occurrence counts . The rows ( or columns ) of this matrix provide one type of word vectors ( those based on word-word co-occurrence ) , but the vectors will be large in general ( linear in the number of distinct words in a corpus ) . Thus , our next step is to run dimensionality reduction . In particular , we will run SVD ( Singular Value Decomposition ) , which is a kind of generalized PCA ( Principal Components Analysis ) to select the top principal components . Here's a visualization of dimensionality reduction with SVD . In this picture our co-occurrence matrix is with rows corresponding to words . We obtain a full matrix decomposition , with the singular values ordered in the diagonal matrix , and our new , shorter length - word vectors in . This reduced-dimensionality co-occurrence representation preserves semantic relationships between words , e.g . doctor and hospital will be closer than doctor and dog . Notes : If you can barely remember what an eigenvalue is , here's a slow , friendly introduction to SVD ( https://davetang.org / file / Singular_Value_Decomposition_Tutorial . pdf ) . If you want to learn more thoroughly about PCA or SVD , feel free to check out lectures 7 ( https://web.stanford.edu/class/cs168/l/l7.pdf ) , 8 ( http://theory.stanford.edu/~tim/s15/l/l8.pdf ) , and 9 ( https://web.stanford.edu/class/cs168/l/l9.pdf ) of CS168 . These course notes provide a great high-level treatment of these exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 4 of 18 3/30/2020 , 9:13 PM Plotting Co-Occurrence Word Embeddings Here , we will be using the Reuters ( business and financial news ) corpus . If you haven't run the import cell at the top of this page , please run it now ( click it and press SHIFT-RETURN ) . The corpus consists of 10,788 news documents totaling 1.3 million words . These documents span 90 categories and are split into train and test . For more details , please see https://www.nltk.org/book/ch02.html ( https://www.nltk.org/book/ch02.html ) . We provide a read_corpus function below that pulls out only articles from the " crude " ( i.e . news articles about oil , gas , etc . ) category . The function also adds < START > and < END > tokens to each of the documents , and lowercases words . You do not have to perform any other kind of pre - processing . In [ ] : def read_corpus ( category = " crude " ) : " " " Read files from the specified Reuter's category . Params : category ( string ) : category name Return : list of lists , with words from each of the processed files " " " files = reuters . fileids ( category ) return [ [ START_TOKEN ] + [ w . lower ( ) for w in list ( reuters . words ( f ) ) ] + [ END_TOKE N ] for f in files ] Let's have a look what these documents are like â€¦ . In [ ] : reuters_corpus = read_corpus ( ) pprint . pprint ( reuters_corpus [ : 3 ] , compact = True , width = 100 ) Question 1.1 : Implement distinct_words [ code ] ( 2 points ) Write a method to work out the distinct words ( word types ) that occur in the corpus . You can do this with for loops , but it's more efficient to do it with Python list comprehensions . In particular , this ( https://coderwall.com/p/rcmaea/flatten-a-list-of-lists- in-one-line-in-python ) may be useful to flatten a list of lists . If you're not familiar with Python list comprehensions in general , here's more information ( https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html ) . You may find it useful to use Python sets ( https://www.w3schools.com/python/python_sets.asp ) to remove duplicate words . In [ ] : def distinct_words ( corpus ) : " " " Determine a list of distinct words for the corpus . Params : corpus ( list of list of strings ) : corpus of documents Return : corpus_words ( list of strings ) : list of distinct words across the corpu s , sorted ( using python ' sorted ' function ) num_corpus_words ( integer ) : number of distinct words across the corpus " " " corpus_words = [ ] num_corpus_words = - 1 # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - return corpus_words , num_corpus_words exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 5 of 18 3/30/2020 , 9:13 PM In [ ] : # - - - - - - - - - - - - - - - - - - - - - # Run this sanity check # Note that this not an exhaustive check for correctness . # - - - - - - - - - - - - - - - - - - - - - # Define toy corpus test_corpus = [ " { } All that glitters isn't gold { } " . format ( START_TOKEN , END_TOKEN ) . split ( " " ) , " { } All's well that ends well { } " . format ( START_TOKEN , END_TOKEN ) . split ( " " ) ] test_corpus_words , num_corpus_words = distinct_words ( test_corpus ) # Correct answers ans_test_corpus_words = sorted ( [ START_TOKEN , " All " , " ends " , " that " , " gold " , " All ' s " , " glitters " , " isn't " , " well " , END_TOKEN ] ) ans_num_corpus_words = len ( ans_test_corpus_words ) # Test correct number of words assert ( num_corpus_words = = ans_num_corpus_words ) , " Incorrect number of distinct wor ds . Correct : { } . Yours : { } " . format ( ans_num_corpus_words , num_corpus_words ) # Test correct words assert ( test_corpus_words = = ans_test_corpus_words ) , " Incorrect corpus_words . \ nCorr ect : { } \ nYours : { } " . format ( str ( ans_test_corpus_words ) , str ( test_corpus_words ) ) # Print Success print ( " - " * 80 ) print ( " Passed All Tests ! " ) print ( " - " * 80 ) Question 1.2 : Implement compute_co_occurrence_matrix [ code ] ( 3 points ) Write a method that constructs a co-occurrence matrix for a certain window-size ( with a default of 4 ) , considering words before and after the word in the center of the window . Here , we start to use numpy ( np ) to represent vectors , matrices , and tensors . If you're not familiar with NumPy , there's a NumPy tutorial in the second half of this cs231n Python NumPy tutorial ( http://cs231n.github.io/python-numpy-tutorial/ ) . exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 6 of 18 3/30/2020 , 9:13 PM In [ ] : def compute_co_occurrence_matrix ( corpus , window_size = 4 ) : " " " Compute co-occurrence matrix for the given corpus and window_size ( default of 4 ) . Note : Each word in a document should be at the center of a window . Words ne ar edges will have a smaller number of co-occurring words . For example , if we take the document " < START > All that glitters is no t gold < END > " with window size of 4 , " All " will co-occur with " < START > " , " that " , " glitters " , " is " , and " no t " . Params : corpus ( list of list of strings ) : corpus of documents window_size ( int ) : size of context window Return : M ( a symmetric numpy matrix of shape ( number of unique words in the cor pus , number of unique words in the corpus ) ) : Co-occurence matrix of word counts . The ordering of the words in the rows / columns should be the same as the ordering of the words given by the distinct_words function . word2Ind ( dict ) : dictionary that maps word to index ( i.e . row / column nu mber ) for matrix M . " " " words , num_words = distinct_words ( corpus ) M = None word2Ind = { } # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - return M , word2Ind exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 7 of 18 3/30/2020 , 9:13 PM In [ ] : # - - - - - - - - - - - - - - - - - - - - - # Run this sanity check # Note that this is not an exhaustive check for correctness . # - - - - - - - - - - - - - - - - - - - - - # Define toy corpus and get student's co-occurrence matrix test_corpus = [ " { } All that glitters isn't gold { } " . format ( START_TOKEN , END_TOKEN ) . split ( " " ) , " { } All's well that ends well { } " . format ( START_TOKEN , END_TOKEN ) . split ( " " ) ] M_test , word2Ind_test = compute_co_occurrence_matrix ( test_corpus , window_size = 1 ) # Correct M and word2Ind M_test_ans = np . array ( [ [ 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 1 . , 0 . , 0 . , 1 . , ] , [ 0 . , 0 . , 1 . , 1 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , ] , [ 0 . , 1 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 1 . , 0 . , ] , [ 0 . , 1 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 1 . , ] , [ 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 1 . , 1 . , ] , [ 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 1 . , 1 . , 0 . , ] , [ 1 . , 0 . , 0 . , 0 . , 0 . , 0 . , 0 . , 1 . , 0 . , 0 . , ] , [ 0 . , 0 . , 0 . , 0 . , 0 . , 1 . , 1 . , 0 . , 0 . , 0 . , ] , [ 0 . , 0 . , 1 . , 0 . , 1 . , 1 . , 0 . , 0 . , 0 . , 1 . , ] , [ 1 . , 0 . , 0 . , 1 . , 1 . , 0 . , 0 . , 0 . , 1 . , 0 . , ] ] ) ans_test_corpus_words = sorted ( [ START_TOKEN , " All " , " ends " , " that " , " gold " , " All ' s " , " glitters " , " isn't " , " well " , END_TOKEN ] ) word2Ind_ans = dict ( zip ( ans_test_corpus_words , range ( len ( ans_test_corpus_words ) ) ) ) # Test correct word2Ind assert ( word2Ind_ans = = word2Ind_test ) , " Your word2Ind is incorrect :\ nCorrect : { } \ n Yours : { } " . format ( word2Ind_ans , word2Ind_test ) # Test correct M shape assert ( M_test . shape = = M_test_ans . shape ) , " M matrix has incorrect shape . \ nCorrect : { } \ nYours : { } " . format ( M_test . shape , M_test_ans . shape ) # Test correct M values for w1 in word2Ind_ans . keys ( ) : idx1 = word2Ind_ans [ w1 ] for w2 in word2Ind_ans . keys ( ) : idx2 = word2Ind_ans [ w2 ] student = M_test [ idx1 , idx2 ] correct = M_test_ans [ idx1 , idx2 ] if student ! = correct : print ( " Correct M : " ) print ( M_test_ans ) print ( " Your M : " ) print ( M_test ) raise AssertionError ( " Incorrect count at index ( { } , { } ) =( { } , { } ) in mat rix M . Yours has { } but should have { } . " . format ( idx1 , idx2 , w1 , w2 , student , correc t ) ) # Print Success print ( " - " * 80 ) print ( " Passed All Tests ! " ) print ( " - " * 80 ) exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 8 of 18 3/30/2020 , 9:13 PM Question 1.3 : Implement reduce_to_k_dim [ code ] ( 1 point ) Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings . Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings . Note : All of numpy , scipy , and scikit-learn ( sklearn ) provide some implementation of SVD , but only scipy and sklearn provide an implementation of Truncated SVD , and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD . So please use sklearn . decomposition . TruncatedSVD ( https://scikit-learn.org/stable/modules / generated / sklearn . decomposition . TruncatedSVD . html ) . In [ ] : def reduce_to_k_dim ( M , k = 2 ) : " " " Reduce a co-occurence count matrix of dimensionality ( num_corpus_words , num _corpus_words ) to a matrix of dimensionality ( num_corpus_words , k ) using the following SVD function from Scikit-Learn : - http://scikit-learn.org/stable/modules/generated/sklearn.decompositio n . TruncatedSVD . html Params : M ( numpy matrix of shape ( number of unique words in the corpus , number of unique words in the corpus ) ) : co-occurence matrix of word counts k ( int ) : embedding size of each word after dimension reduction Return : M_reduced ( numpy matrix of shape ( number of corpus words , k ) ) : matrix o f k-dimensioal word embeddings . In terms of the SVD from math class , this actually returns U * S " " " n_iters = 10 # Use this parameter in your call to ` TruncatedSVD ` M_reduced = None print ( " Running Truncated SVD over % i words . . . " % ( M . shape [ 0 ] ) ) # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - print ( " Done . " ) return M_reduced exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 9 of 18 3/30/2020 , 9:13 PM In [ ] : # - - - - - - - - - - - - - - - - - - - - - # Run this sanity check # Note that this is not an exhaustive check for correctness # In fact we only check that your M_reduced has the right dimensions . # - - - - - - - - - - - - - - - - - - - - - # Define toy corpus and run student code test_corpus = [ " { } All that glitters isn't gold { } " . format ( START_TOKEN , END_TOKEN ) . split ( " " ) , " { } All's well that ends well { } " . format ( START_TOKEN , END_TOKEN ) . split ( " " ) ] M_test , word2Ind_test = compute_co_occurrence_matrix ( test_corpus , window_size = 1 ) M_test_reduced = reduce_to_k_dim ( M_test , k = 2 ) # Test proper dimensions assert ( M_test_reduced . shape [ 0 ] = = 10 ) , " M_reduced has { } rows ; should have { } " . for mat ( M_test_reduced . shape [ 0 ] , 10 ) assert ( M_test_reduced . shape [ 1 ] = = 2 ) , " M_reduced has { } columns ; should have { } " . f ormat ( M_test_reduced . shape [ 1 ] , 2 ) # Print Success print ( " - " * 80 ) print ( " Passed All Tests ! " ) print ( " - " * 80 ) Question 1.4 : Implement plot_embeddings [ code ] ( 1 point ) Here you will write a function to plot a set of 2D vectors in 2D space . For graphs , we will use Matplotlib ( plt ) . For this example , you may find it useful to adapt this code ( https://www.pythonmembers.club/2018/05/08/matplotlib-scatter- plot-annotate-set-text-at-label-each-point / ) . In the future , a good way to make a plot is to look at the Matplotlib gallery ( https://matplotlib.org/gallery/index.html ) , find a plot that looks somewhat like what you want , and adapt the code they give . In [ ] : def plot_embeddings ( M_reduced , word2Ind , words ) : " " " Plot in a scatterplot the embeddings of the words specified in the list " wo rds " . NOTE : do not plot all the words listed in M_reduced / word2Ind . Include a label next to each point . Params : M_reduced ( numpy matrix of shape ( number of unique words in the corpus , 2 ) ) : matrix of 2-dimensioal word embeddings word2Ind ( dict ) : dictionary that maps word to indices for matrix M words ( list of strings ) : words whose embeddings we want to visualize " " " # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 10 of 18 3/30/2020 , 9:13 PM In [ ] : # - - - - - - - - - - - - - - - - - - - - - # Run this sanity check # Note that this is not an exhaustive check for correctness . # The plot produced should look like the " test solution plot " depicted below . # - - - - - - - - - - - - - - - - - - - - - print ( " - " * 80 ) print ( " Outputted Plot : " ) M_reduced_plot_test = np . array ( [ [ 1 , 1 ] , [ - 1 , - 1 ] , [ 1 , - 1 ] , [ - 1 , 1 ] , [ 0 , 0 ] ] ) word2Ind_plot_test = { ' test1 ' : 0 , ' test2 ' : 1 , ' test3 ' : 2 , ' test4 ' : 3 , ' test5 ' : 4 } words = [ ' test1 ' , ' test2 ' , ' test3 ' , ' test4 ' , ' test5 ' ] plot_embeddings ( M_reduced_plot_test , word2Ind_plot_test , words ) print ( " - " * 80 ) * * Test Plot Solution * * Question 1.5 : Co-Occurrence Plot Analysis [ written ] ( 3 points ) Now we will put together all the parts you have written ! We will compute the co-occurrence matrix with fixed window of 4 ( the default window size ) , over the Reuters " crude " ( oil ) corpus . Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word . TruncatedSVD returns U * S , so we need to normalize the returned vectors , so that all the vectors will appear around the unit circle ( therefore closeness is directional closeness ) . Note : The line of code below that does the normalizing uses the NumPy concept of broadcasting . If you don't know about broadcasting , check out Computation on Arrays : Broadcasting by Jake VanderPlas ( https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on- arrays-broadcasting . html ) . Run the below cell to produce the plot . It'll probably take a few seconds to run . What clusters together in 2-dimensional embedding space ? What doesn't cluster together that you might think should have ? Note : " bpd " stands for " barrels per day " and is a commonly used abbreviation in crude oil topic articles . exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 11 of 18 3/30/2020 , 9:13 PM In [ ] : # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # Run This Cell to Produce Your Plot # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - reuters_corpus = read_corpus ( ) M_co_occurrence , word2Ind_co_occurrence = compute_co_occurrence_matrix ( reuters_corp us ) M_reduced_co_occurrence = reduce_to_k_dim ( M_co_occurrence , k = 2 ) # Rescale ( normalize ) the rows to make them each of unit-length M_lengths = np . linalg . norm ( M_reduced_co_occurrence , axis = 1 ) M_normalized = M_reduced_co_occurrence / M_lengths [ : , np . newaxis ] # broadcasting words = [ ' barrels ' , ' bpd ' , ' ecuador ' , ' energy ' , ' industry ' , ' kuwait ' , ' oil ' , ' outpu t ' , ' petroleum ' , ' venezuela ' ] plot_embeddings ( M_normalized , word2Ind_co_occurrence , words ) Write your answer here .
