Part 2 : Prediction-Based Word Vectors ( 15 points ) As discussed in class , more recently prediction-based word vectors have demonstrated better performance , such as word2vec and GloVe ( which also utilizes the benefit of counts ) . Here , we shall explore the embeddings produced by GloVe . Please revisit the class notes and lecture slides for more details on the word2vec and GloVe algorithms . If you're feeling adventurous , challenge yourself and try reading GloVe's original paper ( https://nlp.stanford.edu/pubs/glove.pdf ) . Then run the following cells to load the GloVe vectors into memory . Note : If this is your first time to run these cells , i.e . download the embedding model , it will take about 15 minutes to run . If you've run these cells before , rerunning them will load the model without redownloading it , which will take about 1 to 2 minutes . In [ ] : def load_embedding_model ( ) : " " " Load GloVe Vectors Return : wv_from_bin : All 400000 embeddings , each lengh 200 " " " import gensim . downloader as api wv_from_bin = api . load ( " glove-wiki-gigaword-200 " ) print ( " Loaded vocab size % i " % len ( wv_from_bin . vocab . keys ( ) ) ) return wv_from_bin In [ ] : # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # Run Cell to Load Word Vectors # Note : This will take several minutes # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - wv_from_bin = load_embedding_model ( ) Note : If you are receiving reset by peer error , rerun the cell to restart the download . exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 12 of 18 3/30/2020 , 9:13 PM Reducing dimensionality of Word Embeddings Let's directly compare the GloVe embeddings to those of the co-occurrence matrix . In order to avoid running out of memory , we will work with a sample of 10000 GloVe vectors instead . Run the following cells to : 1 . Put 10000 Glove vectors into a matrix M 2 . Run reduce_to_k_dim ( your Truncated SVD function ) to reduce the vectors from 200-dimensional to 2-dimensional . In [ ] : def get_matrix_of_vectors ( wv_from_bin , required_words =[ ' barrels ' , ' bpd ' , ' ecuador ' , ' energy ' , ' industry ' , ' kuwait ' , ' oil ' , ' output ' , ' petroleum ' , ' venezuela ' ] ) : " " " Put the GloVe vectors into a matrix M . Param : wv_from_bin : KeyedVectors object ; the 400000 GloVe vectors loaded from file Return : M : numpy matrix shape ( num words , 200 ) containing the vectors word2Ind : dictionary mapping each word to its row number in M " " " import random words = list ( wv_from_bin . vocab . keys ( ) ) print ( " Shuffling words . . . " ) random . seed ( 224 ) random . shuffle ( words ) words = words [ : 10000 ] print ( " Putting % i words into word2Ind and matrix M . . . " % len ( words ) ) word2Ind = { } M = [ ] curInd = 0 for w in words : try : M . append ( wv_from_bin . word_vec ( w ) ) word2Ind [ w ] = curInd curInd + = 1 except KeyError : continue for w in required_words : if w in words : continue try : M . append ( wv_from_bin . word_vec ( w ) ) word2Ind [ w ] = curInd curInd + = 1 except KeyError : continue M = np . stack ( M ) print ( " Done . " ) return M , word2Ind In [ ] : # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions # Note : This should be quick to run # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - M , word2Ind = get_matrix_of_vectors ( wv_from_bin ) M_reduced = reduce_to_k_dim ( M , k = 2 ) # Rescale ( normalize ) the rows to make them each of unit-length M_lengths = np . linalg . norm ( M_reduced , axis = 1 ) M_reduced_normalized = M_reduced / M_lengths [ : , np . newaxis ] # broadcasting exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 13 of 18 3/30/2020 , 9:13 PM Note : If you are receiving out of memory issues on your local machine , try closing other applications to free more memory on your device . You may want to try restarting your machine so that you can free up extra memory . Then immediately run the jupyter notebook and see if you can load the word vectors properly . If you still have problems with loading the embeddings onto your local machine after this , please follow the Piazza instructions , as how to run remotely on Stanford Farmshare machines . Question 2.1 : GloVe Plot Analysis [ written ] ( 4 points ) Run the cell below to plot the 2D GloVe embeddings for [ ' barrels ' , ' bpd ' , ' ecuador ' , ' energy ' , ' industry ' , ' kuwait ' , ' oil ' , ' output ' , ' petroleum ' , ' venezuela ' ] . What clusters together in 2-dimensional embedding space ? What doesn't cluster together that you might think should have ? How is the plot different from the one generated earlier from the co-occurrence matrix ? What is a possible reason for causing the difference ? In [ ] : words = [ ' barrels ' , ' bpd ' , ' ecuador ' , ' energy ' , ' industry ' , ' kuwait ' , ' oil ' , ' outpu t ' , ' petroleum ' , ' venezuela ' ] plot_embeddings ( M_reduced_normalized , word2Ind , words ) Write your answer here . Cosine Similarity Now that we have word vectors , we need a way to quantify the similarity between individual words , according to these vectors . One such metric is cosine-similarity . We will be using this to find words that are " close " and " far " from one another . We can think of n-dimensional vectors as points in n-dimensional space . If we take this perspective L1 ( http://mathworld.wolfram.com/L1-Norm.html ) and L2 ( http://mathworld.wolfram.com/L2-Norm.html ) Distances help quantify the amount of space " we must travel " to get between these two points . Another approach is to examine the angle between two vectors . From trigonometry we know that : Instead of computing the actual angle , we can leave the similarity in terms of . Formally the Cosine Similarity ( https://en.wikipedia.org/wiki/Cosine_similarity ) between two vectors and is defined as : exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 14 of 18 3/30/2020 , 9:13 PM Question 2.2 : Words with Multiple Meanings ( 2 points ) [ code + written ] Polysemes and homonyms are words that have more than one meaning ( see this wiki page ( https://en.wikipedia.org / wiki / Polysemy ) to learn more about the difference between polysemes and homonyms ) . Find a word with at least 2 different meanings such that the top-10 most similar words ( according to cosine similarity ) contain related words from both meanings . For example , " leaves " has both " vanishes " and " stalks " in the top 10 , and " scoop " has both " handed_waffle_cone " and " lowdown " . You will probably need to try several polysemous or homonymic words before you find one . Please state the word you discover and the multiple meanings that occur in the top 10 . Why do you think many of the polysemous or homonymic words you tried didn't work ( i.e . the top-10 most similar words only contain one of the meanings of the words ) ? Note : You should use the wv_from_bin . most_similar ( word ) function to get the top 10 similar words . This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word . For further assistance please check the GenSim documentation ( https://radimrehurek.com/gensim/models / keyedvectors . html #gensim . models . keyedvectors . FastTextKeyedVectors . most_similar ) . In [ ] : # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - Write your answer here . Question 2.3 : Synonyms & Antonyms ( 2 points ) [ code + written ] When considering Cosine Similarity , it's often more convenient to think of Cosine Distance , which is simply 1 - Cosine Similarity . Find three words ( w1 , w2 , w3 ) where w1 and w2 are synonyms and w1 and w3 are antonyms , but Cosine Distance ( w1 , w3 ) < Cosine Distance ( w1 , w2 ) . For example , w1 = " happy " is closer to w3 = " sad " than to w2 = " cheerful " . Once you have found your example , please give a possible explanation for why this counter-intuitive result may have happened . You should use the the wv_from_bin . distance ( w1 , w2 ) function here in order to compute the cosine distance between two words . Please see the GenSim documentation ( https://radimrehurek.com/gensim/models / keyedvectors . html #gensim . models . keyedvectors . FastTextKeyedVectors . distance ) for further assistance . In [ ] : # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - Write your answer here . exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 15 of 18 3/30/2020 , 9:13 PM Solving Analogies with Word Vectors Word vectors have been shown to sometimes exhibit the ability to solve analogies . As an example , for the analogy " man : king : : woman : x " ( read : man is to king as woman is to x ) , what is x ? In the cell below , we show you how to use word vectors to find x . The most_similar function finds words that are most similar to the words in the positive list and most dissimilar from the words in the negative list . The answer to the analogy will be the word ranked most similar ( largest numerical value ) . Note : Further Documentation on the most_similar function can be found within the GenSim documentation ( https://radimrehurek.com/gensim/models / keyedvectors . html #gensim . models . keyedvectors . FastTextKeyedVectors . most_similar ) . In [ ] : # Run this cell to answer the analogy - - man : king : : woman : x pprint . pprint ( wv_from_bin . most_similar ( positive =[ ' woman ' , ' king ' ] , negative =[ ' man ' ] ) ) Question 2.4 : Finding Analogies [ code + written ] ( 2 Points ) Find an example of analogy that holds according to these vectors ( i.e . the intended word is ranked top ) . In your solution please state the full analogy in the form x : y : : a : b . If you believe the analogy is complicated , explain why the analogy holds in one or two sentences . Note : You may have to try many analogies to find one that works ! In [ ] : # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - Write your answer here . Question 2.5 : Incorrect Analogy [ code + written ] ( 1 point ) Find an example of analogy that does not hold according to these vectors . In your solution , state the intended analogy in the form x : y : : a : b , and state the ( incorrect ) value of b according to the word vectors . In [ ] : # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - Write your answer here . exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 16 of 18 3/30/2020 , 9:13 PM Question 2.6 : Guided Analysis of Bias in Word Vectors [ written ] ( 1 point ) It's important to be cognizant of the biases ( gender , race , sexual orientation etc . ) implicit in our word embeddings . Bias can be dangerous because it can reinforce stereotypes through applications that employ these models . Run the cell below , to examine ( a ) which terms are most similar to " woman " and " worker " and most dissimilar to " man " , and ( b ) which terms are most similar to " man " and " worker " and most dissimilar to " woman " . Point out the difference between the list of female-associated words and the list of male-associated words , and explain how it is reflecting gender bias . In [ ] : # Run this cell # Here ` positive ` indicates the list of words to be similar to and ` negative ` indic ates the list of words to be # most dissimilar from . pprint . pprint ( wv_from_bin . most_similar ( positive =[ ' woman ' , ' worker ' ] , negative =[ ' man ' ] ) ) print ( ) pprint . pprint ( wv_from_bin . most_similar ( positive =[ ' man ' , ' worker ' ] , negative =[ ' woman ' ] ) ) Write your answer here . Question 2.7 : Independent Analysis of Bias in Word Vectors [ code + written ] ( 1 point ) Use the most_similar function to find another case where some bias is exhibited by the vectors . Please briefly explain the example of bias that you discover . In [ ] : # - - - - - - - - - - - - - - - - - - # Write your implementation here . # - - - - - - - - - - - - - - - - - - Write your answer here . Question 2.8 : Thinking About Bias [ written ] ( 2 points ) What might be the causes of these biases in the word vectors ? You should give least 2 explainations how bias get into the word vectors . How might you be able to investigate / test these causes ? Write your answer here . exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 17 of 18 3/30/2020 , 9:13 PM Submission Instructions 1 . Click the Save button at the top of the Jupyter Notebook . 2 . Select Cell - > All Output - > Clear . This will clear all the outputs from all cells ( but will keep the content of all cells ) . 3 . Select Cell - > Run All . This will run all the cells in order , and will take several minutes . 4 . Once you've rerun everything , select File - > Download as - > PDF via LaTeX ( If you have trouble using " PDF via LaTex " , you can also save the webpage as pdf . Make sure all your solutions especially the coding parts are displayed in the pdf , it's okay if the provided codes get cut off because lines are not wrapped in code cells ) . 5 . Look at the PDF file and make sure all your solutions are there , displayed correctly . The PDF is the only thing your graders will see ! 6 . Submit your PDF on Gradescope . In [ ] : exploring_word_vectors http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring . . . 18 of 18 3/30/2020 , 9:13 PM
