2 . Character-based LSTM decoder for NMT ( 26 points ) We will now add a LSTM-based character-level decoder to our NMT system , based on Luong & Man - ning’s work . 10 The main idea is that when our word-level decoder produces an < UNK > token , we run our character-level decoder ( which you can think of as a character-level conditional language model ) to instead generate the target word one character at a time , as shown in Figure 3 . This will help us to produce rare and out-of-vocabulary target words . Figure 3 : A character-based decoder which is triggered if the word-based decoder produces an UNK . Figure courtesy of Luong & Manning . This graph is just for reference and in our implementation we will use only one layer of LSTM as stated in details below . We now describe the model in three sections : Forward computation of Character Decoder Given a sequence of integers x1 , . . . , xn ∈ Z repre - senting a sequence of characters , we lookup their character embeddings x1 , . . . , xn ∈ Rechar and pass these as input into the ( unidirectional ) LSTM , obtaining hidden states h1 , . . . , hn and cell states c1 , . . . , cn : ht , ct = CharDecoderLSTM ( xt , ht − 1 , ct − 1 ) where ht , ct ∈ Rh ( 12 ) where h is the hidden size of the CharDecoderLSTM . The initial hidden and cell states h0 and c0 are both set to the combined output vector ( refer to Assignment 4 ) for the current timestep of the main word-level NMT decoder . For every timestep t ∈ { 1 , . . . , n } we compute scores ( also called logits ) st ∈ RVchar : st = Wdecht + bdec ∈ RVchar ( 13 ) where the weight matrix Wdec ∈ RVchar × h and the bias vector bdec ∈ RVchar . If we passed st through a softmax function , we would have the probability distribution for the next character in the sequence . 10Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models , Luong and Manning , 2016 . https://arxiv.org/abs/1604.00788 CS 224n Assignment 5 [ updated ] Page 9 of 12 Training of Character Decoder When we train the NMT system , we train the character decoder on every word in the target sentence ( not just the words represented by < UNK > ) . For example , on a particular step of the main NMT decoder , if the target word is music then the input sequence for the CharDecoderLSTM is [ x1 , . . . , xn ] = [ < START > , m , u , s , i , c ] and the target sequence for the CharDecoderLSTM is [ x2 , . . . , xn + 1 ] = [ m , u , s , i , c , < END > ] . We pass the input sequence x1 , . . . , xn , ( along with the initial states h0 and c0 obtained from the combined output vector ) into the CharDecoderLSTM , thus obtaining scores s1 , . . . , sn which we will compare to the target sequence x2 , . . . , xn + 1 . We optimize with respect to sum of the cross-entropy loss : pt = softmax ( st ) ∈ RVchar ∀ t ∈ { 1 , . . . , n } ( 14 ) losschar dec = − n ∑ t = 1 log pt ( xt + 1 ) ∈ R ( 15 ) Note that when we compute losschar dec for a batch of words , we take the sum ( not average ) across the entire batch . On each training iteration , we add losschar dec to the loss of the word-based decoder , so that we simultaneously train the word-based model and character-based decoder . Decoding from the Character Decoder At test time , first we produce a translation from our word - based NMT system in the usual way ( e.g . a decoding algorithm like beam search ) . If the translation contains any < UNK > tokens , then for each of those positions , we use the word-based decoder’s combined output vector to initialize the CharDecoderLSTM’s initial h0 and c0 , then use CharDecoderLSTM to generate a sequence of characters . To generate the sequence of characters , we use the greedy decoding algorithm , which repeatedly chooses the most probable next character , until either the < END > token is produced or we reach a predetermined max length . The algorithm is given below , for a single example ( not batched ) . Algorithm 1 Greedy Decoding Input : Initial states h0 , c0 Output : output word generated by the character decoder ( doesn’t contain < START > or < END > ) 1 : procedure decode greedy 2 : output word ← [ ] 3 : current char ← < START > 4 : for t = 0 , 1 , . . . , max length − 1 do 5 : ht + 1 , ct + 1 ← CharDecoder ( current char , ht , ct ) . use last predicted character as input 6 : st + 1 ← Wdecht + 1 + bdec . compute scores 7 : pt + 1 ← softmax ( st + 1 ) . compute probabilities 8 : current char ← argmaxc pt + 1 ( c ) . the most likely next character 9 : if current char = < END > then 10 : break 11 : output word ← output word + [ current char ] . append this character to output word 12 : return output word Implementation At the end of this section , you will train the full NMT system ( with character-encoder and character - decoder ) . As in the previous assignment , we strongly advise that you first develop the code locally and ensure it does not crash , before attempting to train it on your VM . Finally , make sure that your VM is turned off whenever you are not using it . CS 224n Assignment 5 [ updated ] Page 10 of 12 If your Azure subscription runs out of money your VM will be temporarily locked and inaccessible . If that happens , make a private post on Piazza with your name , email used for Azure , and SUNetID , to request more credits . ( a ) ( 4 points ) ( coding ) Write the forward ( ) function in char decoder.py . This function takes input x1 , . . . , xn and ( h0 , c0 ) ( as described in the Forward computation of the character decoder section ) and returns s1 , . . . , sn and ( hn , cn ) . Run the following for a non-exhaustive sanity check : python sanity_check . py 2a ( b ) ( 5 points ) ( coding ) Write the train forward ( ) function in char decoder.py . This function computes losschar dec summed across the whole batch . Hint : Carefully read the documentation for nn . CrossEntropyLoss . Run the following for a non-exhaustive sanity check : python sanity_check . py 2b ( c ) ( 8 points ) ( coding ) Write the decode greedy ( ) function in char decoder.py to implement the algorithm decode greedy . Note that although Algorithm 1 is described for a single example , your implementation must work over a batch . Algorithm 1 also indicates that you should break when you reach the < END > token , but in the batched case you might find it more convenient to complete all max length steps of the for-loop , then truncate the output words afterwards . Run the following for a non-exhaustive sanity check : python sanity_check . py 2c ( d ) ( 3 points ) ( coding ) Once you have thoroughly checked your implementation of the CharDecoder functions ( checking much more than just the sanity checks ! ) , it’s time to do the ‘ small training run ’ check . Confirm that you’re in the proper conda environment and then execute the following command on your local machine to check if your model overfits to a small subset of the training data . sh run.sh train_local_q2 sh run.sh test_local_q2 Running these should take around 10 minutes ( but this depends on your local machine ) . You should observe the average loss go down to near 0 and average perplexity on train and dev set go to 1 during training . Once you run the test , you should observe BLEU score on the test set higher than 99.00 . If you don’t observe these things , you probably need to go back to debug ! Important : Make sure not to modify the output file ( outputs / test outputs local q2 . txt ) generated by the code – you need this to be included when you run the submission script . ( e ) ( 6 points ) ( coding ) Now that we’ve implemented both the character-based encoder and the character - based decoder , it’s finally time to train the full system ! Connect to your VM and install necessary packages by running : pip install - r gpu_requirements . txt Once your VM is configured and you are in a tmux session , 11 execute : sh run.sh train This should take between 8-12 hours to train on the VM , though time may vary depending on your implementation and whether or not the training procedure hits the early stopping criterion . Run your model on the test set using : sh run.sh test 11Refer to Practical tips for using VMs for more information on tmux CS 224n Assignment 5 [ updated ] Page 11 of 12 and report your test set BLEU score in your assignment write-up . Also ensure that the output file outputs / test outputs . txt is present and unmodified – this will be included in your submission , and we’ll use it to verify your self-reported BLEU score . Given your BLEU score b , here’s how your points will be determined for this sub-problem : BLEU Score Points 0 ≤ b < 35 0 35 ≤ b < 36 2 36 ≤ b 6 
