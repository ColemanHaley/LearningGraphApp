1 . Character-based convolutional encoder for NMT ( 36 points ) In Assignment 4 , we used a simple lookup method to get the representation of a word . If a word is not in our pre-defined vocabulary , then it is represented as the < UNK > token ( which has its own embedding ) . Figure 1 : Lookup-based word embedding model from Assignment 4 , which produces a word embedding of length eword . In this section , we will first describe a method based on Kim et al . ’ s work in Character-Aware Neural Language Models , 2 then we’ll implement it . Specifically , we’ll replace the ‘ Embedding lookup ’ stage in Figure 1 with a sequence of more involved stages , depicted in Figure 2 . Figure 2 : Character-based convolutional encoder , which ultimately produces a word embedding of length eword . Model description and written questions The model in Figure 2 has four main stages , which we’ll describe for a single example ( not a batch ) : 1 . Convert word to character indices . We have a word x ( e.g . Anarchy in Figure 2 ) that we wish to represent . Assume we have a predefined ‘ vocabulary ’ of characters ( for example , all lowercase letters , uppercase letters , numbers , and some punctuation ) . By looking up the index of each character , we can represent a length-l word x as a vector of integers : x = [ c1 , c2 , · · · , cl ] ∈ Zl ( 1 ) where each ci is an integer index into the character vocabulary . 2Character-Aware Neural Language Models , Kim et al . , 2016 . https://arxiv.org/abs/1508.06615 CS 224n Assignment 5 [ updated ] Page 3 of 12 2 . Padding and embedding lookup . Using a special < PAD > ‘ character ’ , we pad ( or truncate ) every word so that it has length mword ( defined as the length of longest word in the batch ) : xpadded = [ c1 , c2 , · · · , cmword ] ∈ Zmword ( 2 ) For each of these characters ci , we lookup a dense character embedding ( which has shape echar ) . This yields a tensor xemb : xemb = CharEmbedding ( xpadded ) ∈ Rmword × echar ( 3 ) We’ll reshape xemb to obtain xreshaped ∈ Rechar × mword before feeding into the convolutional network . 3 3 . Convolutional network . To combine these character embeddings , we’ll use 1-dimensional convo - lutions . The convolutional layer has two hyperparameters : 4 the kernel size k ( also called window size ) , which dictates the size of the window used to compute features , and the number of filters f ( also called number of output features or number of output channels ) . The convolutional layer has a weight matrix W ∈ Rf × echar × k and a bias vector b ∈ Rf . To compute the ith output feature ( where i ∈ { 1 , . . . , f } ) for the tth window of the input , the convolution operation is performed between the input window5 ( xreshaped ) [ : , t : t + k − 1 ] ∈ Rechar × k and the weights W [ i , : , :] ∈ Rechar × k , and the bias term bi ∈ R is added : ( xconv ) i , t = sum ( W [ i , : , :] � ( xreshaped ) [ : , t : t + k − 1 ] ) + bi ∈ R ( 4 ) where � is elementwise multiplication of two matrices with the same shape and sum is the sum of all the elements in the matrix . This operation is performed for every feature i and every window t , where t ∈ { 1 , . . . , mword − k + 1 } . Overall this produces output xconv : xconv = Conv1D ( xreshaped ) ∈ Rf × ( mword − k + 1 ) ( 5 ) For our application , we’ll set f to be equal to eword , the size of the final word embedding for word x ( the rightmost vector in Figure 2 ) . Therefore , xconv ∈ Reword × ( mword − k + 1 ) ( 6 ) Finally , we apply the ReLU function to xconv , then use max-pooling to reduce this to a single vector xconv out ∈ Reword , which is the final output of the Convolutional Network : xconv out = MaxPool ( ReLU ( xconv ) ) ∈ Reword ( 7 ) Here , MaxPool simply takes the maximum across the second dimension . Given a matrix M ∈ Ra × b , then MaxPool ( M ) ∈ Ra with MaxPool ( M ) i = max1 ≤ j ≤ bMij for i ∈ { 1 , . . . , a } . 4 . Highway layer and dropout . As mentioned in Lectures 7 and 11 , Highway Networks6 have a skip-connection controlled by a dynamic gate . Given the input xconv out ∈ Reword , we compute : xproj = ReLU ( Wprojxconv out + bproj ) ∈ Reword ( 8 ) xgate = σ ( Wgatexconv out + bgate ) ∈ Reword ( 9 ) where the weight matrices Wproj , Wgate ∈ Reword × eword , bias vectors bproj , bgate ∈ Reword and σ is the sigmoid function . Next , we obtain the output xhighway by using the gate to combine the projection with the skip-connection : xhighway = xgate � xproj + ( 1 − xgate ) � xconv out ∈ Reword ( 10 ) 3Necessary because the PyTorch Conv1D function performs the convolution only on the last dimension of the input . 4We assume no padding is applied and the stride is 1 . 5In the notation ( xreshaped ) [ : , t : t + k − 1 ] , the range t : t + k − 1 is inclusive , i.e . the width-k window { t , t + 1 , . . . , t + k − 1 } . 6Highway Networks , Srivastava et al . , 2015 . https://arxiv.org/abs/1505.00387 CS 224n Assignment 5 [ updated ] Page 4 of 12 where � denotes element-wise multiplication . Finally , we apply dropout to xhighway : xword emb = Dropout ( xhighway ) ∈ Reword ( 11 ) We’re done ! xword emb is the embedding we will use to represent word x – this will replace the lookup - based word embedding we used in Assignment 4 . ( a ) ( 1 point ) ( written ) We learned in class that recurrent neural architectures can operate over variable length input ( i.e . , the shape of the model parameters is independent of the length of the input sentence ) . Is the same true of convolutional architectures ? Write one sentence to explain why or why not . ( b ) ( 2 points ) ( written ) In step 2 of the character-based embedding model , for each word in a sentence , we pad it to length mword ( the length of longest word in the batch ) . In fact , in our implementation , we also add two special characters to this sequence : one character at the beginning standing for the start of word token , and another character at the end standing for the end of word token . Therefore , x ′ padded = [ c0 , c1 , c2 , · · · , cmword , cmword + 1 ] ∈ Zmword + 2 , where c0 = start of word , and cmword + 1 = end of word . In 1D convolutions , we do padding , i.e . we add some zeros to both sides of our input , so that the kernel sliding over the input can be applied to at least one complete window . In this case , if we use the kernel size k = 5 , what will be the size of the padding ( i.e . the additional number of zeros on each side ) we need for the 1-dimensional convolution , such that there exists at least one window for all possible values of mword in our dataset ? Explain your reasoning . Hints : • What is the smallest possible value that mword can take ? • After padding extra zeros to the input of 1-d convolution layer ( xreshaped ) , the updated x ′ reshaped will have size x ′ reshaped ∈ Rechar × ( mword + 2 + ( 2 ∗ padding ) ) . ( c ) ( 3 points ) ( written ) In step 4 , we introduce a Highway Network with xhighway = xgate � xproj + ( 1 − xgate ) � xconv out . Since xgate is the result of the sigmoid function , it has the range ( 0 , 1 ) . Consider the two extreme cases . If xgate → 0 , then xhighway → xconv out . When xgate → 1 , then xhighway → xproj . This means the Highway layer is smoothly varying its behavior between that of normal linear layer ( xproj ) and that of a layer which simply passes its inputs ( xconv out ) through . Use one or two sentences to explain why this behavior is useful in character embeddings . Based on the definition of xgate = σ ( Wgatexconv out + bgate ) , do you think it is better to initialize bgate to be negative or positive ? Explain your reason briefly . ( d ) ( 2 points ) ( written ) In Lecture 10 , we briefly introduced Transformers , a non-recurrent sequence ( or sequence-to-sequence ) model with a sequence of attention-based transformer blocks . Describe 2 advantages of a Transformer encoder over the LSTM-with-attention encoder in our NMT model ( which we used in both Assignment 4 and Assignment 5 ) . Implementation In the remainder of Section 1 , we will be implementing the character-based encoder in our NMT system . Though you could implement this on top of your own Assignment 4 solution code , for simplicity and fairness we have supplied you7 with a full implementation of the Assignment 4 word-based NMT model ( with some modifications ) ; this is what you will use as a basis for your Assignment 5 code . You will not need to use your VM until Section 2 – the rest of this section can be done on your local machine . In order to run the model code on your local machine , please run the following command to create the proper virtual environment : 7available on Stanford Box ; requires Stanford login CS 224n Assignment 5 [ updated ] Page 5 of 12 conda env create - - file local_env . yml Note that this virtual environment will not be needed on the VM . Run the following to create the correct vocab files : sh run.sh vocab CS 224n Assignment 5 [ updated ] Page 6 of 12 Let’s implement the entire network specified in Figure 2 , from left to right . ( e ) ( 4 points ) ( coding ) Implement to input tensor char ( ) in vocab.py by using 2 methods : • Use words2charindices ( ) in vocab.py to convert each character in all words to its cor - responding index in the character-vocabulary . • Use pad sents char ( ) in utils.py to pad all words to max word length of all words in the batch , and pad all sentences to max sentence length of all sentences in the batch . Then convert the resulting padded sentences to a torch tensor . This corresponds to the first three steps of Figure 2 ( splitting , vocab lookup and padding ) . Ensure you reshape the dimensions so that the output has shape : ( max sentence length , batch size , max word length ) . Run the following for a non-exhaustive sanity check : python sanity_check . py 1e ( f ) ( 4 points ) ( coding and written ) In the empty file highway.py , implement the highway network as a nn . Module class called Highway . 8 • Your module will need a init ( ) and a forward ( ) function ( whose inputs and outputs you decide for yourself ) . • The forward ( ) function will need to map from xconv out to xhighway . • Note that although the model description above is not batched , your forward ( ) function should operate on batches of words . • Make sure that your module uses two nn . Linear layers ( this is important for the autograder ) . There is no provided sanity check for your Highway implementation – instead , you will now write your own code to thoroughly test your implementation . You should do whatever you think is sufficient to convince yourself that your module computes what it’s supposed to compute . Possible ideas include ( and you should do multiple ) : • Write code to check that the input and output have the expected shapes and types . Before you do this , make sure you’ve written docstrings for init ( ) and forward ( ) – you can’t test the expected output if you haven’t clearly laid out what you expect it to be ! • Print out the shape of every intermediate value ; verify all the shapes are correct . • Create a small instance of your highway network ( with small , manageable dimensions ) , manually define some input , manually define the weights and biases , manually calculate what the output should be , and verify that your module does indeed return that value . • Similar to previous , but you could instead print all intermediate values and check each is correct . • If you can think of any ‘ edge case ’ or ‘ unusual ’ inputs , create test cases based on those . Once you’ve finished testing your module , write a short description of the tests you carried out , and why you believe they are sufficient . The 4 points for this question are awarded based on your written description of the tests only . Important : to avoid crashing the autograder , make sure that any print statements are commented out when you submit your code . ( g ) ( 4 points ) ( coding and written ) In the empty file cnn.py , implement the convolutional network as a nn . Module class called CNN . • Your module will need a init ( ) and a forward ( ) function ( whose inputs and outputs you decide for yourself ) . • The forward ( ) function will need to map from xreshaped to xconv out . 8If you’re unsure how to structure a nn . Module , you can start here : https://pytorch.org/tutorials/beginner/ examples_nn / two_layer_net_module . html . After that , you could look at the many examples of nn . Modules in Assign - ments 3 and 4 . CS 224n Assignment 5 [ updated ] Page 7 of 12 • Note that although the model description above is not batched , your forward ( ) function should operate on batches of words . • Make sure that your module has an instance variable of type nn . Conv1d ( this is important for the autograder ) . • Use a kernel size of k = 5 and padding = 1 . As in ( f ) , write code to thoroughly test your implementation . Once you’ve finished testing your module , write a short description of the tests you carried out , and why you believe they are sufficient . As in ( f ) , the 4 points are for your written description only . ( h ) ( 10 points ) ( coding ) Write the init ( ) and forward ( ) functions for the ModelEmbeddings class in model embeddings . py . 9 • The forward ( ) function must map from xpadded to xword emb – note this is for whole batches of sentences , rather than batches of words . • You will need to use the CNN and Highway modules you created . • Don’t forget the dropout layer ! Use 0.3 dropout probability . • Your ModelEmbeddings class should contain one nn . Embedding object ( this is important for the autograder ) . It should also contain a field named self . word embed size . • Remember that we are using echar = 50 . • Depending on your implementation of CNN and Highway , it’s likely that you will need to reshape tensors to get them into the shape required by CNN and Highway , and then reshape again to get the final output of ModelEmbeddings . forward ( ) . • Make sure that you use permute ( ) or transpose ( ) instead of using view ( ) or reshape ( ) to swap dimensions in forward ( ) ( this is important for the autograder ) . Sanity check if the output from your model has the correct shape by running the following : python sanity_check . py 1h The majority of the 10 points are awarded based on a hidden autograder . Though we don’t require it , you should check your implementation using techniques similar to what you did in ( f ) and ( g ) , before you do the ‘ small training run ’ check in ( j ) . ( i ) ( 4 points ) ( coding ) In nmt model.py , complete the forward ( ) method to use character-level padded encodings instead of word-level encodings . This ties together your ModelEmbeddings code with the preprocessing code you wrote . Check your code ! ( j ) ( 2 points ) ( coding ) On your local machine , confirm that you’re in the proper conda eniron - ment then execute the following command . This will train your model on a very small subset of the training data , then test it . Your model should overfit the training data . ( You may need to . contiguous ( ) . view ( ) instead of . view ( ) when reshaping ) . sh run.sh train_local_q1 sh run.sh test_local_q1 Running these should take around 5 minutes ( but this depends on your local machine ) . You should observe the average loss go down to near 0 and average perplexity on train and dev set go to 1 during training . Once you run the test , you should observe BLEU score on the test set higher than 99.00 . If you don’t observe these things , you probably need to go back to debug ! Important : Make sure not to modify the output file ( outputs / test outputs local q1 . txt ) generated by the code – you need this to be included when you run the submission script . 9Note that in this assignment , the full NMT model defines two ModelEmbeddings objects ( one for source and one for target ) , whereas in Assignment 4 , there was a single ModelEmbeddings object that contained two nn . Embedding objects ( one for source and one for target ) .
