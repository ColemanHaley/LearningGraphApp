{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\ftech\fcharset77 Symbol;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 1. Neural Machine Translation with RNNs (45 points) In Machine Translation, our goal is to convert a sentence from the source language (e.g. Spanish) to the target language (e.g. English). In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network with attention, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional LSTM Encoder and a Unidirectional LSTM Decoder. Figure 1: Seq2Seq Model with Multiplicative Attention, shown on the third step of the decoder. Hidden states h enc i and cell states c enc i are defined in the next page. 1 CS 224n Assignment 4 Page 2 of 7 Model description (training procedure) Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding x1, . . . , xm (xi 
\f1 \uc0\u8712 
\f0  R e\'d71 ), where m is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional encoder, yielding hidden states and cell states for both the forwards (
\f1 \uc0\u8594 
\f0 ) and backwards (
\f1 \uc0\u8592 
\f0 ) LSTMs. The forwards and backwards versions are concatenated to give hidden states h enc i and cell states c enc i : h enc i = [
\f1 \uc0\u8592 
\f0 \uc0\u8722 \u8722  h enc i ; \u8722 \u8722 
\f1 \uc0\u8594 
\f0  h enc i ] where h enc i 
\f1 \uc0\u8712 
\f0  R 2h\'d71 , 
\f1 \uc0\u8592 
\f0 \uc0\u8722 \u8722  h enc i , \u8722 \u8722 
\f1 \uc0\u8594 
\f0  h enc i 
\f1 \uc0\u8712 
\f0  R h\'d71 1 \uc0\u8804  i \u8804  m (1) c enc i = [
\f1 \uc0\u8592 
\f0 \uc0\u8722 \u8722  c enc i ; \u8722 \u8722 
\f1 \uc0\u8594 
\f0  c enc i ] where c enc i 
\f1 \uc0\u8712 
\f0  R 2h\'d71 , 
\f1 \uc0\u8592 
\f0 \uc0\u8722 \u8722  c enc i , \u8722 \u8722 
\f1 \uc0\u8594 
\f0  c enc i 
\f1 \uc0\u8712 
\f0  R h\'d71 1 \uc0\u8804  i \u8804  m (2) We then initialize the decoder\'92s first hidden state h dec 0 and cell state c dec 0 with a linear projection of the encoder\'92s final hidden state and final cell state.1 h dec 0 = Wh[ 
\f1 \uc0\u8592 
\f0 \uc0\u8722 \u8722  h enc 1 ; \u8722 \u8722 
\f1 \uc0\u8594 
\f0  h enc m ] where h dec 0 
\f1 \uc0\u8712 
\f0  R h\'d71 ,Wh 
\f1 \uc0\u8712 
\f0  R h\'d72h (3) c dec 0 = Wc[ 
\f1 \uc0\u8592 
\f0 \uc0\u8722 \u8722  c enc 1 ; \u8722 \u8722 
\f1 \uc0\u8594 
\f0  c enc m ] where c dec 0 
\f1 \uc0\u8712 
\f0  R h\'d71 ,Wc 
\f1 \uc0\u8712 
\f0  R h\'d72h (4) With the decoder initialized, we must now feed it a target sentence. On the t th step, we look up the embedding for the t th word, yt 
\f1 \uc0\u8712 
\f0  R e\'d71 . We then concatenate yt with the combined-output vector ot\uc0\u8722 1 
\f1 \uc0\u8712 
\f0  R h\'d71 from the previous timestep (we will explain what this is later down this page!) to produce yt 
\f1 \uc0\u8712 
\f0  R (e+h)\'d71 . Note that for the first target word (i.e. the start token) o0 is a zero-vector. We then feed yt as input to the decoder. h dec t , c dec t = Decoder(yt, h dec t\uc0\u8722 1 , c dec t\u8722 1 ) where h dec t 
\f1 \uc0\u8712 
\f0  R h\'d71 , c dec t 
\f1 \uc0\u8712 
\f0  R h\'d71 (5) (6) We then use h dec t to compute multiplicative attention over h enc 1 , . . . , h enc m : et,i = (h dec t ) TWattProjh enc i where et 
\f1 \uc0\u8712 
\f0  R m\'d71 ,WattProj 
\f1 \uc0\u8712 
\f0  R h\'d72h 1 \uc0\u8804  i \u8804  m (7) \u945 t = softmax(et) where \u945 t 
\f1 \uc0\u8712 
\f0  R m\'d71 (8) at = Xm i=1 \uc0\u945 t,ih enc i where at 
\f1 \uc0\u8712 
\f0  R 2h\'d71 (9) We now concatenate the attention output at with the decoder hidden state h dec t and pass this through a linear layer, tanh, and dropout to attain the combined-output vector ot. ut = [at; h dec t ] where ut 
\f1 \uc0\u8712 
\f0  R 3h\'d71 (10) vt = Wuut where vt 
\f1 \uc0\u8712 
\f0  R h\'d71 ,Wu 
\f1 \uc0\u8712 
\f0  R h\'d73h (11) ot = dropout(tanh(vt)) where ot 
\f1 \uc0\u8712 
\f0  R h\'d71 (12) Then, we produce a probability distribution Pt over target words at the t th timestep: 1 If it\'92s not obvious, think about why we regard [
\f1 \uc0\u8592 
\f0 \uc0\u8722 \u8722  h enc 1 , \u8722 \u8722 
\f1 \uc0\u8594 
\f0 h enc m ] as the \'91final hidden state\'92 of the Encoder. CS 224n Assignment 4 Page 3 of 7 Pt = softmax(Wvocabot) where Pt 
\f1 \uc0\u8712 
\f0  R Vt\'d71 ,Wvocab 
\f1 \uc0\u8712 
\f0  R Vt\'d7h (13) Here, Vt is the size of the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between Pt and gt, where gt is the one-hot vector of the target word at timestep t: Jt(\uc0\u952 ) = CrossEntropy(Pt, gt) (14) Here, \u952  represents all the parameters of the model and Jt(\u952 ) is the loss on step t of the decoder. Now that we have described the model, let\'92s try implementing it for Spanish to English translation! Setting up your Virtual Machine Follow the instructions in the CS224n Azure Guide (link also provided on website and Piazza) in order to create your VM instance. This should take you approximately 45 minutes. Though you will need the GPU to train your model, we strongly advise that you first develop the code locally and ensure that it runs, before attempting to train it on your VM. GPU time is expensive and limited. It takes approximately 4 hours to train the NMT system. We don\'92t want you to accidentally use all your GPU time for debugging your model rather than training and evaluating it. Finally, make sure that your VM is turned off whenever you are not using it. If your Azure subscription runs out of money, your VM will be temporarily locked and inaccessible. If that happens, make a private post on Piazza with your Name, email used for Azure and SUNetID to request more credits. In order to run the model code on your local machine, please run the following command to create the proper virtual environment: conda env create --file local env.yml Note that this virtual environment will not be needed on the VM. Implementation and written questions (a) (2 points) (coding) In order to apply tensor operations, we must ensure that the sentences in a given batch are of the same length. Thus, we must identify the longest sentence in a batch and pad others to be the same length. Implement the pad sents function in utils.py, which shall produce these padded sentences. (b) (3 points) (coding) Implement the init function in model embeddings.py to initialize the necessary source and target embeddings. (c) (4 points) (coding) Implement the init function in nmt model.py to initialize the necessary model embeddings (using the ModelEmbeddings class from model embeddings.py) and layers (LSTM, projection, and dropout) for the NMT system. (d) (8 points) (coding) Implement the encode function in nmt model.py. This function converts the padded source sentences into the tensor X, generates h enc 1 , . . . , h enc m , and computes the initial state h dec 0 and initial cell c dec 0 for the Decoder. You can run a non-comprehensive sanity check by executing: python sanity_check.py 1d CS 224n Assignment 4 Page 4 of 7 (e) (8 points) (coding) Implement the decode function in nmt model.py. This function constructs y\'af and runs the step function over every timestep for the input. You can run a non-comprehensive sanity check by executing: python sanity_check.py 1e (f) (10 points) (coding) Implement the step function in nmt model.py. This function applies the Decoder\'92s LSTM cell for a single timestep, computing the encoding of the target word h dec t , the attention scores et, attention distribution \u945 t, the attention output at, and finally the combined output ot. You can run a non-comprehensive sanity check by executing: python sanity_check.py 1f (g) (3 points) (written) The generate sent masks() function in nmt model.py produces a tensor called enc masks. It has shape (batch size, max source sentence length) and contains 1s in positions corresponding to \'91pad\'92 tokens in the input, and 0s for non-pad tokens. Look at how the masks are used during the attention computation in the step() function (lines 295-296). First explain (in around three sentences) what effect the masks have on the entire attention computation. Then explain (in one or two sentences) why it is necessary to use the masks in this way. Now it\'92s time to get things running! Execute the following to generate the necessary vocab file: sh run.sh vocab As noted earlier, we recommend that you develop the code on your personal computer. Confirm that you are running in the proper conda environment and then execute the following command to train the model on your local machine: sh run.sh train_local Once you have ensured that your code does not crash (i.e. let it run till iter 10 or iter 20), power on your VM from the Azure Web Portal. Then read the Managing Code Deployment to a VM section of our Practical Guide to VMs (link also given on website and Piazza) for instructions on how to upload your code to the VM. Next, install necessary packages to your VM by running: pip install -r gpu_requirements.txt Finally, turn to the Managing Processes on a VM section of the Practical Guide and follow the instructions to create a new tmux session. Concretely, run the following command to create tmux session called nmt. tmux new -s nmt Once your VM is configured and you are in a tmux session, execute: sh run.sh train Once you know your code is running properly, you can detach from session and close your ssh connection to the server. To detach from the session, run: tmux detach You can return to your training model by ssh-ing back into the server and attaching to the tmux session by running: tmux a -t nmt CS 224n Assignment 4 Page 5 of 7 (i) (4 points) Once your model is done training (this should take about 4 hours on the VM), execute the following command to test the model: sh run.sh test Please report the model\'92s corpus BLEU Score. It should be larger than 21. (j) (3 points) (written) In class, we learned about dot product attention, multiplicative attention, and additive attention. Please explain one advantage and one disadvantage of dot product attention compared to multiplicative attention. Then explain one advantage and one disadvantage of additive attention compared to multiplicative attention. As a reminder, dot product attention is et,i = s T t hi , multiplicative attention is et,i = s T t Whi , and additive attention is et,i = v T tanh(W1hi + W2st).}